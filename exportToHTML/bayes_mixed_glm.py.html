<html>
<head>
<title>bayes_mixed_glm.py</title>
<meta http-equiv="Content-Type" content="text/html; charset=utf-8">
<style type="text/css">
.s0 { color: #629755; font-style: italic;}
.s1 { color: #a9b7c6;}
.s2 { color: #cc7832;}
.s3 { color: #808080;}
.s4 { color: #6897bb;}
.s5 { color: #6a8759;}
</style>
</head>
<body bgcolor="#2b2b2b">
<table CELLSPACING=0 CELLPADDING=5 COLS=1 WIDTH="100%" BGCOLOR="#606060" >
<tr><td><center>
<font face="Arial, Helvetica" color="#000000">
bayes_mixed_glm.py</font>
</center></td></tr></table>
<pre><span class="s0">r&quot;&quot;&quot; 
Bayesian inference for generalized linear mixed models. 
 
Currently only families without additional scale or shape parameters 
are supported (binomial and Poisson). 
 
Two estimation approaches are supported: Laplace approximation 
('maximum a posteriori'), and variational Bayes (mean field 
approximation to the posterior distribution). 
 
All realizations of random effects are modeled to be mutually 
independent in this implementation. 
 
The `exog_vc` matrix is the design matrix for the random effects. 
Every column of `exog_vc` corresponds to an independent realization of 
a random effect.  These random effects have mean zero and an unknown 
standard deviation.  The standard deviation parameters are constrained 
to be equal within subsets of the columns. When not using formulas, 
these subsets are specified through the parameter `ident`.  `ident` 
must have the same length as the number of columns of `exog_vc`, and 
two columns whose `ident` values are equal have the same standard 
deviation.  When formulas are used, the columns of `exog_vc` derived 
from a common formula are constrained to have the same standard 
deviation. 
 
In many applications, `exog_vc` will be sparse.  A sparse matrix may 
be passed when constructing a model class.  If a dense matrix is 
passed, it will be converted internally to a sparse matrix.  There 
currently is no way to avoid creating a temporary dense version of 
`exog_vc` when using formulas. 
 
Model and parameterization 
-------------------------- 
The joint density of data and parameters factors as: 
 
.. math:: 
 
    p(y | vc, fep) p(vc | vcp) p(vcp) p(fe) 
 
The terms :math:`p(vcp)` and :math:`p(fe)` are prior distributions 
that are taken to be Gaussian (the :math:`vcp` parameters are log 
standard deviations so the standard deviations have log-normal 
distributions).  The random effects distribution :math:`p(vc | vcp)` 
is independent Gaussian (random effect realizations are independent 
within and between values of the `ident` array).  The model 
:math:`p(y | vc, fep)` depends on the specific GLM being fit. 
&quot;&quot;&quot;</span>

<span class="s2">import </span><span class="s1">numpy </span><span class="s2">as </span><span class="s1">np</span>
<span class="s2">from </span><span class="s1">scipy.optimize </span><span class="s2">import </span><span class="s1">minimize</span>
<span class="s2">from </span><span class="s1">scipy </span><span class="s2">import </span><span class="s1">sparse</span>
<span class="s2">import </span><span class="s1">statsmodels.base.model </span><span class="s2">as </span><span class="s1">base</span>
<span class="s2">from </span><span class="s1">statsmodels.iolib </span><span class="s2">import </span><span class="s1">summary2</span>
<span class="s2">from </span><span class="s1">statsmodels.genmod </span><span class="s2">import </span><span class="s1">families</span>
<span class="s2">import </span><span class="s1">pandas </span><span class="s2">as </span><span class="s1">pd</span>
<span class="s2">import </span><span class="s1">warnings</span>
<span class="s2">import </span><span class="s1">patsy</span>

<span class="s3"># Gauss-Legendre weights</span>
<span class="s1">glw = [</span>
    <span class="s1">[</span><span class="s4">0.2955242247147529</span><span class="s2">, </span><span class="s1">-</span><span class="s4">0.1488743389816312</span><span class="s1">]</span><span class="s2">,</span>
    <span class="s1">[</span><span class="s4">0.2955242247147529</span><span class="s2">, </span><span class="s4">0.1488743389816312</span><span class="s1">]</span><span class="s2">,</span>
    <span class="s1">[</span><span class="s4">0.2692667193099963</span><span class="s2">, </span><span class="s1">-</span><span class="s4">0.4333953941292472</span><span class="s1">]</span><span class="s2">,</span>
    <span class="s1">[</span><span class="s4">0.2692667193099963</span><span class="s2">, </span><span class="s4">0.4333953941292472</span><span class="s1">]</span><span class="s2">,</span>
    <span class="s1">[</span><span class="s4">0.2190863625159820</span><span class="s2">, </span><span class="s1">-</span><span class="s4">0.6794095682990244</span><span class="s1">]</span><span class="s2">,</span>
    <span class="s1">[</span><span class="s4">0.2190863625159820</span><span class="s2">, </span><span class="s4">0.6794095682990244</span><span class="s1">]</span><span class="s2">,</span>
    <span class="s1">[</span><span class="s4">0.1494513491505806</span><span class="s2">, </span><span class="s1">-</span><span class="s4">0.8650633666889845</span><span class="s1">]</span><span class="s2">,</span>
    <span class="s1">[</span><span class="s4">0.1494513491505806</span><span class="s2">, </span><span class="s4">0.8650633666889845</span><span class="s1">]</span><span class="s2">,</span>
    <span class="s1">[</span><span class="s4">0.0666713443086881</span><span class="s2">, </span><span class="s1">-</span><span class="s4">0.9739065285171717</span><span class="s1">]</span><span class="s2">,</span>
    <span class="s1">[</span><span class="s4">0.0666713443086881</span><span class="s2">, </span><span class="s4">0.9739065285171717</span><span class="s1">]</span><span class="s2">,</span>
<span class="s1">]</span>

<span class="s1">_init_doc = </span><span class="s5">r&quot;&quot;&quot; 
    Generalized Linear Mixed Model with Bayesian estimation 
 
    The class implements the Laplace approximation to the posterior 
    distribution (`fit_map`) and a variational Bayes approximation to 
    the posterior (`fit_vb`).  See the two fit method docstrings for 
    more information about the fitting approaches. 
 
    Parameters 
    ---------- 
    endog : array_like 
        Vector of response values. 
    exog : array_like 
        Array of covariates for the fixed effects part of the mean 
        structure. 
    exog_vc : array_like 
        Array of covariates for the random part of the model.  A 
        scipy.sparse array may be provided, or else the passed 
        array will be converted to sparse internally. 
    ident : array_like 
        Array of integer labels showing which random terms (columns 
        of `exog_vc`) have a common variance. 
    vcp_p : float 
        Prior standard deviation for variance component parameters 
        (the prior standard deviation of log(s) is vcp_p, where s is 
        the standard deviation of a random effect). 
    fe_p : float 
        Prior standard deviation for fixed effects parameters. 
    family : statsmodels.genmod.families instance 
        The GLM family. 
    fep_names : list[str] 
        The names of the fixed effects parameters (corresponding to 
        columns of exog).  If None, default names are constructed. 
    vcp_names : list[str] 
        The names of the variance component parameters (corresponding 
        to distinct labels in ident).  If None, default names are 
        constructed. 
    vc_names : list[str] 
        The names of the random effect realizations. 
 
    Returns 
    ------- 
    MixedGLMResults object 
 
    Notes 
    ----- 
    There are three types of values in the posterior distribution: 
    fixed effects parameters (fep), corresponding to the columns of 
    `exog`, random effects realizations (vc), corresponding to the 
    columns of `exog_vc`, and the standard deviations of the random 
    effects realizations (vcp), corresponding to the unique integer 
    labels in `ident`. 
 
    All random effects are modeled as being independent Gaussian 
    values (given the variance structure parameters).  Every column of 
    `exog_vc` has a distinct realized random effect that is used to 
    form the linear predictors.  The elements of `ident` determine the 
    distinct variance structure parameters.  Two random effect 
    realizations that have the same value in `ident` have the same 
    variance.  When fitting with a formula, `ident` is constructed 
    internally (each element of `vc_formulas` yields a distinct label 
    in `ident`). 
 
    The random effect standard deviation parameters (`vcp`) have 
    log-normal prior distributions with mean 0 and standard deviation 
    `vcp_p`. 
 
    Note that for some families, e.g. Binomial, the posterior mode may 
    be difficult to find numerically if `vcp_p` is set to too large of 
    a value.  Setting `vcp_p` to 0.5 seems to work well. 
 
    The prior for the fixed effects parameters is Gaussian with mean 0 
    and standard deviation `fe_p`.  It is recommended that quantitative 
    covariates be standardized. 
 
    Examples 
    --------{example} 
 
 
    References 
    ---------- 
    Introduction to generalized linear mixed models: 
    https://stats.idre.ucla.edu/other/mult-pkg/introduction-to-generalized-linear-mixed-models 
 
    SAS documentation: 
    https://support.sas.com/documentation/cdl/en/statug/63033/HTML/default/viewer.htm#statug_intromix_a0000000215.htm 
 
    An assessment of estimation methods for generalized linear mixed 
    models with binary outcomes 
    https://www.ncbi.nlm.nih.gov/pmc/articles/PMC3866838/ 
    &quot;&quot;&quot;</span>

<span class="s3"># The code in the example should be identical to what appears in</span>
<span class="s3"># the test_doc_examples unit test</span>
<span class="s1">_logit_example = </span><span class="s5">&quot;&quot;&quot; 
    A binomial (logistic) random effects model with random intercepts 
    for villages and random slopes for each year within each village: 
 
    &gt;&gt;&gt; random = {&quot;a&quot;: '0 + C(Village)', &quot;b&quot;: '0 + C(Village)*year_cen'} 
    &gt;&gt;&gt; model = BinomialBayesMixedGLM.from_formula( 
                   'y ~ year_cen', random, data) 
    &gt;&gt;&gt; result = model.fit_vb() 
&quot;&quot;&quot;</span>

<span class="s3"># The code in the example should be identical to what appears in</span>
<span class="s3"># the test_doc_examples unit test</span>
<span class="s1">_poisson_example = </span><span class="s5">&quot;&quot;&quot; 
    A Poisson random effects model with random intercepts for villages 
    and random slopes for each year within each village: 
 
    &gt;&gt;&gt; random = {&quot;a&quot;: '0 + C(Village)', &quot;b&quot;: '0 + C(Village)*year_cen'} 
    &gt;&gt;&gt; model = PoissonBayesMixedGLM.from_formula( 
                    'y ~ year_cen', random, data) 
    &gt;&gt;&gt; result = model.fit_vb() 
&quot;&quot;&quot;</span>


<span class="s2">class </span><span class="s1">_BayesMixedGLM(base.Model):</span>
    <span class="s2">def </span><span class="s1">__init__(self</span><span class="s2">,</span>
                 <span class="s1">endog</span><span class="s2">,</span>
                 <span class="s1">exog</span><span class="s2">,</span>
                 <span class="s1">exog_vc=</span><span class="s2">None,</span>
                 <span class="s1">ident=</span><span class="s2">None,</span>
                 <span class="s1">family=</span><span class="s2">None,</span>
                 <span class="s1">vcp_p=</span><span class="s4">1</span><span class="s2">,</span>
                 <span class="s1">fe_p=</span><span class="s4">2</span><span class="s2">,</span>
                 <span class="s1">fep_names=</span><span class="s2">None,</span>
                 <span class="s1">vcp_names=</span><span class="s2">None,</span>
                 <span class="s1">vc_names=</span><span class="s2">None,</span>
                 <span class="s1">**kwargs):</span>

        <span class="s2">if </span><span class="s1">exog.ndim == </span><span class="s4">1</span><span class="s1">:</span>
            <span class="s2">if </span><span class="s1">isinstance(exog</span><span class="s2">, </span><span class="s1">np.ndarray):</span>
                <span class="s1">exog = exog[:</span><span class="s2">, None</span><span class="s1">]</span>
            <span class="s2">else</span><span class="s1">:</span>
                <span class="s1">exog = pd.DataFrame(exog)</span>

        <span class="s2">if </span><span class="s1">exog.ndim != </span><span class="s4">2</span><span class="s1">:</span>
            <span class="s1">msg = </span><span class="s5">&quot;'exog' must have one or two columns&quot;</span>
            <span class="s2">raise </span><span class="s1">ValueError(msg)</span>

        <span class="s2">if </span><span class="s1">exog_vc.ndim == </span><span class="s4">1</span><span class="s1">:</span>
            <span class="s2">if </span><span class="s1">isinstance(exog_vc</span><span class="s2">, </span><span class="s1">np.ndarray):</span>
                <span class="s1">exog_vc = exog_vc[:</span><span class="s2">, None</span><span class="s1">]</span>
            <span class="s2">else</span><span class="s1">:</span>
                <span class="s1">exog_vc = pd.DataFrame(exog_vc)</span>

        <span class="s2">if </span><span class="s1">exog_vc.ndim != </span><span class="s4">2</span><span class="s1">:</span>
            <span class="s1">msg = </span><span class="s5">&quot;'exog_vc' must have one or two columns&quot;</span>
            <span class="s2">raise </span><span class="s1">ValueError(msg)</span>

        <span class="s1">ident = np.asarray(ident)</span>
        <span class="s2">if </span><span class="s1">ident.ndim != </span><span class="s4">1</span><span class="s1">:</span>
            <span class="s1">msg = </span><span class="s5">&quot;ident must be a one-dimensional array&quot;</span>
            <span class="s2">raise </span><span class="s1">ValueError(msg)</span>

        <span class="s2">if </span><span class="s1">len(ident) != exog_vc.shape[</span><span class="s4">1</span><span class="s1">]:</span>
            <span class="s1">msg = </span><span class="s5">&quot;len(ident) should match the number of columns of exog_vc&quot;</span>
            <span class="s2">raise </span><span class="s1">ValueError(msg)</span>

        <span class="s2">if not </span><span class="s1">np.issubdtype(ident.dtype</span><span class="s2">, </span><span class="s1">np.integer):</span>
            <span class="s1">msg = </span><span class="s5">&quot;ident must have an integer dtype&quot;</span>
            <span class="s2">raise </span><span class="s1">ValueError(msg)</span>

        <span class="s3"># Get the fixed effects parameter names</span>
        <span class="s2">if </span><span class="s1">fep_names </span><span class="s2">is None</span><span class="s1">:</span>
            <span class="s2">if </span><span class="s1">hasattr(exog</span><span class="s2">, </span><span class="s5">&quot;columns&quot;</span><span class="s1">):</span>
                <span class="s1">fep_names = exog.columns.tolist()</span>
            <span class="s2">else</span><span class="s1">:</span>
                <span class="s1">fep_names = [</span><span class="s5">&quot;FE_%d&quot; </span><span class="s1">% (k + </span><span class="s4">1</span><span class="s1">) </span><span class="s2">for </span><span class="s1">k </span><span class="s2">in </span><span class="s1">range(exog.shape[</span><span class="s4">1</span><span class="s1">])]</span>

        <span class="s3"># Get the variance parameter names</span>
        <span class="s2">if </span><span class="s1">vcp_names </span><span class="s2">is None</span><span class="s1">:</span>
            <span class="s1">vcp_names = [</span><span class="s5">&quot;VC_%d&quot; </span><span class="s1">% (k + </span><span class="s4">1</span><span class="s1">) </span><span class="s2">for </span><span class="s1">k </span><span class="s2">in </span><span class="s1">range(int(max(ident)) + </span><span class="s4">1</span><span class="s1">)]</span>
        <span class="s2">else</span><span class="s1">:</span>
            <span class="s2">if </span><span class="s1">len(vcp_names) != len(set(ident)):</span>
                <span class="s1">msg = </span><span class="s5">&quot;The lengths of vcp_names and ident should be the same&quot;</span>
                <span class="s2">raise </span><span class="s1">ValueError(msg)</span>

        <span class="s2">if not </span><span class="s1">sparse.issparse(exog_vc):</span>
            <span class="s1">exog_vc = sparse.csr_matrix(exog_vc)</span>

        <span class="s1">ident = ident.astype(int)</span>
        <span class="s1">vcp_p = float(vcp_p)</span>
        <span class="s1">fe_p = float(fe_p)</span>

        <span class="s3"># Number of fixed effects parameters</span>
        <span class="s2">if </span><span class="s1">exog </span><span class="s2">is None</span><span class="s1">:</span>
            <span class="s1">k_fep = </span><span class="s4">0</span>
        <span class="s2">else</span><span class="s1">:</span>
            <span class="s1">k_fep = exog.shape[</span><span class="s4">1</span><span class="s1">]</span>

        <span class="s3"># Number of variance component structure parameters and</span>
        <span class="s3"># variance component realizations.</span>
        <span class="s2">if </span><span class="s1">exog_vc </span><span class="s2">is None</span><span class="s1">:</span>
            <span class="s1">k_vc = </span><span class="s4">0</span>
            <span class="s1">k_vcp = </span><span class="s4">0</span>
        <span class="s2">else</span><span class="s1">:</span>
            <span class="s1">k_vc = exog_vc.shape[</span><span class="s4">1</span><span class="s1">]</span>
            <span class="s1">k_vcp = max(ident) + </span><span class="s4">1</span>

        <span class="s3"># power might be better but not available in older scipy</span>
        <span class="s1">exog_vc2 = exog_vc.multiply(exog_vc)</span>

        <span class="s1">super(_BayesMixedGLM</span><span class="s2">, </span><span class="s1">self).__init__(endog</span><span class="s2">, </span><span class="s1">exog</span><span class="s2">, </span><span class="s1">**kwargs)</span>

        <span class="s1">self.exog_vc = exog_vc</span>
        <span class="s1">self.exog_vc2 = exog_vc2</span>
        <span class="s1">self.ident = ident</span>
        <span class="s1">self.family = family</span>
        <span class="s1">self.k_fep = k_fep</span>
        <span class="s1">self.k_vc = k_vc</span>
        <span class="s1">self.k_vcp = k_vcp</span>
        <span class="s1">self.fep_names = fep_names</span>
        <span class="s1">self.vcp_names = vcp_names</span>
        <span class="s1">self.vc_names = vc_names</span>
        <span class="s1">self.fe_p = fe_p</span>
        <span class="s1">self.vcp_p = vcp_p</span>
        <span class="s1">self.names = fep_names + vcp_names</span>
        <span class="s2">if </span><span class="s1">vc_names </span><span class="s2">is not None</span><span class="s1">:</span>
            <span class="s1">self.names += vc_names</span>

    <span class="s2">def </span><span class="s1">_unpack(self</span><span class="s2">, </span><span class="s1">vec):</span>

        <span class="s1">ii = </span><span class="s4">0</span>

        <span class="s3"># Fixed effects parameters</span>
        <span class="s1">fep = vec[:ii + self.k_fep]</span>
        <span class="s1">ii += self.k_fep</span>

        <span class="s3"># Variance component structure parameters (standard</span>
        <span class="s3"># deviations).  These are on the log scale.  The standard</span>
        <span class="s3"># deviation for random effect j is exp(vcp[ident[j]]).</span>
        <span class="s1">vcp = vec[ii:ii + self.k_vcp]</span>
        <span class="s1">ii += self.k_vcp</span>

        <span class="s3"># Random effect realizations</span>
        <span class="s1">vc = vec[ii:]</span>

        <span class="s2">return </span><span class="s1">fep</span><span class="s2">, </span><span class="s1">vcp</span><span class="s2">, </span><span class="s1">vc</span>

    <span class="s2">def </span><span class="s1">logposterior(self</span><span class="s2">, </span><span class="s1">params):</span>
        <span class="s0">&quot;&quot;&quot; 
        The overall log-density: log p(y, fe, vc, vcp). 
 
        This differs by an additive constant from the log posterior 
        log p(fe, vc, vcp | y). 
        &quot;&quot;&quot;</span>

        <span class="s1">fep</span><span class="s2">, </span><span class="s1">vcp</span><span class="s2">, </span><span class="s1">vc = self._unpack(params)</span>

        <span class="s3"># Contributions from p(y | x, vc)</span>
        <span class="s1">lp = </span><span class="s4">0</span>
        <span class="s2">if </span><span class="s1">self.k_fep &gt; </span><span class="s4">0</span><span class="s1">:</span>
            <span class="s1">lp += np.dot(self.exog</span><span class="s2">, </span><span class="s1">fep)</span>
        <span class="s2">if </span><span class="s1">self.k_vc &gt; </span><span class="s4">0</span><span class="s1">:</span>
            <span class="s1">lp += self.exog_vc.dot(vc)</span>

        <span class="s1">mu = self.family.link.inverse(lp)</span>
        <span class="s1">ll = self.family.loglike(self.endog</span><span class="s2">, </span><span class="s1">mu)</span>

        <span class="s2">if </span><span class="s1">self.k_vc &gt; </span><span class="s4">0</span><span class="s1">:</span>

            <span class="s3"># Contributions from p(vc | vcp)</span>
            <span class="s1">vcp0 = vcp[self.ident]</span>
            <span class="s1">s = np.exp(vcp0)</span>
            <span class="s1">ll -= </span><span class="s4">0.5 </span><span class="s1">* np.sum(vc**</span><span class="s4">2 </span><span class="s1">/ s**</span><span class="s4">2</span><span class="s1">) + np.sum(vcp0)</span>

            <span class="s3"># Contributions from p(vc)</span>
            <span class="s1">ll -= </span><span class="s4">0.5 </span><span class="s1">* np.sum(vcp**</span><span class="s4">2 </span><span class="s1">/ self.vcp_p**</span><span class="s4">2</span><span class="s1">)</span>

        <span class="s3"># Contributions from p(fep)</span>
        <span class="s2">if </span><span class="s1">self.k_fep &gt; </span><span class="s4">0</span><span class="s1">:</span>
            <span class="s1">ll -= </span><span class="s4">0.5 </span><span class="s1">* np.sum(fep**</span><span class="s4">2 </span><span class="s1">/ self.fe_p**</span><span class="s4">2</span><span class="s1">)</span>

        <span class="s2">return </span><span class="s1">ll</span>

    <span class="s2">def </span><span class="s1">logposterior_grad(self</span><span class="s2">, </span><span class="s1">params):</span>
        <span class="s0">&quot;&quot;&quot; 
        The gradient of the log posterior. 
        &quot;&quot;&quot;</span>

        <span class="s1">fep</span><span class="s2">, </span><span class="s1">vcp</span><span class="s2">, </span><span class="s1">vc = self._unpack(params)</span>

        <span class="s1">lp = </span><span class="s4">0</span>
        <span class="s2">if </span><span class="s1">self.k_fep &gt; </span><span class="s4">0</span><span class="s1">:</span>
            <span class="s1">lp += np.dot(self.exog</span><span class="s2">, </span><span class="s1">fep)</span>
        <span class="s2">if </span><span class="s1">self.k_vc &gt; </span><span class="s4">0</span><span class="s1">:</span>
            <span class="s1">lp += self.exog_vc.dot(vc)</span>

        <span class="s1">mu = self.family.link.inverse(lp)</span>

        <span class="s1">score_factor = (self.endog - mu) / self.family.link.deriv(mu)</span>
        <span class="s1">score_factor /= self.family.variance(mu)</span>

        <span class="s1">te = [</span><span class="s2">None, None, None</span><span class="s1">]</span>

        <span class="s3"># Contributions from p(y | x, z, vc)</span>
        <span class="s2">if </span><span class="s1">self.k_fep &gt; </span><span class="s4">0</span><span class="s1">:</span>
            <span class="s1">te[</span><span class="s4">0</span><span class="s1">] = np.dot(score_factor</span><span class="s2">, </span><span class="s1">self.exog)</span>
        <span class="s2">if </span><span class="s1">self.k_vc &gt; </span><span class="s4">0</span><span class="s1">:</span>
            <span class="s1">te[</span><span class="s4">2</span><span class="s1">] = self.exog_vc.transpose().dot(score_factor)</span>

        <span class="s2">if </span><span class="s1">self.k_vc &gt; </span><span class="s4">0</span><span class="s1">:</span>
            <span class="s3"># Contributions from p(vc | vcp)</span>
            <span class="s3"># vcp0 = vcp[self.ident]</span>
            <span class="s3"># s = np.exp(vcp0)</span>
            <span class="s3"># ll -= 0.5 * np.sum(vc**2 / s**2) + np.sum(vcp0)</span>
            <span class="s1">vcp0 = vcp[self.ident]</span>
            <span class="s1">s = np.exp(vcp0)</span>
            <span class="s1">u = vc**</span><span class="s4">2 </span><span class="s1">/ s**</span><span class="s4">2 </span><span class="s1">- </span><span class="s4">1</span>
            <span class="s1">te[</span><span class="s4">1</span><span class="s1">] = np.bincount(self.ident</span><span class="s2">, </span><span class="s1">weights=u)</span>
            <span class="s1">te[</span><span class="s4">2</span><span class="s1">] -= vc / s**</span><span class="s4">2</span>

            <span class="s3"># Contributions from p(vcp)</span>
            <span class="s3"># ll -= 0.5 * np.sum(vcp**2 / self.vcp_p**2)</span>
            <span class="s1">te[</span><span class="s4">1</span><span class="s1">] -= vcp / self.vcp_p**</span><span class="s4">2</span>

        <span class="s3"># Contributions from p(fep)</span>
        <span class="s2">if </span><span class="s1">self.k_fep &gt; </span><span class="s4">0</span><span class="s1">:</span>
            <span class="s1">te[</span><span class="s4">0</span><span class="s1">] -= fep / self.fe_p**</span><span class="s4">2</span>

        <span class="s1">te = [x </span><span class="s2">for </span><span class="s1">x </span><span class="s2">in </span><span class="s1">te </span><span class="s2">if </span><span class="s1">x </span><span class="s2">is not None</span><span class="s1">]</span>

        <span class="s2">return </span><span class="s1">np.concatenate(te)</span>

    <span class="s2">def </span><span class="s1">_get_start(self):</span>
        <span class="s1">start_fep = np.zeros(self.k_fep)</span>
        <span class="s1">start_vcp = np.ones(self.k_vcp)</span>
        <span class="s1">start_vc = np.random.normal(size=self.k_vc)</span>
        <span class="s1">start = np.concatenate((start_fep</span><span class="s2">, </span><span class="s1">start_vcp</span><span class="s2">, </span><span class="s1">start_vc))</span>
        <span class="s2">return </span><span class="s1">start</span>

    <span class="s1">@classmethod</span>
    <span class="s2">def </span><span class="s1">from_formula(cls</span><span class="s2">,</span>
                     <span class="s1">formula</span><span class="s2">,</span>
                     <span class="s1">vc_formulas</span><span class="s2">,</span>
                     <span class="s1">data</span><span class="s2">,</span>
                     <span class="s1">family=</span><span class="s2">None,</span>
                     <span class="s1">vcp_p=</span><span class="s4">1</span><span class="s2">,</span>
                     <span class="s1">fe_p=</span><span class="s4">2</span><span class="s1">):</span>
        <span class="s0">&quot;&quot;&quot; 
        Fit a BayesMixedGLM using a formula. 
 
        Parameters 
        ---------- 
        formula : str 
            Formula for the endog and fixed effects terms (use ~ to 
            separate dependent and independent expressions). 
        vc_formulas : dictionary 
            vc_formulas[name] is a one-sided formula that creates one 
            collection of random effects with a common variance 
            parameter.  If using categorical (factor) variables to 
            produce variance components, note that generally `0 + ...` 
            should be used so that an intercept is not included. 
        data : data frame 
            The data to which the formulas are applied. 
        family : genmod.families instance 
            A GLM family. 
        vcp_p : float 
            The prior standard deviation for the logarithms of the standard 
            deviations of the random effects. 
        fe_p : float 
            The prior standard deviation for the fixed effects parameters. 
        &quot;&quot;&quot;</span>

        <span class="s1">ident = []</span>
        <span class="s1">exog_vc = []</span>
        <span class="s1">vcp_names = []</span>
        <span class="s1">j = </span><span class="s4">0</span>
        <span class="s2">for </span><span class="s1">na</span><span class="s2">, </span><span class="s1">fml </span><span class="s2">in </span><span class="s1">vc_formulas.items():</span>
            <span class="s1">mat = patsy.dmatrix(fml</span><span class="s2">, </span><span class="s1">data</span><span class="s2">, </span><span class="s1">return_type=</span><span class="s5">'dataframe'</span><span class="s1">)</span>
            <span class="s1">exog_vc.append(mat)</span>
            <span class="s1">vcp_names.append(na)</span>
            <span class="s1">ident.append(j * np.ones(mat.shape[</span><span class="s4">1</span><span class="s1">]</span><span class="s2">, </span><span class="s1">dtype=np.int_))</span>
            <span class="s1">j += </span><span class="s4">1</span>
        <span class="s1">exog_vc = pd.concat(exog_vc</span><span class="s2">, </span><span class="s1">axis=</span><span class="s4">1</span><span class="s1">)</span>
        <span class="s1">vc_names = exog_vc.columns.tolist()</span>

        <span class="s1">ident = np.concatenate(ident)</span>

        <span class="s1">model = super(_BayesMixedGLM</span><span class="s2">, </span><span class="s1">cls).from_formula(</span>
            <span class="s1">formula</span><span class="s2">,</span>
            <span class="s1">data=data</span><span class="s2">,</span>
            <span class="s1">family=family</span><span class="s2">,</span>
            <span class="s1">subset=</span><span class="s2">None,</span>
            <span class="s1">exog_vc=exog_vc</span><span class="s2">,</span>
            <span class="s1">ident=ident</span><span class="s2">,</span>
            <span class="s1">vc_names=vc_names</span><span class="s2">,</span>
            <span class="s1">vcp_names=vcp_names</span><span class="s2">,</span>
            <span class="s1">fe_p=fe_p</span><span class="s2">,</span>
            <span class="s1">vcp_p=vcp_p)</span>

        <span class="s2">return </span><span class="s1">model</span>

    <span class="s2">def </span><span class="s1">fit(self</span><span class="s2">, </span><span class="s1">method=</span><span class="s5">&quot;BFGS&quot;</span><span class="s2">, </span><span class="s1">minim_opts=</span><span class="s2">None</span><span class="s1">):</span>
        <span class="s0">&quot;&quot;&quot; 
        fit is equivalent to fit_map. 
 
        See fit_map for parameter information. 
 
        Use `fit_vb` to fit the model using variational Bayes. 
        &quot;&quot;&quot;</span>
        <span class="s1">self.fit_map(method</span><span class="s2">, </span><span class="s1">minim_opts)</span>

    <span class="s2">def </span><span class="s1">fit_map(self</span><span class="s2">, </span><span class="s1">method=</span><span class="s5">&quot;BFGS&quot;</span><span class="s2">, </span><span class="s1">minim_opts=</span><span class="s2">None, </span><span class="s1">scale_fe=</span><span class="s2">False</span><span class="s1">):</span>
        <span class="s0">&quot;&quot;&quot; 
        Construct the Laplace approximation to the posterior distribution. 
 
        Parameters 
        ---------- 
        method : str 
            Optimization method for finding the posterior mode. 
        minim_opts : dict 
            Options passed to scipy.minimize. 
        scale_fe : bool 
            If True, the columns of the fixed effects design matrix 
            are centered and scaled to unit variance before fitting 
            the model.  The results are back-transformed so that the 
            results are presented on the original scale. 
 
        Returns 
        ------- 
        BayesMixedGLMResults instance. 
        &quot;&quot;&quot;</span>

        <span class="s2">if </span><span class="s1">scale_fe:</span>
            <span class="s1">mn = self.exog.mean(</span><span class="s4">0</span><span class="s1">)</span>
            <span class="s1">sc = self.exog.std(</span><span class="s4">0</span><span class="s1">)</span>
            <span class="s1">self._exog_save = self.exog</span>
            <span class="s1">self.exog = self.exog.copy()</span>
            <span class="s1">ixs = np.flatnonzero(sc &gt; </span><span class="s4">1e-8</span><span class="s1">)</span>
            <span class="s1">self.exog[:</span><span class="s2">, </span><span class="s1">ixs] -= mn[ixs]</span>
            <span class="s1">self.exog[:</span><span class="s2">, </span><span class="s1">ixs] /= sc[ixs]</span>

        <span class="s2">def </span><span class="s1">fun(params):</span>
            <span class="s2">return </span><span class="s1">-self.logposterior(params)</span>

        <span class="s2">def </span><span class="s1">grad(params):</span>
            <span class="s2">return </span><span class="s1">-self.logposterior_grad(params)</span>

        <span class="s1">start = self._get_start()</span>

        <span class="s1">r = minimize(fun</span><span class="s2">, </span><span class="s1">start</span><span class="s2">, </span><span class="s1">method=method</span><span class="s2">, </span><span class="s1">jac=grad</span><span class="s2">, </span><span class="s1">options=minim_opts)</span>
        <span class="s2">if not </span><span class="s1">r.success:</span>
            <span class="s1">msg = (</span><span class="s5">&quot;Laplace fitting did not converge, |gradient|=%.6f&quot; </span><span class="s1">%</span>
                   <span class="s1">np.sqrt(np.sum(r.jac**</span><span class="s4">2</span><span class="s1">)))</span>
            <span class="s1">warnings.warn(msg)</span>

        <span class="s2">from </span><span class="s1">statsmodels.tools.numdiff </span><span class="s2">import </span><span class="s1">approx_fprime</span>
        <span class="s1">hess = approx_fprime(r.x</span><span class="s2">, </span><span class="s1">grad)</span>
        <span class="s1">cov = np.linalg.inv(hess)</span>

        <span class="s1">params = r.x</span>

        <span class="s2">if </span><span class="s1">scale_fe:</span>
            <span class="s1">self.exog = self._exog_save</span>
            <span class="s2">del </span><span class="s1">self._exog_save</span>
            <span class="s1">params[ixs] /= sc[ixs]</span>
            <span class="s1">cov[ixs</span><span class="s2">, </span><span class="s1">:][:</span><span class="s2">, </span><span class="s1">ixs] /= np.outer(sc[ixs]</span><span class="s2">, </span><span class="s1">sc[ixs])</span>

        <span class="s2">return </span><span class="s1">BayesMixedGLMResults(self</span><span class="s2">, </span><span class="s1">params</span><span class="s2">, </span><span class="s1">cov</span><span class="s2">, </span><span class="s1">optim_retvals=r)</span>

    <span class="s2">def </span><span class="s1">predict(self</span><span class="s2">, </span><span class="s1">params</span><span class="s2">, </span><span class="s1">exog=</span><span class="s2">None, </span><span class="s1">linear=</span><span class="s2">False</span><span class="s1">):</span>
        <span class="s0">&quot;&quot;&quot; 
        Return the fitted mean structure. 
 
        Parameters 
        ---------- 
        params : array_like 
            The parameter vector, may be the full parameter vector, or may 
            be truncated to include only the mean parameters. 
        exog : array_like 
            The design matrix for the mean structure.  If omitted, use the 
            model's design matrix. 
        linear : bool 
            If True, return the linear predictor without passing through the 
            link function. 
 
        Returns 
        ------- 
        A 1-dimensional array of predicted values 
        &quot;&quot;&quot;</span>

        <span class="s2">if </span><span class="s1">exog </span><span class="s2">is None</span><span class="s1">:</span>
            <span class="s1">exog = self.exog</span>

        <span class="s1">q = exog.shape[</span><span class="s4">1</span><span class="s1">]</span>
        <span class="s1">pr = np.dot(exog</span><span class="s2">, </span><span class="s1">params[</span><span class="s4">0</span><span class="s1">:q])</span>

        <span class="s2">if not </span><span class="s1">linear:</span>
            <span class="s1">pr = self.family.link.inverse(pr)</span>

        <span class="s2">return </span><span class="s1">pr</span>


<span class="s2">class </span><span class="s1">_VariationalBayesMixedGLM:</span>
    <span class="s0">&quot;&quot;&quot; 
    A mixin providing generic (not family-specific) methods for 
    variational Bayes mean field fitting. 
    &quot;&quot;&quot;</span>

    <span class="s3"># Integration range (from -rng to +rng).  The integrals are with</span>
    <span class="s3"># respect to a standard Gaussian distribution so (-5, 5) will be</span>
    <span class="s3"># sufficient in many cases.</span>
    <span class="s1">rng = </span><span class="s4">5</span>

    <span class="s1">verbose = </span><span class="s2">False</span>

    <span class="s3"># Returns the mean and variance of the linear predictor under the</span>
    <span class="s3"># given distribution parameters.</span>
    <span class="s2">def </span><span class="s1">_lp_stats(self</span><span class="s2">, </span><span class="s1">fep_mean</span><span class="s2">, </span><span class="s1">fep_sd</span><span class="s2">, </span><span class="s1">vc_mean</span><span class="s2">, </span><span class="s1">vc_sd):</span>

        <span class="s1">tm = np.dot(self.exog</span><span class="s2">, </span><span class="s1">fep_mean)</span>
        <span class="s1">tv = np.dot(self.exog**</span><span class="s4">2</span><span class="s2">, </span><span class="s1">fep_sd**</span><span class="s4">2</span><span class="s1">)</span>
        <span class="s1">tm += self.exog_vc.dot(vc_mean)</span>
        <span class="s1">tv += self.exog_vc2.dot(vc_sd**</span><span class="s4">2</span><span class="s1">)</span>

        <span class="s2">return </span><span class="s1">tm</span><span class="s2">, </span><span class="s1">tv</span>

    <span class="s2">def </span><span class="s1">vb_elbo_base(self</span><span class="s2">, </span><span class="s1">h</span><span class="s2">, </span><span class="s1">tm</span><span class="s2">, </span><span class="s1">fep_mean</span><span class="s2">, </span><span class="s1">vcp_mean</span><span class="s2">, </span><span class="s1">vc_mean</span><span class="s2">, </span><span class="s1">fep_sd</span><span class="s2">, </span><span class="s1">vcp_sd</span><span class="s2">,</span>
                     <span class="s1">vc_sd):</span>
        <span class="s0">&quot;&quot;&quot; 
        Returns the evidence lower bound (ELBO) for the model. 
 
        This function calculates the family-specific ELBO function 
        based on information provided from a subclass. 
 
        Parameters 
        ---------- 
        h : function mapping 1d vector to 1d vector 
            The contribution of the model to the ELBO function can be 
            expressed as y_i*lp_i + Eh_i(z), where y_i and lp_i are 
            the response and linear predictor for observation i, and z 
            is a standard normal random variable.  This formulation 
            can be achieved for any GLM with a canonical link 
            function. 
        &quot;&quot;&quot;</span>

        <span class="s3"># p(y | vc) contributions</span>
        <span class="s1">iv = </span><span class="s4">0</span>
        <span class="s2">for </span><span class="s1">w </span><span class="s2">in </span><span class="s1">glw:</span>
            <span class="s1">z = self.rng * w[</span><span class="s4">1</span><span class="s1">]</span>
            <span class="s1">iv += w[</span><span class="s4">0</span><span class="s1">] * h(z) * np.exp(-z**</span><span class="s4">2 </span><span class="s1">/ </span><span class="s4">2</span><span class="s1">)</span>
        <span class="s1">iv /= np.sqrt(</span><span class="s4">2 </span><span class="s1">* np.pi)</span>
        <span class="s1">iv *= self.rng</span>
        <span class="s1">iv += self.endog * tm</span>
        <span class="s1">iv = iv.sum()</span>

        <span class="s3"># p(vc | vcp) * p(vcp) * p(fep) contributions</span>
        <span class="s1">iv += self._elbo_common(fep_mean</span><span class="s2">, </span><span class="s1">fep_sd</span><span class="s2">, </span><span class="s1">vcp_mean</span><span class="s2">, </span><span class="s1">vcp_sd</span><span class="s2">, </span><span class="s1">vc_mean</span><span class="s2">,</span>
                                <span class="s1">vc_sd)</span>

        <span class="s1">r = (iv + np.sum(np.log(fep_sd)) + np.sum(np.log(vcp_sd)) + np.sum(</span>
            <span class="s1">np.log(vc_sd)))</span>

        <span class="s2">return </span><span class="s1">r</span>

    <span class="s2">def </span><span class="s1">vb_elbo_grad_base(self</span><span class="s2">, </span><span class="s1">h</span><span class="s2">, </span><span class="s1">tm</span><span class="s2">, </span><span class="s1">tv</span><span class="s2">, </span><span class="s1">fep_mean</span><span class="s2">, </span><span class="s1">vcp_mean</span><span class="s2">, </span><span class="s1">vc_mean</span><span class="s2">, </span><span class="s1">fep_sd</span><span class="s2">,</span>
                          <span class="s1">vcp_sd</span><span class="s2">, </span><span class="s1">vc_sd):</span>
        <span class="s0">&quot;&quot;&quot; 
        Return the gradient of the ELBO function. 
 
        See vb_elbo_base for parameters. 
        &quot;&quot;&quot;</span>

        <span class="s1">fep_mean_grad = </span><span class="s4">0.</span>
        <span class="s1">fep_sd_grad = </span><span class="s4">0.</span>
        <span class="s1">vcp_mean_grad = </span><span class="s4">0.</span>
        <span class="s1">vcp_sd_grad = </span><span class="s4">0.</span>
        <span class="s1">vc_mean_grad = </span><span class="s4">0.</span>
        <span class="s1">vc_sd_grad = </span><span class="s4">0.</span>

        <span class="s3"># p(y | vc) contributions</span>
        <span class="s2">for </span><span class="s1">w </span><span class="s2">in </span><span class="s1">glw:</span>
            <span class="s1">z = self.rng * w[</span><span class="s4">1</span><span class="s1">]</span>
            <span class="s1">u = h(z) * np.exp(-z**</span><span class="s4">2 </span><span class="s1">/ </span><span class="s4">2</span><span class="s1">) / np.sqrt(</span><span class="s4">2 </span><span class="s1">* np.pi)</span>
            <span class="s1">r = u / np.sqrt(tv)</span>
            <span class="s1">fep_mean_grad += w[</span><span class="s4">0</span><span class="s1">] * np.dot(u</span><span class="s2">, </span><span class="s1">self.exog)</span>
            <span class="s1">vc_mean_grad += w[</span><span class="s4">0</span><span class="s1">] * self.exog_vc.transpose().dot(u)</span>
            <span class="s1">fep_sd_grad += w[</span><span class="s4">0</span><span class="s1">] * z * np.dot(r</span><span class="s2">, </span><span class="s1">self.exog**</span><span class="s4">2 </span><span class="s1">* fep_sd)</span>
            <span class="s1">v = self.exog_vc2.multiply(vc_sd).transpose().dot(r)</span>
            <span class="s1">v = np.squeeze(np.asarray(v))</span>
            <span class="s1">vc_sd_grad += w[</span><span class="s4">0</span><span class="s1">] * z * v</span>

        <span class="s1">fep_mean_grad *= self.rng</span>
        <span class="s1">vc_mean_grad *= self.rng</span>
        <span class="s1">fep_sd_grad *= self.rng</span>
        <span class="s1">vc_sd_grad *= self.rng</span>
        <span class="s1">fep_mean_grad += np.dot(self.endog</span><span class="s2">, </span><span class="s1">self.exog)</span>
        <span class="s1">vc_mean_grad += self.exog_vc.transpose().dot(self.endog)</span>

        <span class="s1">(fep_mean_grad_i</span><span class="s2">, </span><span class="s1">fep_sd_grad_i</span><span class="s2">, </span><span class="s1">vcp_mean_grad_i</span><span class="s2">, </span><span class="s1">vcp_sd_grad_i</span><span class="s2">,</span>
         <span class="s1">vc_mean_grad_i</span><span class="s2">, </span><span class="s1">vc_sd_grad_i) = self._elbo_grad_common(</span>
             <span class="s1">fep_mean</span><span class="s2">, </span><span class="s1">fep_sd</span><span class="s2">, </span><span class="s1">vcp_mean</span><span class="s2">, </span><span class="s1">vcp_sd</span><span class="s2">, </span><span class="s1">vc_mean</span><span class="s2">, </span><span class="s1">vc_sd)</span>

        <span class="s1">fep_mean_grad += fep_mean_grad_i</span>
        <span class="s1">fep_sd_grad += fep_sd_grad_i</span>
        <span class="s1">vcp_mean_grad += vcp_mean_grad_i</span>
        <span class="s1">vcp_sd_grad += vcp_sd_grad_i</span>
        <span class="s1">vc_mean_grad += vc_mean_grad_i</span>
        <span class="s1">vc_sd_grad += vc_sd_grad_i</span>

        <span class="s1">fep_sd_grad += </span><span class="s4">1 </span><span class="s1">/ fep_sd</span>
        <span class="s1">vcp_sd_grad += </span><span class="s4">1 </span><span class="s1">/ vcp_sd</span>
        <span class="s1">vc_sd_grad += </span><span class="s4">1 </span><span class="s1">/ vc_sd</span>

        <span class="s1">mean_grad = np.concatenate((fep_mean_grad</span><span class="s2">, </span><span class="s1">vcp_mean_grad</span><span class="s2">,</span>
                                    <span class="s1">vc_mean_grad))</span>
        <span class="s1">sd_grad = np.concatenate((fep_sd_grad</span><span class="s2">, </span><span class="s1">vcp_sd_grad</span><span class="s2">, </span><span class="s1">vc_sd_grad))</span>

        <span class="s2">if </span><span class="s1">self.verbose:</span>
            <span class="s1">print(</span>
                <span class="s5">&quot;|G|=%f&quot; </span><span class="s1">% np.sqrt(np.sum(mean_grad**</span><span class="s4">2</span><span class="s1">) + np.sum(sd_grad**</span><span class="s4">2</span><span class="s1">)))</span>

        <span class="s2">return </span><span class="s1">mean_grad</span><span class="s2">, </span><span class="s1">sd_grad</span>

    <span class="s2">def </span><span class="s1">fit_vb(self</span><span class="s2">,</span>
               <span class="s1">mean=</span><span class="s2">None,</span>
               <span class="s1">sd=</span><span class="s2">None,</span>
               <span class="s1">fit_method=</span><span class="s5">&quot;BFGS&quot;</span><span class="s2">,</span>
               <span class="s1">minim_opts=</span><span class="s2">None,</span>
               <span class="s1">scale_fe=</span><span class="s2">False,</span>
               <span class="s1">verbose=</span><span class="s2">False</span><span class="s1">):</span>
        <span class="s0">&quot;&quot;&quot; 
        Fit a model using the variational Bayes mean field approximation. 
 
        Parameters 
        ---------- 
        mean : array_like 
            Starting value for VB mean vector 
        sd : array_like 
            Starting value for VB standard deviation vector 
        fit_method : str 
            Algorithm for scipy.minimize 
        minim_opts : dict 
            Options passed to scipy.minimize 
        scale_fe : bool 
            If true, the columns of the fixed effects design matrix 
            are centered and scaled to unit variance before fitting 
            the model.  The results are back-transformed so that the 
            results are presented on the original scale. 
        verbose : bool 
            If True, print the gradient norm to the screen each time 
            it is calculated. 
 
        Notes 
        ----- 
        The goal is to find a factored Gaussian approximation 
        q1*q2*...  to the posterior distribution, approximately 
        minimizing the KL divergence from the factored approximation 
        to the actual posterior.  The KL divergence, or ELBO function 
        has the form 
 
            E* log p(y, fe, vcp, vc) - E* log q 
 
        where E* is expectation with respect to the product of qj. 
 
        References 
        ---------- 
        Blei, Kucukelbir, McAuliffe (2017).  Variational Inference: A 
        review for Statisticians 
        https://arxiv.org/pdf/1601.00670.pdf 
        &quot;&quot;&quot;</span>

        <span class="s1">self.verbose = verbose</span>

        <span class="s2">if </span><span class="s1">scale_fe:</span>
            <span class="s1">mn = self.exog.mean(</span><span class="s4">0</span><span class="s1">)</span>
            <span class="s1">sc = self.exog.std(</span><span class="s4">0</span><span class="s1">)</span>
            <span class="s1">self._exog_save = self.exog</span>
            <span class="s1">self.exog = self.exog.copy()</span>
            <span class="s1">ixs = np.flatnonzero(sc &gt; </span><span class="s4">1e-8</span><span class="s1">)</span>
            <span class="s1">self.exog[:</span><span class="s2">, </span><span class="s1">ixs] -= mn[ixs]</span>
            <span class="s1">self.exog[:</span><span class="s2">, </span><span class="s1">ixs] /= sc[ixs]</span>

        <span class="s1">n = self.k_fep + self.k_vcp + self.k_vc</span>
        <span class="s1">ml = self.k_fep + self.k_vcp + self.k_vc</span>
        <span class="s2">if </span><span class="s1">mean </span><span class="s2">is None</span><span class="s1">:</span>
            <span class="s1">m = np.zeros(n)</span>
        <span class="s2">else</span><span class="s1">:</span>
            <span class="s2">if </span><span class="s1">len(mean) != ml:</span>
                <span class="s2">raise </span><span class="s1">ValueError(</span>
                    <span class="s5">&quot;mean has incorrect length, %d != %d&quot; </span><span class="s1">% (len(mean)</span><span class="s2">, </span><span class="s1">ml))</span>
            <span class="s1">m = mean.copy()</span>
        <span class="s2">if </span><span class="s1">sd </span><span class="s2">is None</span><span class="s1">:</span>
            <span class="s1">s = -</span><span class="s4">0.5 </span><span class="s1">+ </span><span class="s4">0.1 </span><span class="s1">* np.random.normal(size=n)</span>
        <span class="s2">else</span><span class="s1">:</span>
            <span class="s2">if </span><span class="s1">len(sd) != ml:</span>
                <span class="s2">raise </span><span class="s1">ValueError(</span>
                    <span class="s5">&quot;sd has incorrect length, %d != %d&quot; </span><span class="s1">% (len(sd)</span><span class="s2">, </span><span class="s1">ml))</span>

            <span class="s3"># s is parametrized on the log-scale internally when</span>
            <span class="s3"># optimizing the ELBO function (this is transparent to the</span>
            <span class="s3"># caller)</span>
            <span class="s1">s = np.log(sd)</span>

        <span class="s3"># Do not allow the variance parameter starting mean values to</span>
        <span class="s3"># be too small.</span>
        <span class="s1">i1</span><span class="s2">, </span><span class="s1">i2 = self.k_fep</span><span class="s2">, </span><span class="s1">self.k_fep + self.k_vcp</span>
        <span class="s1">m[i1:i2] = np.where(m[i1:i2] &lt; -</span><span class="s4">1</span><span class="s2">, </span><span class="s1">-</span><span class="s4">1</span><span class="s2">, </span><span class="s1">m[i1:i2])</span>

        <span class="s3"># Do not allow the posterior standard deviation starting values</span>
        <span class="s3"># to be too small.</span>
        <span class="s1">s = np.where(s &lt; -</span><span class="s4">1</span><span class="s2">, </span><span class="s1">-</span><span class="s4">1</span><span class="s2">, </span><span class="s1">s)</span>

        <span class="s2">def </span><span class="s1">elbo(x):</span>
            <span class="s1">n = len(x) // </span><span class="s4">2</span>
            <span class="s2">return </span><span class="s1">-self.vb_elbo(x[:n]</span><span class="s2">, </span><span class="s1">np.exp(x[n:]))</span>

        <span class="s2">def </span><span class="s1">elbo_grad(x):</span>
            <span class="s1">n = len(x) // </span><span class="s4">2</span>
            <span class="s1">gm</span><span class="s2">, </span><span class="s1">gs = self.vb_elbo_grad(x[:n]</span><span class="s2">, </span><span class="s1">np.exp(x[n:]))</span>
            <span class="s1">gs *= np.exp(x[n:])</span>
            <span class="s2">return </span><span class="s1">-np.concatenate((gm</span><span class="s2">, </span><span class="s1">gs))</span>

        <span class="s1">start = np.concatenate((m</span><span class="s2">, </span><span class="s1">s))</span>
        <span class="s1">mm = minimize(</span>
            <span class="s1">elbo</span><span class="s2">, </span><span class="s1">start</span><span class="s2">, </span><span class="s1">jac=elbo_grad</span><span class="s2">, </span><span class="s1">method=fit_method</span><span class="s2">, </span><span class="s1">options=minim_opts)</span>
        <span class="s2">if not </span><span class="s1">mm.success:</span>
            <span class="s1">warnings.warn(</span><span class="s5">&quot;VB fitting did not converge&quot;</span><span class="s1">)</span>

        <span class="s1">n = len(mm.x) // </span><span class="s4">2</span>
        <span class="s1">params = mm.x[</span><span class="s4">0</span><span class="s1">:n]</span>
        <span class="s1">va = np.exp(</span><span class="s4">2 </span><span class="s1">* mm.x[n:])</span>

        <span class="s2">if </span><span class="s1">scale_fe:</span>
            <span class="s1">self.exog = self._exog_save</span>
            <span class="s2">del </span><span class="s1">self._exog_save</span>
            <span class="s1">params[ixs] /= sc[ixs]</span>
            <span class="s1">va[ixs] /= sc[ixs]**</span><span class="s4">2</span>

        <span class="s2">return </span><span class="s1">BayesMixedGLMResults(self</span><span class="s2">, </span><span class="s1">params</span><span class="s2">, </span><span class="s1">va</span><span class="s2">, </span><span class="s1">mm)</span>

    <span class="s3"># Handle terms in the ELBO that are common to all models.</span>
    <span class="s2">def </span><span class="s1">_elbo_common(self</span><span class="s2">, </span><span class="s1">fep_mean</span><span class="s2">, </span><span class="s1">fep_sd</span><span class="s2">, </span><span class="s1">vcp_mean</span><span class="s2">, </span><span class="s1">vcp_sd</span><span class="s2">, </span><span class="s1">vc_mean</span><span class="s2">, </span><span class="s1">vc_sd):</span>

        <span class="s1">iv = </span><span class="s4">0</span>

        <span class="s3"># p(vc | vcp) contributions</span>
        <span class="s1">m = vcp_mean[self.ident]</span>
        <span class="s1">s = vcp_sd[self.ident]</span>
        <span class="s1">iv -= np.sum((vc_mean**</span><span class="s4">2 </span><span class="s1">+ vc_sd**</span><span class="s4">2</span><span class="s1">) * np.exp(</span><span class="s4">2 </span><span class="s1">* (s**</span><span class="s4">2 </span><span class="s1">- m))) / </span><span class="s4">2</span>
        <span class="s1">iv -= np.sum(m)</span>

        <span class="s3"># p(vcp) contributions</span>
        <span class="s1">iv -= </span><span class="s4">0.5 </span><span class="s1">* (vcp_mean**</span><span class="s4">2 </span><span class="s1">+ vcp_sd**</span><span class="s4">2</span><span class="s1">).sum() / self.vcp_p**</span><span class="s4">2</span>

        <span class="s3"># p(b) contributions</span>
        <span class="s1">iv -= </span><span class="s4">0.5 </span><span class="s1">* (fep_mean**</span><span class="s4">2 </span><span class="s1">+ fep_sd**</span><span class="s4">2</span><span class="s1">).sum() / self.fe_p**</span><span class="s4">2</span>

        <span class="s2">return </span><span class="s1">iv</span>

    <span class="s2">def </span><span class="s1">_elbo_grad_common(self</span><span class="s2">, </span><span class="s1">fep_mean</span><span class="s2">, </span><span class="s1">fep_sd</span><span class="s2">, </span><span class="s1">vcp_mean</span><span class="s2">, </span><span class="s1">vcp_sd</span><span class="s2">, </span><span class="s1">vc_mean</span><span class="s2">,</span>
                          <span class="s1">vc_sd):</span>

        <span class="s3"># p(vc | vcp) contributions</span>
        <span class="s1">m = vcp_mean[self.ident]</span>
        <span class="s1">s = vcp_sd[self.ident]</span>
        <span class="s1">u = vc_mean**</span><span class="s4">2 </span><span class="s1">+ vc_sd**</span><span class="s4">2</span>
        <span class="s1">ve = np.exp(</span><span class="s4">2 </span><span class="s1">* (s**</span><span class="s4">2 </span><span class="s1">- m))</span>
        <span class="s1">dm = u * ve - </span><span class="s4">1</span>
        <span class="s1">ds = -</span><span class="s4">2 </span><span class="s1">* u * ve * s</span>
        <span class="s1">vcp_mean_grad = np.bincount(self.ident</span><span class="s2">, </span><span class="s1">weights=dm)</span>
        <span class="s1">vcp_sd_grad = np.bincount(self.ident</span><span class="s2">, </span><span class="s1">weights=ds)</span>

        <span class="s1">vc_mean_grad = -vc_mean.copy() * ve</span>
        <span class="s1">vc_sd_grad = -vc_sd.copy() * ve</span>

        <span class="s3"># p(vcp) contributions</span>
        <span class="s1">vcp_mean_grad -= vcp_mean / self.vcp_p**</span><span class="s4">2</span>
        <span class="s1">vcp_sd_grad -= vcp_sd / self.vcp_p**</span><span class="s4">2</span>

        <span class="s3"># p(b) contributions</span>
        <span class="s1">fep_mean_grad = -fep_mean.copy() / self.fe_p**</span><span class="s4">2</span>
        <span class="s1">fep_sd_grad = -fep_sd.copy() / self.fe_p**</span><span class="s4">2</span>

        <span class="s2">return </span><span class="s1">(fep_mean_grad</span><span class="s2">, </span><span class="s1">fep_sd_grad</span><span class="s2">, </span><span class="s1">vcp_mean_grad</span><span class="s2">, </span><span class="s1">vcp_sd_grad</span><span class="s2">,</span>
                <span class="s1">vc_mean_grad</span><span class="s2">, </span><span class="s1">vc_sd_grad)</span>


<span class="s2">class </span><span class="s1">BayesMixedGLMResults:</span>
    <span class="s0">&quot;&quot;&quot; 
    Class to hold results from a Bayesian estimation of a Mixed GLM model. 
 
    Attributes 
    ---------- 
    fe_mean : array_like 
        Posterior mean of the fixed effects coefficients. 
    fe_sd : array_like 
        Posterior standard deviation of the fixed effects coefficients 
    vcp_mean : array_like 
        Posterior mean of the logged variance component standard 
        deviations. 
    vcp_sd : array_like 
        Posterior standard deviation of the logged variance component 
        standard deviations. 
    vc_mean : array_like 
        Posterior mean of the random coefficients 
    vc_sd : array_like 
        Posterior standard deviation of the random coefficients 
    &quot;&quot;&quot;</span>

    <span class="s2">def </span><span class="s1">__init__(self</span><span class="s2">, </span><span class="s1">model</span><span class="s2">, </span><span class="s1">params</span><span class="s2">, </span><span class="s1">cov_params</span><span class="s2">, </span><span class="s1">optim_retvals=</span><span class="s2">None</span><span class="s1">):</span>

        <span class="s1">self.model = model</span>
        <span class="s1">self.params = params</span>
        <span class="s1">self._cov_params = cov_params</span>
        <span class="s1">self.optim_retvals = optim_retvals</span>

        <span class="s1">self.fe_mean</span><span class="s2">, </span><span class="s1">self.vcp_mean</span><span class="s2">, </span><span class="s1">self.vc_mean = (model._unpack(params))</span>

        <span class="s2">if </span><span class="s1">cov_params.ndim == </span><span class="s4">2</span><span class="s1">:</span>
            <span class="s1">cp = np.diag(cov_params)</span>
        <span class="s2">else</span><span class="s1">:</span>
            <span class="s1">cp = cov_params</span>
        <span class="s1">self.fe_sd</span><span class="s2">, </span><span class="s1">self.vcp_sd</span><span class="s2">, </span><span class="s1">self.vc_sd = model._unpack(cp)</span>
        <span class="s1">self.fe_sd = np.sqrt(self.fe_sd)</span>
        <span class="s1">self.vcp_sd = np.sqrt(self.vcp_sd)</span>
        <span class="s1">self.vc_sd = np.sqrt(self.vc_sd)</span>

    <span class="s2">def </span><span class="s1">cov_params(self):</span>

        <span class="s2">if </span><span class="s1">hasattr(self.model.data</span><span class="s2">, </span><span class="s5">&quot;frame&quot;</span><span class="s1">):</span>
            <span class="s3"># Return the covariance matrix as a dataframe or series</span>
            <span class="s1">na = (self.model.fep_names + self.model.vcp_names +</span>
                  <span class="s1">self.model.vc_names)</span>
            <span class="s2">if </span><span class="s1">self._cov_params.ndim == </span><span class="s4">2</span><span class="s1">:</span>
                <span class="s2">return </span><span class="s1">pd.DataFrame(self._cov_params</span><span class="s2">, </span><span class="s1">index=na</span><span class="s2">, </span><span class="s1">columns=na)</span>
            <span class="s2">else</span><span class="s1">:</span>
                <span class="s2">return </span><span class="s1">pd.Series(self._cov_params</span><span class="s2">, </span><span class="s1">index=na)</span>

        <span class="s3"># Return the covariance matrix as a ndarray</span>
        <span class="s2">return </span><span class="s1">self._cov_params</span>

    <span class="s2">def </span><span class="s1">summary(self):</span>

        <span class="s1">df = pd.DataFrame()</span>
        <span class="s1">m = self.model.k_fep + self.model.k_vcp</span>
        <span class="s1">df[</span><span class="s5">&quot;Type&quot;</span><span class="s1">] = ([</span><span class="s5">&quot;M&quot; </span><span class="s2">for </span><span class="s1">k </span><span class="s2">in </span><span class="s1">range(self.model.k_fep)] +</span>
                      <span class="s1">[</span><span class="s5">&quot;V&quot; </span><span class="s2">for </span><span class="s1">k </span><span class="s2">in </span><span class="s1">range(self.model.k_vcp)])</span>

        <span class="s1">df[</span><span class="s5">&quot;Post. Mean&quot;</span><span class="s1">] = self.params[</span><span class="s4">0</span><span class="s1">:m]</span>

        <span class="s2">if </span><span class="s1">self._cov_params.ndim == </span><span class="s4">2</span><span class="s1">:</span>
            <span class="s1">v = np.diag(self._cov_params)[</span><span class="s4">0</span><span class="s1">:m]</span>
            <span class="s1">df[</span><span class="s5">&quot;Post. SD&quot;</span><span class="s1">] = np.sqrt(v)</span>
        <span class="s2">else</span><span class="s1">:</span>
            <span class="s1">df[</span><span class="s5">&quot;Post. SD&quot;</span><span class="s1">] = np.sqrt(self._cov_params[</span><span class="s4">0</span><span class="s1">:m])</span>

        <span class="s3"># Convert variance parameters to natural scale</span>
        <span class="s1">df[</span><span class="s5">&quot;SD&quot;</span><span class="s1">] = np.exp(df[</span><span class="s5">&quot;Post. Mean&quot;</span><span class="s1">])</span>
        <span class="s1">df[</span><span class="s5">&quot;SD (LB)&quot;</span><span class="s1">] = np.exp(df[</span><span class="s5">&quot;Post. Mean&quot;</span><span class="s1">] - </span><span class="s4">2 </span><span class="s1">* df[</span><span class="s5">&quot;Post. SD&quot;</span><span class="s1">])</span>
        <span class="s1">df[</span><span class="s5">&quot;SD (UB)&quot;</span><span class="s1">] = np.exp(df[</span><span class="s5">&quot;Post. Mean&quot;</span><span class="s1">] + </span><span class="s4">2 </span><span class="s1">* df[</span><span class="s5">&quot;Post. SD&quot;</span><span class="s1">])</span>
        <span class="s1">df[</span><span class="s5">&quot;SD&quot;</span><span class="s1">] = [</span><span class="s5">&quot;%.3f&quot; </span><span class="s1">% x </span><span class="s2">for </span><span class="s1">x </span><span class="s2">in </span><span class="s1">df.SD]</span>
        <span class="s1">df[</span><span class="s5">&quot;SD (LB)&quot;</span><span class="s1">] = [</span><span class="s5">&quot;%.3f&quot; </span><span class="s1">% x </span><span class="s2">for </span><span class="s1">x </span><span class="s2">in </span><span class="s1">df[</span><span class="s5">&quot;SD (LB)&quot;</span><span class="s1">]]</span>
        <span class="s1">df[</span><span class="s5">&quot;SD (UB)&quot;</span><span class="s1">] = [</span><span class="s5">&quot;%.3f&quot; </span><span class="s1">% x </span><span class="s2">for </span><span class="s1">x </span><span class="s2">in </span><span class="s1">df[</span><span class="s5">&quot;SD (UB)&quot;</span><span class="s1">]]</span>
        <span class="s1">df.loc[df.index &lt; self.model.k_fep</span><span class="s2">, </span><span class="s5">&quot;SD&quot;</span><span class="s1">] = </span><span class="s5">&quot;&quot;</span>
        <span class="s1">df.loc[df.index &lt; self.model.k_fep</span><span class="s2">, </span><span class="s5">&quot;SD (LB)&quot;</span><span class="s1">] = </span><span class="s5">&quot;&quot;</span>
        <span class="s1">df.loc[df.index &lt; self.model.k_fep</span><span class="s2">, </span><span class="s5">&quot;SD (UB)&quot;</span><span class="s1">] = </span><span class="s5">&quot;&quot;</span>

        <span class="s1">df.index = self.model.fep_names + self.model.vcp_names</span>

        <span class="s1">summ = summary2.Summary()</span>
        <span class="s1">summ.add_title(self.model.family.__class__.__name__ +</span>
                       <span class="s5">&quot; Mixed GLM Results&quot;</span><span class="s1">)</span>
        <span class="s1">summ.add_df(df)</span>

        <span class="s1">summ.add_text(</span><span class="s5">&quot;Parameter types are mean structure (M) and &quot;</span>
                      <span class="s5">&quot;variance structure (V)&quot;</span><span class="s1">)</span>
        <span class="s1">summ.add_text(</span><span class="s5">&quot;Variance parameters are modeled as log &quot;</span>
                      <span class="s5">&quot;standard deviations&quot;</span><span class="s1">)</span>

        <span class="s2">return </span><span class="s1">summ</span>

    <span class="s2">def </span><span class="s1">random_effects(self</span><span class="s2">, </span><span class="s1">term=</span><span class="s2">None</span><span class="s1">):</span>
        <span class="s0">&quot;&quot;&quot; 
        Posterior mean and standard deviation of random effects. 
 
        Parameters 
        ---------- 
        term : int or None 
            If None, results for all random effects are returned.  If 
            an integer, returns results for a given set of random 
            effects.  The value of `term` refers to an element of the 
            `ident` vector, or to a position in the `vc_formulas` 
            list. 
 
        Returns 
        ------- 
        Data frame of posterior means and posterior standard 
        deviations of random effects. 
        &quot;&quot;&quot;</span>

        <span class="s1">z = self.vc_mean</span>
        <span class="s1">s = self.vc_sd</span>
        <span class="s1">na = self.model.vc_names</span>

        <span class="s2">if </span><span class="s1">term </span><span class="s2">is not None</span><span class="s1">:</span>
            <span class="s1">termix = self.model.vcp_names.index(term)</span>
            <span class="s1">ii = np.flatnonzero(self.model.ident == termix)</span>
            <span class="s1">z = z[ii]</span>
            <span class="s1">s = s[ii]</span>
            <span class="s1">na = [na[i] </span><span class="s2">for </span><span class="s1">i </span><span class="s2">in </span><span class="s1">ii]</span>

        <span class="s1">x = pd.DataFrame({</span><span class="s5">&quot;Mean&quot;</span><span class="s1">: z</span><span class="s2">, </span><span class="s5">&quot;SD&quot;</span><span class="s1">: s})</span>

        <span class="s2">if </span><span class="s1">na </span><span class="s2">is not None</span><span class="s1">:</span>
            <span class="s1">x.index = na</span>

        <span class="s2">return </span><span class="s1">x</span>

    <span class="s2">def </span><span class="s1">predict(self</span><span class="s2">, </span><span class="s1">exog=</span><span class="s2">None, </span><span class="s1">linear=</span><span class="s2">False</span><span class="s1">):</span>
        <span class="s0">&quot;&quot;&quot; 
        Return predicted values for the mean structure. 
 
        Parameters 
        ---------- 
        exog : array_like 
            The design matrix for the mean structure.  If None, 
            use the model's design matrix. 
        linear : bool 
            If True, returns the linear predictor, otherwise 
            transform the linear predictor using the link function. 
 
        Returns 
        ------- 
        A one-dimensional array of fitted values. 
        &quot;&quot;&quot;</span>

        <span class="s2">return </span><span class="s1">self.model.predict(self.params</span><span class="s2">, </span><span class="s1">exog</span><span class="s2">, </span><span class="s1">linear)</span>


<span class="s2">class </span><span class="s1">BinomialBayesMixedGLM(_VariationalBayesMixedGLM</span><span class="s2">, </span><span class="s1">_BayesMixedGLM):</span>

    <span class="s1">__doc__ = _init_doc.format(example=_logit_example)</span>

    <span class="s2">def </span><span class="s1">__init__(self</span><span class="s2">,</span>
                 <span class="s1">endog</span><span class="s2">,</span>
                 <span class="s1">exog</span><span class="s2">,</span>
                 <span class="s1">exog_vc</span><span class="s2">,</span>
                 <span class="s1">ident</span><span class="s2">,</span>
                 <span class="s1">vcp_p=</span><span class="s4">1</span><span class="s2">,</span>
                 <span class="s1">fe_p=</span><span class="s4">2</span><span class="s2">,</span>
                 <span class="s1">fep_names=</span><span class="s2">None,</span>
                 <span class="s1">vcp_names=</span><span class="s2">None,</span>
                 <span class="s1">vc_names=</span><span class="s2">None</span><span class="s1">):</span>

        <span class="s1">super(BinomialBayesMixedGLM</span><span class="s2">, </span><span class="s1">self).__init__(</span>
            <span class="s1">endog</span><span class="s2">,</span>
            <span class="s1">exog</span><span class="s2">,</span>
            <span class="s1">exog_vc=exog_vc</span><span class="s2">,</span>
            <span class="s1">ident=ident</span><span class="s2">,</span>
            <span class="s1">vcp_p=vcp_p</span><span class="s2">,</span>
            <span class="s1">fe_p=fe_p</span><span class="s2">,</span>
            <span class="s1">family=families.Binomial()</span><span class="s2">,</span>
            <span class="s1">fep_names=fep_names</span><span class="s2">,</span>
            <span class="s1">vcp_names=vcp_names</span><span class="s2">,</span>
            <span class="s1">vc_names=vc_names)</span>

        <span class="s2">if not </span><span class="s1">np.all(np.unique(endog) == np.r_[</span><span class="s4">0</span><span class="s2">, </span><span class="s4">1</span><span class="s1">]):</span>
            <span class="s1">msg = </span><span class="s5">&quot;endog values must be 0 and 1, and not all identical&quot;</span>
            <span class="s2">raise </span><span class="s1">ValueError(msg)</span>

    <span class="s1">@classmethod</span>
    <span class="s2">def </span><span class="s1">from_formula(cls</span><span class="s2">, </span><span class="s1">formula</span><span class="s2">, </span><span class="s1">vc_formulas</span><span class="s2">, </span><span class="s1">data</span><span class="s2">, </span><span class="s1">vcp_p=</span><span class="s4">1</span><span class="s2">, </span><span class="s1">fe_p=</span><span class="s4">2</span><span class="s1">):</span>

        <span class="s1">fam = families.Binomial()</span>
        <span class="s1">x = _BayesMixedGLM.from_formula(</span>
            <span class="s1">formula</span><span class="s2">, </span><span class="s1">vc_formulas</span><span class="s2">, </span><span class="s1">data</span><span class="s2">, </span><span class="s1">family=fam</span><span class="s2">, </span><span class="s1">vcp_p=vcp_p</span><span class="s2">, </span><span class="s1">fe_p=fe_p)</span>

        <span class="s3"># Copy over to the intended class structure</span>
        <span class="s1">mod = BinomialBayesMixedGLM(</span>
            <span class="s1">x.endog</span><span class="s2">,</span>
            <span class="s1">x.exog</span><span class="s2">,</span>
            <span class="s1">exog_vc=x.exog_vc</span><span class="s2">,</span>
            <span class="s1">ident=x.ident</span><span class="s2">,</span>
            <span class="s1">vcp_p=x.vcp_p</span><span class="s2">,</span>
            <span class="s1">fe_p=x.fe_p</span><span class="s2">,</span>
            <span class="s1">fep_names=x.fep_names</span><span class="s2">,</span>
            <span class="s1">vcp_names=x.vcp_names</span><span class="s2">,</span>
            <span class="s1">vc_names=x.vc_names)</span>
        <span class="s1">mod.data = x.data</span>

        <span class="s2">return </span><span class="s1">mod</span>

    <span class="s2">def </span><span class="s1">vb_elbo(self</span><span class="s2">, </span><span class="s1">vb_mean</span><span class="s2">, </span><span class="s1">vb_sd):</span>
        <span class="s0">&quot;&quot;&quot; 
        Returns the evidence lower bound (ELBO) for the model. 
        &quot;&quot;&quot;</span>

        <span class="s1">fep_mean</span><span class="s2">, </span><span class="s1">vcp_mean</span><span class="s2">, </span><span class="s1">vc_mean = self._unpack(vb_mean)</span>
        <span class="s1">fep_sd</span><span class="s2">, </span><span class="s1">vcp_sd</span><span class="s2">, </span><span class="s1">vc_sd = self._unpack(vb_sd)</span>
        <span class="s1">tm</span><span class="s2">, </span><span class="s1">tv = self._lp_stats(fep_mean</span><span class="s2">, </span><span class="s1">fep_sd</span><span class="s2">, </span><span class="s1">vc_mean</span><span class="s2">, </span><span class="s1">vc_sd)</span>

        <span class="s2">def </span><span class="s1">h(z):</span>
            <span class="s2">return </span><span class="s1">-np.log(</span><span class="s4">1 </span><span class="s1">+ np.exp(tm + np.sqrt(tv) * z))</span>

        <span class="s2">return </span><span class="s1">self.vb_elbo_base(h</span><span class="s2">, </span><span class="s1">tm</span><span class="s2">, </span><span class="s1">fep_mean</span><span class="s2">, </span><span class="s1">vcp_mean</span><span class="s2">, </span><span class="s1">vc_mean</span><span class="s2">, </span><span class="s1">fep_sd</span><span class="s2">,</span>
                                 <span class="s1">vcp_sd</span><span class="s2">, </span><span class="s1">vc_sd)</span>

    <span class="s2">def </span><span class="s1">vb_elbo_grad(self</span><span class="s2">, </span><span class="s1">vb_mean</span><span class="s2">, </span><span class="s1">vb_sd):</span>
        <span class="s0">&quot;&quot;&quot; 
        Returns the gradient of the model's evidence lower bound (ELBO). 
        &quot;&quot;&quot;</span>

        <span class="s1">fep_mean</span><span class="s2">, </span><span class="s1">vcp_mean</span><span class="s2">, </span><span class="s1">vc_mean = self._unpack(vb_mean)</span>
        <span class="s1">fep_sd</span><span class="s2">, </span><span class="s1">vcp_sd</span><span class="s2">, </span><span class="s1">vc_sd = self._unpack(vb_sd)</span>
        <span class="s1">tm</span><span class="s2">, </span><span class="s1">tv = self._lp_stats(fep_mean</span><span class="s2">, </span><span class="s1">fep_sd</span><span class="s2">, </span><span class="s1">vc_mean</span><span class="s2">, </span><span class="s1">vc_sd)</span>

        <span class="s2">def </span><span class="s1">h(z):</span>
            <span class="s1">u = tm + np.sqrt(tv) * z</span>
            <span class="s1">x = np.zeros_like(u)</span>
            <span class="s1">ii = np.flatnonzero(u &gt; </span><span class="s4">0</span><span class="s1">)</span>
            <span class="s1">uu = u[ii]</span>
            <span class="s1">x[ii] = </span><span class="s4">1 </span><span class="s1">/ (</span><span class="s4">1 </span><span class="s1">+ np.exp(-uu))</span>
            <span class="s1">ii = np.flatnonzero(u &lt;= </span><span class="s4">0</span><span class="s1">)</span>
            <span class="s1">uu = u[ii]</span>
            <span class="s1">x[ii] = np.exp(uu) / (</span><span class="s4">1 </span><span class="s1">+ np.exp(uu))</span>
            <span class="s2">return </span><span class="s1">-x</span>

        <span class="s2">return </span><span class="s1">self.vb_elbo_grad_base(h</span><span class="s2">, </span><span class="s1">tm</span><span class="s2">, </span><span class="s1">tv</span><span class="s2">, </span><span class="s1">fep_mean</span><span class="s2">, </span><span class="s1">vcp_mean</span><span class="s2">, </span><span class="s1">vc_mean</span><span class="s2">,</span>
                                      <span class="s1">fep_sd</span><span class="s2">, </span><span class="s1">vcp_sd</span><span class="s2">, </span><span class="s1">vc_sd)</span>


<span class="s2">class </span><span class="s1">PoissonBayesMixedGLM(_VariationalBayesMixedGLM</span><span class="s2">, </span><span class="s1">_BayesMixedGLM):</span>

    <span class="s1">__doc__ = _init_doc.format(example=_poisson_example)</span>

    <span class="s2">def </span><span class="s1">__init__(self</span><span class="s2">,</span>
                 <span class="s1">endog</span><span class="s2">,</span>
                 <span class="s1">exog</span><span class="s2">,</span>
                 <span class="s1">exog_vc</span><span class="s2">,</span>
                 <span class="s1">ident</span><span class="s2">,</span>
                 <span class="s1">vcp_p=</span><span class="s4">1</span><span class="s2">,</span>
                 <span class="s1">fe_p=</span><span class="s4">2</span><span class="s2">,</span>
                 <span class="s1">fep_names=</span><span class="s2">None,</span>
                 <span class="s1">vcp_names=</span><span class="s2">None,</span>
                 <span class="s1">vc_names=</span><span class="s2">None</span><span class="s1">):</span>

        <span class="s1">super(PoissonBayesMixedGLM</span><span class="s2">, </span><span class="s1">self).__init__(</span>
            <span class="s1">endog=endog</span><span class="s2">,</span>
            <span class="s1">exog=exog</span><span class="s2">,</span>
            <span class="s1">exog_vc=exog_vc</span><span class="s2">,</span>
            <span class="s1">ident=ident</span><span class="s2">,</span>
            <span class="s1">vcp_p=vcp_p</span><span class="s2">,</span>
            <span class="s1">fe_p=fe_p</span><span class="s2">,</span>
            <span class="s1">family=families.Poisson()</span><span class="s2">,</span>
            <span class="s1">fep_names=fep_names</span><span class="s2">,</span>
            <span class="s1">vcp_names=vcp_names</span><span class="s2">,</span>
            <span class="s1">vc_names=vc_names)</span>

    <span class="s1">@classmethod</span>
    <span class="s2">def </span><span class="s1">from_formula(cls</span><span class="s2">,</span>
                     <span class="s1">formula</span><span class="s2">,</span>
                     <span class="s1">vc_formulas</span><span class="s2">,</span>
                     <span class="s1">data</span><span class="s2">,</span>
                     <span class="s1">vcp_p=</span><span class="s4">1</span><span class="s2">,</span>
                     <span class="s1">fe_p=</span><span class="s4">2</span><span class="s2">,</span>
                     <span class="s1">vcp_names=</span><span class="s2">None,</span>
                     <span class="s1">vc_names=</span><span class="s2">None</span><span class="s1">):</span>

        <span class="s1">fam = families.Poisson()</span>
        <span class="s1">x = _BayesMixedGLM.from_formula(</span>
            <span class="s1">formula</span><span class="s2">,</span>
            <span class="s1">vc_formulas</span><span class="s2">,</span>
            <span class="s1">data</span><span class="s2">,</span>
            <span class="s1">family=fam</span><span class="s2">,</span>
            <span class="s1">vcp_p=vcp_p</span><span class="s2">,</span>
            <span class="s1">fe_p=fe_p)</span>

        <span class="s3"># Copy over to the intended class structure</span>
        <span class="s1">mod = PoissonBayesMixedGLM(</span>
            <span class="s1">endog=x.endog</span><span class="s2">,</span>
            <span class="s1">exog=x.exog</span><span class="s2">,</span>
            <span class="s1">exog_vc=x.exog_vc</span><span class="s2">,</span>
            <span class="s1">ident=x.ident</span><span class="s2">,</span>
            <span class="s1">vcp_p=x.vcp_p</span><span class="s2">,</span>
            <span class="s1">fe_p=x.fe_p</span><span class="s2">,</span>
            <span class="s1">fep_names=x.fep_names</span><span class="s2">,</span>
            <span class="s1">vcp_names=x.vcp_names</span><span class="s2">,</span>
            <span class="s1">vc_names=x.vc_names)</span>
        <span class="s1">mod.data = x.data</span>

        <span class="s2">return </span><span class="s1">mod</span>

    <span class="s2">def </span><span class="s1">vb_elbo(self</span><span class="s2">, </span><span class="s1">vb_mean</span><span class="s2">, </span><span class="s1">vb_sd):</span>
        <span class="s0">&quot;&quot;&quot; 
        Returns the evidence lower bound (ELBO) for the model. 
        &quot;&quot;&quot;</span>

        <span class="s1">fep_mean</span><span class="s2">, </span><span class="s1">vcp_mean</span><span class="s2">, </span><span class="s1">vc_mean = self._unpack(vb_mean)</span>
        <span class="s1">fep_sd</span><span class="s2">, </span><span class="s1">vcp_sd</span><span class="s2">, </span><span class="s1">vc_sd = self._unpack(vb_sd)</span>
        <span class="s1">tm</span><span class="s2">, </span><span class="s1">tv = self._lp_stats(fep_mean</span><span class="s2">, </span><span class="s1">fep_sd</span><span class="s2">, </span><span class="s1">vc_mean</span><span class="s2">, </span><span class="s1">vc_sd)</span>

        <span class="s2">def </span><span class="s1">h(z):</span>
            <span class="s2">return </span><span class="s1">-np.exp(tm + np.sqrt(tv) * z)</span>

        <span class="s2">return </span><span class="s1">self.vb_elbo_base(h</span><span class="s2">, </span><span class="s1">tm</span><span class="s2">, </span><span class="s1">fep_mean</span><span class="s2">, </span><span class="s1">vcp_mean</span><span class="s2">, </span><span class="s1">vc_mean</span><span class="s2">, </span><span class="s1">fep_sd</span><span class="s2">,</span>
                                 <span class="s1">vcp_sd</span><span class="s2">, </span><span class="s1">vc_sd)</span>

    <span class="s2">def </span><span class="s1">vb_elbo_grad(self</span><span class="s2">, </span><span class="s1">vb_mean</span><span class="s2">, </span><span class="s1">vb_sd):</span>
        <span class="s0">&quot;&quot;&quot; 
        Returns the gradient of the model's evidence lower bound (ELBO). 
        &quot;&quot;&quot;</span>

        <span class="s1">fep_mean</span><span class="s2">, </span><span class="s1">vcp_mean</span><span class="s2">, </span><span class="s1">vc_mean = self._unpack(vb_mean)</span>
        <span class="s1">fep_sd</span><span class="s2">, </span><span class="s1">vcp_sd</span><span class="s2">, </span><span class="s1">vc_sd = self._unpack(vb_sd)</span>
        <span class="s1">tm</span><span class="s2">, </span><span class="s1">tv = self._lp_stats(fep_mean</span><span class="s2">, </span><span class="s1">fep_sd</span><span class="s2">, </span><span class="s1">vc_mean</span><span class="s2">, </span><span class="s1">vc_sd)</span>

        <span class="s2">def </span><span class="s1">h(z):</span>
            <span class="s1">y = -np.exp(tm + np.sqrt(tv) * z)</span>
            <span class="s2">return </span><span class="s1">y</span>

        <span class="s2">return </span><span class="s1">self.vb_elbo_grad_base(h</span><span class="s2">, </span><span class="s1">tm</span><span class="s2">, </span><span class="s1">tv</span><span class="s2">, </span><span class="s1">fep_mean</span><span class="s2">, </span><span class="s1">vcp_mean</span><span class="s2">, </span><span class="s1">vc_mean</span><span class="s2">,</span>
                                      <span class="s1">fep_sd</span><span class="s2">, </span><span class="s1">vcp_sd</span><span class="s2">, </span><span class="s1">vc_sd)</span>
</pre>
</body>
</html>