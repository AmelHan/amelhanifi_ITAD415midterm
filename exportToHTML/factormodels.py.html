<html>
<head>
<title>factormodels.py</title>
<meta http-equiv="Content-Type" content="text/html; charset=utf-8">
<style type="text/css">
.s0 { color: #808080;}
.s1 { color: #a9b7c6;}
.s2 { color: #629755; font-style: italic;}
.s3 { color: #cc7832;}
.s4 { color: #6897bb;}
.s5 { color: #6a8759;}
</style>
</head>
<body bgcolor="#2b2b2b">
<table CELLSPACING=0 CELLPADDING=5 COLS=1 WIDTH="100%" BGCOLOR="#606060" >
<tr><td><center>
<font face="Arial, Helvetica" color="#000000">
factormodels.py</font>
</center></td></tr></table>
<pre><span class="s0"># -*- coding: utf-8 -*-</span>
<span class="s2">&quot;&quot;&quot; 
Created on Sun Nov 14 08:21:41 2010 
 
Author: josef-pktd 
License: BSD (3-clause) 
&quot;&quot;&quot;</span>

<span class="s3">import </span><span class="s1">numpy </span><span class="s3">as </span><span class="s1">np</span>
<span class="s3">import </span><span class="s1">statsmodels.api </span><span class="s3">as </span><span class="s1">sm</span>
<span class="s3">from </span><span class="s1">statsmodels.sandbox.tools </span><span class="s3">import </span><span class="s1">pca</span>
<span class="s3">from </span><span class="s1">statsmodels.sandbox.tools.cross_val </span><span class="s3">import </span><span class="s1">LeaveOneOut</span>

<span class="s0">#converting example Principal Component Regression to a class</span>
<span class="s0">#from sandbox/example_pca_regression.py</span>


<span class="s3">class </span><span class="s1">FactorModelUnivariate:</span>
    <span class="s2">''' 
 
    Todo: 
    check treatment of const, make it optional ? 
        add hasconst (0 or 1), needed when selecting nfact+hasconst 
    options are arguments in calc_factors, should be more public instead 
    cross-validation is slow for large number of observations 
    '''</span>
    <span class="s3">def </span><span class="s1">__init__(self</span><span class="s3">, </span><span class="s1">endog</span><span class="s3">, </span><span class="s1">exog):</span>
        <span class="s0">#do this in a superclass?</span>
        <span class="s1">self.endog = np.asarray(endog)</span>
        <span class="s1">self.exog = np.asarray(exog)</span>


    <span class="s3">def </span><span class="s1">calc_factors(self</span><span class="s3">, </span><span class="s1">x=</span><span class="s3">None, </span><span class="s1">keepdim=</span><span class="s4">0</span><span class="s3">, </span><span class="s1">addconst=</span><span class="s3">True</span><span class="s1">):</span>
        <span class="s2">'''get factor decomposition of exogenous variables 
 
        This uses principal component analysis to obtain the factors. The number 
        of factors kept is the maximum that will be considered in the regression. 
        '''</span>
        <span class="s3">if </span><span class="s1">x </span><span class="s3">is None</span><span class="s1">:</span>
            <span class="s1">x = self.exog</span>
        <span class="s3">else</span><span class="s1">:</span>
            <span class="s1">x = np.asarray(x)</span>
        <span class="s1">xred</span><span class="s3">, </span><span class="s1">fact</span><span class="s3">, </span><span class="s1">evals</span><span class="s3">, </span><span class="s1">evecs  = pca(x</span><span class="s3">, </span><span class="s1">keepdim=keepdim</span><span class="s3">, </span><span class="s1">normalize=</span><span class="s4">1</span><span class="s1">)</span>
        <span class="s1">self.exog_reduced = xred</span>
        <span class="s0">#self.factors = fact</span>
        <span class="s3">if </span><span class="s1">addconst:</span>
            <span class="s1">self.factors = sm.add_constant(fact</span><span class="s3">, </span><span class="s1">prepend=</span><span class="s3">True</span><span class="s1">)</span>
            <span class="s1">self.hasconst = </span><span class="s4">1  </span><span class="s0">#needs to be int</span>
        <span class="s3">else</span><span class="s1">:</span>
            <span class="s1">self.factors = fact</span>
            <span class="s1">self.hasconst = </span><span class="s4">0  </span><span class="s0">#needs to be int</span>

        <span class="s1">self.evals = evals</span>
        <span class="s1">self.evecs = evecs</span>

    <span class="s3">def </span><span class="s1">fit_fixed_nfact(self</span><span class="s3">, </span><span class="s1">nfact):</span>
        <span class="s3">if not </span><span class="s1">hasattr(self</span><span class="s3">, </span><span class="s5">'factors_wconst'</span><span class="s1">):</span>
            <span class="s1">self.calc_factors()</span>
        <span class="s3">return </span><span class="s1">sm.OLS(self.endog</span><span class="s3">, </span><span class="s1">self.factors[:</span><span class="s3">,</span><span class="s1">:nfact+</span><span class="s4">1</span><span class="s1">]).fit()</span>

    <span class="s3">def </span><span class="s1">fit_find_nfact(self</span><span class="s3">, </span><span class="s1">maxfact=</span><span class="s3">None, </span><span class="s1">skip_crossval=</span><span class="s3">True, </span><span class="s1">cv_iter=</span><span class="s3">None</span><span class="s1">):</span>
        <span class="s2">'''estimate the model and selection criteria for up to maxfact factors 
 
        The selection criteria that are calculated are AIC, BIC, and R2_adj. and 
        additionally cross-validation prediction error sum of squares if `skip_crossval` 
        is false. Cross-validation is not used by default because it can be 
        time consuming to calculate. 
 
        By default the cross-validation method is Leave-one-out on the full dataset. 
        A different cross-validation sample can be specified as an argument to 
        cv_iter. 
 
        Results are attached in `results_find_nfact` 
 
 
 
        '''</span>
        <span class="s0">#print 'OLS on Factors'</span>
        <span class="s3">if not </span><span class="s1">hasattr(self</span><span class="s3">, </span><span class="s5">'factors'</span><span class="s1">):</span>
            <span class="s1">self.calc_factors()</span>

        <span class="s1">hasconst = self.hasconst</span>
        <span class="s3">if </span><span class="s1">maxfact </span><span class="s3">is None</span><span class="s1">:</span>
            <span class="s1">maxfact = self.factors.shape[</span><span class="s4">1</span><span class="s1">] - hasconst</span>

        <span class="s3">if </span><span class="s1">(maxfact+hasconst) &lt; </span><span class="s4">1</span><span class="s1">:</span>
            <span class="s3">raise </span><span class="s1">ValueError(</span><span class="s5">'nothing to do, number of factors (incl. constant) should ' </span><span class="s1">+</span>
                             <span class="s5">'be at least 1'</span><span class="s1">)</span>

        <span class="s0">#temporary safety</span>
        <span class="s1">maxfact = min(maxfact</span><span class="s3">, </span><span class="s4">10</span><span class="s1">)</span>

        <span class="s1">y0 = self.endog</span>
        <span class="s1">results = []</span>
        <span class="s0">#xred, fact, eva, eve  = pca(x0, keepdim=0, normalize=1)</span>
        <span class="s3">for </span><span class="s1">k </span><span class="s3">in </span><span class="s1">range(</span><span class="s4">1</span><span class="s3">, </span><span class="s1">maxfact+hasconst): </span><span class="s0">#k includes now the constnat</span>
            <span class="s0">#xred, fact, eva, eve  = pca(x0, keepdim=k, normalize=1)</span>
            <span class="s0"># this is faster and same result</span>
            <span class="s1">fact = self.factors[:</span><span class="s3">,</span><span class="s1">:k]</span>
            <span class="s1">res = sm.OLS(y0</span><span class="s3">, </span><span class="s1">fact).fit()</span>
        <span class="s0">##    print 'k =', k</span>
        <span class="s0">##    print res.params</span>
        <span class="s0">##    print 'aic:  ', res.aic</span>
        <span class="s0">##    print 'bic:  ', res.bic</span>
        <span class="s0">##    print 'llf:  ', res.llf</span>
        <span class="s0">##    print 'R2    ', res.rsquared</span>
        <span class="s0">##    print 'R2 adj', res.rsquared_adj</span>

            <span class="s3">if not </span><span class="s1">skip_crossval:</span>
                <span class="s3">if </span><span class="s1">cv_iter </span><span class="s3">is None</span><span class="s1">:</span>
                    <span class="s1">cv_iter = LeaveOneOut(len(y0))</span>
                <span class="s1">prederr2 = </span><span class="s4">0.</span>
                <span class="s3">for </span><span class="s1">inidx</span><span class="s3">, </span><span class="s1">outidx </span><span class="s3">in </span><span class="s1">cv_iter:</span>
                    <span class="s1">res_l1o = sm.OLS(y0[inidx]</span><span class="s3">, </span><span class="s1">fact[inidx</span><span class="s3">,</span><span class="s1">:]).fit()</span>
                    <span class="s0">#print data.endog[outidx], res.model.predict(data.exog[outidx,:]),</span>
                    <span class="s1">prederr2 += (y0[outidx] -</span>
                                 <span class="s1">res_l1o.model.predict(res_l1o.params</span><span class="s3">, </span><span class="s1">fact[outidx</span><span class="s3">,</span><span class="s1">:]))**</span><span class="s4">2.</span>
            <span class="s3">else</span><span class="s1">:</span>
                <span class="s1">prederr2 = np.nan</span>

            <span class="s1">results.append([k</span><span class="s3">, </span><span class="s1">res.aic</span><span class="s3">, </span><span class="s1">res.bic</span><span class="s3">, </span><span class="s1">res.rsquared_adj</span><span class="s3">, </span><span class="s1">prederr2])</span>

        <span class="s1">self.results_find_nfact = results = np.array(results)</span>
        <span class="s1">self.best_nfact = np.r_[(np.argmin(results[:</span><span class="s3">,</span><span class="s4">1</span><span class="s1">:</span><span class="s4">3</span><span class="s1">]</span><span class="s3">,</span><span class="s4">0</span><span class="s1">)</span><span class="s3">, </span><span class="s1">np.argmax(results[:</span><span class="s3">,</span><span class="s4">3</span><span class="s1">]</span><span class="s3">,</span><span class="s4">0</span><span class="s1">)</span><span class="s3">,</span>
                     <span class="s1">np.argmin(results[:</span><span class="s3">,</span><span class="s1">-</span><span class="s4">1</span><span class="s1">]</span><span class="s3">,</span><span class="s4">0</span><span class="s1">))]</span>

    <span class="s3">def </span><span class="s1">summary_find_nfact(self):</span>
        <span class="s2">'''provides a summary for the selection of the number of factors 
 
        Returns 
        ------- 
        sumstr : str 
            summary of the results for selecting the number of factors 
 
        '''</span>
        <span class="s3">if not </span><span class="s1">hasattr(self</span><span class="s3">, </span><span class="s5">'results_find_nfact'</span><span class="s1">):</span>
            <span class="s1">self.fit_find_nfact()</span>


        <span class="s1">results = self.results_find_nfact</span>
        <span class="s1">sumstr = </span><span class="s5">''</span>
        <span class="s1">sumstr += </span><span class="s5">'</span><span class="s3">\n</span><span class="s5">' </span><span class="s1">+ </span><span class="s5">'Best result for k, by AIC, BIC, R2_adj, L1O'</span>
<span class="s0">#        best = np.r_[(np.argmin(results[:,1:3],0), np.argmax(results[:,3],0),</span>
<span class="s0">#                     np.argmin(results[:,-1],0))]</span>

        <span class="s1">sumstr += </span><span class="s5">'</span><span class="s3">\n</span><span class="s5">' </span><span class="s1">+ </span><span class="s5">' '</span><span class="s1">*</span><span class="s4">19 </span><span class="s1">+ </span><span class="s5">'%5d %4d %6d %5d' </span><span class="s1">% tuple(self.best_nfact)</span>

        <span class="s3">from </span><span class="s1">statsmodels.iolib.table </span><span class="s3">import </span><span class="s1">SimpleTable</span>

        <span class="s1">headers = </span><span class="s5">'k, AIC, BIC, R2_adj, L1O'</span><span class="s1">.split(</span><span class="s5">', '</span><span class="s1">)</span>
        <span class="s1">numformat = [</span><span class="s5">'%6d'</span><span class="s1">] + [</span><span class="s5">'%10.3f'</span><span class="s1">]*</span><span class="s4">4 </span><span class="s0">#'%10.4f'</span>
        <span class="s1">txt_fmt1 = dict(data_fmts = numformat)</span>
        <span class="s1">tabl = SimpleTable(results</span><span class="s3">, </span><span class="s1">headers</span><span class="s3">, None, </span><span class="s1">txt_fmt=txt_fmt1)</span>

        <span class="s1">sumstr += </span><span class="s5">'</span><span class="s3">\n</span><span class="s5">' </span><span class="s1">+ </span><span class="s5">&quot;PCA regression on simulated data,&quot;</span>
        <span class="s1">sumstr += </span><span class="s5">'</span><span class="s3">\n</span><span class="s5">' </span><span class="s1">+ </span><span class="s5">&quot;DGP: 2 factors and 4 explanatory variables&quot;</span>
        <span class="s1">sumstr += </span><span class="s5">'</span><span class="s3">\n</span><span class="s5">' </span><span class="s1">+ tabl.__str__()</span>
        <span class="s1">sumstr += </span><span class="s5">'</span><span class="s3">\n</span><span class="s5">' </span><span class="s1">+ </span><span class="s5">&quot;Notes: k is number of components of PCA,&quot;</span>
        <span class="s1">sumstr += </span><span class="s5">'</span><span class="s3">\n</span><span class="s5">' </span><span class="s1">+ </span><span class="s5">&quot;       constant is added additionally&quot;</span>
        <span class="s1">sumstr += </span><span class="s5">'</span><span class="s3">\n</span><span class="s5">' </span><span class="s1">+ </span><span class="s5">&quot;       k=0 means regression on constant only&quot;</span>
        <span class="s1">sumstr += </span><span class="s5">'</span><span class="s3">\n</span><span class="s5">' </span><span class="s1">+ </span><span class="s5">&quot;       L1O: sum of squared prediction errors for leave-one-out&quot;</span>
        <span class="s3">return </span><span class="s1">sumstr</span>


<span class="s3">if </span><span class="s1">__name__ == </span><span class="s5">'__main__'</span><span class="s1">:</span>

    <span class="s1">examples = [</span><span class="s4">1</span><span class="s1">]</span>
    <span class="s3">if </span><span class="s4">1 </span><span class="s3">in </span><span class="s1">examples:</span>
        <span class="s1">nobs = </span><span class="s4">500</span>
        <span class="s1">f0 = np.c_[np.random.normal(size=(nobs</span><span class="s3">,</span><span class="s4">2</span><span class="s1">))</span><span class="s3">, </span><span class="s1">np.ones((nobs</span><span class="s3">,</span><span class="s4">1</span><span class="s1">))]</span>
        <span class="s1">f2xcoef = np.c_[np.repeat(np.eye(</span><span class="s4">2</span><span class="s1">)</span><span class="s3">,</span><span class="s4">2</span><span class="s3">,</span><span class="s4">0</span><span class="s1">)</span><span class="s3">,</span><span class="s1">np.arange(</span><span class="s4">4</span><span class="s1">)[::-</span><span class="s4">1</span><span class="s1">]].T</span>
        <span class="s1">f2xcoef = np.array([[ </span><span class="s4">1.</span><span class="s3">,  </span><span class="s4">1.</span><span class="s3">,  </span><span class="s4">0.</span><span class="s3">,  </span><span class="s4">0.</span><span class="s1">]</span><span class="s3">,</span>
                            <span class="s1">[ </span><span class="s4">0.</span><span class="s3">,  </span><span class="s4">0.</span><span class="s3">,  </span><span class="s4">1.</span><span class="s3">,  </span><span class="s4">1.</span><span class="s1">]</span><span class="s3">,</span>
                            <span class="s1">[ </span><span class="s4">3.</span><span class="s3">,  </span><span class="s4">2.</span><span class="s3">,  </span><span class="s4">1.</span><span class="s3">,  </span><span class="s4">0.</span><span class="s1">]])</span>
        <span class="s1">f2xcoef = np.array([[ </span><span class="s4">0.1</span><span class="s3">,  </span><span class="s4">3.</span><span class="s3">,  </span><span class="s4">1.</span><span class="s3">,    </span><span class="s4">0.</span><span class="s1">]</span><span class="s3">,</span>
                            <span class="s1">[ </span><span class="s4">0.</span><span class="s3">,  </span><span class="s4">0.</span><span class="s3">,  </span><span class="s4">1.5</span><span class="s3">,   </span><span class="s4">0.1</span><span class="s1">]</span><span class="s3">,</span>
                            <span class="s1">[ </span><span class="s4">3.</span><span class="s3">,  </span><span class="s4">2.</span><span class="s3">,  </span><span class="s4">1.</span><span class="s3">,    </span><span class="s4">0.</span><span class="s1">]])</span>
        <span class="s1">x0 = np.dot(f0</span><span class="s3">, </span><span class="s1">f2xcoef)</span>
        <span class="s1">x0 += </span><span class="s4">0.1</span><span class="s1">*np.random.normal(size=x0.shape)</span>
        <span class="s1">ytrue = np.dot(f0</span><span class="s3">,</span><span class="s1">[</span><span class="s4">1.</span><span class="s3">, </span><span class="s4">1.</span><span class="s3">, </span><span class="s4">1.</span><span class="s1">])</span>
        <span class="s1">y0 = ytrue + </span><span class="s4">0.1</span><span class="s1">*np.random.normal(size=ytrue.shape)</span>

        <span class="s1">mod = FactorModelUnivariate(y0</span><span class="s3">, </span><span class="s1">x0)</span>
        <span class="s1">print(mod.summary_find_nfact())</span>
        <span class="s1">print(</span><span class="s5">&quot;with cross validation - slower&quot;</span><span class="s1">)</span>
        <span class="s1">mod.fit_find_nfact(maxfact=</span><span class="s3">None, </span><span class="s1">skip_crossval=</span><span class="s3">False, </span><span class="s1">cv_iter=</span><span class="s3">None</span><span class="s1">)</span>
        <span class="s1">print(mod.summary_find_nfact())</span>
</pre>
</body>
</html>