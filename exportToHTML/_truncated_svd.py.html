<html>
<head>
<title>_truncated_svd.py</title>
<meta http-equiv="Content-Type" content="text/html; charset=utf-8">
<style type="text/css">
.s0 { color: #629755; font-style: italic;}
.s1 { color: #a9b7c6;}
.s2 { color: #808080;}
.s3 { color: #cc7832;}
.s4 { color: #6a8759;}
.s5 { color: #6897bb;}
</style>
</head>
<body bgcolor="#2b2b2b">
<table CELLSPACING=0 CELLPADDING=5 COLS=1 WIDTH="100%" BGCOLOR="#606060" >
<tr><td><center>
<font face="Arial, Helvetica" color="#000000">
_truncated_svd.py</font>
</center></td></tr></table>
<pre><span class="s0">&quot;&quot;&quot;Truncated SVD for sparse matrices, aka latent semantic analysis (LSA). 
&quot;&quot;&quot;</span>

<span class="s2"># Author: Lars Buitinck</span>
<span class="s2">#         Olivier Grisel &lt;olivier.grisel@ensta.org&gt;</span>
<span class="s2">#         Michael Becker &lt;mike@beckerfuffle.com&gt;</span>
<span class="s2"># License: 3-clause BSD.</span>

<span class="s3">from </span><span class="s1">numbers </span><span class="s3">import </span><span class="s1">Integral</span><span class="s3">, </span><span class="s1">Real</span>

<span class="s3">import </span><span class="s1">numpy </span><span class="s3">as </span><span class="s1">np</span>
<span class="s3">import </span><span class="s1">scipy.sparse </span><span class="s3">as </span><span class="s1">sp</span>
<span class="s3">from </span><span class="s1">scipy.sparse.linalg </span><span class="s3">import </span><span class="s1">svds</span>

<span class="s3">from </span><span class="s1">..base </span><span class="s3">import </span><span class="s1">(</span>
    <span class="s1">BaseEstimator</span><span class="s3">,</span>
    <span class="s1">ClassNamePrefixFeaturesOutMixin</span><span class="s3">,</span>
    <span class="s1">TransformerMixin</span><span class="s3">,</span>
    <span class="s1">_fit_context</span><span class="s3">,</span>
<span class="s1">)</span>
<span class="s3">from </span><span class="s1">..utils </span><span class="s3">import </span><span class="s1">check_array</span><span class="s3">, </span><span class="s1">check_random_state</span>
<span class="s3">from </span><span class="s1">..utils._arpack </span><span class="s3">import </span><span class="s1">_init_arpack_v0</span>
<span class="s3">from </span><span class="s1">..utils._param_validation </span><span class="s3">import </span><span class="s1">Interval</span><span class="s3">, </span><span class="s1">StrOptions</span>
<span class="s3">from </span><span class="s1">..utils.extmath </span><span class="s3">import </span><span class="s1">randomized_svd</span><span class="s3">, </span><span class="s1">safe_sparse_dot</span><span class="s3">, </span><span class="s1">svd_flip</span>
<span class="s3">from </span><span class="s1">..utils.sparsefuncs </span><span class="s3">import </span><span class="s1">mean_variance_axis</span>
<span class="s3">from </span><span class="s1">..utils.validation </span><span class="s3">import </span><span class="s1">check_is_fitted</span>

<span class="s1">__all__ = [</span><span class="s4">&quot;TruncatedSVD&quot;</span><span class="s1">]</span>


<span class="s3">class </span><span class="s1">TruncatedSVD(ClassNamePrefixFeaturesOutMixin</span><span class="s3">, </span><span class="s1">TransformerMixin</span><span class="s3">, </span><span class="s1">BaseEstimator):</span>
    <span class="s0">&quot;&quot;&quot;Dimensionality reduction using truncated SVD (aka LSA). 
 
    This transformer performs linear dimensionality reduction by means of 
    truncated singular value decomposition (SVD). Contrary to PCA, this 
    estimator does not center the data before computing the singular value 
    decomposition. This means it can work with sparse matrices 
    efficiently. 
 
    In particular, truncated SVD works on term count/tf-idf matrices as 
    returned by the vectorizers in :mod:`sklearn.feature_extraction.text`. In 
    that context, it is known as latent semantic analysis (LSA). 
 
    This estimator supports two algorithms: a fast randomized SVD solver, and 
    a &quot;naive&quot; algorithm that uses ARPACK as an eigensolver on `X * X.T` or 
    `X.T * X`, whichever is more efficient. 
 
    Read more in the :ref:`User Guide &lt;LSA&gt;`. 
 
    Parameters 
    ---------- 
    n_components : int, default=2 
        Desired dimensionality of output data. 
        If algorithm='arpack', must be strictly less than the number of features. 
        If algorithm='randomized', must be less than or equal to the number of features. 
        The default value is useful for visualisation. For LSA, a value of 
        100 is recommended. 
 
    algorithm : {'arpack', 'randomized'}, default='randomized' 
        SVD solver to use. Either &quot;arpack&quot; for the ARPACK wrapper in SciPy 
        (scipy.sparse.linalg.svds), or &quot;randomized&quot; for the randomized 
        algorithm due to Halko (2009). 
 
    n_iter : int, default=5 
        Number of iterations for randomized SVD solver. Not used by ARPACK. The 
        default is larger than the default in 
        :func:`~sklearn.utils.extmath.randomized_svd` to handle sparse 
        matrices that may have large slowly decaying spectrum. 
 
    n_oversamples : int, default=10 
        Number of oversamples for randomized SVD solver. Not used by ARPACK. 
        See :func:`~sklearn.utils.extmath.randomized_svd` for a complete 
        description. 
 
        .. versionadded:: 1.1 
 
    power_iteration_normalizer : {'auto', 'QR', 'LU', 'none'}, default='auto' 
        Power iteration normalizer for randomized SVD solver. 
        Not used by ARPACK. See :func:`~sklearn.utils.extmath.randomized_svd` 
        for more details. 
 
        .. versionadded:: 1.1 
 
    random_state : int, RandomState instance or None, default=None 
        Used during randomized svd. Pass an int for reproducible results across 
        multiple function calls. 
        See :term:`Glossary &lt;random_state&gt;`. 
 
    tol : float, default=0.0 
        Tolerance for ARPACK. 0 means machine precision. Ignored by randomized 
        SVD solver. 
 
    Attributes 
    ---------- 
    components_ : ndarray of shape (n_components, n_features) 
        The right singular vectors of the input data. 
 
    explained_variance_ : ndarray of shape (n_components,) 
        The variance of the training samples transformed by a projection to 
        each component. 
 
    explained_variance_ratio_ : ndarray of shape (n_components,) 
        Percentage of variance explained by each of the selected components. 
 
    singular_values_ : ndarray of shape (n_components,) 
        The singular values corresponding to each of the selected components. 
        The singular values are equal to the 2-norms of the ``n_components`` 
        variables in the lower-dimensional space. 
 
    n_features_in_ : int 
        Number of features seen during :term:`fit`. 
 
        .. versionadded:: 0.24 
 
    feature_names_in_ : ndarray of shape (`n_features_in_`,) 
        Names of features seen during :term:`fit`. Defined only when `X` 
        has feature names that are all strings. 
 
        .. versionadded:: 1.0 
 
    See Also 
    -------- 
    DictionaryLearning : Find a dictionary that sparsely encodes data. 
    FactorAnalysis : A simple linear generative model with 
        Gaussian latent variables. 
    IncrementalPCA : Incremental principal components analysis. 
    KernelPCA : Kernel Principal component analysis. 
    NMF : Non-Negative Matrix Factorization. 
    PCA : Principal component analysis. 
 
    Notes 
    ----- 
    SVD suffers from a problem called &quot;sign indeterminacy&quot;, which means the 
    sign of the ``components_`` and the output from transform depend on the 
    algorithm and random state. To work around this, fit instances of this 
    class to data once, then keep the instance around to do transformations. 
 
    References 
    ---------- 
    :arxiv:`Halko, et al. (2009). &quot;Finding structure with randomness: 
    Stochastic algorithms for constructing approximate matrix decompositions&quot; 
    &lt;0909.4061&gt;` 
 
    Examples 
    -------- 
    &gt;&gt;&gt; from sklearn.decomposition import TruncatedSVD 
    &gt;&gt;&gt; from scipy.sparse import csr_matrix 
    &gt;&gt;&gt; import numpy as np 
    &gt;&gt;&gt; np.random.seed(0) 
    &gt;&gt;&gt; X_dense = np.random.rand(100, 100) 
    &gt;&gt;&gt; X_dense[:, 2 * np.arange(50)] = 0 
    &gt;&gt;&gt; X = csr_matrix(X_dense) 
    &gt;&gt;&gt; svd = TruncatedSVD(n_components=5, n_iter=7, random_state=42) 
    &gt;&gt;&gt; svd.fit(X) 
    TruncatedSVD(n_components=5, n_iter=7, random_state=42) 
    &gt;&gt;&gt; print(svd.explained_variance_ratio_) 
    [0.0157... 0.0512... 0.0499... 0.0479... 0.0453...] 
    &gt;&gt;&gt; print(svd.explained_variance_ratio_.sum()) 
    0.2102... 
    &gt;&gt;&gt; print(svd.singular_values_) 
    [35.2410...  4.5981...   4.5420...  4.4486...  4.3288...] 
    &quot;&quot;&quot;</span>

    <span class="s1">_parameter_constraints: dict = {</span>
        <span class="s4">&quot;n_components&quot;</span><span class="s1">: [Interval(Integral</span><span class="s3">, </span><span class="s5">1</span><span class="s3">, None, </span><span class="s1">closed=</span><span class="s4">&quot;left&quot;</span><span class="s1">)]</span><span class="s3">,</span>
        <span class="s4">&quot;algorithm&quot;</span><span class="s1">: [StrOptions({</span><span class="s4">&quot;arpack&quot;</span><span class="s3">, </span><span class="s4">&quot;randomized&quot;</span><span class="s1">})]</span><span class="s3">,</span>
        <span class="s4">&quot;n_iter&quot;</span><span class="s1">: [Interval(Integral</span><span class="s3">, </span><span class="s5">0</span><span class="s3">, None, </span><span class="s1">closed=</span><span class="s4">&quot;left&quot;</span><span class="s1">)]</span><span class="s3">,</span>
        <span class="s4">&quot;n_oversamples&quot;</span><span class="s1">: [Interval(Integral</span><span class="s3">, </span><span class="s5">1</span><span class="s3">, None, </span><span class="s1">closed=</span><span class="s4">&quot;left&quot;</span><span class="s1">)]</span><span class="s3">,</span>
        <span class="s4">&quot;power_iteration_normalizer&quot;</span><span class="s1">: [StrOptions({</span><span class="s4">&quot;auto&quot;</span><span class="s3">, </span><span class="s4">&quot;OR&quot;</span><span class="s3">, </span><span class="s4">&quot;LU&quot;</span><span class="s3">, </span><span class="s4">&quot;none&quot;</span><span class="s1">})]</span><span class="s3">,</span>
        <span class="s4">&quot;random_state&quot;</span><span class="s1">: [</span><span class="s4">&quot;random_state&quot;</span><span class="s1">]</span><span class="s3">,</span>
        <span class="s4">&quot;tol&quot;</span><span class="s1">: [Interval(Real</span><span class="s3">, </span><span class="s5">0</span><span class="s3">, None, </span><span class="s1">closed=</span><span class="s4">&quot;left&quot;</span><span class="s1">)]</span><span class="s3">,</span>
    <span class="s1">}</span>

    <span class="s3">def </span><span class="s1">__init__(</span>
        <span class="s1">self</span><span class="s3">,</span>
        <span class="s1">n_components=</span><span class="s5">2</span><span class="s3">,</span>
        <span class="s1">*</span><span class="s3">,</span>
        <span class="s1">algorithm=</span><span class="s4">&quot;randomized&quot;</span><span class="s3">,</span>
        <span class="s1">n_iter=</span><span class="s5">5</span><span class="s3">,</span>
        <span class="s1">n_oversamples=</span><span class="s5">10</span><span class="s3">,</span>
        <span class="s1">power_iteration_normalizer=</span><span class="s4">&quot;auto&quot;</span><span class="s3">,</span>
        <span class="s1">random_state=</span><span class="s3">None,</span>
        <span class="s1">tol=</span><span class="s5">0.0</span><span class="s3">,</span>
    <span class="s1">):</span>
        <span class="s1">self.algorithm = algorithm</span>
        <span class="s1">self.n_components = n_components</span>
        <span class="s1">self.n_iter = n_iter</span>
        <span class="s1">self.n_oversamples = n_oversamples</span>
        <span class="s1">self.power_iteration_normalizer = power_iteration_normalizer</span>
        <span class="s1">self.random_state = random_state</span>
        <span class="s1">self.tol = tol</span>

    <span class="s3">def </span><span class="s1">fit(self</span><span class="s3">, </span><span class="s1">X</span><span class="s3">, </span><span class="s1">y=</span><span class="s3">None</span><span class="s1">):</span>
        <span class="s0">&quot;&quot;&quot;Fit model on training data X. 
 
        Parameters 
        ---------- 
        X : {array-like, sparse matrix} of shape (n_samples, n_features) 
            Training data. 
 
        y : Ignored 
            Not used, present here for API consistency by convention. 
 
        Returns 
        ------- 
        self : object 
            Returns the transformer object. 
        &quot;&quot;&quot;</span>
        <span class="s1">self.fit_transform(X)</span>
        <span class="s3">return </span><span class="s1">self</span>

    <span class="s1">@_fit_context(prefer_skip_nested_validation=</span><span class="s3">True</span><span class="s1">)</span>
    <span class="s3">def </span><span class="s1">fit_transform(self</span><span class="s3">, </span><span class="s1">X</span><span class="s3">, </span><span class="s1">y=</span><span class="s3">None</span><span class="s1">):</span>
        <span class="s0">&quot;&quot;&quot;Fit model to X and perform dimensionality reduction on X. 
 
        Parameters 
        ---------- 
        X : {array-like, sparse matrix} of shape (n_samples, n_features) 
            Training data. 
 
        y : Ignored 
            Not used, present here for API consistency by convention. 
 
        Returns 
        ------- 
        X_new : ndarray of shape (n_samples, n_components) 
            Reduced version of X. This will always be a dense array. 
        &quot;&quot;&quot;</span>
        <span class="s1">X = self._validate_data(X</span><span class="s3">, </span><span class="s1">accept_sparse=[</span><span class="s4">&quot;csr&quot;</span><span class="s3">, </span><span class="s4">&quot;csc&quot;</span><span class="s1">]</span><span class="s3">, </span><span class="s1">ensure_min_features=</span><span class="s5">2</span><span class="s1">)</span>
        <span class="s1">random_state = check_random_state(self.random_state)</span>

        <span class="s3">if </span><span class="s1">self.algorithm == </span><span class="s4">&quot;arpack&quot;</span><span class="s1">:</span>
            <span class="s1">v0 = _init_arpack_v0(min(X.shape)</span><span class="s3">, </span><span class="s1">random_state)</span>
            <span class="s1">U</span><span class="s3">, </span><span class="s1">Sigma</span><span class="s3">, </span><span class="s1">VT = svds(X</span><span class="s3">, </span><span class="s1">k=self.n_components</span><span class="s3">, </span><span class="s1">tol=self.tol</span><span class="s3">, </span><span class="s1">v0=v0)</span>
            <span class="s2"># svds doesn't abide by scipy.linalg.svd/randomized_svd</span>
            <span class="s2"># conventions, so reverse its outputs.</span>
            <span class="s1">Sigma = Sigma[::-</span><span class="s5">1</span><span class="s1">]</span>
            <span class="s1">U</span><span class="s3">, </span><span class="s1">VT = svd_flip(U[:</span><span class="s3">, </span><span class="s1">::-</span><span class="s5">1</span><span class="s1">]</span><span class="s3">, </span><span class="s1">VT[::-</span><span class="s5">1</span><span class="s1">])</span>

        <span class="s3">elif </span><span class="s1">self.algorithm == </span><span class="s4">&quot;randomized&quot;</span><span class="s1">:</span>
            <span class="s3">if </span><span class="s1">self.n_components &gt; X.shape[</span><span class="s5">1</span><span class="s1">]:</span>
                <span class="s3">raise </span><span class="s1">ValueError(</span>
                    <span class="s4">f&quot;n_components(</span><span class="s3">{</span><span class="s1">self.n_components</span><span class="s3">}</span><span class="s4">) must be &lt;=&quot;</span>
                    <span class="s4">f&quot; n_features(</span><span class="s3">{</span><span class="s1">X.shape[</span><span class="s5">1</span><span class="s1">]</span><span class="s3">}</span><span class="s4">).&quot;</span>
                <span class="s1">)</span>
            <span class="s1">U</span><span class="s3">, </span><span class="s1">Sigma</span><span class="s3">, </span><span class="s1">VT = randomized_svd(</span>
                <span class="s1">X</span><span class="s3">,</span>
                <span class="s1">self.n_components</span><span class="s3">,</span>
                <span class="s1">n_iter=self.n_iter</span><span class="s3">,</span>
                <span class="s1">n_oversamples=self.n_oversamples</span><span class="s3">,</span>
                <span class="s1">power_iteration_normalizer=self.power_iteration_normalizer</span><span class="s3">,</span>
                <span class="s1">random_state=random_state</span><span class="s3">,</span>
            <span class="s1">)</span>

        <span class="s1">self.components_ = VT</span>

        <span class="s2"># As a result of the SVD approximation error on X ~ U @ Sigma @ V.T,</span>
        <span class="s2"># X @ V is not the same as U @ Sigma</span>
        <span class="s3">if </span><span class="s1">self.algorithm == </span><span class="s4">&quot;randomized&quot; </span><span class="s3">or </span><span class="s1">(</span>
            <span class="s1">self.algorithm == </span><span class="s4">&quot;arpack&quot; </span><span class="s3">and </span><span class="s1">self.tol &gt; </span><span class="s5">0</span>
        <span class="s1">):</span>
            <span class="s1">X_transformed = safe_sparse_dot(X</span><span class="s3">, </span><span class="s1">self.components_.T)</span>
        <span class="s3">else</span><span class="s1">:</span>
            <span class="s1">X_transformed = U * Sigma</span>

        <span class="s2"># Calculate explained variance &amp; explained variance ratio</span>
        <span class="s1">self.explained_variance_ = exp_var = np.var(X_transformed</span><span class="s3">, </span><span class="s1">axis=</span><span class="s5">0</span><span class="s1">)</span>
        <span class="s3">if </span><span class="s1">sp.issparse(X):</span>
            <span class="s1">_</span><span class="s3">, </span><span class="s1">full_var = mean_variance_axis(X</span><span class="s3">, </span><span class="s1">axis=</span><span class="s5">0</span><span class="s1">)</span>
            <span class="s1">full_var = full_var.sum()</span>
        <span class="s3">else</span><span class="s1">:</span>
            <span class="s1">full_var = np.var(X</span><span class="s3">, </span><span class="s1">axis=</span><span class="s5">0</span><span class="s1">).sum()</span>
        <span class="s1">self.explained_variance_ratio_ = exp_var / full_var</span>
        <span class="s1">self.singular_values_ = Sigma  </span><span class="s2"># Store the singular values.</span>

        <span class="s3">return </span><span class="s1">X_transformed</span>

    <span class="s3">def </span><span class="s1">transform(self</span><span class="s3">, </span><span class="s1">X):</span>
        <span class="s0">&quot;&quot;&quot;Perform dimensionality reduction on X. 
 
        Parameters 
        ---------- 
        X : {array-like, sparse matrix} of shape (n_samples, n_features) 
            New data. 
 
        Returns 
        ------- 
        X_new : ndarray of shape (n_samples, n_components) 
            Reduced version of X. This will always be a dense array. 
        &quot;&quot;&quot;</span>
        <span class="s1">check_is_fitted(self)</span>
        <span class="s1">X = self._validate_data(X</span><span class="s3">, </span><span class="s1">accept_sparse=[</span><span class="s4">&quot;csr&quot;</span><span class="s3">, </span><span class="s4">&quot;csc&quot;</span><span class="s1">]</span><span class="s3">, </span><span class="s1">reset=</span><span class="s3">False</span><span class="s1">)</span>
        <span class="s3">return </span><span class="s1">safe_sparse_dot(X</span><span class="s3">, </span><span class="s1">self.components_.T)</span>

    <span class="s3">def </span><span class="s1">inverse_transform(self</span><span class="s3">, </span><span class="s1">X):</span>
        <span class="s0">&quot;&quot;&quot;Transform X back to its original space. 
 
        Returns an array X_original whose transform would be X. 
 
        Parameters 
        ---------- 
        X : array-like of shape (n_samples, n_components) 
            New data. 
 
        Returns 
        ------- 
        X_original : ndarray of shape (n_samples, n_features) 
            Note that this is always a dense array. 
        &quot;&quot;&quot;</span>
        <span class="s1">X = check_array(X)</span>
        <span class="s3">return </span><span class="s1">np.dot(X</span><span class="s3">, </span><span class="s1">self.components_)</span>

    <span class="s3">def </span><span class="s1">_more_tags(self):</span>
        <span class="s3">return </span><span class="s1">{</span><span class="s4">&quot;preserves_dtype&quot;</span><span class="s1">: [np.float64</span><span class="s3">, </span><span class="s1">np.float32]}</span>

    <span class="s1">@property</span>
    <span class="s3">def </span><span class="s1">_n_features_out(self):</span>
        <span class="s0">&quot;&quot;&quot;Number of transformed output features.&quot;&quot;&quot;</span>
        <span class="s3">return </span><span class="s1">self.components_.shape[</span><span class="s5">0</span><span class="s1">]</span>
</pre>
</body>
</html>