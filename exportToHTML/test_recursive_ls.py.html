<html>
<head>
<title>test_recursive_ls.py</title>
<meta http-equiv="Content-Type" content="text/html; charset=utf-8">
<style type="text/css">
.s0 { color: #629755; font-style: italic;}
.s1 { color: #a9b7c6;}
.s2 { color: #cc7832;}
.s3 { color: #6a8759;}
.s4 { color: #808080;}
.s5 { color: #6897bb;}
</style>
</head>
<body bgcolor="#2b2b2b">
<table CELLSPACING=0 CELLPADDING=5 COLS=1 WIDTH="100%" BGCOLOR="#606060" >
<tr><td><center>
<font face="Arial, Helvetica" color="#000000">
test_recursive_ls.py</font>
</center></td></tr></table>
<pre><span class="s0">&quot;&quot;&quot; 
Tests for recursive least squares models 
 
Author: Chad Fulton 
License: Simplified-BSD 
&quot;&quot;&quot;</span>
<span class="s2">import </span><span class="s1">os</span>

<span class="s2">import </span><span class="s1">numpy </span><span class="s2">as </span><span class="s1">np</span>
<span class="s2">from </span><span class="s1">numpy.testing </span><span class="s2">import </span><span class="s1">assert_allclose</span><span class="s2">, </span><span class="s1">assert_equal</span><span class="s2">, </span><span class="s1">assert_raises</span>
<span class="s2">import </span><span class="s1">pandas </span><span class="s2">as </span><span class="s1">pd</span>
<span class="s2">import </span><span class="s1">pytest</span>
<span class="s2">from </span><span class="s1">scipy.stats </span><span class="s2">import </span><span class="s1">norm</span>

<span class="s2">from </span><span class="s1">statsmodels.datasets </span><span class="s2">import </span><span class="s1">macrodata</span>
<span class="s2">from </span><span class="s1">statsmodels.genmod.api </span><span class="s2">import </span><span class="s1">GLM</span>
<span class="s2">from </span><span class="s1">statsmodels.regression.linear_model </span><span class="s2">import </span><span class="s1">OLS</span>
<span class="s2">from </span><span class="s1">statsmodels.regression.recursive_ls </span><span class="s2">import </span><span class="s1">RecursiveLS</span>
<span class="s2">from </span><span class="s1">statsmodels.stats.diagnostic </span><span class="s2">import </span><span class="s1">recursive_olsresiduals</span>
<span class="s2">from </span><span class="s1">statsmodels.tools </span><span class="s2">import </span><span class="s1">add_constant</span>
<span class="s2">from </span><span class="s1">statsmodels.tools.eval_measures </span><span class="s2">import </span><span class="s1">aic</span><span class="s2">, </span><span class="s1">bic</span>
<span class="s2">from </span><span class="s1">statsmodels.tools.sm_exceptions </span><span class="s2">import </span><span class="s1">ValueWarning</span>

<span class="s1">current_path = os.path.dirname(os.path.abspath(__file__))</span>

<span class="s1">results_R_path = </span><span class="s3">'results' </span><span class="s1">+ os.sep + </span><span class="s3">'results_rls_R.csv'</span>
<span class="s1">results_R = pd.read_csv(current_path + os.sep + results_R_path)</span>

<span class="s1">results_stata_path = </span><span class="s3">'results' </span><span class="s1">+ os.sep + </span><span class="s3">'results_rls_stata.csv'</span>
<span class="s1">results_stata = pd.read_csv(current_path + os.sep + results_stata_path)</span>

<span class="s1">dta = macrodata.load_pandas().data</span>
<span class="s1">dta.index = pd.date_range(start=</span><span class="s3">'1959-01-01'</span><span class="s2">, </span><span class="s1">end=</span><span class="s3">'2009-07-01'</span><span class="s2">, </span><span class="s1">freq=</span><span class="s3">'QS'</span><span class="s1">)</span>

<span class="s1">endog = dta[</span><span class="s3">'cpi'</span><span class="s1">]</span>
<span class="s1">exog = add_constant(dta[</span><span class="s3">'m1'</span><span class="s1">])</span>


<span class="s2">def </span><span class="s1">test_endog():</span>
    <span class="s4"># Tests for numpy input</span>
    <span class="s1">mod = RecursiveLS(endog.values</span><span class="s2">, </span><span class="s1">exog.values)</span>
    <span class="s1">res = mod.fit()</span>

    <span class="s4"># Test the RLS estimates against OLS estimates</span>
    <span class="s1">mod_ols = OLS(endog</span><span class="s2">, </span><span class="s1">exog)</span>
    <span class="s1">res_ols = mod_ols.fit()</span>
    <span class="s1">assert_allclose(res.params</span><span class="s2">, </span><span class="s1">res_ols.params)</span>

    <span class="s4"># Tests for 1-dim exog</span>
    <span class="s1">mod = RecursiveLS(endog</span><span class="s2">, </span><span class="s1">dta[</span><span class="s3">'m1'</span><span class="s1">].values)</span>
    <span class="s1">res = mod.fit()</span>

    <span class="s4"># Test the RLS estimates against OLS estimates</span>
    <span class="s1">mod_ols = OLS(endog</span><span class="s2">, </span><span class="s1">dta[</span><span class="s3">'m1'</span><span class="s1">])</span>
    <span class="s1">res_ols = mod_ols.fit()</span>
    <span class="s1">assert_allclose(res.params</span><span class="s2">, </span><span class="s1">res_ols.params)</span>

<span class="s2">def </span><span class="s1">test_ols():</span>
    <span class="s4"># More comprehensive tests against OLS estimates</span>
    <span class="s1">mod = RecursiveLS(endog</span><span class="s2">, </span><span class="s1">dta[</span><span class="s3">'m1'</span><span class="s1">])</span>
    <span class="s1">res = mod.fit()</span>

    <span class="s1">mod_ols = OLS(endog</span><span class="s2">, </span><span class="s1">dta[</span><span class="s3">'m1'</span><span class="s1">])</span>
    <span class="s1">res_ols = mod_ols.fit()</span>

    <span class="s4"># Regression coefficients, standard errors, and estimated scale</span>
    <span class="s1">assert_allclose(res.params</span><span class="s2">, </span><span class="s1">res_ols.params)</span>
    <span class="s1">assert_allclose(res.bse</span><span class="s2">, </span><span class="s1">res_ols.bse)</span>
    <span class="s4"># Note: scale here is computed according to Harvey, 1989, 4.2.5, and is</span>
    <span class="s4"># the called the ML estimator and sometimes (e.g. later in section 5)</span>
    <span class="s4"># denoted \tilde \sigma_*^2</span>
    <span class="s1">assert_allclose(res.filter_results.obs_cov[</span><span class="s5">0</span><span class="s2">, </span><span class="s5">0</span><span class="s1">]</span><span class="s2">, </span><span class="s1">res_ols.scale)</span>

    <span class="s4"># OLS residuals are equivalent to smoothed forecast errors</span>
    <span class="s4"># (the latter are defined as e_t|T by Harvey, 1989, 5.4.5)</span>
    <span class="s4"># (this follows since the smoothed state simply contains the</span>
    <span class="s4"># full-information estimates of the regression coefficients)</span>
    <span class="s1">actual = (mod.endog[:</span><span class="s2">, </span><span class="s5">0</span><span class="s1">] -</span>
              <span class="s1">np.sum(mod[</span><span class="s3">'design'</span><span class="s2">, </span><span class="s5">0</span><span class="s2">, </span><span class="s1">:</span><span class="s2">, </span><span class="s1">:] * res.smoothed_state</span><span class="s2">, </span><span class="s1">axis=</span><span class="s5">0</span><span class="s1">))</span>
    <span class="s1">assert_allclose(actual</span><span class="s2">, </span><span class="s1">res_ols.resid)</span>

    <span class="s4"># Given the estimate of scale as `sum(v_t^2 / f_t) / (T - d)` (see</span>
    <span class="s4"># Harvey, 1989, 4.2.5 on p. 183), then llf_recursive is equivalent to the</span>
    <span class="s4"># full OLS loglikelihood (i.e. without the scale concentrated out).</span>
    <span class="s1">desired = mod_ols.loglike(res_ols.params</span><span class="s2">, </span><span class="s1">scale=res_ols.scale)</span>
    <span class="s1">assert_allclose(res.llf_recursive</span><span class="s2">, </span><span class="s1">desired)</span>
    <span class="s4"># Alternatively, we can constrcut the concentrated OLS loglikelihood</span>
    <span class="s4"># by computing the scale term with `nobs` in the denominator rather than</span>
    <span class="s4"># `nobs - d`.</span>
    <span class="s1">scale_alternative = np.sum((</span>
        <span class="s1">res.standardized_forecasts_error[</span><span class="s5">0</span><span class="s2">, </span><span class="s5">1</span><span class="s1">:] *</span>
        <span class="s1">res.filter_results.obs_cov[</span><span class="s5">0</span><span class="s2">, </span><span class="s5">0</span><span class="s1">]**</span><span class="s5">0.5</span><span class="s1">)**</span><span class="s5">2</span><span class="s1">) / mod.nobs</span>
    <span class="s1">llf_alternative = np.log(norm.pdf(res.resid_recursive</span><span class="s2">, </span><span class="s1">loc=</span><span class="s5">0</span><span class="s2">,</span>
                                      <span class="s1">scale=scale_alternative**</span><span class="s5">0.5</span><span class="s1">)).sum()</span>
    <span class="s1">assert_allclose(llf_alternative</span><span class="s2">, </span><span class="s1">res_ols.llf)</span>

    <span class="s4"># Prediction</span>
    <span class="s1">actual = res.forecast(</span><span class="s5">10</span><span class="s2">, </span><span class="s1">design=np.ones((</span><span class="s5">1</span><span class="s2">, </span><span class="s5">1</span><span class="s2">, </span><span class="s5">10</span><span class="s1">)))</span>
    <span class="s1">assert_allclose(actual</span><span class="s2">, </span><span class="s1">res_ols.predict(np.ones((</span><span class="s5">10</span><span class="s2">, </span><span class="s5">1</span><span class="s1">))))</span>

    <span class="s4"># Sums of squares, R^2</span>
    <span class="s1">assert_allclose(res.ess</span><span class="s2">, </span><span class="s1">res_ols.ess)</span>
    <span class="s1">assert_allclose(res.ssr</span><span class="s2">, </span><span class="s1">res_ols.ssr)</span>
    <span class="s1">assert_allclose(res.centered_tss</span><span class="s2">, </span><span class="s1">res_ols.centered_tss)</span>
    <span class="s1">assert_allclose(res.uncentered_tss</span><span class="s2">, </span><span class="s1">res_ols.uncentered_tss)</span>
    <span class="s1">assert_allclose(res.rsquared</span><span class="s2">, </span><span class="s1">res_ols.rsquared)</span>

    <span class="s4"># Mean squares</span>
    <span class="s1">assert_allclose(res.mse_model</span><span class="s2">, </span><span class="s1">res_ols.mse_model)</span>
    <span class="s1">assert_allclose(res.mse_resid</span><span class="s2">, </span><span class="s1">res_ols.mse_resid)</span>
    <span class="s1">assert_allclose(res.mse_total</span><span class="s2">, </span><span class="s1">res_ols.mse_total)</span>

    <span class="s4"># Hypothesis tests</span>
    <span class="s1">actual = res.t_test(</span><span class="s3">'m1 = 0'</span><span class="s1">)</span>
    <span class="s1">desired = res_ols.t_test(</span><span class="s3">'m1 = 0'</span><span class="s1">)</span>
    <span class="s1">assert_allclose(actual.statistic</span><span class="s2">, </span><span class="s1">desired.statistic)</span>
    <span class="s1">assert_allclose(actual.pvalue</span><span class="s2">, </span><span class="s1">desired.pvalue</span><span class="s2">, </span><span class="s1">atol=</span><span class="s5">1e-15</span><span class="s1">)</span>

    <span class="s1">actual = res.f_test(</span><span class="s3">'m1 = 0'</span><span class="s1">)</span>
    <span class="s1">desired = res_ols.f_test(</span><span class="s3">'m1 = 0'</span><span class="s1">)</span>
    <span class="s1">assert_allclose(actual.statistic</span><span class="s2">, </span><span class="s1">desired.statistic)</span>
    <span class="s1">assert_allclose(actual.pvalue</span><span class="s2">, </span><span class="s1">desired.pvalue</span><span class="s2">, </span><span class="s1">atol=</span><span class="s5">1e-15</span><span class="s1">)</span>

    <span class="s4"># Information criteria</span>
    <span class="s4"># Note: the llf and llf_obs given in the results are based on the Kalman</span>
    <span class="s4"># filter and so the ic given in results will not be identical to the</span>
    <span class="s4"># OLS versions. Additionally, llf_recursive is comparable to the</span>
    <span class="s4"># non-concentrated llf, and not the concentrated llf that is by default</span>
    <span class="s4"># used in OLS. Compute new ic based on llf_alternative to compare.</span>
    <span class="s1">actual_aic = aic(llf_alternative</span><span class="s2">, </span><span class="s1">res.nobs_effective</span><span class="s2">, </span><span class="s1">res.df_model)</span>
    <span class="s1">assert_allclose(actual_aic</span><span class="s2">, </span><span class="s1">res_ols.aic)</span>
    <span class="s1">actual_bic = bic(llf_alternative</span><span class="s2">, </span><span class="s1">res.nobs_effective</span><span class="s2">, </span><span class="s1">res.df_model)</span>
    <span class="s1">assert_allclose(actual_bic</span><span class="s2">, </span><span class="s1">res_ols.bic)</span>


<span class="s2">def </span><span class="s1">test_glm(constraints=</span><span class="s2">None</span><span class="s1">):</span>
    <span class="s4"># More comprehensive tests against GLM estimates (this is sort of redundant</span>
    <span class="s4"># given `test_ols`, but this is mostly to complement the tests in</span>
    <span class="s4"># `test_glm_constrained`)</span>
    <span class="s1">endog = dta.infl</span>
    <span class="s1">exog = add_constant(dta[[</span><span class="s3">'unemp'</span><span class="s2">, </span><span class="s3">'m1'</span><span class="s1">]])</span>

    <span class="s1">mod = RecursiveLS(endog</span><span class="s2">, </span><span class="s1">exog</span><span class="s2">, </span><span class="s1">constraints=constraints)</span>
    <span class="s1">res = mod.fit()</span>

    <span class="s1">mod_glm = GLM(endog</span><span class="s2">, </span><span class="s1">exog)</span>
    <span class="s2">if </span><span class="s1">constraints </span><span class="s2">is None</span><span class="s1">:</span>
        <span class="s1">res_glm = mod_glm.fit()</span>
    <span class="s2">else</span><span class="s1">:</span>
        <span class="s1">res_glm = mod_glm.fit_constrained(constraints=constraints)</span>

    <span class="s4"># Regression coefficients, standard errors, and estimated scale</span>
    <span class="s1">assert_allclose(res.params</span><span class="s2">, </span><span class="s1">res_glm.params)</span>
    <span class="s1">assert_allclose(res.bse</span><span class="s2">, </span><span class="s1">res_glm.bse</span><span class="s2">, </span><span class="s1">atol=</span><span class="s5">1e-6</span><span class="s1">)</span>
    <span class="s4"># Note: scale here is computed according to Harvey, 1989, 4.2.5, and is</span>
    <span class="s4"># the called the ML estimator and sometimes (e.g. later in section 5)</span>
    <span class="s4"># denoted \tilde \sigma_*^2</span>
    <span class="s1">assert_allclose(res.filter_results.obs_cov[</span><span class="s5">0</span><span class="s2">, </span><span class="s5">0</span><span class="s1">]</span><span class="s2">, </span><span class="s1">res_glm.scale)</span>

    <span class="s4"># DoF</span>
    <span class="s4"># Note: GLM does not include intercept in DoF, so modify by -1</span>
    <span class="s1">assert_equal(res.df_model - </span><span class="s5">1</span><span class="s2">, </span><span class="s1">res_glm.df_model)</span>

    <span class="s4"># OLS residuals are equivalent to smoothed forecast errors</span>
    <span class="s4"># (the latter are defined as e_t|T by Harvey, 1989, 5.4.5)</span>
    <span class="s4"># (this follows since the smoothed state simply contains the</span>
    <span class="s4"># full-information estimates of the regression coefficients)</span>
    <span class="s1">actual = (mod.endog[:</span><span class="s2">, </span><span class="s5">0</span><span class="s1">] -</span>
              <span class="s1">np.sum(mod[</span><span class="s3">'design'</span><span class="s2">, </span><span class="s5">0</span><span class="s2">, </span><span class="s1">:</span><span class="s2">, </span><span class="s1">:] * res.smoothed_state</span><span class="s2">, </span><span class="s1">axis=</span><span class="s5">0</span><span class="s1">))</span>
    <span class="s1">assert_allclose(actual</span><span class="s2">, </span><span class="s1">res_glm.resid_response</span><span class="s2">, </span><span class="s1">atol=</span><span class="s5">1e-7</span><span class="s1">)</span>

    <span class="s4"># Given the estimate of scale as `sum(v_t^2 / f_t) / (T - d)` (see</span>
    <span class="s4"># Harvey, 1989, 4.2.5 on p. 183), then llf_recursive is equivalent to the</span>
    <span class="s4"># full OLS loglikelihood (i.e. without the scale concentrated out).</span>
    <span class="s1">desired = mod_glm.loglike(res_glm.params</span><span class="s2">, </span><span class="s1">scale=res_glm.scale)</span>
    <span class="s1">assert_allclose(res.llf_recursive</span><span class="s2">, </span><span class="s1">desired)</span>
    <span class="s4"># Alternatively, we can construct the concentrated OLS loglikelihood</span>
    <span class="s4"># by computing the scale term with `nobs` in the denominator rather than</span>
    <span class="s4"># `nobs - d`.</span>
    <span class="s1">scale_alternative = np.sum((</span>
        <span class="s1">res.standardized_forecasts_error[</span><span class="s5">0</span><span class="s2">, </span><span class="s5">1</span><span class="s1">:] *</span>
        <span class="s1">res.filter_results.obs_cov[</span><span class="s5">0</span><span class="s2">, </span><span class="s5">0</span><span class="s1">]**</span><span class="s5">0.5</span><span class="s1">)**</span><span class="s5">2</span><span class="s1">) / mod.nobs</span>
    <span class="s1">llf_alternative = np.log(norm.pdf(res.resid_recursive</span><span class="s2">, </span><span class="s1">loc=</span><span class="s5">0</span><span class="s2">,</span>
                                      <span class="s1">scale=scale_alternative**</span><span class="s5">0.5</span><span class="s1">)).sum()</span>
    <span class="s1">assert_allclose(llf_alternative</span><span class="s2">, </span><span class="s1">res_glm.llf)</span>

    <span class="s4"># Prediction</span>
    <span class="s4"># TODO: prediction in this case is not working.</span>
    <span class="s2">if </span><span class="s1">constraints </span><span class="s2">is None</span><span class="s1">:</span>
        <span class="s1">design = np.ones((</span><span class="s5">1</span><span class="s2">, </span><span class="s5">3</span><span class="s2">, </span><span class="s5">10</span><span class="s1">))</span>
        <span class="s1">actual = res.forecast(</span><span class="s5">10</span><span class="s2">, </span><span class="s1">design=design)</span>
        <span class="s1">assert_allclose(actual</span><span class="s2">, </span><span class="s1">res_glm.predict(np.ones((</span><span class="s5">10</span><span class="s2">, </span><span class="s5">3</span><span class="s1">))))</span>
    <span class="s2">else</span><span class="s1">:</span>
        <span class="s1">design = np.ones((</span><span class="s5">2</span><span class="s2">, </span><span class="s5">3</span><span class="s2">, </span><span class="s5">10</span><span class="s1">))</span>
        <span class="s1">assert_raises(NotImplementedError</span><span class="s2">, </span><span class="s1">res.forecast</span><span class="s2">, </span><span class="s5">10</span><span class="s2">, </span><span class="s1">design=design)</span>

    <span class="s4"># Hypothesis tests</span>
    <span class="s1">actual = res.t_test(</span><span class="s3">'m1 = 0'</span><span class="s1">)</span>
    <span class="s1">desired = res_glm.t_test(</span><span class="s3">'m1 = 0'</span><span class="s1">)</span>
    <span class="s1">assert_allclose(actual.statistic</span><span class="s2">, </span><span class="s1">desired.statistic)</span>
    <span class="s1">assert_allclose(actual.pvalue</span><span class="s2">, </span><span class="s1">desired.pvalue</span><span class="s2">, </span><span class="s1">atol=</span><span class="s5">1e-15</span><span class="s1">)</span>

    <span class="s1">actual = res.f_test(</span><span class="s3">'m1 = 0'</span><span class="s1">)</span>
    <span class="s1">desired = res_glm.f_test(</span><span class="s3">'m1 = 0'</span><span class="s1">)</span>
    <span class="s1">assert_allclose(actual.statistic</span><span class="s2">, </span><span class="s1">desired.statistic)</span>
    <span class="s1">assert_allclose(actual.pvalue</span><span class="s2">, </span><span class="s1">desired.pvalue)</span>

    <span class="s4"># Information criteria</span>
    <span class="s4"># Note: the llf and llf_obs given in the results are based on the Kalman</span>
    <span class="s4"># filter and so the ic given in results will not be identical to the</span>
    <span class="s4"># OLS versions. Additionally, llf_recursive is comparable to the</span>
    <span class="s4"># non-concentrated llf, and not the concentrated llf that is by default</span>
    <span class="s4"># used in OLS. Compute new ic based on llf_alternative to compare.</span>
    <span class="s1">actual_aic = aic(llf_alternative</span><span class="s2">, </span><span class="s1">res.nobs_effective</span><span class="s2">, </span><span class="s1">res.df_model)</span>
    <span class="s1">assert_allclose(actual_aic</span><span class="s2">, </span><span class="s1">res_glm.aic)</span>
    <span class="s4"># See gh#1733 for details on why the BIC does not match while AIC does</span>
    <span class="s4"># actual_bic = bic(llf_alternative, res.nobs_effective, res.df_model)</span>
    <span class="s4"># assert_allclose(actual_bic, res_glm.bic)</span>

<span class="s2">def </span><span class="s1">test_glm_constrained():</span>
    <span class="s1">test_glm(constraints=</span><span class="s3">'m1 + unemp = 1'</span><span class="s1">)</span>


<span class="s2">def </span><span class="s1">test_filter():</span>
    <span class="s4"># Basic test for filtering</span>
    <span class="s1">mod = RecursiveLS(endog</span><span class="s2">, </span><span class="s1">exog)</span>
    <span class="s1">res = mod.filter()</span>

    <span class="s4"># Test the RLS estimates against OLS estimates</span>
    <span class="s1">mod_ols = OLS(endog</span><span class="s2">, </span><span class="s1">exog)</span>
    <span class="s1">res_ols = mod_ols.fit()</span>
    <span class="s1">assert_allclose(res.params</span><span class="s2">, </span><span class="s1">res_ols.params)</span>


<span class="s2">def </span><span class="s1">test_estimates():</span>
    <span class="s1">mod = RecursiveLS(endog</span><span class="s2">, </span><span class="s1">exog)</span>
    <span class="s1">res = mod.fit()</span>

    <span class="s4"># Test for start_params</span>
    <span class="s1">assert_equal(mod.start_params</span><span class="s2">, </span><span class="s5">0</span><span class="s1">)</span>


    <span class="s4"># Test the RLS coefficient estimates against those from R (quantreg)</span>
    <span class="s4"># Due to initialization issues, we get more agreement as we get</span>
    <span class="s4"># farther from the initial values.</span>
    <span class="s1">assert_allclose(res.recursive_coefficients.filtered[:</span><span class="s2">, </span><span class="s5">2</span><span class="s1">:</span><span class="s5">10</span><span class="s1">].T</span><span class="s2">,</span>
                    <span class="s1">results_R.iloc[:</span><span class="s5">8</span><span class="s1">][[</span><span class="s3">'beta1'</span><span class="s2">, </span><span class="s3">'beta2'</span><span class="s1">]]</span><span class="s2">, </span><span class="s1">rtol=</span><span class="s5">1e-5</span><span class="s1">)</span>
    <span class="s1">assert_allclose(res.recursive_coefficients.filtered[:</span><span class="s2">, </span><span class="s5">9</span><span class="s1">:</span><span class="s5">20</span><span class="s1">].T</span><span class="s2">,</span>
                    <span class="s1">results_R.iloc[</span><span class="s5">7</span><span class="s1">:</span><span class="s5">18</span><span class="s1">][[</span><span class="s3">'beta1'</span><span class="s2">, </span><span class="s3">'beta2'</span><span class="s1">]])</span>
    <span class="s1">assert_allclose(res.recursive_coefficients.filtered[:</span><span class="s2">, </span><span class="s5">19</span><span class="s1">:].T</span><span class="s2">,</span>
                    <span class="s1">results_R.iloc[</span><span class="s5">17</span><span class="s1">:][[</span><span class="s3">'beta1'</span><span class="s2">, </span><span class="s3">'beta2'</span><span class="s1">]])</span>

    <span class="s4"># Test the RLS estimates against OLS estimates</span>
    <span class="s1">mod_ols = OLS(endog</span><span class="s2">, </span><span class="s1">exog)</span>
    <span class="s1">res_ols = mod_ols.fit()</span>
    <span class="s1">assert_allclose(res.params</span><span class="s2">, </span><span class="s1">res_ols.params)</span>


<span class="s1">@pytest.mark.matplotlib</span>
<span class="s2">def </span><span class="s1">test_plots(close_figures):</span>
    <span class="s1">exog = add_constant(dta[[</span><span class="s3">'m1'</span><span class="s2">, </span><span class="s3">'pop'</span><span class="s1">]])</span>
    <span class="s1">mod = RecursiveLS(endog</span><span class="s2">, </span><span class="s1">exog)</span>
    <span class="s1">res = mod.fit()</span>

    <span class="s4"># Basic plot</span>
    <span class="s2">try</span><span class="s1">:</span>
        <span class="s2">from </span><span class="s1">pandas.plotting </span><span class="s2">import </span><span class="s1">register_matplotlib_converters</span>
        <span class="s1">register_matplotlib_converters()</span>
    <span class="s2">except </span><span class="s1">ImportError:</span>
        <span class="s2">pass</span>
    <span class="s1">fig = res.plot_recursive_coefficient()</span>

    <span class="s4"># Specific variable</span>
    <span class="s1">fig = res.plot_recursive_coefficient(variables=[</span><span class="s3">'m1'</span><span class="s1">])</span>

    <span class="s4"># All variables</span>
    <span class="s1">fig = res.plot_recursive_coefficient(variables=[</span><span class="s5">0</span><span class="s2">, </span><span class="s3">'m1'</span><span class="s2">, </span><span class="s3">'pop'</span><span class="s1">])</span>

    <span class="s4"># Basic plot</span>
    <span class="s1">fig = res.plot_cusum()</span>

    <span class="s4"># Other alphas</span>
    <span class="s2">for </span><span class="s1">alpha </span><span class="s2">in </span><span class="s1">[</span><span class="s5">0.01</span><span class="s2">, </span><span class="s5">0.10</span><span class="s1">]:</span>
        <span class="s1">fig = res.plot_cusum(alpha=alpha)</span>

    <span class="s4"># Invalid alpha</span>
    <span class="s1">assert_raises(ValueError</span><span class="s2">, </span><span class="s1">res.plot_cusum</span><span class="s2">, </span><span class="s1">alpha=</span><span class="s5">0.123</span><span class="s1">)</span>

    <span class="s4"># Basic plot</span>
    <span class="s1">fig = res.plot_cusum_squares()</span>

    <span class="s4"># Numpy input (no dates)</span>
    <span class="s1">mod = RecursiveLS(endog.values</span><span class="s2">, </span><span class="s1">exog.values)</span>
    <span class="s1">res = mod.fit()</span>

    <span class="s4"># Basic plot</span>
    <span class="s1">fig = res.plot_recursive_coefficient()</span>

    <span class="s4"># Basic plot</span>
    <span class="s1">fig = res.plot_cusum()</span>

    <span class="s4"># Basic plot</span>
    <span class="s1">fig = res.plot_cusum_squares()</span>


<span class="s2">def </span><span class="s1">test_from_formula():</span>
    <span class="s2">with </span><span class="s1">pytest.warns(ValueWarning</span><span class="s2">, </span><span class="s1">match=</span><span class="s3">&quot;No frequency information&quot;</span><span class="s1">):</span>
        <span class="s1">mod = RecursiveLS.from_formula(</span><span class="s3">'cpi ~ m1'</span><span class="s2">, </span><span class="s1">data=dta)</span>

    <span class="s1">res = mod.fit()</span>

    <span class="s4"># Test the RLS estimates against OLS estimates</span>
    <span class="s1">mod_ols = OLS.from_formula(</span><span class="s3">'cpi ~ m1'</span><span class="s2">, </span><span class="s1">data=dta)</span>
    <span class="s1">res_ols = mod_ols.fit()</span>
    <span class="s1">assert_allclose(res.params</span><span class="s2">, </span><span class="s1">res_ols.params)</span>


<span class="s2">def </span><span class="s1">test_resid_recursive():</span>
    <span class="s1">mod = RecursiveLS(endog</span><span class="s2">, </span><span class="s1">exog)</span>
    <span class="s1">res = mod.fit()</span>

    <span class="s4"># Test the recursive residuals against those from R (strucchange)</span>
    <span class="s1">assert_allclose(res.resid_recursive[</span><span class="s5">2</span><span class="s1">:</span><span class="s5">10</span><span class="s1">].T</span><span class="s2">,</span>
                    <span class="s1">results_R.iloc[:</span><span class="s5">8</span><span class="s1">][</span><span class="s3">'rec_resid'</span><span class="s1">])</span>
    <span class="s1">assert_allclose(res.resid_recursive[</span><span class="s5">9</span><span class="s1">:</span><span class="s5">20</span><span class="s1">].T</span><span class="s2">,</span>
                    <span class="s1">results_R.iloc[</span><span class="s5">7</span><span class="s1">:</span><span class="s5">18</span><span class="s1">][</span><span class="s3">'rec_resid'</span><span class="s1">])</span>
    <span class="s1">assert_allclose(res.resid_recursive[</span><span class="s5">19</span><span class="s1">:].T</span><span class="s2">,</span>
                    <span class="s1">results_R.iloc[</span><span class="s5">17</span><span class="s1">:][</span><span class="s3">'rec_resid'</span><span class="s1">])</span>

    <span class="s4"># Test the RLS estimates against those from Stata (cusum6)</span>
    <span class="s1">assert_allclose(res.resid_recursive[</span><span class="s5">3</span><span class="s1">:]</span><span class="s2">,</span>
                    <span class="s1">results_stata.iloc[</span><span class="s5">3</span><span class="s1">:][</span><span class="s3">'rr'</span><span class="s1">]</span><span class="s2">, </span><span class="s1">atol=</span><span class="s5">1e-5</span><span class="s2">, </span><span class="s1">rtol=</span><span class="s5">1e-5</span><span class="s1">)</span>

    <span class="s4"># Test the RLS estimates against statsmodels estimates</span>
    <span class="s1">mod_ols = OLS(endog</span><span class="s2">, </span><span class="s1">exog)</span>
    <span class="s1">res_ols = mod_ols.fit()</span>
    <span class="s1">desired_resid_recursive = recursive_olsresiduals(res_ols)[</span><span class="s5">4</span><span class="s1">][</span><span class="s5">2</span><span class="s1">:]</span>
    <span class="s1">assert_allclose(res.resid_recursive[</span><span class="s5">2</span><span class="s1">:]</span><span class="s2">, </span><span class="s1">desired_resid_recursive)</span>


<span class="s2">def </span><span class="s1">test_recursive_olsresiduals_bad_input(reset_randomstate):</span>
    <span class="s2">from </span><span class="s1">statsmodels.tsa.arima.model </span><span class="s2">import </span><span class="s1">ARIMA</span>
    <span class="s1">e = np.random.standard_normal(</span><span class="s5">250</span><span class="s1">)</span>
    <span class="s1">y = e.copy()</span>
    <span class="s2">for </span><span class="s1">i </span><span class="s2">in </span><span class="s1">range(</span><span class="s5">1</span><span class="s2">, </span><span class="s1">y.shape[</span><span class="s5">0</span><span class="s1">]):</span>
        <span class="s1">y[i] += </span><span class="s5">0.1 </span><span class="s1">+ </span><span class="s5">0.8 </span><span class="s1">* y[i - </span><span class="s5">1</span><span class="s1">] + e[i]</span>
    <span class="s1">res = ARIMA(y[</span><span class="s5">20</span><span class="s1">:]</span><span class="s2">, </span><span class="s1">order=(</span><span class="s5">1</span><span class="s2">,</span><span class="s5">0</span><span class="s2">,</span><span class="s5">0</span><span class="s1">)</span><span class="s2">, </span><span class="s1">trend=</span><span class="s3">&quot;c&quot;</span><span class="s1">).fit()</span>
    <span class="s2">with </span><span class="s1">pytest.raises(TypeError</span><span class="s2">, </span><span class="s1">match=</span><span class="s3">&quot;res a regression results instance&quot;</span><span class="s1">):</span>
        <span class="s1">recursive_olsresiduals(res)</span>


<span class="s2">def </span><span class="s1">test_cusum():</span>
    <span class="s1">mod = RecursiveLS(endog</span><span class="s2">, </span><span class="s1">exog)</span>
    <span class="s1">res = mod.fit()</span>

    <span class="s4"># Test the cusum statistics against those from R (strucchange)</span>
    <span class="s4"># These values are not even close to ours, to Statas, or to the alternate</span>
    <span class="s4"># statsmodels values</span>
    <span class="s4"># assert_allclose(res.cusum, results_R['cusum'])</span>

    <span class="s4"># Test the cusum statistics against Stata (cusum6)</span>
    <span class="s4"># Note: cusum6 excludes the first 3 elements due to OLS initialization</span>
    <span class="s4"># whereas we exclude only the first 2. Also there are initialization</span>
    <span class="s4"># differences (as seen above in the recursive residuals).</span>
    <span class="s4"># Here we explicitly reverse engineer our cusum to match their to show the</span>
    <span class="s4"># equivalence</span>
    <span class="s1">d = res.nobs_diffuse</span>
    <span class="s1">cusum = res.cusum * np.std(res.resid_recursive[d:]</span><span class="s2">, </span><span class="s1">ddof=</span><span class="s5">1</span><span class="s1">)</span>
    <span class="s1">cusum -= res.resid_recursive[d]</span>
    <span class="s1">cusum /= np.std(res.resid_recursive[d+</span><span class="s5">1</span><span class="s1">:]</span><span class="s2">, </span><span class="s1">ddof=</span><span class="s5">1</span><span class="s1">)</span>
    <span class="s1">cusum = cusum[</span><span class="s5">1</span><span class="s1">:]</span>
    <span class="s1">assert_allclose(cusum</span><span class="s2">, </span><span class="s1">results_stata.iloc[</span><span class="s5">3</span><span class="s1">:][</span><span class="s3">'cusum'</span><span class="s1">]</span><span class="s2">, </span><span class="s1">atol=</span><span class="s5">1e-6</span><span class="s2">, </span><span class="s1">rtol=</span><span class="s5">1e-5</span><span class="s1">)</span>

    <span class="s4"># Test the cusum statistics against statsmodels estimates</span>
    <span class="s1">mod_ols = OLS(endog</span><span class="s2">, </span><span class="s1">exog)</span>
    <span class="s1">res_ols = mod_ols.fit()</span>
    <span class="s1">desired_cusum = recursive_olsresiduals(res_ols)[-</span><span class="s5">2</span><span class="s1">][</span><span class="s5">1</span><span class="s1">:]</span>
    <span class="s1">assert_allclose(res.cusum</span><span class="s2">, </span><span class="s1">desired_cusum</span><span class="s2">, </span><span class="s1">rtol=</span><span class="s5">1e-6</span><span class="s1">)</span>

    <span class="s4"># Test the cusum bounds against Stata (cusum6)</span>
    <span class="s4"># Again note that cusum6 excludes the first 3 elements, so we need to</span>
    <span class="s4"># change the ddof and points.</span>
    <span class="s1">actual_bounds = res._cusum_significance_bounds(</span>
        <span class="s1">alpha=</span><span class="s5">0.05</span><span class="s2">, </span><span class="s1">ddof=</span><span class="s5">1</span><span class="s2">, </span><span class="s1">points=np.arange(d+</span><span class="s5">1</span><span class="s2">, </span><span class="s1">res.nobs))</span>
    <span class="s1">desired_bounds = results_stata.iloc[</span><span class="s5">3</span><span class="s1">:][[</span><span class="s3">'lw'</span><span class="s2">, </span><span class="s3">'uw'</span><span class="s1">]].T</span>
    <span class="s1">assert_allclose(actual_bounds</span><span class="s2">, </span><span class="s1">desired_bounds</span><span class="s2">, </span><span class="s1">rtol=</span><span class="s5">1e-6</span><span class="s1">)</span>

    <span class="s4"># Test the cusum bounds against statsmodels</span>
    <span class="s1">actual_bounds = res._cusum_significance_bounds(</span>
        <span class="s1">alpha=</span><span class="s5">0.05</span><span class="s2">, </span><span class="s1">ddof=</span><span class="s5">0</span><span class="s2">, </span><span class="s1">points=np.arange(d</span><span class="s2">, </span><span class="s1">res.nobs))</span>
    <span class="s1">desired_bounds = recursive_olsresiduals(res_ols)[-</span><span class="s5">1</span><span class="s1">]</span>
    <span class="s1">assert_allclose(actual_bounds</span><span class="s2">, </span><span class="s1">desired_bounds)</span>

    <span class="s4"># Test for invalid calls</span>
    <span class="s1">assert_raises(ValueError</span><span class="s2">, </span><span class="s1">res._cusum_squares_significance_bounds</span><span class="s2">,</span>
                  <span class="s1">alpha=</span><span class="s5">0.123</span><span class="s1">)</span>


<span class="s2">def </span><span class="s1">test_stata():</span>
    <span class="s4"># Test the cusum and cusumsq statistics against Stata (cusum6)</span>
    <span class="s1">mod = RecursiveLS(endog</span><span class="s2">, </span><span class="s1">exog</span><span class="s2">, </span><span class="s1">loglikelihood_burn=</span><span class="s5">3</span><span class="s1">)</span>
    <span class="s2">with </span><span class="s1">pytest.warns(UserWarning):</span>
        <span class="s1">res = mod.fit()</span>
    <span class="s1">d = max(res.nobs_diffuse</span><span class="s2">, </span><span class="s1">res.loglikelihood_burn)</span>

    <span class="s1">assert_allclose(res.resid_recursive[</span><span class="s5">3</span><span class="s1">:]</span><span class="s2">, </span><span class="s1">results_stata.iloc[</span><span class="s5">3</span><span class="s1">:][</span><span class="s3">'rr'</span><span class="s1">]</span><span class="s2">,</span>
                    <span class="s1">atol=</span><span class="s5">1e-5</span><span class="s2">, </span><span class="s1">rtol=</span><span class="s5">1e-5</span><span class="s1">)</span>
    <span class="s1">assert_allclose(res.cusum</span><span class="s2">, </span><span class="s1">results_stata.iloc[</span><span class="s5">3</span><span class="s1">:][</span><span class="s3">'cusum'</span><span class="s1">]</span><span class="s2">, </span><span class="s1">atol=</span><span class="s5">1e-5</span><span class="s1">)</span>
    <span class="s1">assert_allclose(res.cusum_squares</span><span class="s2">, </span><span class="s1">results_stata.iloc[</span><span class="s5">3</span><span class="s1">:][</span><span class="s3">'cusum2'</span><span class="s1">]</span><span class="s2">,</span>
                    <span class="s1">atol=</span><span class="s5">1e-5</span><span class="s1">)</span>

    <span class="s1">actual_bounds = res._cusum_significance_bounds(</span>
        <span class="s1">alpha=</span><span class="s5">0.05</span><span class="s2">, </span><span class="s1">ddof=</span><span class="s5">0</span><span class="s2">, </span><span class="s1">points=np.arange(d+</span><span class="s5">1</span><span class="s2">, </span><span class="s1">res.nobs+</span><span class="s5">1</span><span class="s1">))</span>
    <span class="s1">desired_bounds = results_stata.iloc[</span><span class="s5">3</span><span class="s1">:][[</span><span class="s3">'lw'</span><span class="s2">, </span><span class="s3">'uw'</span><span class="s1">]].T</span>
    <span class="s1">assert_allclose(actual_bounds</span><span class="s2">, </span><span class="s1">desired_bounds</span><span class="s2">, </span><span class="s1">atol=</span><span class="s5">1e-5</span><span class="s1">)</span>

    <span class="s4"># Note: Stata uses a set of tabulated critical values whereas we use an</span>
    <span class="s4"># approximation formula, so this test is quite imprecise</span>
    <span class="s1">actual_bounds = res._cusum_squares_significance_bounds(</span>
        <span class="s1">alpha=</span><span class="s5">0.05</span><span class="s2">, </span><span class="s1">points=np.arange(d+</span><span class="s5">1</span><span class="s2">, </span><span class="s1">res.nobs+</span><span class="s5">1</span><span class="s1">))</span>
    <span class="s1">desired_bounds = results_stata.iloc[</span><span class="s5">3</span><span class="s1">:][[</span><span class="s3">'lww'</span><span class="s2">, </span><span class="s3">'uww'</span><span class="s1">]].T</span>
    <span class="s1">assert_allclose(actual_bounds</span><span class="s2">, </span><span class="s1">desired_bounds</span><span class="s2">, </span><span class="s1">atol=</span><span class="s5">1e-2</span><span class="s1">)</span>


<span class="s2">def </span><span class="s1">test_constraints_stata():</span>
    <span class="s1">endog = dta[</span><span class="s3">'infl'</span><span class="s1">]</span>
    <span class="s1">exog = add_constant(dta[[</span><span class="s3">'m1'</span><span class="s2">, </span><span class="s3">'unemp'</span><span class="s1">]])</span>

    <span class="s1">mod = RecursiveLS(endog</span><span class="s2">, </span><span class="s1">exog</span><span class="s2">, </span><span class="s1">constraints=</span><span class="s3">'m1 + unemp = 1'</span><span class="s1">)</span>
    <span class="s1">res = mod.fit()</span>

    <span class="s4"># See tests/results/test_rls.do</span>
    <span class="s1">desired = [-</span><span class="s5">0.7001083844336</span><span class="s2">, </span><span class="s1">-</span><span class="s5">0.0018477514060</span><span class="s2">, </span><span class="s5">1.0018477514060</span><span class="s1">]</span>
    <span class="s1">assert_allclose(res.params</span><span class="s2">, </span><span class="s1">desired)</span>

    <span class="s4"># See tests/results/test_rls.do</span>
    <span class="s1">desired = [</span><span class="s5">.4699552366</span><span class="s2">, </span><span class="s5">.0005369357</span><span class="s2">, </span><span class="s5">.0005369357</span><span class="s1">]</span>
    <span class="s1">assert_allclose(res.bse[</span><span class="s5">0</span><span class="s1">]</span><span class="s2">, </span><span class="s1">desired[</span><span class="s5">0</span><span class="s1">]</span><span class="s2">, </span><span class="s1">atol=</span><span class="s5">1e-1</span><span class="s1">)</span>
    <span class="s1">assert_allclose(res.bse[</span><span class="s5">1</span><span class="s1">:]</span><span class="s2">, </span><span class="s1">desired[</span><span class="s5">1</span><span class="s1">:]</span><span class="s2">, </span><span class="s1">atol=</span><span class="s5">1e-4</span><span class="s1">)</span>

    <span class="s4"># See tests/results/test_rls.do</span>
    <span class="s1">desired = -</span><span class="s5">534.4292052931121</span>
    <span class="s4"># Note that to compute what Stata reports as the llf, we need to use a</span>
    <span class="s4"># different denominator for estimating the scale, and then compute the</span>
    <span class="s4"># llf from the alternative recursive residuals</span>
    <span class="s1">scale_alternative = np.sum((</span>
        <span class="s1">res.standardized_forecasts_error[</span><span class="s5">0</span><span class="s2">, </span><span class="s5">1</span><span class="s1">:] *</span>
        <span class="s1">res.filter_results.obs_cov[</span><span class="s5">0</span><span class="s2">, </span><span class="s5">0</span><span class="s1">]**</span><span class="s5">0.5</span><span class="s1">)**</span><span class="s5">2</span><span class="s1">) / mod.nobs</span>
    <span class="s1">llf_alternative = np.log(norm.pdf(res.resid_recursive</span><span class="s2">, </span><span class="s1">loc=</span><span class="s5">0</span><span class="s2">,</span>
                                      <span class="s1">scale=scale_alternative**</span><span class="s5">0.5</span><span class="s1">)).sum()</span>
    <span class="s1">assert_allclose(llf_alternative</span><span class="s2">, </span><span class="s1">desired)</span>


<span class="s2">def </span><span class="s1">test_multiple_constraints():</span>
    <span class="s1">endog = dta[</span><span class="s3">'infl'</span><span class="s1">]</span>
    <span class="s1">exog = add_constant(dta[[</span><span class="s3">'m1'</span><span class="s2">, </span><span class="s3">'unemp'</span><span class="s2">, </span><span class="s3">'cpi'</span><span class="s1">]])</span>

    <span class="s1">constraints = [</span>
        <span class="s3">'m1 + unemp = 1'</span><span class="s2">,</span>
        <span class="s3">'cpi = 0'</span><span class="s2">,</span>
    <span class="s1">]</span>

    <span class="s1">mod = RecursiveLS(endog</span><span class="s2">, </span><span class="s1">exog</span><span class="s2">, </span><span class="s1">constraints=constraints)</span>
    <span class="s1">res = mod.fit()</span>

    <span class="s4"># See tests/results/test_rls.do</span>
    <span class="s1">desired = [-</span><span class="s5">0.7001083844336</span><span class="s2">, </span><span class="s1">-</span><span class="s5">0.0018477514060</span><span class="s2">, </span><span class="s5">1.0018477514060</span><span class="s2">, </span><span class="s5">0</span><span class="s1">]</span>
    <span class="s1">assert_allclose(res.params</span><span class="s2">, </span><span class="s1">desired</span><span class="s2">, </span><span class="s1">atol=</span><span class="s5">1e-10</span><span class="s1">)</span>

    <span class="s4"># See tests/results/test_rls.do</span>
    <span class="s1">desired = [</span><span class="s5">.4699552366</span><span class="s2">, </span><span class="s5">.0005369357</span><span class="s2">, </span><span class="s5">.0005369357</span><span class="s2">, </span><span class="s5">0</span><span class="s1">]</span>
    <span class="s1">assert_allclose(res.bse[</span><span class="s5">0</span><span class="s1">]</span><span class="s2">, </span><span class="s1">desired[</span><span class="s5">0</span><span class="s1">]</span><span class="s2">, </span><span class="s1">atol=</span><span class="s5">1e-1</span><span class="s1">)</span>
    <span class="s1">assert_allclose(res.bse[</span><span class="s5">1</span><span class="s1">:-</span><span class="s5">1</span><span class="s1">]</span><span class="s2">, </span><span class="s1">desired[</span><span class="s5">1</span><span class="s1">:-</span><span class="s5">1</span><span class="s1">]</span><span class="s2">, </span><span class="s1">atol=</span><span class="s5">1e-4</span><span class="s1">)</span>

    <span class="s4"># See tests/results/test_rls.do</span>
    <span class="s1">desired = -</span><span class="s5">534.4292052931121</span>
    <span class="s4"># Note that to compute what Stata reports as the llf, we need to use a</span>
    <span class="s4"># different denominator for estimating the scale, and then compute the</span>
    <span class="s4"># llf from the alternative recursive residuals</span>
    <span class="s1">scale_alternative = np.sum((</span>
        <span class="s1">res.standardized_forecasts_error[</span><span class="s5">0</span><span class="s2">, </span><span class="s5">1</span><span class="s1">:] *</span>
        <span class="s1">res.filter_results.obs_cov[</span><span class="s5">0</span><span class="s2">, </span><span class="s5">0</span><span class="s1">]**</span><span class="s5">0.5</span><span class="s1">)**</span><span class="s5">2</span><span class="s1">) / mod.nobs</span>
    <span class="s1">llf_alternative = np.log(norm.pdf(res.resid_recursive</span><span class="s2">, </span><span class="s1">loc=</span><span class="s5">0</span><span class="s2">,</span>
                                      <span class="s1">scale=scale_alternative**</span><span class="s5">0.5</span><span class="s1">)).sum()</span>
    <span class="s1">assert_allclose(llf_alternative</span><span class="s2">, </span><span class="s1">desired)</span>


<span class="s2">def </span><span class="s1">test_fix_params():</span>
    <span class="s1">mod = RecursiveLS([</span><span class="s5">0</span><span class="s2">, </span><span class="s5">1</span><span class="s2">, </span><span class="s5">0</span><span class="s2">, </span><span class="s5">1</span><span class="s1">]</span><span class="s2">, </span><span class="s1">[</span><span class="s5">1</span><span class="s2">, </span><span class="s5">1</span><span class="s2">, </span><span class="s5">1</span><span class="s2">, </span><span class="s5">1</span><span class="s1">])</span>
    <span class="s2">with </span><span class="s1">pytest.raises(ValueError</span><span class="s2">, </span><span class="s1">match=(</span><span class="s3">'Linear constraints on coefficients'</span>
                                          <span class="s3">' should be given'</span><span class="s1">)):</span>
        <span class="s2">with </span><span class="s1">mod.fix_params({</span><span class="s3">'const'</span><span class="s1">: </span><span class="s5">0.1</span><span class="s1">}):</span>
            <span class="s1">mod.fit()</span>

    <span class="s2">with </span><span class="s1">pytest.raises(ValueError</span><span class="s2">, </span><span class="s1">match=(</span><span class="s3">'Linear constraints on coefficients'</span>
                                          <span class="s3">' should be given'</span><span class="s1">)):</span>
        <span class="s1">mod.fit_constrained({</span><span class="s3">'const'</span><span class="s1">: </span><span class="s5">0.1</span><span class="s1">})</span>
</pre>
</body>
</html>