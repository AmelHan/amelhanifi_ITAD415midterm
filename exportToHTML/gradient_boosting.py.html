<html>
<head>
<title>gradient_boosting.py</title>
<meta http-equiv="Content-Type" content="text/html; charset=utf-8">
<style type="text/css">
.s0 { color: #629755; font-style: italic;}
.s1 { color: #a9b7c6;}
.s2 { color: #808080;}
.s3 { color: #cc7832;}
.s4 { color: #6a8759;}
.s5 { color: #6897bb;}
</style>
</head>
<body bgcolor="#2b2b2b">
<table CELLSPACING=0 CELLPADDING=5 COLS=1 WIDTH="100%" BGCOLOR="#606060" >
<tr><td><center>
<font face="Arial, Helvetica" color="#000000">
gradient_boosting.py</font>
</center></td></tr></table>
<pre><span class="s0">&quot;&quot;&quot;Fast Gradient Boosting decision trees for classification and regression.&quot;&quot;&quot;</span>
<span class="s2"># Author: Nicolas Hug</span>

<span class="s3">import </span><span class="s1">itertools</span>
<span class="s3">from </span><span class="s1">abc </span><span class="s3">import </span><span class="s1">ABC</span><span class="s3">, </span><span class="s1">abstractmethod</span>
<span class="s3">from </span><span class="s1">functools </span><span class="s3">import </span><span class="s1">partial</span>
<span class="s3">from </span><span class="s1">numbers </span><span class="s3">import </span><span class="s1">Integral</span><span class="s3">, </span><span class="s1">Real</span>
<span class="s3">from </span><span class="s1">timeit </span><span class="s3">import </span><span class="s1">default_timer </span><span class="s3">as </span><span class="s1">time</span>

<span class="s3">import </span><span class="s1">numpy </span><span class="s3">as </span><span class="s1">np</span>

<span class="s3">from </span><span class="s1">..._loss.loss </span><span class="s3">import </span><span class="s1">(</span>
    <span class="s1">_LOSSES</span><span class="s3">,</span>
    <span class="s1">BaseLoss</span><span class="s3">,</span>
    <span class="s1">HalfBinomialLoss</span><span class="s3">,</span>
    <span class="s1">HalfGammaLoss</span><span class="s3">,</span>
    <span class="s1">HalfMultinomialLoss</span><span class="s3">,</span>
    <span class="s1">HalfPoissonLoss</span><span class="s3">,</span>
    <span class="s1">PinballLoss</span><span class="s3">,</span>
<span class="s1">)</span>
<span class="s3">from </span><span class="s1">...base </span><span class="s3">import </span><span class="s1">(</span>
    <span class="s1">BaseEstimator</span><span class="s3">,</span>
    <span class="s1">ClassifierMixin</span><span class="s3">,</span>
    <span class="s1">RegressorMixin</span><span class="s3">,</span>
    <span class="s1">_fit_context</span><span class="s3">,</span>
    <span class="s1">is_classifier</span><span class="s3">,</span>
<span class="s1">)</span>
<span class="s3">from </span><span class="s1">...metrics </span><span class="s3">import </span><span class="s1">check_scoring</span>
<span class="s3">from </span><span class="s1">...model_selection </span><span class="s3">import </span><span class="s1">train_test_split</span>
<span class="s3">from </span><span class="s1">...preprocessing </span><span class="s3">import </span><span class="s1">LabelEncoder</span>
<span class="s3">from </span><span class="s1">...utils </span><span class="s3">import </span><span class="s1">check_random_state</span><span class="s3">, </span><span class="s1">compute_sample_weight</span><span class="s3">, </span><span class="s1">resample</span>
<span class="s3">from </span><span class="s1">...utils._openmp_helpers </span><span class="s3">import </span><span class="s1">_openmp_effective_n_threads</span>
<span class="s3">from </span><span class="s1">...utils._param_validation </span><span class="s3">import </span><span class="s1">Interval</span><span class="s3">, </span><span class="s1">RealNotInt</span><span class="s3">, </span><span class="s1">StrOptions</span>
<span class="s3">from </span><span class="s1">...utils.multiclass </span><span class="s3">import </span><span class="s1">check_classification_targets</span>
<span class="s3">from </span><span class="s1">...utils.validation </span><span class="s3">import </span><span class="s1">(</span>
    <span class="s1">_check_monotonic_cst</span><span class="s3">,</span>
    <span class="s1">_check_sample_weight</span><span class="s3">,</span>
    <span class="s1">check_consistent_length</span><span class="s3">,</span>
    <span class="s1">check_is_fitted</span><span class="s3">,</span>
<span class="s1">)</span>
<span class="s3">from </span><span class="s1">._gradient_boosting </span><span class="s3">import </span><span class="s1">_update_raw_predictions</span>
<span class="s3">from </span><span class="s1">.binning </span><span class="s3">import </span><span class="s1">_BinMapper</span>
<span class="s3">from </span><span class="s1">.common </span><span class="s3">import </span><span class="s1">G_H_DTYPE</span><span class="s3">, </span><span class="s1">X_DTYPE</span><span class="s3">, </span><span class="s1">Y_DTYPE</span>
<span class="s3">from </span><span class="s1">.grower </span><span class="s3">import </span><span class="s1">TreeGrower</span>

<span class="s1">_LOSSES = _LOSSES.copy()</span>
<span class="s1">_LOSSES.update(</span>
    <span class="s1">{</span>
        <span class="s4">&quot;poisson&quot;</span><span class="s1">: HalfPoissonLoss</span><span class="s3">,</span>
        <span class="s4">&quot;gamma&quot;</span><span class="s1">: HalfGammaLoss</span><span class="s3">,</span>
        <span class="s4">&quot;quantile&quot;</span><span class="s1">: PinballLoss</span><span class="s3">,</span>
    <span class="s1">}</span>
<span class="s1">)</span>


<span class="s3">def </span><span class="s1">_update_leaves_values(loss</span><span class="s3">, </span><span class="s1">grower</span><span class="s3">, </span><span class="s1">y_true</span><span class="s3">, </span><span class="s1">raw_prediction</span><span class="s3">, </span><span class="s1">sample_weight):</span>
    <span class="s0">&quot;&quot;&quot;Update the leaf values to be predicted by the tree. 
 
    Update equals: 
        loss.fit_intercept_only(y_true - raw_prediction) 
 
    This is only applied if loss.need_update_leaves_values is True. 
    Note: It only works, if the loss is a function of the residual, as is the 
    case for AbsoluteError and PinballLoss. Otherwise, one would need to get 
    the minimum of loss(y_true, raw_prediction + x) in x. A few examples: 
      - AbsoluteError: median(y_true - raw_prediction). 
      - PinballLoss: quantile(y_true - raw_prediction). 
    See also notes about need_update_leaves_values in BaseLoss. 
    &quot;&quot;&quot;</span>
    <span class="s2"># TODO: Ideally this should be computed in parallel over the leaves using something</span>
    <span class="s2"># similar to _update_raw_predictions(), but this requires a cython version of</span>
    <span class="s2"># median().</span>
    <span class="s3">for </span><span class="s1">leaf </span><span class="s3">in </span><span class="s1">grower.finalized_leaves:</span>
        <span class="s1">indices = leaf.sample_indices</span>
        <span class="s3">if </span><span class="s1">sample_weight </span><span class="s3">is None</span><span class="s1">:</span>
            <span class="s1">sw = </span><span class="s3">None</span>
        <span class="s3">else</span><span class="s1">:</span>
            <span class="s1">sw = sample_weight[indices]</span>
        <span class="s1">update = loss.fit_intercept_only(</span>
            <span class="s1">y_true=y_true[indices] - raw_prediction[indices]</span><span class="s3">,</span>
            <span class="s1">sample_weight=sw</span><span class="s3">,</span>
        <span class="s1">)</span>
        <span class="s1">leaf.value = grower.shrinkage * update</span>
        <span class="s2"># Note that the regularization is ignored here</span>


<span class="s3">class </span><span class="s1">BaseHistGradientBoosting(BaseEstimator</span><span class="s3">, </span><span class="s1">ABC):</span>
    <span class="s0">&quot;&quot;&quot;Base class for histogram-based gradient boosting estimators.&quot;&quot;&quot;</span>

    <span class="s1">_parameter_constraints: dict = {</span>
        <span class="s4">&quot;loss&quot;</span><span class="s1">: [BaseLoss]</span><span class="s3">,</span>
        <span class="s4">&quot;learning_rate&quot;</span><span class="s1">: [Interval(Real</span><span class="s3">, </span><span class="s5">0</span><span class="s3">, None, </span><span class="s1">closed=</span><span class="s4">&quot;neither&quot;</span><span class="s1">)]</span><span class="s3">,</span>
        <span class="s4">&quot;max_iter&quot;</span><span class="s1">: [Interval(Integral</span><span class="s3">, </span><span class="s5">1</span><span class="s3">, None, </span><span class="s1">closed=</span><span class="s4">&quot;left&quot;</span><span class="s1">)]</span><span class="s3">,</span>
        <span class="s4">&quot;max_leaf_nodes&quot;</span><span class="s1">: [Interval(Integral</span><span class="s3">, </span><span class="s5">2</span><span class="s3">, None, </span><span class="s1">closed=</span><span class="s4">&quot;left&quot;</span><span class="s1">)</span><span class="s3">, None</span><span class="s1">]</span><span class="s3">,</span>
        <span class="s4">&quot;max_depth&quot;</span><span class="s1">: [Interval(Integral</span><span class="s3">, </span><span class="s5">1</span><span class="s3">, None, </span><span class="s1">closed=</span><span class="s4">&quot;left&quot;</span><span class="s1">)</span><span class="s3">, None</span><span class="s1">]</span><span class="s3">,</span>
        <span class="s4">&quot;min_samples_leaf&quot;</span><span class="s1">: [Interval(Integral</span><span class="s3">, </span><span class="s5">1</span><span class="s3">, None, </span><span class="s1">closed=</span><span class="s4">&quot;left&quot;</span><span class="s1">)]</span><span class="s3">,</span>
        <span class="s4">&quot;l2_regularization&quot;</span><span class="s1">: [Interval(Real</span><span class="s3">, </span><span class="s5">0</span><span class="s3">, None, </span><span class="s1">closed=</span><span class="s4">&quot;left&quot;</span><span class="s1">)]</span><span class="s3">,</span>
        <span class="s4">&quot;monotonic_cst&quot;</span><span class="s1">: [</span><span class="s4">&quot;array-like&quot;</span><span class="s3">, </span><span class="s1">dict</span><span class="s3">, None</span><span class="s1">]</span><span class="s3">,</span>
        <span class="s4">&quot;interaction_cst&quot;</span><span class="s1">: [</span>
            <span class="s1">list</span><span class="s3">,</span>
            <span class="s1">tuple</span><span class="s3">,</span>
            <span class="s1">StrOptions({</span><span class="s4">&quot;pairwise&quot;</span><span class="s3">, </span><span class="s4">&quot;no_interactions&quot;</span><span class="s1">})</span><span class="s3">,</span>
            <span class="s3">None,</span>
        <span class="s1">]</span><span class="s3">,</span>
        <span class="s4">&quot;n_iter_no_change&quot;</span><span class="s1">: [Interval(Integral</span><span class="s3">, </span><span class="s5">1</span><span class="s3">, None, </span><span class="s1">closed=</span><span class="s4">&quot;left&quot;</span><span class="s1">)]</span><span class="s3">,</span>
        <span class="s4">&quot;validation_fraction&quot;</span><span class="s1">: [</span>
            <span class="s1">Interval(RealNotInt</span><span class="s3">, </span><span class="s5">0</span><span class="s3">, </span><span class="s5">1</span><span class="s3">, </span><span class="s1">closed=</span><span class="s4">&quot;neither&quot;</span><span class="s1">)</span><span class="s3">,</span>
            <span class="s1">Interval(Integral</span><span class="s3">, </span><span class="s5">1</span><span class="s3">, None, </span><span class="s1">closed=</span><span class="s4">&quot;left&quot;</span><span class="s1">)</span><span class="s3">,</span>
            <span class="s3">None,</span>
        <span class="s1">]</span><span class="s3">,</span>
        <span class="s4">&quot;tol&quot;</span><span class="s1">: [Interval(Real</span><span class="s3">, </span><span class="s5">0</span><span class="s3">, None, </span><span class="s1">closed=</span><span class="s4">&quot;left&quot;</span><span class="s1">)]</span><span class="s3">,</span>
        <span class="s4">&quot;max_bins&quot;</span><span class="s1">: [Interval(Integral</span><span class="s3">, </span><span class="s5">2</span><span class="s3">, </span><span class="s5">255</span><span class="s3">, </span><span class="s1">closed=</span><span class="s4">&quot;both&quot;</span><span class="s1">)]</span><span class="s3">,</span>
        <span class="s4">&quot;categorical_features&quot;</span><span class="s1">: [</span><span class="s4">&quot;array-like&quot;</span><span class="s3">, None</span><span class="s1">]</span><span class="s3">,</span>
        <span class="s4">&quot;warm_start&quot;</span><span class="s1">: [</span><span class="s4">&quot;boolean&quot;</span><span class="s1">]</span><span class="s3">,</span>
        <span class="s4">&quot;early_stopping&quot;</span><span class="s1">: [StrOptions({</span><span class="s4">&quot;auto&quot;</span><span class="s1">})</span><span class="s3">, </span><span class="s4">&quot;boolean&quot;</span><span class="s1">]</span><span class="s3">,</span>
        <span class="s4">&quot;scoring&quot;</span><span class="s1">: [str</span><span class="s3">, </span><span class="s1">callable</span><span class="s3">, None</span><span class="s1">]</span><span class="s3">,</span>
        <span class="s4">&quot;verbose&quot;</span><span class="s1">: [</span><span class="s4">&quot;verbose&quot;</span><span class="s1">]</span><span class="s3">,</span>
        <span class="s4">&quot;random_state&quot;</span><span class="s1">: [</span><span class="s4">&quot;random_state&quot;</span><span class="s1">]</span><span class="s3">,</span>
    <span class="s1">}</span>

    <span class="s1">@abstractmethod</span>
    <span class="s3">def </span><span class="s1">__init__(</span>
        <span class="s1">self</span><span class="s3">,</span>
        <span class="s1">loss</span><span class="s3">,</span>
        <span class="s1">*</span><span class="s3">,</span>
        <span class="s1">learning_rate</span><span class="s3">,</span>
        <span class="s1">max_iter</span><span class="s3">,</span>
        <span class="s1">max_leaf_nodes</span><span class="s3">,</span>
        <span class="s1">max_depth</span><span class="s3">,</span>
        <span class="s1">min_samples_leaf</span><span class="s3">,</span>
        <span class="s1">l2_regularization</span><span class="s3">,</span>
        <span class="s1">max_bins</span><span class="s3">,</span>
        <span class="s1">categorical_features</span><span class="s3">,</span>
        <span class="s1">monotonic_cst</span><span class="s3">,</span>
        <span class="s1">interaction_cst</span><span class="s3">,</span>
        <span class="s1">warm_start</span><span class="s3">,</span>
        <span class="s1">early_stopping</span><span class="s3">,</span>
        <span class="s1">scoring</span><span class="s3">,</span>
        <span class="s1">validation_fraction</span><span class="s3">,</span>
        <span class="s1">n_iter_no_change</span><span class="s3">,</span>
        <span class="s1">tol</span><span class="s3">,</span>
        <span class="s1">verbose</span><span class="s3">,</span>
        <span class="s1">random_state</span><span class="s3">,</span>
    <span class="s1">):</span>
        <span class="s1">self.loss = loss</span>
        <span class="s1">self.learning_rate = learning_rate</span>
        <span class="s1">self.max_iter = max_iter</span>
        <span class="s1">self.max_leaf_nodes = max_leaf_nodes</span>
        <span class="s1">self.max_depth = max_depth</span>
        <span class="s1">self.min_samples_leaf = min_samples_leaf</span>
        <span class="s1">self.l2_regularization = l2_regularization</span>
        <span class="s1">self.max_bins = max_bins</span>
        <span class="s1">self.monotonic_cst = monotonic_cst</span>
        <span class="s1">self.interaction_cst = interaction_cst</span>
        <span class="s1">self.categorical_features = categorical_features</span>
        <span class="s1">self.warm_start = warm_start</span>
        <span class="s1">self.early_stopping = early_stopping</span>
        <span class="s1">self.scoring = scoring</span>
        <span class="s1">self.validation_fraction = validation_fraction</span>
        <span class="s1">self.n_iter_no_change = n_iter_no_change</span>
        <span class="s1">self.tol = tol</span>
        <span class="s1">self.verbose = verbose</span>
        <span class="s1">self.random_state = random_state</span>

    <span class="s3">def </span><span class="s1">_validate_parameters(self):</span>
        <span class="s0">&quot;&quot;&quot;Validate parameters passed to __init__. 
 
        The parameters that are directly passed to the grower are checked in 
        TreeGrower.&quot;&quot;&quot;</span>
        <span class="s3">if </span><span class="s1">self.monotonic_cst </span><span class="s3">is not None and </span><span class="s1">self.n_trees_per_iteration_ != </span><span class="s5">1</span><span class="s1">:</span>
            <span class="s3">raise </span><span class="s1">ValueError(</span>
                <span class="s4">&quot;monotonic constraints are not supported for multiclass classification.&quot;</span>
            <span class="s1">)</span>

    <span class="s3">def </span><span class="s1">_finalize_sample_weight(self</span><span class="s3">, </span><span class="s1">sample_weight</span><span class="s3">, </span><span class="s1">y):</span>
        <span class="s0">&quot;&quot;&quot;Finalize sample weight. 
 
        Used by subclasses to adjust sample_weights. This is useful for implementing 
        class weights. 
        &quot;&quot;&quot;</span>
        <span class="s3">return </span><span class="s1">sample_weight</span>

    <span class="s3">def </span><span class="s1">_check_categories(self</span><span class="s3">, </span><span class="s1">X):</span>
        <span class="s0">&quot;&quot;&quot;Check and validate categorical features in X 
 
        Return 
        ------ 
        is_categorical : ndarray of shape (n_features,) or None, dtype=bool 
            Indicates whether a feature is categorical. If no feature is 
            categorical, this is None. 
        known_categories : list of size n_features or None 
            The list contains, for each feature: 
                - an array of shape (n_categories,) with the unique cat values 
                - None if the feature is not categorical 
            None if no feature is categorical. 
        &quot;&quot;&quot;</span>
        <span class="s3">if </span><span class="s1">self.categorical_features </span><span class="s3">is None</span><span class="s1">:</span>
            <span class="s3">return None, None</span>

        <span class="s1">categorical_features = np.asarray(self.categorical_features)</span>

        <span class="s3">if </span><span class="s1">categorical_features.size == </span><span class="s5">0</span><span class="s1">:</span>
            <span class="s3">return None, None</span>

        <span class="s3">if </span><span class="s1">categorical_features.dtype.kind </span><span class="s3">not in </span><span class="s1">(</span><span class="s4">&quot;i&quot;</span><span class="s3">, </span><span class="s4">&quot;b&quot;</span><span class="s3">, </span><span class="s4">&quot;U&quot;</span><span class="s3">, </span><span class="s4">&quot;O&quot;</span><span class="s1">):</span>
            <span class="s3">raise </span><span class="s1">ValueError(</span>
                <span class="s4">&quot;categorical_features must be an array-like of bool, int or &quot;</span>
                <span class="s4">f&quot;str, got: </span><span class="s3">{</span><span class="s1">categorical_features.dtype.name</span><span class="s3">}</span><span class="s4">.&quot;</span>
            <span class="s1">)</span>

        <span class="s3">if </span><span class="s1">categorical_features.dtype.kind == </span><span class="s4">&quot;O&quot;</span><span class="s1">:</span>
            <span class="s1">types = set(type(f) </span><span class="s3">for </span><span class="s1">f </span><span class="s3">in </span><span class="s1">categorical_features)</span>
            <span class="s3">if </span><span class="s1">types != {str}:</span>
                <span class="s3">raise </span><span class="s1">ValueError(</span>
                    <span class="s4">&quot;categorical_features must be an array-like of bool, int or &quot;</span>
                    <span class="s4">f&quot;str, got: </span><span class="s3">{</span><span class="s4">', '</span><span class="s1">.join(sorted(t.__name__ </span><span class="s3">for </span><span class="s1">t </span><span class="s3">in </span><span class="s1">types))</span><span class="s3">}</span><span class="s4">.&quot;</span>
                <span class="s1">)</span>

        <span class="s1">n_features = X.shape[</span><span class="s5">1</span><span class="s1">]</span>

        <span class="s3">if </span><span class="s1">categorical_features.dtype.kind </span><span class="s3">in </span><span class="s1">(</span><span class="s4">&quot;U&quot;</span><span class="s3">, </span><span class="s4">&quot;O&quot;</span><span class="s1">):</span>
            <span class="s2"># check for feature names</span>
            <span class="s3">if not </span><span class="s1">hasattr(self</span><span class="s3">, </span><span class="s4">&quot;feature_names_in_&quot;</span><span class="s1">):</span>
                <span class="s3">raise </span><span class="s1">ValueError(</span>
                    <span class="s4">&quot;categorical_features should be passed as an array of &quot;</span>
                    <span class="s4">&quot;integers or as a boolean mask when the model is fitted &quot;</span>
                    <span class="s4">&quot;on data without feature names.&quot;</span>
                <span class="s1">)</span>
            <span class="s1">is_categorical = np.zeros(n_features</span><span class="s3">, </span><span class="s1">dtype=bool)</span>
            <span class="s1">feature_names = self.feature_names_in_.tolist()</span>
            <span class="s3">for </span><span class="s1">feature_name </span><span class="s3">in </span><span class="s1">categorical_features:</span>
                <span class="s3">try</span><span class="s1">:</span>
                    <span class="s1">is_categorical[feature_names.index(feature_name)] = </span><span class="s3">True</span>
                <span class="s3">except </span><span class="s1">ValueError </span><span class="s3">as </span><span class="s1">e:</span>
                    <span class="s3">raise </span><span class="s1">ValueError(</span>
                        <span class="s4">f&quot;categorical_features has a item value '</span><span class="s3">{</span><span class="s1">feature_name</span><span class="s3">}</span><span class="s4">' &quot;</span>
                        <span class="s4">&quot;which is not a valid feature name of the training &quot;</span>
                        <span class="s4">f&quot;data. Observed feature names: </span><span class="s3">{</span><span class="s1">feature_names</span><span class="s3">}</span><span class="s4">&quot;</span>
                    <span class="s1">) </span><span class="s3">from </span><span class="s1">e</span>
        <span class="s3">elif </span><span class="s1">categorical_features.dtype.kind == </span><span class="s4">&quot;i&quot;</span><span class="s1">:</span>
            <span class="s2"># check for categorical features as indices</span>
            <span class="s3">if </span><span class="s1">(</span>
                <span class="s1">np.max(categorical_features) &gt;= n_features</span>
                <span class="s3">or </span><span class="s1">np.min(categorical_features) &lt; </span><span class="s5">0</span>
            <span class="s1">):</span>
                <span class="s3">raise </span><span class="s1">ValueError(</span>
                    <span class="s4">&quot;categorical_features set as integer &quot;</span>
                    <span class="s4">&quot;indices must be in [0, n_features - 1]&quot;</span>
                <span class="s1">)</span>
            <span class="s1">is_categorical = np.zeros(n_features</span><span class="s3">, </span><span class="s1">dtype=bool)</span>
            <span class="s1">is_categorical[categorical_features] = </span><span class="s3">True</span>
        <span class="s3">else</span><span class="s1">:</span>
            <span class="s3">if </span><span class="s1">categorical_features.shape[</span><span class="s5">0</span><span class="s1">] != n_features:</span>
                <span class="s3">raise </span><span class="s1">ValueError(</span>
                    <span class="s4">&quot;categorical_features set as a boolean mask &quot;</span>
                    <span class="s4">&quot;must have shape (n_features,), got: &quot;</span>
                    <span class="s4">f&quot;</span><span class="s3">{</span><span class="s1">categorical_features.shape</span><span class="s3">}</span><span class="s4">&quot;</span>
                <span class="s1">)</span>
            <span class="s1">is_categorical = categorical_features</span>

        <span class="s3">if not </span><span class="s1">np.any(is_categorical):</span>
            <span class="s3">return None, None</span>

        <span class="s2"># Compute the known categories in the training data. We cannot do this</span>
        <span class="s2"># in the BinMapper because it only gets a fraction of the training data</span>
        <span class="s2"># when early stopping is enabled.</span>
        <span class="s1">known_categories = []</span>

        <span class="s3">for </span><span class="s1">f_idx </span><span class="s3">in </span><span class="s1">range(n_features):</span>
            <span class="s3">if </span><span class="s1">is_categorical[f_idx]:</span>
                <span class="s1">categories = np.unique(X[:</span><span class="s3">, </span><span class="s1">f_idx])</span>
                <span class="s1">missing = np.isnan(categories)</span>
                <span class="s3">if </span><span class="s1">missing.any():</span>
                    <span class="s1">categories = categories[~missing]</span>

                <span class="s2"># Treat negative values for categorical features as missing values.</span>
                <span class="s1">negative_categories = categories &lt; </span><span class="s5">0</span>
                <span class="s3">if </span><span class="s1">negative_categories.any():</span>
                    <span class="s1">categories = categories[~negative_categories]</span>

                <span class="s3">if </span><span class="s1">hasattr(self</span><span class="s3">, </span><span class="s4">&quot;feature_names_in_&quot;</span><span class="s1">):</span>
                    <span class="s1">feature_name = </span><span class="s4">f&quot;'</span><span class="s3">{</span><span class="s1">self.feature_names_in_[f_idx]</span><span class="s3">}</span><span class="s4">'&quot;</span>
                <span class="s3">else</span><span class="s1">:</span>
                    <span class="s1">feature_name = </span><span class="s4">f&quot;at index </span><span class="s3">{</span><span class="s1">f_idx</span><span class="s3">}</span><span class="s4">&quot;</span>

                <span class="s3">if </span><span class="s1">categories.size &gt; self.max_bins:</span>
                    <span class="s3">raise </span><span class="s1">ValueError(</span>
                        <span class="s4">f&quot;Categorical feature </span><span class="s3">{</span><span class="s1">feature_name</span><span class="s3">} </span><span class="s4">is expected to &quot;</span>
                        <span class="s4">f&quot;have a cardinality &lt;= </span><span class="s3">{</span><span class="s1">self.max_bins</span><span class="s3">} </span><span class="s4">but actually &quot;</span>
                        <span class="s4">f&quot;has a cardinality of </span><span class="s3">{</span><span class="s1">categories.size</span><span class="s3">}</span><span class="s4">.&quot;</span>
                    <span class="s1">)</span>

                <span class="s3">if </span><span class="s1">(categories &gt;= self.max_bins).any():</span>
                    <span class="s3">raise </span><span class="s1">ValueError(</span>
                        <span class="s4">f&quot;Categorical feature </span><span class="s3">{</span><span class="s1">feature_name</span><span class="s3">} </span><span class="s4">is expected to &quot;</span>
                        <span class="s4">f&quot;be encoded with values &lt; </span><span class="s3">{</span><span class="s1">self.max_bins</span><span class="s3">} </span><span class="s4">but the &quot;</span>
                        <span class="s4">&quot;largest value for the encoded categories is &quot;</span>
                        <span class="s4">f&quot;</span><span class="s3">{</span><span class="s1">categories.max()</span><span class="s3">}</span><span class="s4">.&quot;</span>
                    <span class="s1">)</span>
            <span class="s3">else</span><span class="s1">:</span>
                <span class="s1">categories = </span><span class="s3">None</span>
            <span class="s1">known_categories.append(categories)</span>

        <span class="s3">return </span><span class="s1">is_categorical</span><span class="s3">, </span><span class="s1">known_categories</span>

    <span class="s3">def </span><span class="s1">_check_interaction_cst(self</span><span class="s3">, </span><span class="s1">n_features):</span>
        <span class="s0">&quot;&quot;&quot;Check and validation for interaction constraints.&quot;&quot;&quot;</span>
        <span class="s3">if </span><span class="s1">self.interaction_cst </span><span class="s3">is None</span><span class="s1">:</span>
            <span class="s3">return None</span>

        <span class="s3">if </span><span class="s1">self.interaction_cst == </span><span class="s4">&quot;no_interactions&quot;</span><span class="s1">:</span>
            <span class="s1">interaction_cst = [[i] </span><span class="s3">for </span><span class="s1">i </span><span class="s3">in </span><span class="s1">range(n_features)]</span>
        <span class="s3">elif </span><span class="s1">self.interaction_cst == </span><span class="s4">&quot;pairwise&quot;</span><span class="s1">:</span>
            <span class="s1">interaction_cst = itertools.combinations(range(n_features)</span><span class="s3">, </span><span class="s5">2</span><span class="s1">)</span>
        <span class="s3">else</span><span class="s1">:</span>
            <span class="s1">interaction_cst = self.interaction_cst</span>

        <span class="s3">try</span><span class="s1">:</span>
            <span class="s1">constraints = [set(group) </span><span class="s3">for </span><span class="s1">group </span><span class="s3">in </span><span class="s1">interaction_cst]</span>
        <span class="s3">except </span><span class="s1">TypeError:</span>
            <span class="s3">raise </span><span class="s1">ValueError(</span>
                <span class="s4">&quot;Interaction constraints must be a sequence of tuples or lists, got:&quot;</span>
                <span class="s4">f&quot; </span><span class="s3">{</span><span class="s1">self.interaction_cst</span><span class="s3">!r}</span><span class="s4">.&quot;</span>
            <span class="s1">)</span>

        <span class="s3">for </span><span class="s1">group </span><span class="s3">in </span><span class="s1">constraints:</span>
            <span class="s3">for </span><span class="s1">x </span><span class="s3">in </span><span class="s1">group:</span>
                <span class="s3">if not </span><span class="s1">(isinstance(x</span><span class="s3">, </span><span class="s1">Integral) </span><span class="s3">and </span><span class="s5">0 </span><span class="s1">&lt;= x &lt; n_features):</span>
                    <span class="s3">raise </span><span class="s1">ValueError(</span>
                        <span class="s4">&quot;Interaction constraints must consist of integer indices in&quot;</span>
                        <span class="s4">f&quot; [0, n_features - 1] = [0, </span><span class="s3">{</span><span class="s1">n_features - </span><span class="s5">1</span><span class="s3">}</span><span class="s4">], specifying the&quot;</span>
                        <span class="s4">&quot; position of features, got invalid indices:&quot;</span>
                        <span class="s4">f&quot; </span><span class="s3">{</span><span class="s1">group</span><span class="s3">!r}</span><span class="s4">&quot;</span>
                    <span class="s1">)</span>

        <span class="s2"># Add all not listed features as own group by default.</span>
        <span class="s1">rest = set(range(n_features)) - set().union(*constraints)</span>
        <span class="s3">if </span><span class="s1">len(rest) &gt; </span><span class="s5">0</span><span class="s1">:</span>
            <span class="s1">constraints.append(rest)</span>

        <span class="s3">return </span><span class="s1">constraints</span>

    <span class="s1">@_fit_context(prefer_skip_nested_validation=</span><span class="s3">True</span><span class="s1">)</span>
    <span class="s3">def </span><span class="s1">fit(self</span><span class="s3">, </span><span class="s1">X</span><span class="s3">, </span><span class="s1">y</span><span class="s3">, </span><span class="s1">sample_weight=</span><span class="s3">None</span><span class="s1">):</span>
        <span class="s0">&quot;&quot;&quot;Fit the gradient boosting model. 
 
        Parameters 
        ---------- 
        X : array-like of shape (n_samples, n_features) 
            The input samples. 
 
        y : array-like of shape (n_samples,) 
            Target values. 
 
        sample_weight : array-like of shape (n_samples,) default=None 
            Weights of training data. 
 
            .. versionadded:: 0.23 
 
        Returns 
        ------- 
        self : object 
            Fitted estimator. 
        &quot;&quot;&quot;</span>
        <span class="s1">fit_start_time = time()</span>
        <span class="s1">acc_find_split_time = </span><span class="s5">0.0  </span><span class="s2"># time spent finding the best splits</span>
        <span class="s1">acc_apply_split_time = </span><span class="s5">0.0  </span><span class="s2"># time spent splitting nodes</span>
        <span class="s1">acc_compute_hist_time = </span><span class="s5">0.0  </span><span class="s2"># time spent computing histograms</span>
        <span class="s2"># time spent predicting X for gradient and hessians update</span>
        <span class="s1">acc_prediction_time = </span><span class="s5">0.0</span>
        <span class="s1">X</span><span class="s3">, </span><span class="s1">y = self._validate_data(X</span><span class="s3">, </span><span class="s1">y</span><span class="s3">, </span><span class="s1">dtype=[X_DTYPE]</span><span class="s3">, </span><span class="s1">force_all_finite=</span><span class="s3">False</span><span class="s1">)</span>
        <span class="s1">y = self._encode_y(y)</span>
        <span class="s1">check_consistent_length(X</span><span class="s3">, </span><span class="s1">y)</span>
        <span class="s2"># Do not create unit sample weights by default to later skip some</span>
        <span class="s2"># computation</span>
        <span class="s3">if </span><span class="s1">sample_weight </span><span class="s3">is not None</span><span class="s1">:</span>
            <span class="s1">sample_weight = _check_sample_weight(sample_weight</span><span class="s3">, </span><span class="s1">X</span><span class="s3">, </span><span class="s1">dtype=np.float64)</span>
            <span class="s2"># TODO: remove when PDP supports sample weights</span>
            <span class="s1">self._fitted_with_sw = </span><span class="s3">True</span>

        <span class="s1">sample_weight = self._finalize_sample_weight(sample_weight</span><span class="s3">, </span><span class="s1">y)</span>

        <span class="s1">rng = check_random_state(self.random_state)</span>

        <span class="s2"># When warm starting, we want to reuse the same seed that was used</span>
        <span class="s2"># the first time fit was called (e.g. for subsampling or for the</span>
        <span class="s2"># train/val split).</span>
        <span class="s3">if not </span><span class="s1">(self.warm_start </span><span class="s3">and </span><span class="s1">self._is_fitted()):</span>
            <span class="s1">self._random_seed = rng.randint(np.iinfo(np.uint32).max</span><span class="s3">, </span><span class="s1">dtype=</span><span class="s4">&quot;u8&quot;</span><span class="s1">)</span>

        <span class="s1">self._validate_parameters()</span>
        <span class="s1">monotonic_cst = _check_monotonic_cst(self</span><span class="s3">, </span><span class="s1">self.monotonic_cst)</span>

        <span class="s2"># used for validation in predict</span>
        <span class="s1">n_samples</span><span class="s3">, </span><span class="s1">self._n_features = X.shape</span>

        <span class="s1">self.is_categorical_</span><span class="s3">, </span><span class="s1">known_categories = self._check_categories(X)</span>

        <span class="s2"># Encode constraints into a list of sets of features indices (integers).</span>
        <span class="s1">interaction_cst = self._check_interaction_cst(self._n_features)</span>

        <span class="s2"># we need this stateful variable to tell raw_predict() that it was</span>
        <span class="s2"># called from fit() (this current method), and that the data it has</span>
        <span class="s2"># received is pre-binned.</span>
        <span class="s2"># predicting is faster on pre-binned data, so we want early stopping</span>
        <span class="s2"># predictions to be made on pre-binned data. Unfortunately the _scorer</span>
        <span class="s2"># can only call predict() or predict_proba(), not raw_predict(), and</span>
        <span class="s2"># there's no way to tell the scorer that it needs to predict binned</span>
        <span class="s2"># data.</span>
        <span class="s1">self._in_fit = </span><span class="s3">True</span>

        <span class="s2"># `_openmp_effective_n_threads` is used to take cgroups CPU quotes</span>
        <span class="s2"># into account when determine the maximum number of threads to use.</span>
        <span class="s1">n_threads = _openmp_effective_n_threads()</span>

        <span class="s3">if </span><span class="s1">isinstance(self.loss</span><span class="s3">, </span><span class="s1">str):</span>
            <span class="s1">self._loss = self._get_loss(sample_weight=sample_weight)</span>
        <span class="s3">elif </span><span class="s1">isinstance(self.loss</span><span class="s3">, </span><span class="s1">BaseLoss):</span>
            <span class="s1">self._loss = self.loss</span>

        <span class="s3">if </span><span class="s1">self.early_stopping == </span><span class="s4">&quot;auto&quot;</span><span class="s1">:</span>
            <span class="s1">self.do_early_stopping_ = n_samples &gt; </span><span class="s5">10000</span>
        <span class="s3">else</span><span class="s1">:</span>
            <span class="s1">self.do_early_stopping_ = self.early_stopping</span>

        <span class="s2"># create validation data if needed</span>
        <span class="s1">self._use_validation_data = self.validation_fraction </span><span class="s3">is not None</span>
        <span class="s3">if </span><span class="s1">self.do_early_stopping_ </span><span class="s3">and </span><span class="s1">self._use_validation_data:</span>
            <span class="s2"># stratify for classification</span>
            <span class="s2"># instead of checking predict_proba, loss.n_classes &gt;= 2 would also work</span>
            <span class="s1">stratify = y </span><span class="s3">if </span><span class="s1">hasattr(self._loss</span><span class="s3">, </span><span class="s4">&quot;predict_proba&quot;</span><span class="s1">) </span><span class="s3">else None</span>

            <span class="s2"># Save the state of the RNG for the training and validation split.</span>
            <span class="s2"># This is needed in order to have the same split when using</span>
            <span class="s2"># warm starting.</span>

            <span class="s3">if </span><span class="s1">sample_weight </span><span class="s3">is None</span><span class="s1">:</span>
                <span class="s1">X_train</span><span class="s3">, </span><span class="s1">X_val</span><span class="s3">, </span><span class="s1">y_train</span><span class="s3">, </span><span class="s1">y_val = train_test_split(</span>
                    <span class="s1">X</span><span class="s3">,</span>
                    <span class="s1">y</span><span class="s3">,</span>
                    <span class="s1">test_size=self.validation_fraction</span><span class="s3">,</span>
                    <span class="s1">stratify=stratify</span><span class="s3">,</span>
                    <span class="s1">random_state=self._random_seed</span><span class="s3">,</span>
                <span class="s1">)</span>
                <span class="s1">sample_weight_train = sample_weight_val = </span><span class="s3">None</span>
            <span class="s3">else</span><span class="s1">:</span>
                <span class="s2"># TODO: incorporate sample_weight in sampling here, as well as</span>
                <span class="s2"># stratify</span>
                <span class="s1">(</span>
                    <span class="s1">X_train</span><span class="s3">,</span>
                    <span class="s1">X_val</span><span class="s3">,</span>
                    <span class="s1">y_train</span><span class="s3">,</span>
                    <span class="s1">y_val</span><span class="s3">,</span>
                    <span class="s1">sample_weight_train</span><span class="s3">,</span>
                    <span class="s1">sample_weight_val</span><span class="s3">,</span>
                <span class="s1">) = train_test_split(</span>
                    <span class="s1">X</span><span class="s3">,</span>
                    <span class="s1">y</span><span class="s3">,</span>
                    <span class="s1">sample_weight</span><span class="s3">,</span>
                    <span class="s1">test_size=self.validation_fraction</span><span class="s3">,</span>
                    <span class="s1">stratify=stratify</span><span class="s3">,</span>
                    <span class="s1">random_state=self._random_seed</span><span class="s3">,</span>
                <span class="s1">)</span>
        <span class="s3">else</span><span class="s1">:</span>
            <span class="s1">X_train</span><span class="s3">, </span><span class="s1">y_train</span><span class="s3">, </span><span class="s1">sample_weight_train = X</span><span class="s3">, </span><span class="s1">y</span><span class="s3">, </span><span class="s1">sample_weight</span>
            <span class="s1">X_val = y_val = sample_weight_val = </span><span class="s3">None</span>

        <span class="s2"># Bin the data</span>
        <span class="s2"># For ease of use of the API, the user-facing GBDT classes accept the</span>
        <span class="s2"># parameter max_bins, which doesn't take into account the bin for</span>
        <span class="s2"># missing values (which is always allocated). However, since max_bins</span>
        <span class="s2"># isn't the true maximal number of bins, all other private classes</span>
        <span class="s2"># (binmapper, histbuilder...) accept n_bins instead, which is the</span>
        <span class="s2"># actual total number of bins. Everywhere in the code, the</span>
        <span class="s2"># convention is that n_bins == max_bins + 1</span>
        <span class="s1">n_bins = self.max_bins + </span><span class="s5">1  </span><span class="s2"># + 1 for missing values</span>
        <span class="s1">self._bin_mapper = _BinMapper(</span>
            <span class="s1">n_bins=n_bins</span><span class="s3">,</span>
            <span class="s1">is_categorical=self.is_categorical_</span><span class="s3">,</span>
            <span class="s1">known_categories=known_categories</span><span class="s3">,</span>
            <span class="s1">random_state=self._random_seed</span><span class="s3">,</span>
            <span class="s1">n_threads=n_threads</span><span class="s3">,</span>
        <span class="s1">)</span>
        <span class="s1">X_binned_train = self._bin_data(X_train</span><span class="s3">, </span><span class="s1">is_training_data=</span><span class="s3">True</span><span class="s1">)</span>
        <span class="s3">if </span><span class="s1">X_val </span><span class="s3">is not None</span><span class="s1">:</span>
            <span class="s1">X_binned_val = self._bin_data(X_val</span><span class="s3">, </span><span class="s1">is_training_data=</span><span class="s3">False</span><span class="s1">)</span>
        <span class="s3">else</span><span class="s1">:</span>
            <span class="s1">X_binned_val = </span><span class="s3">None</span>

        <span class="s2"># Uses binned data to check for missing values</span>
        <span class="s1">has_missing_values = (</span>
            <span class="s1">(X_binned_train == self._bin_mapper.missing_values_bin_idx_)</span>
            <span class="s1">.any(axis=</span><span class="s5">0</span><span class="s1">)</span>
            <span class="s1">.astype(np.uint8)</span>
        <span class="s1">)</span>

        <span class="s3">if </span><span class="s1">self.verbose:</span>
            <span class="s1">print(</span><span class="s4">&quot;Fitting gradient boosted rounds:&quot;</span><span class="s1">)</span>

        <span class="s1">n_samples = X_binned_train.shape[</span><span class="s5">0</span><span class="s1">]</span>

        <span class="s2"># First time calling fit, or no warm start</span>
        <span class="s3">if not </span><span class="s1">(self._is_fitted() </span><span class="s3">and </span><span class="s1">self.warm_start):</span>
            <span class="s2"># Clear random state and score attributes</span>
            <span class="s1">self._clear_state()</span>

            <span class="s2"># initialize raw_predictions: those are the accumulated values</span>
            <span class="s2"># predicted by the trees for the training data. raw_predictions has</span>
            <span class="s2"># shape (n_samples, n_trees_per_iteration) where</span>
            <span class="s2"># n_trees_per_iterations is n_classes in multiclass classification,</span>
            <span class="s2"># else 1.</span>
            <span class="s2"># self._baseline_prediction has shape (1, n_trees_per_iteration)</span>
            <span class="s1">self._baseline_prediction = self._loss.fit_intercept_only(</span>
                <span class="s1">y_true=y_train</span><span class="s3">, </span><span class="s1">sample_weight=sample_weight_train</span>
            <span class="s1">).reshape((</span><span class="s5">1</span><span class="s3">, </span><span class="s1">-</span><span class="s5">1</span><span class="s1">))</span>
            <span class="s1">raw_predictions = np.zeros(</span>
                <span class="s1">shape=(n_samples</span><span class="s3">, </span><span class="s1">self.n_trees_per_iteration_)</span><span class="s3">,</span>
                <span class="s1">dtype=self._baseline_prediction.dtype</span><span class="s3">,</span>
                <span class="s1">order=</span><span class="s4">&quot;F&quot;</span><span class="s3">,</span>
            <span class="s1">)</span>
            <span class="s1">raw_predictions += self._baseline_prediction</span>

            <span class="s2"># predictors is a matrix (list of lists) of TreePredictor objects</span>
            <span class="s2"># with shape (n_iter_, n_trees_per_iteration)</span>
            <span class="s1">self._predictors = predictors = []</span>

            <span class="s2"># Initialize structures and attributes related to early stopping</span>
            <span class="s1">self._scorer = </span><span class="s3">None  </span><span class="s2"># set if scoring != loss</span>
            <span class="s1">raw_predictions_val = </span><span class="s3">None  </span><span class="s2"># set if scoring == loss and use val</span>
            <span class="s1">self.train_score_ = []</span>
            <span class="s1">self.validation_score_ = []</span>

            <span class="s3">if </span><span class="s1">self.do_early_stopping_:</span>
                <span class="s2"># populate train_score and validation_score with the</span>
                <span class="s2"># predictions of the initial model (before the first tree)</span>

                <span class="s3">if </span><span class="s1">self.scoring == </span><span class="s4">&quot;loss&quot;</span><span class="s1">:</span>
                    <span class="s2"># we're going to compute scoring w.r.t the loss. As losses</span>
                    <span class="s2"># take raw predictions as input (unlike the scorers), we</span>
                    <span class="s2"># can optimize a bit and avoid repeating computing the</span>
                    <span class="s2"># predictions of the previous trees. We'll reuse</span>
                    <span class="s2"># raw_predictions (as it's needed for training anyway) for</span>
                    <span class="s2"># evaluating the training loss, and create</span>
                    <span class="s2"># raw_predictions_val for storing the raw predictions of</span>
                    <span class="s2"># the validation data.</span>

                    <span class="s3">if </span><span class="s1">self._use_validation_data:</span>
                        <span class="s1">raw_predictions_val = np.zeros(</span>
                            <span class="s1">shape=(X_binned_val.shape[</span><span class="s5">0</span><span class="s1">]</span><span class="s3">, </span><span class="s1">self.n_trees_per_iteration_)</span><span class="s3">,</span>
                            <span class="s1">dtype=self._baseline_prediction.dtype</span><span class="s3">,</span>
                            <span class="s1">order=</span><span class="s4">&quot;F&quot;</span><span class="s3">,</span>
                        <span class="s1">)</span>

                        <span class="s1">raw_predictions_val += self._baseline_prediction</span>

                    <span class="s1">self._check_early_stopping_loss(</span>
                        <span class="s1">raw_predictions=raw_predictions</span><span class="s3">,</span>
                        <span class="s1">y_train=y_train</span><span class="s3">,</span>
                        <span class="s1">sample_weight_train=sample_weight_train</span><span class="s3">,</span>
                        <span class="s1">raw_predictions_val=raw_predictions_val</span><span class="s3">,</span>
                        <span class="s1">y_val=y_val</span><span class="s3">,</span>
                        <span class="s1">sample_weight_val=sample_weight_val</span><span class="s3">,</span>
                        <span class="s1">n_threads=n_threads</span><span class="s3">,</span>
                    <span class="s1">)</span>
                <span class="s3">else</span><span class="s1">:</span>
                    <span class="s1">self._scorer = check_scoring(self</span><span class="s3">, </span><span class="s1">self.scoring)</span>
                    <span class="s2"># _scorer is a callable with signature (est, X, y) and</span>
                    <span class="s2"># calls est.predict() or est.predict_proba() depending on</span>
                    <span class="s2"># its nature.</span>
                    <span class="s2"># Unfortunately, each call to _scorer() will compute</span>
                    <span class="s2"># the predictions of all the trees. So we use a subset of</span>
                    <span class="s2"># the training set to compute train scores.</span>

                    <span class="s2"># Compute the subsample set</span>
                    <span class="s1">(</span>
                        <span class="s1">X_binned_small_train</span><span class="s3">,</span>
                        <span class="s1">y_small_train</span><span class="s3">,</span>
                        <span class="s1">sample_weight_small_train</span><span class="s3">,</span>
                    <span class="s1">) = self._get_small_trainset(</span>
                        <span class="s1">X_binned_train</span><span class="s3">, </span><span class="s1">y_train</span><span class="s3">, </span><span class="s1">sample_weight_train</span><span class="s3">, </span><span class="s1">self._random_seed</span>
                    <span class="s1">)</span>

                    <span class="s1">self._check_early_stopping_scorer(</span>
                        <span class="s1">X_binned_small_train</span><span class="s3">,</span>
                        <span class="s1">y_small_train</span><span class="s3">,</span>
                        <span class="s1">sample_weight_small_train</span><span class="s3">,</span>
                        <span class="s1">X_binned_val</span><span class="s3">,</span>
                        <span class="s1">y_val</span><span class="s3">,</span>
                        <span class="s1">sample_weight_val</span><span class="s3">,</span>
                    <span class="s1">)</span>
            <span class="s1">begin_at_stage = </span><span class="s5">0</span>

        <span class="s2"># warm start: this is not the first time fit was called</span>
        <span class="s3">else</span><span class="s1">:</span>
            <span class="s2"># Check that the maximum number of iterations is not smaller</span>
            <span class="s2"># than the number of iterations from the previous fit</span>
            <span class="s3">if </span><span class="s1">self.max_iter &lt; self.n_iter_:</span>
                <span class="s3">raise </span><span class="s1">ValueError(</span>
                    <span class="s4">&quot;max_iter=%d must be larger than or equal to &quot;</span>
                    <span class="s4">&quot;n_iter_=%d when warm_start==True&quot; </span><span class="s1">% (self.max_iter</span><span class="s3">, </span><span class="s1">self.n_iter_)</span>
                <span class="s1">)</span>

            <span class="s2"># Convert array attributes to lists</span>
            <span class="s1">self.train_score_ = self.train_score_.tolist()</span>
            <span class="s1">self.validation_score_ = self.validation_score_.tolist()</span>

            <span class="s2"># Compute raw predictions</span>
            <span class="s1">raw_predictions = self._raw_predict(X_binned_train</span><span class="s3">, </span><span class="s1">n_threads=n_threads)</span>
            <span class="s3">if </span><span class="s1">self.do_early_stopping_ </span><span class="s3">and </span><span class="s1">self._use_validation_data:</span>
                <span class="s1">raw_predictions_val = self._raw_predict(</span>
                    <span class="s1">X_binned_val</span><span class="s3">, </span><span class="s1">n_threads=n_threads</span>
                <span class="s1">)</span>
            <span class="s3">else</span><span class="s1">:</span>
                <span class="s1">raw_predictions_val = </span><span class="s3">None</span>

            <span class="s3">if </span><span class="s1">self.do_early_stopping_ </span><span class="s3">and </span><span class="s1">self.scoring != </span><span class="s4">&quot;loss&quot;</span><span class="s1">:</span>
                <span class="s2"># Compute the subsample set</span>
                <span class="s1">(</span>
                    <span class="s1">X_binned_small_train</span><span class="s3">,</span>
                    <span class="s1">y_small_train</span><span class="s3">,</span>
                    <span class="s1">sample_weight_small_train</span><span class="s3">,</span>
                <span class="s1">) = self._get_small_trainset(</span>
                    <span class="s1">X_binned_train</span><span class="s3">, </span><span class="s1">y_train</span><span class="s3">, </span><span class="s1">sample_weight_train</span><span class="s3">, </span><span class="s1">self._random_seed</span>
                <span class="s1">)</span>

            <span class="s2"># Get the predictors from the previous fit</span>
            <span class="s1">predictors = self._predictors</span>

            <span class="s1">begin_at_stage = self.n_iter_</span>

        <span class="s2"># initialize gradients and hessians (empty arrays).</span>
        <span class="s2"># shape = (n_samples, n_trees_per_iteration).</span>
        <span class="s1">gradient</span><span class="s3">, </span><span class="s1">hessian = self._loss.init_gradient_and_hessian(</span>
            <span class="s1">n_samples=n_samples</span><span class="s3">, </span><span class="s1">dtype=G_H_DTYPE</span><span class="s3">, </span><span class="s1">order=</span><span class="s4">&quot;F&quot;</span>
        <span class="s1">)</span>

        <span class="s3">for </span><span class="s1">iteration </span><span class="s3">in </span><span class="s1">range(begin_at_stage</span><span class="s3">, </span><span class="s1">self.max_iter):</span>
            <span class="s3">if </span><span class="s1">self.verbose:</span>
                <span class="s1">iteration_start_time = time()</span>
                <span class="s1">print(</span>
                    <span class="s4">&quot;[{}/{}] &quot;</span><span class="s1">.format(iteration + </span><span class="s5">1</span><span class="s3">, </span><span class="s1">self.max_iter)</span><span class="s3">, </span><span class="s1">end=</span><span class="s4">&quot;&quot;</span><span class="s3">, </span><span class="s1">flush=</span><span class="s3">True</span>
                <span class="s1">)</span>

            <span class="s2"># Update gradients and hessians, inplace</span>
            <span class="s2"># Note that self._loss expects shape (n_samples,) for</span>
            <span class="s2"># n_trees_per_iteration = 1 else shape (n_samples, n_trees_per_iteration).</span>
            <span class="s3">if </span><span class="s1">self._loss.constant_hessian:</span>
                <span class="s1">self._loss.gradient(</span>
                    <span class="s1">y_true=y_train</span><span class="s3">,</span>
                    <span class="s1">raw_prediction=raw_predictions</span><span class="s3">,</span>
                    <span class="s1">sample_weight=sample_weight_train</span><span class="s3">,</span>
                    <span class="s1">gradient_out=gradient</span><span class="s3">,</span>
                    <span class="s1">n_threads=n_threads</span><span class="s3">,</span>
                <span class="s1">)</span>
            <span class="s3">else</span><span class="s1">:</span>
                <span class="s1">self._loss.gradient_hessian(</span>
                    <span class="s1">y_true=y_train</span><span class="s3">,</span>
                    <span class="s1">raw_prediction=raw_predictions</span><span class="s3">,</span>
                    <span class="s1">sample_weight=sample_weight_train</span><span class="s3">,</span>
                    <span class="s1">gradient_out=gradient</span><span class="s3">,</span>
                    <span class="s1">hessian_out=hessian</span><span class="s3">,</span>
                    <span class="s1">n_threads=n_threads</span><span class="s3">,</span>
                <span class="s1">)</span>

            <span class="s2"># Append a list since there may be more than 1 predictor per iter</span>
            <span class="s1">predictors.append([])</span>

            <span class="s2"># 2-d views of shape (n_samples, n_trees_per_iteration_) or (n_samples, 1)</span>
            <span class="s2"># on gradient and hessian to simplify the loop over n_trees_per_iteration_.</span>
            <span class="s3">if </span><span class="s1">gradient.ndim == </span><span class="s5">1</span><span class="s1">:</span>
                <span class="s1">g_view = gradient.reshape((-</span><span class="s5">1</span><span class="s3">, </span><span class="s5">1</span><span class="s1">))</span>
                <span class="s1">h_view = hessian.reshape((-</span><span class="s5">1</span><span class="s3">, </span><span class="s5">1</span><span class="s1">))</span>
            <span class="s3">else</span><span class="s1">:</span>
                <span class="s1">g_view = gradient</span>
                <span class="s1">h_view = hessian</span>

            <span class="s2"># Build `n_trees_per_iteration` trees.</span>
            <span class="s3">for </span><span class="s1">k </span><span class="s3">in </span><span class="s1">range(self.n_trees_per_iteration_):</span>
                <span class="s1">grower = TreeGrower(</span>
                    <span class="s1">X_binned=X_binned_train</span><span class="s3">,</span>
                    <span class="s1">gradients=g_view[:</span><span class="s3">, </span><span class="s1">k]</span><span class="s3">,</span>
                    <span class="s1">hessians=h_view[:</span><span class="s3">, </span><span class="s1">k]</span><span class="s3">,</span>
                    <span class="s1">n_bins=n_bins</span><span class="s3">,</span>
                    <span class="s1">n_bins_non_missing=self._bin_mapper.n_bins_non_missing_</span><span class="s3">,</span>
                    <span class="s1">has_missing_values=has_missing_values</span><span class="s3">,</span>
                    <span class="s1">is_categorical=self.is_categorical_</span><span class="s3">,</span>
                    <span class="s1">monotonic_cst=monotonic_cst</span><span class="s3">,</span>
                    <span class="s1">interaction_cst=interaction_cst</span><span class="s3">,</span>
                    <span class="s1">max_leaf_nodes=self.max_leaf_nodes</span><span class="s3">,</span>
                    <span class="s1">max_depth=self.max_depth</span><span class="s3">,</span>
                    <span class="s1">min_samples_leaf=self.min_samples_leaf</span><span class="s3">,</span>
                    <span class="s1">l2_regularization=self.l2_regularization</span><span class="s3">,</span>
                    <span class="s1">shrinkage=self.learning_rate</span><span class="s3">,</span>
                    <span class="s1">n_threads=n_threads</span><span class="s3">,</span>
                <span class="s1">)</span>
                <span class="s1">grower.grow()</span>

                <span class="s1">acc_apply_split_time += grower.total_apply_split_time</span>
                <span class="s1">acc_find_split_time += grower.total_find_split_time</span>
                <span class="s1">acc_compute_hist_time += grower.total_compute_hist_time</span>

                <span class="s3">if </span><span class="s1">self._loss.need_update_leaves_values:</span>
                    <span class="s1">_update_leaves_values(</span>
                        <span class="s1">loss=self._loss</span><span class="s3">,</span>
                        <span class="s1">grower=grower</span><span class="s3">,</span>
                        <span class="s1">y_true=y_train</span><span class="s3">,</span>
                        <span class="s1">raw_prediction=raw_predictions[:</span><span class="s3">, </span><span class="s1">k]</span><span class="s3">,</span>
                        <span class="s1">sample_weight=sample_weight_train</span><span class="s3">,</span>
                    <span class="s1">)</span>

                <span class="s1">predictor = grower.make_predictor(</span>
                    <span class="s1">binning_thresholds=self._bin_mapper.bin_thresholds_</span>
                <span class="s1">)</span>
                <span class="s1">predictors[-</span><span class="s5">1</span><span class="s1">].append(predictor)</span>

                <span class="s2"># Update raw_predictions with the predictions of the newly</span>
                <span class="s2"># created tree.</span>
                <span class="s1">tic_pred = time()</span>
                <span class="s1">_update_raw_predictions(raw_predictions[:</span><span class="s3">, </span><span class="s1">k]</span><span class="s3">, </span><span class="s1">grower</span><span class="s3">, </span><span class="s1">n_threads)</span>
                <span class="s1">toc_pred = time()</span>
                <span class="s1">acc_prediction_time += toc_pred - tic_pred</span>

            <span class="s1">should_early_stop = </span><span class="s3">False</span>
            <span class="s3">if </span><span class="s1">self.do_early_stopping_:</span>
                <span class="s3">if </span><span class="s1">self.scoring == </span><span class="s4">&quot;loss&quot;</span><span class="s1">:</span>
                    <span class="s2"># Update raw_predictions_val with the newest tree(s)</span>
                    <span class="s3">if </span><span class="s1">self._use_validation_data:</span>
                        <span class="s3">for </span><span class="s1">k</span><span class="s3">, </span><span class="s1">pred </span><span class="s3">in </span><span class="s1">enumerate(self._predictors[-</span><span class="s5">1</span><span class="s1">]):</span>
                            <span class="s1">raw_predictions_val[:</span><span class="s3">, </span><span class="s1">k] += pred.predict_binned(</span>
                                <span class="s1">X_binned_val</span><span class="s3">,</span>
                                <span class="s1">self._bin_mapper.missing_values_bin_idx_</span><span class="s3">,</span>
                                <span class="s1">n_threads</span><span class="s3">,</span>
                            <span class="s1">)</span>

                    <span class="s1">should_early_stop = self._check_early_stopping_loss(</span>
                        <span class="s1">raw_predictions=raw_predictions</span><span class="s3">,</span>
                        <span class="s1">y_train=y_train</span><span class="s3">,</span>
                        <span class="s1">sample_weight_train=sample_weight_train</span><span class="s3">,</span>
                        <span class="s1">raw_predictions_val=raw_predictions_val</span><span class="s3">,</span>
                        <span class="s1">y_val=y_val</span><span class="s3">,</span>
                        <span class="s1">sample_weight_val=sample_weight_val</span><span class="s3">,</span>
                        <span class="s1">n_threads=n_threads</span><span class="s3">,</span>
                    <span class="s1">)</span>

                <span class="s3">else</span><span class="s1">:</span>
                    <span class="s1">should_early_stop = self._check_early_stopping_scorer(</span>
                        <span class="s1">X_binned_small_train</span><span class="s3">,</span>
                        <span class="s1">y_small_train</span><span class="s3">,</span>
                        <span class="s1">sample_weight_small_train</span><span class="s3">,</span>
                        <span class="s1">X_binned_val</span><span class="s3">,</span>
                        <span class="s1">y_val</span><span class="s3">,</span>
                        <span class="s1">sample_weight_val</span><span class="s3">,</span>
                    <span class="s1">)</span>

            <span class="s3">if </span><span class="s1">self.verbose:</span>
                <span class="s1">self._print_iteration_stats(iteration_start_time)</span>

            <span class="s2"># maybe we could also early stop if all the trees are stumps?</span>
            <span class="s3">if </span><span class="s1">should_early_stop:</span>
                <span class="s3">break</span>

        <span class="s3">if </span><span class="s1">self.verbose:</span>
            <span class="s1">duration = time() - fit_start_time</span>
            <span class="s1">n_total_leaves = sum(</span>
                <span class="s1">predictor.get_n_leaf_nodes()</span>
                <span class="s3">for </span><span class="s1">predictors_at_ith_iteration </span><span class="s3">in </span><span class="s1">self._predictors</span>
                <span class="s3">for </span><span class="s1">predictor </span><span class="s3">in </span><span class="s1">predictors_at_ith_iteration</span>
            <span class="s1">)</span>
            <span class="s1">n_predictors = sum(</span>
                <span class="s1">len(predictors_at_ith_iteration)</span>
                <span class="s3">for </span><span class="s1">predictors_at_ith_iteration </span><span class="s3">in </span><span class="s1">self._predictors</span>
            <span class="s1">)</span>
            <span class="s1">print(</span>
                <span class="s4">&quot;Fit {} trees in {:.3f} s, ({} total leaves)&quot;</span><span class="s1">.format(</span>
                    <span class="s1">n_predictors</span><span class="s3">, </span><span class="s1">duration</span><span class="s3">, </span><span class="s1">n_total_leaves</span>
                <span class="s1">)</span>
            <span class="s1">)</span>
            <span class="s1">print(</span>
                <span class="s4">&quot;{:&lt;32} {:.3f}s&quot;</span><span class="s1">.format(</span>
                    <span class="s4">&quot;Time spent computing histograms:&quot;</span><span class="s3">, </span><span class="s1">acc_compute_hist_time</span>
                <span class="s1">)</span>
            <span class="s1">)</span>
            <span class="s1">print(</span>
                <span class="s4">&quot;{:&lt;32} {:.3f}s&quot;</span><span class="s1">.format(</span>
                    <span class="s4">&quot;Time spent finding best splits:&quot;</span><span class="s3">, </span><span class="s1">acc_find_split_time</span>
                <span class="s1">)</span>
            <span class="s1">)</span>
            <span class="s1">print(</span>
                <span class="s4">&quot;{:&lt;32} {:.3f}s&quot;</span><span class="s1">.format(</span>
                    <span class="s4">&quot;Time spent applying splits:&quot;</span><span class="s3">, </span><span class="s1">acc_apply_split_time</span>
                <span class="s1">)</span>
            <span class="s1">)</span>
            <span class="s1">print(</span>
                <span class="s4">&quot;{:&lt;32} {:.3f}s&quot;</span><span class="s1">.format(</span><span class="s4">&quot;Time spent predicting:&quot;</span><span class="s3">, </span><span class="s1">acc_prediction_time)</span>
            <span class="s1">)</span>

        <span class="s1">self.train_score_ = np.asarray(self.train_score_)</span>
        <span class="s1">self.validation_score_ = np.asarray(self.validation_score_)</span>
        <span class="s3">del </span><span class="s1">self._in_fit  </span><span class="s2"># hard delete so we're sure it can't be used anymore</span>
        <span class="s3">return </span><span class="s1">self</span>

    <span class="s3">def </span><span class="s1">_is_fitted(self):</span>
        <span class="s3">return </span><span class="s1">len(getattr(self</span><span class="s3">, </span><span class="s4">&quot;_predictors&quot;</span><span class="s3">, </span><span class="s1">[])) &gt; </span><span class="s5">0</span>

    <span class="s3">def </span><span class="s1">_clear_state(self):</span>
        <span class="s0">&quot;&quot;&quot;Clear the state of the gradient boosting model.&quot;&quot;&quot;</span>
        <span class="s3">for </span><span class="s1">var </span><span class="s3">in </span><span class="s1">(</span><span class="s4">&quot;train_score_&quot;</span><span class="s3">, </span><span class="s4">&quot;validation_score_&quot;</span><span class="s1">):</span>
            <span class="s3">if </span><span class="s1">hasattr(self</span><span class="s3">, </span><span class="s1">var):</span>
                <span class="s1">delattr(self</span><span class="s3">, </span><span class="s1">var)</span>

    <span class="s3">def </span><span class="s1">_get_small_trainset(self</span><span class="s3">, </span><span class="s1">X_binned_train</span><span class="s3">, </span><span class="s1">y_train</span><span class="s3">, </span><span class="s1">sample_weight_train</span><span class="s3">, </span><span class="s1">seed):</span>
        <span class="s0">&quot;&quot;&quot;Compute the indices of the subsample set and return this set. 
 
        For efficiency, we need to subsample the training set to compute scores 
        with scorers. 
        &quot;&quot;&quot;</span>
        <span class="s2"># TODO: incorporate sample_weights here in `resample`</span>
        <span class="s1">subsample_size = </span><span class="s5">10000</span>
        <span class="s3">if </span><span class="s1">X_binned_train.shape[</span><span class="s5">0</span><span class="s1">] &gt; subsample_size:</span>
            <span class="s1">indices = np.arange(X_binned_train.shape[</span><span class="s5">0</span><span class="s1">])</span>
            <span class="s1">stratify = y_train </span><span class="s3">if </span><span class="s1">is_classifier(self) </span><span class="s3">else None</span>
            <span class="s1">indices = resample(</span>
                <span class="s1">indices</span><span class="s3">,</span>
                <span class="s1">n_samples=subsample_size</span><span class="s3">,</span>
                <span class="s1">replace=</span><span class="s3">False,</span>
                <span class="s1">random_state=seed</span><span class="s3">,</span>
                <span class="s1">stratify=stratify</span><span class="s3">,</span>
            <span class="s1">)</span>
            <span class="s1">X_binned_small_train = X_binned_train[indices]</span>
            <span class="s1">y_small_train = y_train[indices]</span>
            <span class="s3">if </span><span class="s1">sample_weight_train </span><span class="s3">is not None</span><span class="s1">:</span>
                <span class="s1">sample_weight_small_train = sample_weight_train[indices]</span>
            <span class="s3">else</span><span class="s1">:</span>
                <span class="s1">sample_weight_small_train = </span><span class="s3">None</span>
            <span class="s1">X_binned_small_train = np.ascontiguousarray(X_binned_small_train)</span>
            <span class="s3">return </span><span class="s1">(X_binned_small_train</span><span class="s3">, </span><span class="s1">y_small_train</span><span class="s3">, </span><span class="s1">sample_weight_small_train)</span>
        <span class="s3">else</span><span class="s1">:</span>
            <span class="s3">return </span><span class="s1">X_binned_train</span><span class="s3">, </span><span class="s1">y_train</span><span class="s3">, </span><span class="s1">sample_weight_train</span>

    <span class="s3">def </span><span class="s1">_check_early_stopping_scorer(</span>
        <span class="s1">self</span><span class="s3">,</span>
        <span class="s1">X_binned_small_train</span><span class="s3">,</span>
        <span class="s1">y_small_train</span><span class="s3">,</span>
        <span class="s1">sample_weight_small_train</span><span class="s3">,</span>
        <span class="s1">X_binned_val</span><span class="s3">,</span>
        <span class="s1">y_val</span><span class="s3">,</span>
        <span class="s1">sample_weight_val</span><span class="s3">,</span>
    <span class="s1">):</span>
        <span class="s0">&quot;&quot;&quot;Check if fitting should be early-stopped based on scorer. 
 
        Scores are computed on validation data or on training data. 
        &quot;&quot;&quot;</span>
        <span class="s3">if </span><span class="s1">is_classifier(self):</span>
            <span class="s1">y_small_train = self.classes_[y_small_train.astype(int)]</span>

        <span class="s3">if </span><span class="s1">sample_weight_small_train </span><span class="s3">is None</span><span class="s1">:</span>
            <span class="s1">self.train_score_.append(</span>
                <span class="s1">self._scorer(self</span><span class="s3">, </span><span class="s1">X_binned_small_train</span><span class="s3">, </span><span class="s1">y_small_train)</span>
            <span class="s1">)</span>
        <span class="s3">else</span><span class="s1">:</span>
            <span class="s1">self.train_score_.append(</span>
                <span class="s1">self._scorer(</span>
                    <span class="s1">self</span><span class="s3">,</span>
                    <span class="s1">X_binned_small_train</span><span class="s3">,</span>
                    <span class="s1">y_small_train</span><span class="s3">,</span>
                    <span class="s1">sample_weight=sample_weight_small_train</span><span class="s3">,</span>
                <span class="s1">)</span>
            <span class="s1">)</span>

        <span class="s3">if </span><span class="s1">self._use_validation_data:</span>
            <span class="s3">if </span><span class="s1">is_classifier(self):</span>
                <span class="s1">y_val = self.classes_[y_val.astype(int)]</span>
            <span class="s3">if </span><span class="s1">sample_weight_val </span><span class="s3">is None</span><span class="s1">:</span>
                <span class="s1">self.validation_score_.append(self._scorer(self</span><span class="s3">, </span><span class="s1">X_binned_val</span><span class="s3">, </span><span class="s1">y_val))</span>
            <span class="s3">else</span><span class="s1">:</span>
                <span class="s1">self.validation_score_.append(</span>
                    <span class="s1">self._scorer(</span>
                        <span class="s1">self</span><span class="s3">, </span><span class="s1">X_binned_val</span><span class="s3">, </span><span class="s1">y_val</span><span class="s3">, </span><span class="s1">sample_weight=sample_weight_val</span>
                    <span class="s1">)</span>
                <span class="s1">)</span>
            <span class="s3">return </span><span class="s1">self._should_stop(self.validation_score_)</span>
        <span class="s3">else</span><span class="s1">:</span>
            <span class="s3">return </span><span class="s1">self._should_stop(self.train_score_)</span>

    <span class="s3">def </span><span class="s1">_check_early_stopping_loss(</span>
        <span class="s1">self</span><span class="s3">,</span>
        <span class="s1">raw_predictions</span><span class="s3">,</span>
        <span class="s1">y_train</span><span class="s3">,</span>
        <span class="s1">sample_weight_train</span><span class="s3">,</span>
        <span class="s1">raw_predictions_val</span><span class="s3">,</span>
        <span class="s1">y_val</span><span class="s3">,</span>
        <span class="s1">sample_weight_val</span><span class="s3">,</span>
        <span class="s1">n_threads=</span><span class="s5">1</span><span class="s3">,</span>
    <span class="s1">):</span>
        <span class="s0">&quot;&quot;&quot;Check if fitting should be early-stopped based on loss. 
 
        Scores are computed on validation data or on training data. 
        &quot;&quot;&quot;</span>
        <span class="s1">self.train_score_.append(</span>
            <span class="s1">-self._loss(</span>
                <span class="s1">y_true=y_train</span><span class="s3">,</span>
                <span class="s1">raw_prediction=raw_predictions</span><span class="s3">,</span>
                <span class="s1">sample_weight=sample_weight_train</span><span class="s3">,</span>
                <span class="s1">n_threads=n_threads</span><span class="s3">,</span>
            <span class="s1">)</span>
        <span class="s1">)</span>

        <span class="s3">if </span><span class="s1">self._use_validation_data:</span>
            <span class="s1">self.validation_score_.append(</span>
                <span class="s1">-self._loss(</span>
                    <span class="s1">y_true=y_val</span><span class="s3">,</span>
                    <span class="s1">raw_prediction=raw_predictions_val</span><span class="s3">,</span>
                    <span class="s1">sample_weight=sample_weight_val</span><span class="s3">,</span>
                    <span class="s1">n_threads=n_threads</span><span class="s3">,</span>
                <span class="s1">)</span>
            <span class="s1">)</span>
            <span class="s3">return </span><span class="s1">self._should_stop(self.validation_score_)</span>
        <span class="s3">else</span><span class="s1">:</span>
            <span class="s3">return </span><span class="s1">self._should_stop(self.train_score_)</span>

    <span class="s3">def </span><span class="s1">_should_stop(self</span><span class="s3">, </span><span class="s1">scores):</span>
        <span class="s0">&quot;&quot;&quot; 
        Return True (do early stopping) if the last n scores aren't better 
        than the (n-1)th-to-last score, up to some tolerance. 
        &quot;&quot;&quot;</span>
        <span class="s1">reference_position = self.n_iter_no_change + </span><span class="s5">1</span>
        <span class="s3">if </span><span class="s1">len(scores) &lt; reference_position:</span>
            <span class="s3">return False</span>

        <span class="s2"># A higher score is always better. Higher tol means that it will be</span>
        <span class="s2"># harder for subsequent iteration to be considered an improvement upon</span>
        <span class="s2"># the reference score, and therefore it is more likely to early stop</span>
        <span class="s2"># because of the lack of significant improvement.</span>
        <span class="s1">reference_score = scores[-reference_position] + self.tol</span>
        <span class="s1">recent_scores = scores[-reference_position + </span><span class="s5">1 </span><span class="s1">:]</span>
        <span class="s1">recent_improvements = [score &gt; reference_score </span><span class="s3">for </span><span class="s1">score </span><span class="s3">in </span><span class="s1">recent_scores]</span>
        <span class="s3">return not </span><span class="s1">any(recent_improvements)</span>

    <span class="s3">def </span><span class="s1">_bin_data(self</span><span class="s3">, </span><span class="s1">X</span><span class="s3">, </span><span class="s1">is_training_data):</span>
        <span class="s0">&quot;&quot;&quot;Bin data X. 
 
        If is_training_data, then fit the _bin_mapper attribute. 
        Else, the binned data is converted to a C-contiguous array. 
        &quot;&quot;&quot;</span>

        <span class="s1">description = </span><span class="s4">&quot;training&quot; </span><span class="s3">if </span><span class="s1">is_training_data </span><span class="s3">else </span><span class="s4">&quot;validation&quot;</span>
        <span class="s3">if </span><span class="s1">self.verbose:</span>
            <span class="s1">print(</span>
                <span class="s4">&quot;Binning {:.3f} GB of {} data: &quot;</span><span class="s1">.format(X.nbytes / </span><span class="s5">1e9</span><span class="s3">, </span><span class="s1">description)</span><span class="s3">,</span>
                <span class="s1">end=</span><span class="s4">&quot;&quot;</span><span class="s3">,</span>
                <span class="s1">flush=</span><span class="s3">True,</span>
            <span class="s1">)</span>
        <span class="s1">tic = time()</span>
        <span class="s3">if </span><span class="s1">is_training_data:</span>
            <span class="s1">X_binned = self._bin_mapper.fit_transform(X)  </span><span class="s2"># F-aligned array</span>
        <span class="s3">else</span><span class="s1">:</span>
            <span class="s1">X_binned = self._bin_mapper.transform(X)  </span><span class="s2"># F-aligned array</span>
            <span class="s2"># We convert the array to C-contiguous since predicting is faster</span>
            <span class="s2"># with this layout (training is faster on F-arrays though)</span>
            <span class="s1">X_binned = np.ascontiguousarray(X_binned)</span>
        <span class="s1">toc = time()</span>
        <span class="s3">if </span><span class="s1">self.verbose:</span>
            <span class="s1">duration = toc - tic</span>
            <span class="s1">print(</span><span class="s4">&quot;{:.3f} s&quot;</span><span class="s1">.format(duration))</span>

        <span class="s3">return </span><span class="s1">X_binned</span>

    <span class="s3">def </span><span class="s1">_print_iteration_stats(self</span><span class="s3">, </span><span class="s1">iteration_start_time):</span>
        <span class="s0">&quot;&quot;&quot;Print info about the current fitting iteration.&quot;&quot;&quot;</span>
        <span class="s1">log_msg = </span><span class="s4">&quot;&quot;</span>

        <span class="s1">predictors_of_ith_iteration = [</span>
            <span class="s1">predictors_list</span>
            <span class="s3">for </span><span class="s1">predictors_list </span><span class="s3">in </span><span class="s1">self._predictors[-</span><span class="s5">1</span><span class="s1">]</span>
            <span class="s3">if </span><span class="s1">predictors_list</span>
        <span class="s1">]</span>
        <span class="s1">n_trees = len(predictors_of_ith_iteration)</span>
        <span class="s1">max_depth = max(</span>
            <span class="s1">predictor.get_max_depth() </span><span class="s3">for </span><span class="s1">predictor </span><span class="s3">in </span><span class="s1">predictors_of_ith_iteration</span>
        <span class="s1">)</span>
        <span class="s1">n_leaves = sum(</span>
            <span class="s1">predictor.get_n_leaf_nodes() </span><span class="s3">for </span><span class="s1">predictor </span><span class="s3">in </span><span class="s1">predictors_of_ith_iteration</span>
        <span class="s1">)</span>

        <span class="s3">if </span><span class="s1">n_trees == </span><span class="s5">1</span><span class="s1">:</span>
            <span class="s1">log_msg += </span><span class="s4">&quot;{} tree, {} leaves, &quot;</span><span class="s1">.format(n_trees</span><span class="s3">, </span><span class="s1">n_leaves)</span>
        <span class="s3">else</span><span class="s1">:</span>
            <span class="s1">log_msg += </span><span class="s4">&quot;{} trees, {} leaves &quot;</span><span class="s1">.format(n_trees</span><span class="s3">, </span><span class="s1">n_leaves)</span>
            <span class="s1">log_msg += </span><span class="s4">&quot;({} on avg), &quot;</span><span class="s1">.format(int(n_leaves / n_trees))</span>

        <span class="s1">log_msg += </span><span class="s4">&quot;max depth = {}, &quot;</span><span class="s1">.format(max_depth)</span>

        <span class="s3">if </span><span class="s1">self.do_early_stopping_:</span>
            <span class="s3">if </span><span class="s1">self.scoring == </span><span class="s4">&quot;loss&quot;</span><span class="s1">:</span>
                <span class="s1">factor = -</span><span class="s5">1  </span><span class="s2"># score_ arrays contain the negative loss</span>
                <span class="s1">name = </span><span class="s4">&quot;loss&quot;</span>
            <span class="s3">else</span><span class="s1">:</span>
                <span class="s1">factor = </span><span class="s5">1</span>
                <span class="s1">name = </span><span class="s4">&quot;score&quot;</span>
            <span class="s1">log_msg += </span><span class="s4">&quot;train {}: {:.5f}, &quot;</span><span class="s1">.format(name</span><span class="s3">, </span><span class="s1">factor * self.train_score_[-</span><span class="s5">1</span><span class="s1">])</span>
            <span class="s3">if </span><span class="s1">self._use_validation_data:</span>
                <span class="s1">log_msg += </span><span class="s4">&quot;val {}: {:.5f}, &quot;</span><span class="s1">.format(</span>
                    <span class="s1">name</span><span class="s3">, </span><span class="s1">factor * self.validation_score_[-</span><span class="s5">1</span><span class="s1">]</span>
                <span class="s1">)</span>

        <span class="s1">iteration_time = time() - iteration_start_time</span>
        <span class="s1">log_msg += </span><span class="s4">&quot;in {:0.3f}s&quot;</span><span class="s1">.format(iteration_time)</span>

        <span class="s1">print(log_msg)</span>

    <span class="s3">def </span><span class="s1">_raw_predict(self</span><span class="s3">, </span><span class="s1">X</span><span class="s3">, </span><span class="s1">n_threads=</span><span class="s3">None</span><span class="s1">):</span>
        <span class="s0">&quot;&quot;&quot;Return the sum of the leaves values over all predictors. 
 
        Parameters 
        ---------- 
        X : array-like of shape (n_samples, n_features) 
            The input samples. 
        n_threads : int, default=None 
            Number of OpenMP threads to use. `_openmp_effective_n_threads` is called 
            to determine the effective number of threads use, which takes cgroups CPU 
            quotes into account. See the docstring of `_openmp_effective_n_threads` 
            for details. 
 
        Returns 
        ------- 
        raw_predictions : array, shape (n_samples, n_trees_per_iteration) 
            The raw predicted values. 
        &quot;&quot;&quot;</span>
        <span class="s1">is_binned = getattr(self</span><span class="s3">, </span><span class="s4">&quot;_in_fit&quot;</span><span class="s3">, False</span><span class="s1">)</span>
        <span class="s3">if not </span><span class="s1">is_binned:</span>
            <span class="s1">X = self._validate_data(</span>
                <span class="s1">X</span><span class="s3">, </span><span class="s1">dtype=X_DTYPE</span><span class="s3">, </span><span class="s1">force_all_finite=</span><span class="s3">False, </span><span class="s1">reset=</span><span class="s3">False</span>
            <span class="s1">)</span>
        <span class="s1">check_is_fitted(self)</span>
        <span class="s3">if </span><span class="s1">X.shape[</span><span class="s5">1</span><span class="s1">] != self._n_features:</span>
            <span class="s3">raise </span><span class="s1">ValueError(</span>
                <span class="s4">&quot;X has {} features but this estimator was trained with &quot;</span>
                <span class="s4">&quot;{} features.&quot;</span><span class="s1">.format(X.shape[</span><span class="s5">1</span><span class="s1">]</span><span class="s3">, </span><span class="s1">self._n_features)</span>
            <span class="s1">)</span>
        <span class="s1">n_samples = X.shape[</span><span class="s5">0</span><span class="s1">]</span>
        <span class="s1">raw_predictions = np.zeros(</span>
            <span class="s1">shape=(n_samples</span><span class="s3">, </span><span class="s1">self.n_trees_per_iteration_)</span><span class="s3">,</span>
            <span class="s1">dtype=self._baseline_prediction.dtype</span><span class="s3">,</span>
            <span class="s1">order=</span><span class="s4">&quot;F&quot;</span><span class="s3">,</span>
        <span class="s1">)</span>
        <span class="s1">raw_predictions += self._baseline_prediction</span>

        <span class="s2"># We intentionally decouple the number of threads used at prediction</span>
        <span class="s2"># time from the number of threads used at fit time because the model</span>
        <span class="s2"># can be deployed on a different machine for prediction purposes.</span>
        <span class="s1">n_threads = _openmp_effective_n_threads(n_threads)</span>
        <span class="s1">self._predict_iterations(</span>
            <span class="s1">X</span><span class="s3">, </span><span class="s1">self._predictors</span><span class="s3">, </span><span class="s1">raw_predictions</span><span class="s3">, </span><span class="s1">is_binned</span><span class="s3">, </span><span class="s1">n_threads</span>
        <span class="s1">)</span>
        <span class="s3">return </span><span class="s1">raw_predictions</span>

    <span class="s3">def </span><span class="s1">_predict_iterations(self</span><span class="s3">, </span><span class="s1">X</span><span class="s3">, </span><span class="s1">predictors</span><span class="s3">, </span><span class="s1">raw_predictions</span><span class="s3">, </span><span class="s1">is_binned</span><span class="s3">, </span><span class="s1">n_threads):</span>
        <span class="s0">&quot;&quot;&quot;Add the predictions of the predictors to raw_predictions.&quot;&quot;&quot;</span>
        <span class="s3">if not </span><span class="s1">is_binned:</span>
            <span class="s1">(</span>
                <span class="s1">known_cat_bitsets</span><span class="s3">,</span>
                <span class="s1">f_idx_map</span><span class="s3">,</span>
            <span class="s1">) = self._bin_mapper.make_known_categories_bitsets()</span>

        <span class="s3">for </span><span class="s1">predictors_of_ith_iteration </span><span class="s3">in </span><span class="s1">predictors:</span>
            <span class="s3">for </span><span class="s1">k</span><span class="s3">, </span><span class="s1">predictor </span><span class="s3">in </span><span class="s1">enumerate(predictors_of_ith_iteration):</span>
                <span class="s3">if </span><span class="s1">is_binned:</span>
                    <span class="s1">predict = partial(</span>
                        <span class="s1">predictor.predict_binned</span><span class="s3">,</span>
                        <span class="s1">missing_values_bin_idx=self._bin_mapper.missing_values_bin_idx_</span><span class="s3">,</span>
                        <span class="s1">n_threads=n_threads</span><span class="s3">,</span>
                    <span class="s1">)</span>
                <span class="s3">else</span><span class="s1">:</span>
                    <span class="s1">predict = partial(</span>
                        <span class="s1">predictor.predict</span><span class="s3">,</span>
                        <span class="s1">known_cat_bitsets=known_cat_bitsets</span><span class="s3">,</span>
                        <span class="s1">f_idx_map=f_idx_map</span><span class="s3">,</span>
                        <span class="s1">n_threads=n_threads</span><span class="s3">,</span>
                    <span class="s1">)</span>
                <span class="s1">raw_predictions[:</span><span class="s3">, </span><span class="s1">k] += predict(X)</span>

    <span class="s3">def </span><span class="s1">_staged_raw_predict(self</span><span class="s3">, </span><span class="s1">X):</span>
        <span class="s0">&quot;&quot;&quot;Compute raw predictions of ``X`` for each iteration. 
 
        This method allows monitoring (i.e. determine error on testing set) 
        after each stage. 
 
        Parameters 
        ---------- 
        X : array-like of shape (n_samples, n_features) 
            The input samples. 
 
        Yields 
        ------ 
        raw_predictions : generator of ndarray of shape \ 
            (n_samples, n_trees_per_iteration) 
            The raw predictions of the input samples. The order of the 
            classes corresponds to that in the attribute :term:`classes_`. 
        &quot;&quot;&quot;</span>
        <span class="s1">X = self._validate_data(X</span><span class="s3">, </span><span class="s1">dtype=X_DTYPE</span><span class="s3">, </span><span class="s1">force_all_finite=</span><span class="s3">False, </span><span class="s1">reset=</span><span class="s3">False</span><span class="s1">)</span>
        <span class="s1">check_is_fitted(self)</span>
        <span class="s3">if </span><span class="s1">X.shape[</span><span class="s5">1</span><span class="s1">] != self._n_features:</span>
            <span class="s3">raise </span><span class="s1">ValueError(</span>
                <span class="s4">&quot;X has {} features but this estimator was trained with &quot;</span>
                <span class="s4">&quot;{} features.&quot;</span><span class="s1">.format(X.shape[</span><span class="s5">1</span><span class="s1">]</span><span class="s3">, </span><span class="s1">self._n_features)</span>
            <span class="s1">)</span>
        <span class="s1">n_samples = X.shape[</span><span class="s5">0</span><span class="s1">]</span>
        <span class="s1">raw_predictions = np.zeros(</span>
            <span class="s1">shape=(n_samples</span><span class="s3">, </span><span class="s1">self.n_trees_per_iteration_)</span><span class="s3">,</span>
            <span class="s1">dtype=self._baseline_prediction.dtype</span><span class="s3">,</span>
            <span class="s1">order=</span><span class="s4">&quot;F&quot;</span><span class="s3">,</span>
        <span class="s1">)</span>
        <span class="s1">raw_predictions += self._baseline_prediction</span>

        <span class="s2"># We intentionally decouple the number of threads used at prediction</span>
        <span class="s2"># time from the number of threads used at fit time because the model</span>
        <span class="s2"># can be deployed on a different machine for prediction purposes.</span>
        <span class="s1">n_threads = _openmp_effective_n_threads()</span>
        <span class="s3">for </span><span class="s1">iteration </span><span class="s3">in </span><span class="s1">range(len(self._predictors)):</span>
            <span class="s1">self._predict_iterations(</span>
                <span class="s1">X</span><span class="s3">,</span>
                <span class="s1">self._predictors[iteration : iteration + </span><span class="s5">1</span><span class="s1">]</span><span class="s3">,</span>
                <span class="s1">raw_predictions</span><span class="s3">,</span>
                <span class="s1">is_binned=</span><span class="s3">False,</span>
                <span class="s1">n_threads=n_threads</span><span class="s3">,</span>
            <span class="s1">)</span>
            <span class="s3">yield </span><span class="s1">raw_predictions.copy()</span>

    <span class="s3">def </span><span class="s1">_compute_partial_dependence_recursion(self</span><span class="s3">, </span><span class="s1">grid</span><span class="s3">, </span><span class="s1">target_features):</span>
        <span class="s0">&quot;&quot;&quot;Fast partial dependence computation. 
 
        Parameters 
        ---------- 
        grid : ndarray, shape (n_samples, n_target_features) 
            The grid points on which the partial dependence should be 
            evaluated. 
        target_features : ndarray, shape (n_target_features) 
            The set of target features for which the partial dependence 
            should be evaluated. 
 
        Returns 
        ------- 
        averaged_predictions : ndarray, shape \ 
                (n_trees_per_iteration, n_samples) 
            The value of the partial dependence function on each grid point. 
        &quot;&quot;&quot;</span>

        <span class="s3">if </span><span class="s1">getattr(self</span><span class="s3">, </span><span class="s4">&quot;_fitted_with_sw&quot;</span><span class="s3">, False</span><span class="s1">):</span>
            <span class="s3">raise </span><span class="s1">NotImplementedError(</span>
                <span class="s4">&quot;{} does not support partial dependence &quot;</span>
                <span class="s4">&quot;plots with the 'recursion' method when &quot;</span>
                <span class="s4">&quot;sample weights were given during fit &quot;</span>
                <span class="s4">&quot;time.&quot;</span><span class="s1">.format(self.__class__.__name__)</span>
            <span class="s1">)</span>

        <span class="s1">grid = np.asarray(grid</span><span class="s3">, </span><span class="s1">dtype=X_DTYPE</span><span class="s3">, </span><span class="s1">order=</span><span class="s4">&quot;C&quot;</span><span class="s1">)</span>
        <span class="s1">averaged_predictions = np.zeros(</span>
            <span class="s1">(self.n_trees_per_iteration_</span><span class="s3">, </span><span class="s1">grid.shape[</span><span class="s5">0</span><span class="s1">])</span><span class="s3">, </span><span class="s1">dtype=Y_DTYPE</span>
        <span class="s1">)</span>

        <span class="s3">for </span><span class="s1">predictors_of_ith_iteration </span><span class="s3">in </span><span class="s1">self._predictors:</span>
            <span class="s3">for </span><span class="s1">k</span><span class="s3">, </span><span class="s1">predictor </span><span class="s3">in </span><span class="s1">enumerate(predictors_of_ith_iteration):</span>
                <span class="s1">predictor.compute_partial_dependence(</span>
                    <span class="s1">grid</span><span class="s3">, </span><span class="s1">target_features</span><span class="s3">, </span><span class="s1">averaged_predictions[k]</span>
                <span class="s1">)</span>
        <span class="s2"># Note that the learning rate is already accounted for in the leaves</span>
        <span class="s2"># values.</span>

        <span class="s3">return </span><span class="s1">averaged_predictions</span>

    <span class="s3">def </span><span class="s1">_more_tags(self):</span>
        <span class="s3">return </span><span class="s1">{</span><span class="s4">&quot;allow_nan&quot;</span><span class="s1">: </span><span class="s3">True</span><span class="s1">}</span>

    <span class="s1">@abstractmethod</span>
    <span class="s3">def </span><span class="s1">_get_loss(self</span><span class="s3">, </span><span class="s1">sample_weight):</span>
        <span class="s3">pass</span>

    <span class="s1">@abstractmethod</span>
    <span class="s3">def </span><span class="s1">_encode_y(self</span><span class="s3">, </span><span class="s1">y=</span><span class="s3">None</span><span class="s1">):</span>
        <span class="s3">pass</span>

    <span class="s1">@property</span>
    <span class="s3">def </span><span class="s1">n_iter_(self):</span>
        <span class="s0">&quot;&quot;&quot;Number of iterations of the boosting process.&quot;&quot;&quot;</span>
        <span class="s1">check_is_fitted(self)</span>
        <span class="s3">return </span><span class="s1">len(self._predictors)</span>


<span class="s3">class </span><span class="s1">HistGradientBoostingRegressor(RegressorMixin</span><span class="s3">, </span><span class="s1">BaseHistGradientBoosting):</span>
    <span class="s0">&quot;&quot;&quot;Histogram-based Gradient Boosting Regression Tree. 
 
    This estimator is much faster than 
    :class:`GradientBoostingRegressor&lt;sklearn.ensemble.GradientBoostingRegressor&gt;` 
    for big datasets (n_samples &gt;= 10 000). 
 
    This estimator has native support for missing values (NaNs). During 
    training, the tree grower learns at each split point whether samples 
    with missing values should go to the left or right child, based on the 
    potential gain. When predicting, samples with missing values are 
    assigned to the left or right child consequently. If no missing values 
    were encountered for a given feature during training, then samples with 
    missing values are mapped to whichever child has the most samples. 
 
    This implementation is inspired by 
    `LightGBM &lt;https://github.com/Microsoft/LightGBM&gt;`_. 
 
    Read more in the :ref:`User Guide &lt;histogram_based_gradient_boosting&gt;`. 
 
    .. versionadded:: 0.21 
 
    Parameters 
    ---------- 
    loss : {'squared_error', 'absolute_error', 'gamma', 'poisson', 'quantile'}, \ 
            default='squared_error' 
        The loss function to use in the boosting process. Note that the 
        &quot;squared error&quot;, &quot;gamma&quot; and &quot;poisson&quot; losses actually implement 
        &quot;half least squares loss&quot;, &quot;half gamma deviance&quot; and &quot;half poisson 
        deviance&quot; to simplify the computation of the gradient. Furthermore, 
        &quot;gamma&quot; and &quot;poisson&quot; losses internally use a log-link, &quot;gamma&quot; 
        requires ``y &gt; 0`` and &quot;poisson&quot; requires ``y &gt;= 0``. 
        &quot;quantile&quot; uses the pinball loss. 
 
        .. versionchanged:: 0.23 
           Added option 'poisson'. 
 
        .. versionchanged:: 1.1 
           Added option 'quantile'. 
 
        .. versionchanged:: 1.3 
           Added option 'gamma'. 
 
    quantile : float, default=None 
        If loss is &quot;quantile&quot;, this parameter specifies which quantile to be estimated 
        and must be between 0 and 1. 
    learning_rate : float, default=0.1 
        The learning rate, also known as *shrinkage*. This is used as a 
        multiplicative factor for the leaves values. Use ``1`` for no 
        shrinkage. 
    max_iter : int, default=100 
        The maximum number of iterations of the boosting process, i.e. the 
        maximum number of trees. 
    max_leaf_nodes : int or None, default=31 
        The maximum number of leaves for each tree. Must be strictly greater 
        than 1. If None, there is no maximum limit. 
    max_depth : int or None, default=None 
        The maximum depth of each tree. The depth of a tree is the number of 
        edges to go from the root to the deepest leaf. 
        Depth isn't constrained by default. 
    min_samples_leaf : int, default=20 
        The minimum number of samples per leaf. For small datasets with less 
        than a few hundred samples, it is recommended to lower this value 
        since only very shallow trees would be built. 
    l2_regularization : float, default=0 
        The L2 regularization parameter. Use ``0`` for no regularization 
        (default). 
    max_bins : int, default=255 
        The maximum number of bins to use for non-missing values. Before 
        training, each feature of the input array `X` is binned into 
        integer-valued bins, which allows for a much faster training stage. 
        Features with a small number of unique values may use less than 
        ``max_bins`` bins. In addition to the ``max_bins`` bins, one more bin 
        is always reserved for missing values. Must be no larger than 255. 
    categorical_features : array-like of {bool, int, str} of shape (n_features) \ 
            or shape (n_categorical_features,), default=None 
        Indicates the categorical features. 
 
        - None : no feature will be considered categorical. 
        - boolean array-like : boolean mask indicating categorical features. 
        - integer array-like : integer indices indicating categorical 
          features. 
        - str array-like: names of categorical features (assuming the training 
          data has feature names). 
 
        For each categorical feature, there must be at most `max_bins` unique 
        categories, and each categorical value must be less then `max_bins - 1`. 
        Negative values for categorical features are treated as missing values. 
        All categorical values are converted to floating point numbers. 
        This means that categorical values of 1.0 and 1 are treated as 
        the same category. 
 
        Read more in the :ref:`User Guide &lt;categorical_support_gbdt&gt;`. 
 
        .. versionadded:: 0.24 
 
        .. versionchanged:: 1.2 
           Added support for feature names. 
 
    monotonic_cst : array-like of int of shape (n_features) or dict, default=None 
        Monotonic constraint to enforce on each feature are specified using the 
        following integer values: 
 
        - 1: monotonic increase 
        - 0: no constraint 
        - -1: monotonic decrease 
 
        If a dict with str keys, map feature to monotonic constraints by name. 
        If an array, the features are mapped to constraints by position. See 
        :ref:`monotonic_cst_features_names` for a usage example. 
 
        The constraints are only valid for binary classifications and hold 
        over the probability of the positive class. 
        Read more in the :ref:`User Guide &lt;monotonic_cst_gbdt&gt;`. 
 
        .. versionadded:: 0.23 
 
        .. versionchanged:: 1.2 
           Accept dict of constraints with feature names as keys. 
 
    interaction_cst : {&quot;pairwise&quot;, &quot;no_interactions&quot;} or sequence of lists/tuples/sets \ 
            of int, default=None 
        Specify interaction constraints, the sets of features which can 
        interact with each other in child node splits. 
 
        Each item specifies the set of feature indices that are allowed 
        to interact with each other. If there are more features than 
        specified in these constraints, they are treated as if they were 
        specified as an additional set. 
 
        The strings &quot;pairwise&quot; and &quot;no_interactions&quot; are shorthands for 
        allowing only pairwise or no interactions, respectively. 
 
        For instance, with 5 features in total, `interaction_cst=[{0, 1}]` 
        is equivalent to `interaction_cst=[{0, 1}, {2, 3, 4}]`, 
        and specifies that each branch of a tree will either only split 
        on features 0 and 1 or only split on features 2, 3 and 4. 
 
        .. versionadded:: 1.2 
 
    warm_start : bool, default=False 
        When set to ``True``, reuse the solution of the previous call to fit 
        and add more estimators to the ensemble. For results to be valid, the 
        estimator should be re-trained on the same data only. 
        See :term:`the Glossary &lt;warm_start&gt;`. 
    early_stopping : 'auto' or bool, default='auto' 
        If 'auto', early stopping is enabled if the sample size is larger than 
        10000. If True, early stopping is enabled, otherwise early stopping is 
        disabled. 
 
        .. versionadded:: 0.23 
 
    scoring : str or callable or None, default='loss' 
        Scoring parameter to use for early stopping. It can be a single 
        string (see :ref:`scoring_parameter`) or a callable (see 
        :ref:`scoring`). If None, the estimator's default scorer is used. If 
        ``scoring='loss'``, early stopping is checked w.r.t the loss value. 
        Only used if early stopping is performed. 
    validation_fraction : int or float or None, default=0.1 
        Proportion (or absolute size) of training data to set aside as 
        validation data for early stopping. If None, early stopping is done on 
        the training data. Only used if early stopping is performed. 
    n_iter_no_change : int, default=10 
        Used to determine when to &quot;early stop&quot;. The fitting process is 
        stopped when none of the last ``n_iter_no_change`` scores are better 
        than the ``n_iter_no_change - 1`` -th-to-last one, up to some 
        tolerance. Only used if early stopping is performed. 
    tol : float, default=1e-7 
        The absolute tolerance to use when comparing scores during early 
        stopping. The higher the tolerance, the more likely we are to early 
        stop: higher tolerance means that it will be harder for subsequent 
        iterations to be considered an improvement upon the reference score. 
    verbose : int, default=0 
        The verbosity level. If not zero, print some information about the 
        fitting process. 
    random_state : int, RandomState instance or None, default=None 
        Pseudo-random number generator to control the subsampling in the 
        binning process, and the train/validation data split if early stopping 
        is enabled. 
        Pass an int for reproducible output across multiple function calls. 
        See :term:`Glossary &lt;random_state&gt;`. 
 
    Attributes 
    ---------- 
    do_early_stopping_ : bool 
        Indicates whether early stopping is used during training. 
    n_iter_ : int 
        The number of iterations as selected by early stopping, depending on 
        the `early_stopping` parameter. Otherwise it corresponds to max_iter. 
    n_trees_per_iteration_ : int 
        The number of tree that are built at each iteration. For regressors, 
        this is always 1. 
    train_score_ : ndarray, shape (n_iter_+1,) 
        The scores at each iteration on the training data. The first entry 
        is the score of the ensemble before the first iteration. Scores are 
        computed according to the ``scoring`` parameter. If ``scoring`` is 
        not 'loss', scores are computed on a subset of at most 10 000 
        samples. Empty if no early stopping. 
    validation_score_ : ndarray, shape (n_iter_+1,) 
        The scores at each iteration on the held-out validation data. The 
        first entry is the score of the ensemble before the first iteration. 
        Scores are computed according to the ``scoring`` parameter. Empty if 
        no early stopping or if ``validation_fraction`` is None. 
    is_categorical_ : ndarray, shape (n_features, ) or None 
        Boolean mask for the categorical features. ``None`` if there are no 
        categorical features. 
    n_features_in_ : int 
        Number of features seen during :term:`fit`. 
 
        .. versionadded:: 0.24 
    feature_names_in_ : ndarray of shape (`n_features_in_`,) 
        Names of features seen during :term:`fit`. Defined only when `X` 
        has feature names that are all strings. 
 
        .. versionadded:: 1.0 
 
    See Also 
    -------- 
    GradientBoostingRegressor : Exact gradient boosting method that does not 
        scale as good on datasets with a large number of samples. 
    sklearn.tree.DecisionTreeRegressor : A decision tree regressor. 
    RandomForestRegressor : A meta-estimator that fits a number of decision 
        tree regressors on various sub-samples of the dataset and uses 
        averaging to improve the statistical performance and control 
        over-fitting. 
    AdaBoostRegressor : A meta-estimator that begins by fitting a regressor 
        on the original dataset and then fits additional copies of the 
        regressor on the same dataset but where the weights of instances are 
        adjusted according to the error of the current prediction. As such, 
        subsequent regressors focus more on difficult cases. 
 
    Examples 
    -------- 
    &gt;&gt;&gt; from sklearn.ensemble import HistGradientBoostingRegressor 
    &gt;&gt;&gt; from sklearn.datasets import load_diabetes 
    &gt;&gt;&gt; X, y = load_diabetes(return_X_y=True) 
    &gt;&gt;&gt; est = HistGradientBoostingRegressor().fit(X, y) 
    &gt;&gt;&gt; est.score(X, y) 
    0.92... 
    &quot;&quot;&quot;</span>

    <span class="s1">_parameter_constraints: dict = {</span>
        <span class="s1">**BaseHistGradientBoosting._parameter_constraints</span><span class="s3">,</span>
        <span class="s4">&quot;loss&quot;</span><span class="s1">: [</span>
            <span class="s1">StrOptions(</span>
                <span class="s1">{</span>
                    <span class="s4">&quot;squared_error&quot;</span><span class="s3">,</span>
                    <span class="s4">&quot;absolute_error&quot;</span><span class="s3">,</span>
                    <span class="s4">&quot;poisson&quot;</span><span class="s3">,</span>
                    <span class="s4">&quot;gamma&quot;</span><span class="s3">,</span>
                    <span class="s4">&quot;quantile&quot;</span><span class="s3">,</span>
                <span class="s1">}</span>
            <span class="s1">)</span><span class="s3">,</span>
            <span class="s1">BaseLoss</span><span class="s3">,</span>
        <span class="s1">]</span><span class="s3">,</span>
        <span class="s4">&quot;quantile&quot;</span><span class="s1">: [Interval(Real</span><span class="s3">, </span><span class="s5">0</span><span class="s3">, </span><span class="s5">1</span><span class="s3">, </span><span class="s1">closed=</span><span class="s4">&quot;both&quot;</span><span class="s1">)</span><span class="s3">, None</span><span class="s1">]</span><span class="s3">,</span>
    <span class="s1">}</span>

    <span class="s3">def </span><span class="s1">__init__(</span>
        <span class="s1">self</span><span class="s3">,</span>
        <span class="s1">loss=</span><span class="s4">&quot;squared_error&quot;</span><span class="s3">,</span>
        <span class="s1">*</span><span class="s3">,</span>
        <span class="s1">quantile=</span><span class="s3">None,</span>
        <span class="s1">learning_rate=</span><span class="s5">0.1</span><span class="s3">,</span>
        <span class="s1">max_iter=</span><span class="s5">100</span><span class="s3">,</span>
        <span class="s1">max_leaf_nodes=</span><span class="s5">31</span><span class="s3">,</span>
        <span class="s1">max_depth=</span><span class="s3">None,</span>
        <span class="s1">min_samples_leaf=</span><span class="s5">20</span><span class="s3">,</span>
        <span class="s1">l2_regularization=</span><span class="s5">0.0</span><span class="s3">,</span>
        <span class="s1">max_bins=</span><span class="s5">255</span><span class="s3">,</span>
        <span class="s1">categorical_features=</span><span class="s3">None,</span>
        <span class="s1">monotonic_cst=</span><span class="s3">None,</span>
        <span class="s1">interaction_cst=</span><span class="s3">None,</span>
        <span class="s1">warm_start=</span><span class="s3">False,</span>
        <span class="s1">early_stopping=</span><span class="s4">&quot;auto&quot;</span><span class="s3">,</span>
        <span class="s1">scoring=</span><span class="s4">&quot;loss&quot;</span><span class="s3">,</span>
        <span class="s1">validation_fraction=</span><span class="s5">0.1</span><span class="s3">,</span>
        <span class="s1">n_iter_no_change=</span><span class="s5">10</span><span class="s3">,</span>
        <span class="s1">tol=</span><span class="s5">1e-7</span><span class="s3">,</span>
        <span class="s1">verbose=</span><span class="s5">0</span><span class="s3">,</span>
        <span class="s1">random_state=</span><span class="s3">None,</span>
    <span class="s1">):</span>
        <span class="s1">super(HistGradientBoostingRegressor</span><span class="s3">, </span><span class="s1">self).__init__(</span>
            <span class="s1">loss=loss</span><span class="s3">,</span>
            <span class="s1">learning_rate=learning_rate</span><span class="s3">,</span>
            <span class="s1">max_iter=max_iter</span><span class="s3">,</span>
            <span class="s1">max_leaf_nodes=max_leaf_nodes</span><span class="s3">,</span>
            <span class="s1">max_depth=max_depth</span><span class="s3">,</span>
            <span class="s1">min_samples_leaf=min_samples_leaf</span><span class="s3">,</span>
            <span class="s1">l2_regularization=l2_regularization</span><span class="s3">,</span>
            <span class="s1">max_bins=max_bins</span><span class="s3">,</span>
            <span class="s1">monotonic_cst=monotonic_cst</span><span class="s3">,</span>
            <span class="s1">interaction_cst=interaction_cst</span><span class="s3">,</span>
            <span class="s1">categorical_features=categorical_features</span><span class="s3">,</span>
            <span class="s1">early_stopping=early_stopping</span><span class="s3">,</span>
            <span class="s1">warm_start=warm_start</span><span class="s3">,</span>
            <span class="s1">scoring=scoring</span><span class="s3">,</span>
            <span class="s1">validation_fraction=validation_fraction</span><span class="s3">,</span>
            <span class="s1">n_iter_no_change=n_iter_no_change</span><span class="s3">,</span>
            <span class="s1">tol=tol</span><span class="s3">,</span>
            <span class="s1">verbose=verbose</span><span class="s3">,</span>
            <span class="s1">random_state=random_state</span><span class="s3">,</span>
        <span class="s1">)</span>
        <span class="s1">self.quantile = quantile</span>

    <span class="s3">def </span><span class="s1">predict(self</span><span class="s3">, </span><span class="s1">X):</span>
        <span class="s0">&quot;&quot;&quot;Predict values for X. 
 
        Parameters 
        ---------- 
        X : array-like, shape (n_samples, n_features) 
            The input samples. 
 
        Returns 
        ------- 
        y : ndarray, shape (n_samples,) 
            The predicted values. 
        &quot;&quot;&quot;</span>
        <span class="s1">check_is_fitted(self)</span>
        <span class="s2"># Return inverse link of raw predictions after converting</span>
        <span class="s2"># shape (n_samples, 1) to (n_samples,)</span>
        <span class="s3">return </span><span class="s1">self._loss.link.inverse(self._raw_predict(X).ravel())</span>

    <span class="s3">def </span><span class="s1">staged_predict(self</span><span class="s3">, </span><span class="s1">X):</span>
        <span class="s0">&quot;&quot;&quot;Predict regression target for each iteration. 
 
        This method allows monitoring (i.e. determine error on testing set) 
        after each stage. 
 
        .. versionadded:: 0.24 
 
        Parameters 
        ---------- 
        X : array-like of shape (n_samples, n_features) 
            The input samples. 
 
        Yields 
        ------ 
        y : generator of ndarray of shape (n_samples,) 
            The predicted values of the input samples, for each iteration. 
        &quot;&quot;&quot;</span>
        <span class="s3">for </span><span class="s1">raw_predictions </span><span class="s3">in </span><span class="s1">self._staged_raw_predict(X):</span>
            <span class="s3">yield </span><span class="s1">self._loss.link.inverse(raw_predictions.ravel())</span>

    <span class="s3">def </span><span class="s1">_encode_y(self</span><span class="s3">, </span><span class="s1">y):</span>
        <span class="s2"># Just convert y to the expected dtype</span>
        <span class="s1">self.n_trees_per_iteration_ = </span><span class="s5">1</span>
        <span class="s1">y = y.astype(Y_DTYPE</span><span class="s3">, </span><span class="s1">copy=</span><span class="s3">False</span><span class="s1">)</span>
        <span class="s3">if </span><span class="s1">self.loss == </span><span class="s4">&quot;gamma&quot;</span><span class="s1">:</span>
            <span class="s2"># Ensure y &gt; 0</span>
            <span class="s3">if not </span><span class="s1">np.all(y &gt; </span><span class="s5">0</span><span class="s1">):</span>
                <span class="s3">raise </span><span class="s1">ValueError(</span><span class="s4">&quot;loss='gamma' requires strictly positive y.&quot;</span><span class="s1">)</span>
        <span class="s3">elif </span><span class="s1">self.loss == </span><span class="s4">&quot;poisson&quot;</span><span class="s1">:</span>
            <span class="s2"># Ensure y &gt;= 0 and sum(y) &gt; 0</span>
            <span class="s3">if not </span><span class="s1">(np.all(y &gt;= </span><span class="s5">0</span><span class="s1">) </span><span class="s3">and </span><span class="s1">np.sum(y) &gt; </span><span class="s5">0</span><span class="s1">):</span>
                <span class="s3">raise </span><span class="s1">ValueError(</span>
                    <span class="s4">&quot;loss='poisson' requires non-negative y and sum(y) &gt; 0.&quot;</span>
                <span class="s1">)</span>
        <span class="s3">return </span><span class="s1">y</span>

    <span class="s3">def </span><span class="s1">_get_loss(self</span><span class="s3">, </span><span class="s1">sample_weight):</span>
        <span class="s3">if </span><span class="s1">self.loss == </span><span class="s4">&quot;quantile&quot;</span><span class="s1">:</span>
            <span class="s3">return </span><span class="s1">_LOSSES[self.loss](</span>
                <span class="s1">sample_weight=sample_weight</span><span class="s3">, </span><span class="s1">quantile=self.quantile</span>
            <span class="s1">)</span>
        <span class="s3">else</span><span class="s1">:</span>
            <span class="s3">return </span><span class="s1">_LOSSES[self.loss](sample_weight=sample_weight)</span>


<span class="s3">class </span><span class="s1">HistGradientBoostingClassifier(ClassifierMixin</span><span class="s3">, </span><span class="s1">BaseHistGradientBoosting):</span>
    <span class="s0">&quot;&quot;&quot;Histogram-based Gradient Boosting Classification Tree. 
 
    This estimator is much faster than 
    :class:`GradientBoostingClassifier&lt;sklearn.ensemble.GradientBoostingClassifier&gt;` 
    for big datasets (n_samples &gt;= 10 000). 
 
    This estimator has native support for missing values (NaNs). During 
    training, the tree grower learns at each split point whether samples 
    with missing values should go to the left or right child, based on the 
    potential gain. When predicting, samples with missing values are 
    assigned to the left or right child consequently. If no missing values 
    were encountered for a given feature during training, then samples with 
    missing values are mapped to whichever child has the most samples. 
 
    This implementation is inspired by 
    `LightGBM &lt;https://github.com/Microsoft/LightGBM&gt;`_. 
 
    Read more in the :ref:`User Guide &lt;histogram_based_gradient_boosting&gt;`. 
 
    .. versionadded:: 0.21 
 
    Parameters 
    ---------- 
    loss : {'log_loss'}, default='log_loss' 
        The loss function to use in the boosting process. 
 
        For binary classification problems, 'log_loss' is also known as logistic loss, 
        binomial deviance or binary crossentropy. Internally, the model fits one tree 
        per boosting iteration and uses the logistic sigmoid function (expit) as 
        inverse link function to compute the predicted positive class probability. 
 
        For multiclass classification problems, 'log_loss' is also known as multinomial 
        deviance or categorical crossentropy. Internally, the model fits one tree per 
        boosting iteration and per class and uses the softmax function as inverse link 
        function to compute the predicted probabilities of the classes. 
 
    learning_rate : float, default=0.1 
        The learning rate, also known as *shrinkage*. This is used as a 
        multiplicative factor for the leaves values. Use ``1`` for no 
        shrinkage. 
    max_iter : int, default=100 
        The maximum number of iterations of the boosting process, i.e. the 
        maximum number of trees for binary classification. For multiclass 
        classification, `n_classes` trees per iteration are built. 
    max_leaf_nodes : int or None, default=31 
        The maximum number of leaves for each tree. Must be strictly greater 
        than 1. If None, there is no maximum limit. 
    max_depth : int or None, default=None 
        The maximum depth of each tree. The depth of a tree is the number of 
        edges to go from the root to the deepest leaf. 
        Depth isn't constrained by default. 
    min_samples_leaf : int, default=20 
        The minimum number of samples per leaf. For small datasets with less 
        than a few hundred samples, it is recommended to lower this value 
        since only very shallow trees would be built. 
    l2_regularization : float, default=0 
        The L2 regularization parameter. Use 0 for no regularization. 
    max_bins : int, default=255 
        The maximum number of bins to use for non-missing values. Before 
        training, each feature of the input array `X` is binned into 
        integer-valued bins, which allows for a much faster training stage. 
        Features with a small number of unique values may use less than 
        ``max_bins`` bins. In addition to the ``max_bins`` bins, one more bin 
        is always reserved for missing values. Must be no larger than 255. 
    categorical_features : array-like of {bool, int, str} of shape (n_features) \ 
            or shape (n_categorical_features,), default=None 
        Indicates the categorical features. 
 
        - None : no feature will be considered categorical. 
        - boolean array-like : boolean mask indicating categorical features. 
        - integer array-like : integer indices indicating categorical 
          features. 
        - str array-like: names of categorical features (assuming the training 
          data has feature names). 
 
        For each categorical feature, there must be at most `max_bins` unique 
        categories, and each categorical value must be less then `max_bins - 1`. 
        Negative values for categorical features are treated as missing values. 
        All categorical values are converted to floating point numbers. 
        This means that categorical values of 1.0 and 1 are treated as 
        the same category. 
 
        Read more in the :ref:`User Guide &lt;categorical_support_gbdt&gt;`. 
 
        .. versionadded:: 0.24 
 
        .. versionchanged:: 1.2 
           Added support for feature names. 
 
    monotonic_cst : array-like of int of shape (n_features) or dict, default=None 
        Monotonic constraint to enforce on each feature are specified using the 
        following integer values: 
 
        - 1: monotonic increase 
        - 0: no constraint 
        - -1: monotonic decrease 
 
        If a dict with str keys, map feature to monotonic constraints by name. 
        If an array, the features are mapped to constraints by position. See 
        :ref:`monotonic_cst_features_names` for a usage example. 
 
        The constraints are only valid for binary classifications and hold 
        over the probability of the positive class. 
        Read more in the :ref:`User Guide &lt;monotonic_cst_gbdt&gt;`. 
 
        .. versionadded:: 0.23 
 
        .. versionchanged:: 1.2 
           Accept dict of constraints with feature names as keys. 
 
    interaction_cst : {&quot;pairwise&quot;, &quot;no_interactions&quot;} or sequence of lists/tuples/sets \ 
            of int, default=None 
        Specify interaction constraints, the sets of features which can 
        interact with each other in child node splits. 
 
        Each item specifies the set of feature indices that are allowed 
        to interact with each other. If there are more features than 
        specified in these constraints, they are treated as if they were 
        specified as an additional set. 
 
        The strings &quot;pairwise&quot; and &quot;no_interactions&quot; are shorthands for 
        allowing only pairwise or no interactions, respectively. 
 
        For instance, with 5 features in total, `interaction_cst=[{0, 1}]` 
        is equivalent to `interaction_cst=[{0, 1}, {2, 3, 4}]`, 
        and specifies that each branch of a tree will either only split 
        on features 0 and 1 or only split on features 2, 3 and 4. 
 
        .. versionadded:: 1.2 
 
    warm_start : bool, default=False 
        When set to ``True``, reuse the solution of the previous call to fit 
        and add more estimators to the ensemble. For results to be valid, the 
        estimator should be re-trained on the same data only. 
        See :term:`the Glossary &lt;warm_start&gt;`. 
    early_stopping : 'auto' or bool, default='auto' 
        If 'auto', early stopping is enabled if the sample size is larger than 
        10000. If True, early stopping is enabled, otherwise early stopping is 
        disabled. 
 
        .. versionadded:: 0.23 
 
    scoring : str or callable or None, default='loss' 
        Scoring parameter to use for early stopping. It can be a single 
        string (see :ref:`scoring_parameter`) or a callable (see 
        :ref:`scoring`). If None, the estimator's default scorer 
        is used. If ``scoring='loss'``, early stopping is checked 
        w.r.t the loss value. Only used if early stopping is performed. 
    validation_fraction : int or float or None, default=0.1 
        Proportion (or absolute size) of training data to set aside as 
        validation data for early stopping. If None, early stopping is done on 
        the training data. Only used if early stopping is performed. 
    n_iter_no_change : int, default=10 
        Used to determine when to &quot;early stop&quot;. The fitting process is 
        stopped when none of the last ``n_iter_no_change`` scores are better 
        than the ``n_iter_no_change - 1`` -th-to-last one, up to some 
        tolerance. Only used if early stopping is performed. 
    tol : float, default=1e-7 
        The absolute tolerance to use when comparing scores. The higher the 
        tolerance, the more likely we are to early stop: higher tolerance 
        means that it will be harder for subsequent iterations to be 
        considered an improvement upon the reference score. 
    verbose : int, default=0 
        The verbosity level. If not zero, print some information about the 
        fitting process. 
    random_state : int, RandomState instance or None, default=None 
        Pseudo-random number generator to control the subsampling in the 
        binning process, and the train/validation data split if early stopping 
        is enabled. 
        Pass an int for reproducible output across multiple function calls. 
        See :term:`Glossary &lt;random_state&gt;`. 
    class_weight : dict or 'balanced', default=None 
        Weights associated with classes in the form `{class_label: weight}`. 
        If not given, all classes are supposed to have weight one. 
        The &quot;balanced&quot; mode uses the values of y to automatically adjust 
        weights inversely proportional to class frequencies in the input data 
        as `n_samples / (n_classes * np.bincount(y))`. 
        Note that these weights will be multiplied with sample_weight (passed 
        through the fit method) if `sample_weight` is specified. 
 
        .. versionadded:: 1.2 
 
    Attributes 
    ---------- 
    classes_ : array, shape = (n_classes,) 
        Class labels. 
    do_early_stopping_ : bool 
        Indicates whether early stopping is used during training. 
    n_iter_ : int 
        The number of iterations as selected by early stopping, depending on 
        the `early_stopping` parameter. Otherwise it corresponds to max_iter. 
    n_trees_per_iteration_ : int 
        The number of tree that are built at each iteration. This is equal to 1 
        for binary classification, and to ``n_classes`` for multiclass 
        classification. 
    train_score_ : ndarray, shape (n_iter_+1,) 
        The scores at each iteration on the training data. The first entry 
        is the score of the ensemble before the first iteration. Scores are 
        computed according to the ``scoring`` parameter. If ``scoring`` is 
        not 'loss', scores are computed on a subset of at most 10 000 
        samples. Empty if no early stopping. 
    validation_score_ : ndarray, shape (n_iter_+1,) 
        The scores at each iteration on the held-out validation data. The 
        first entry is the score of the ensemble before the first iteration. 
        Scores are computed according to the ``scoring`` parameter. Empty if 
        no early stopping or if ``validation_fraction`` is None. 
    is_categorical_ : ndarray, shape (n_features, ) or None 
        Boolean mask for the categorical features. ``None`` if there are no 
        categorical features. 
    n_features_in_ : int 
        Number of features seen during :term:`fit`. 
 
        .. versionadded:: 0.24 
    feature_names_in_ : ndarray of shape (`n_features_in_`,) 
        Names of features seen during :term:`fit`. Defined only when `X` 
        has feature names that are all strings. 
 
        .. versionadded:: 1.0 
 
    See Also 
    -------- 
    GradientBoostingClassifier : Exact gradient boosting method that does not 
        scale as good on datasets with a large number of samples. 
    sklearn.tree.DecisionTreeClassifier : A decision tree classifier. 
    RandomForestClassifier : A meta-estimator that fits a number of decision 
        tree classifiers on various sub-samples of the dataset and uses 
        averaging to improve the predictive accuracy and control over-fitting. 
    AdaBoostClassifier : A meta-estimator that begins by fitting a classifier 
        on the original dataset and then fits additional copies of the 
        classifier on the same dataset where the weights of incorrectly 
        classified instances are adjusted such that subsequent classifiers 
        focus more on difficult cases. 
 
    Examples 
    -------- 
    &gt;&gt;&gt; from sklearn.ensemble import HistGradientBoostingClassifier 
    &gt;&gt;&gt; from sklearn.datasets import load_iris 
    &gt;&gt;&gt; X, y = load_iris(return_X_y=True) 
    &gt;&gt;&gt; clf = HistGradientBoostingClassifier().fit(X, y) 
    &gt;&gt;&gt; clf.score(X, y) 
    1.0 
    &quot;&quot;&quot;</span>

    <span class="s1">_parameter_constraints: dict = {</span>
        <span class="s1">**BaseHistGradientBoosting._parameter_constraints</span><span class="s3">,</span>
        <span class="s4">&quot;loss&quot;</span><span class="s1">: [StrOptions({</span><span class="s4">&quot;log_loss&quot;</span><span class="s1">})</span><span class="s3">, </span><span class="s1">BaseLoss]</span><span class="s3">,</span>
        <span class="s4">&quot;class_weight&quot;</span><span class="s1">: [dict</span><span class="s3">, </span><span class="s1">StrOptions({</span><span class="s4">&quot;balanced&quot;</span><span class="s1">})</span><span class="s3">, None</span><span class="s1">]</span><span class="s3">,</span>
    <span class="s1">}</span>

    <span class="s3">def </span><span class="s1">__init__(</span>
        <span class="s1">self</span><span class="s3">,</span>
        <span class="s1">loss=</span><span class="s4">&quot;log_loss&quot;</span><span class="s3">,</span>
        <span class="s1">*</span><span class="s3">,</span>
        <span class="s1">learning_rate=</span><span class="s5">0.1</span><span class="s3">,</span>
        <span class="s1">max_iter=</span><span class="s5">100</span><span class="s3">,</span>
        <span class="s1">max_leaf_nodes=</span><span class="s5">31</span><span class="s3">,</span>
        <span class="s1">max_depth=</span><span class="s3">None,</span>
        <span class="s1">min_samples_leaf=</span><span class="s5">20</span><span class="s3">,</span>
        <span class="s1">l2_regularization=</span><span class="s5">0.0</span><span class="s3">,</span>
        <span class="s1">max_bins=</span><span class="s5">255</span><span class="s3">,</span>
        <span class="s1">categorical_features=</span><span class="s3">None,</span>
        <span class="s1">monotonic_cst=</span><span class="s3">None,</span>
        <span class="s1">interaction_cst=</span><span class="s3">None,</span>
        <span class="s1">warm_start=</span><span class="s3">False,</span>
        <span class="s1">early_stopping=</span><span class="s4">&quot;auto&quot;</span><span class="s3">,</span>
        <span class="s1">scoring=</span><span class="s4">&quot;loss&quot;</span><span class="s3">,</span>
        <span class="s1">validation_fraction=</span><span class="s5">0.1</span><span class="s3">,</span>
        <span class="s1">n_iter_no_change=</span><span class="s5">10</span><span class="s3">,</span>
        <span class="s1">tol=</span><span class="s5">1e-7</span><span class="s3">,</span>
        <span class="s1">verbose=</span><span class="s5">0</span><span class="s3">,</span>
        <span class="s1">random_state=</span><span class="s3">None,</span>
        <span class="s1">class_weight=</span><span class="s3">None,</span>
    <span class="s1">):</span>
        <span class="s1">super(HistGradientBoostingClassifier</span><span class="s3">, </span><span class="s1">self).__init__(</span>
            <span class="s1">loss=loss</span><span class="s3">,</span>
            <span class="s1">learning_rate=learning_rate</span><span class="s3">,</span>
            <span class="s1">max_iter=max_iter</span><span class="s3">,</span>
            <span class="s1">max_leaf_nodes=max_leaf_nodes</span><span class="s3">,</span>
            <span class="s1">max_depth=max_depth</span><span class="s3">,</span>
            <span class="s1">min_samples_leaf=min_samples_leaf</span><span class="s3">,</span>
            <span class="s1">l2_regularization=l2_regularization</span><span class="s3">,</span>
            <span class="s1">max_bins=max_bins</span><span class="s3">,</span>
            <span class="s1">categorical_features=categorical_features</span><span class="s3">,</span>
            <span class="s1">monotonic_cst=monotonic_cst</span><span class="s3">,</span>
            <span class="s1">interaction_cst=interaction_cst</span><span class="s3">,</span>
            <span class="s1">warm_start=warm_start</span><span class="s3">,</span>
            <span class="s1">early_stopping=early_stopping</span><span class="s3">,</span>
            <span class="s1">scoring=scoring</span><span class="s3">,</span>
            <span class="s1">validation_fraction=validation_fraction</span><span class="s3">,</span>
            <span class="s1">n_iter_no_change=n_iter_no_change</span><span class="s3">,</span>
            <span class="s1">tol=tol</span><span class="s3">,</span>
            <span class="s1">verbose=verbose</span><span class="s3">,</span>
            <span class="s1">random_state=random_state</span><span class="s3">,</span>
        <span class="s1">)</span>
        <span class="s1">self.class_weight = class_weight</span>

    <span class="s3">def </span><span class="s1">_finalize_sample_weight(self</span><span class="s3">, </span><span class="s1">sample_weight</span><span class="s3">, </span><span class="s1">y):</span>
        <span class="s0">&quot;&quot;&quot;Adjust sample_weights with class_weights.&quot;&quot;&quot;</span>
        <span class="s3">if </span><span class="s1">self.class_weight </span><span class="s3">is None</span><span class="s1">:</span>
            <span class="s3">return </span><span class="s1">sample_weight</span>

        <span class="s1">expanded_class_weight = compute_sample_weight(self.class_weight</span><span class="s3">, </span><span class="s1">y)</span>

        <span class="s3">if </span><span class="s1">sample_weight </span><span class="s3">is not None</span><span class="s1">:</span>
            <span class="s3">return </span><span class="s1">sample_weight * expanded_class_weight</span>
        <span class="s3">else</span><span class="s1">:</span>
            <span class="s3">return </span><span class="s1">expanded_class_weight</span>

    <span class="s3">def </span><span class="s1">predict(self</span><span class="s3">, </span><span class="s1">X):</span>
        <span class="s0">&quot;&quot;&quot;Predict classes for X. 
 
        Parameters 
        ---------- 
        X : array-like, shape (n_samples, n_features) 
            The input samples. 
 
        Returns 
        ------- 
        y : ndarray, shape (n_samples,) 
            The predicted classes. 
        &quot;&quot;&quot;</span>
        <span class="s2"># TODO: This could be done in parallel</span>
        <span class="s1">encoded_classes = np.argmax(self.predict_proba(X)</span><span class="s3">, </span><span class="s1">axis=</span><span class="s5">1</span><span class="s1">)</span>
        <span class="s3">return </span><span class="s1">self.classes_[encoded_classes]</span>

    <span class="s3">def </span><span class="s1">staged_predict(self</span><span class="s3">, </span><span class="s1">X):</span>
        <span class="s0">&quot;&quot;&quot;Predict classes at each iteration. 
 
        This method allows monitoring (i.e. determine error on testing set) 
        after each stage. 
 
        .. versionadded:: 0.24 
 
        Parameters 
        ---------- 
        X : array-like of shape (n_samples, n_features) 
            The input samples. 
 
        Yields 
        ------ 
        y : generator of ndarray of shape (n_samples,) 
            The predicted classes of the input samples, for each iteration. 
        &quot;&quot;&quot;</span>
        <span class="s3">for </span><span class="s1">proba </span><span class="s3">in </span><span class="s1">self.staged_predict_proba(X):</span>
            <span class="s1">encoded_classes = np.argmax(proba</span><span class="s3">, </span><span class="s1">axis=</span><span class="s5">1</span><span class="s1">)</span>
            <span class="s3">yield </span><span class="s1">self.classes_.take(encoded_classes</span><span class="s3">, </span><span class="s1">axis=</span><span class="s5">0</span><span class="s1">)</span>

    <span class="s3">def </span><span class="s1">predict_proba(self</span><span class="s3">, </span><span class="s1">X):</span>
        <span class="s0">&quot;&quot;&quot;Predict class probabilities for X. 
 
        Parameters 
        ---------- 
        X : array-like, shape (n_samples, n_features) 
            The input samples. 
 
        Returns 
        ------- 
        p : ndarray, shape (n_samples, n_classes) 
            The class probabilities of the input samples. 
        &quot;&quot;&quot;</span>
        <span class="s1">raw_predictions = self._raw_predict(X)</span>
        <span class="s3">return </span><span class="s1">self._loss.predict_proba(raw_predictions)</span>

    <span class="s3">def </span><span class="s1">staged_predict_proba(self</span><span class="s3">, </span><span class="s1">X):</span>
        <span class="s0">&quot;&quot;&quot;Predict class probabilities at each iteration. 
 
        This method allows monitoring (i.e. determine error on testing set) 
        after each stage. 
 
        Parameters 
        ---------- 
        X : array-like of shape (n_samples, n_features) 
            The input samples. 
 
        Yields 
        ------ 
        y : generator of ndarray of shape (n_samples,) 
            The predicted class probabilities of the input samples, 
            for each iteration. 
        &quot;&quot;&quot;</span>
        <span class="s3">for </span><span class="s1">raw_predictions </span><span class="s3">in </span><span class="s1">self._staged_raw_predict(X):</span>
            <span class="s3">yield </span><span class="s1">self._loss.predict_proba(raw_predictions)</span>

    <span class="s3">def </span><span class="s1">decision_function(self</span><span class="s3">, </span><span class="s1">X):</span>
        <span class="s0">&quot;&quot;&quot;Compute the decision function of ``X``. 
 
        Parameters 
        ---------- 
        X : array-like, shape (n_samples, n_features) 
            The input samples. 
 
        Returns 
        ------- 
        decision : ndarray, shape (n_samples,) or \ 
                (n_samples, n_trees_per_iteration) 
            The raw predicted values (i.e. the sum of the trees leaves) for 
            each sample. n_trees_per_iteration is equal to the number of 
            classes in multiclass classification. 
        &quot;&quot;&quot;</span>
        <span class="s1">decision = self._raw_predict(X)</span>
        <span class="s3">if </span><span class="s1">decision.shape[</span><span class="s5">1</span><span class="s1">] == </span><span class="s5">1</span><span class="s1">:</span>
            <span class="s1">decision = decision.ravel()</span>
        <span class="s3">return </span><span class="s1">decision</span>

    <span class="s3">def </span><span class="s1">staged_decision_function(self</span><span class="s3">, </span><span class="s1">X):</span>
        <span class="s0">&quot;&quot;&quot;Compute decision function of ``X`` for each iteration. 
 
        This method allows monitoring (i.e. determine error on testing set) 
        after each stage. 
 
        Parameters 
        ---------- 
        X : array-like of shape (n_samples, n_features) 
            The input samples. 
 
        Yields 
        ------ 
        decision : generator of ndarray of shape (n_samples,) or \ 
                (n_samples, n_trees_per_iteration) 
            The decision function of the input samples, which corresponds to 
            the raw values predicted from the trees of the ensemble . The 
            classes corresponds to that in the attribute :term:`classes_`. 
        &quot;&quot;&quot;</span>
        <span class="s3">for </span><span class="s1">staged_decision </span><span class="s3">in </span><span class="s1">self._staged_raw_predict(X):</span>
            <span class="s3">if </span><span class="s1">staged_decision.shape[</span><span class="s5">1</span><span class="s1">] == </span><span class="s5">1</span><span class="s1">:</span>
                <span class="s1">staged_decision = staged_decision.ravel()</span>
            <span class="s3">yield </span><span class="s1">staged_decision</span>

    <span class="s3">def </span><span class="s1">_encode_y(self</span><span class="s3">, </span><span class="s1">y):</span>
        <span class="s2"># encode classes into 0 ... n_classes - 1 and sets attributes classes_</span>
        <span class="s2"># and n_trees_per_iteration_</span>
        <span class="s1">check_classification_targets(y)</span>

        <span class="s1">label_encoder = LabelEncoder()</span>
        <span class="s1">encoded_y = label_encoder.fit_transform(y)</span>
        <span class="s1">self.classes_ = label_encoder.classes_</span>
        <span class="s1">n_classes = self.classes_.shape[</span><span class="s5">0</span><span class="s1">]</span>
        <span class="s2"># only 1 tree for binary classification. For multiclass classification,</span>
        <span class="s2"># we build 1 tree per class.</span>
        <span class="s1">self.n_trees_per_iteration_ = </span><span class="s5">1 </span><span class="s3">if </span><span class="s1">n_classes &lt;= </span><span class="s5">2 </span><span class="s3">else </span><span class="s1">n_classes</span>
        <span class="s1">encoded_y = encoded_y.astype(Y_DTYPE</span><span class="s3">, </span><span class="s1">copy=</span><span class="s3">False</span><span class="s1">)</span>
        <span class="s3">return </span><span class="s1">encoded_y</span>

    <span class="s3">def </span><span class="s1">_get_loss(self</span><span class="s3">, </span><span class="s1">sample_weight):</span>
        <span class="s2"># At this point self.loss == &quot;log_loss&quot;</span>
        <span class="s3">if </span><span class="s1">self.n_trees_per_iteration_ == </span><span class="s5">1</span><span class="s1">:</span>
            <span class="s3">return </span><span class="s1">HalfBinomialLoss(sample_weight=sample_weight)</span>
        <span class="s3">else</span><span class="s1">:</span>
            <span class="s3">return </span><span class="s1">HalfMultinomialLoss(</span>
                <span class="s1">sample_weight=sample_weight</span><span class="s3">, </span><span class="s1">n_classes=self.n_trees_per_iteration_</span>
            <span class="s1">)</span>
</pre>
</body>
</html>