<html>
<head>
<title>test_loss.py</title>
<meta http-equiv="Content-Type" content="text/html; charset=utf-8">
<style type="text/css">
.s0 { color: #cc7832;}
.s1 { color: #a9b7c6;}
.s2 { color: #808080;}
.s3 { color: #6897bb;}
.s4 { color: #6a8759;}
.s5 { color: #629755; font-style: italic;}
</style>
</head>
<body bgcolor="#2b2b2b">
<table CELLSPACING=0 CELLPADDING=5 COLS=1 WIDTH="100%" BGCOLOR="#606060" >
<tr><td><center>
<font face="Arial, Helvetica" color="#000000">
test_loss.py</font>
</center></td></tr></table>
<pre><span class="s0">import </span><span class="s1">pickle</span>

<span class="s0">import </span><span class="s1">numpy </span><span class="s0">as </span><span class="s1">np</span>
<span class="s0">import </span><span class="s1">pytest</span>
<span class="s0">from </span><span class="s1">numpy.testing </span><span class="s0">import </span><span class="s1">assert_allclose</span><span class="s0">, </span><span class="s1">assert_array_equal</span>
<span class="s0">from </span><span class="s1">pytest </span><span class="s0">import </span><span class="s1">approx</span>
<span class="s0">from </span><span class="s1">scipy.optimize </span><span class="s0">import </span><span class="s1">(</span>
    <span class="s1">LinearConstraint</span><span class="s0">,</span>
    <span class="s1">minimize</span><span class="s0">,</span>
    <span class="s1">minimize_scalar</span><span class="s0">,</span>
    <span class="s1">newton</span><span class="s0">,</span>
<span class="s1">)</span>
<span class="s0">from </span><span class="s1">scipy.special </span><span class="s0">import </span><span class="s1">logsumexp</span>

<span class="s0">from </span><span class="s1">sklearn._loss.link </span><span class="s0">import </span><span class="s1">IdentityLink</span><span class="s0">, </span><span class="s1">_inclusive_low_high</span>
<span class="s0">from </span><span class="s1">sklearn._loss.loss </span><span class="s0">import </span><span class="s1">(</span>
    <span class="s1">_LOSSES</span><span class="s0">,</span>
    <span class="s1">AbsoluteError</span><span class="s0">,</span>
    <span class="s1">BaseLoss</span><span class="s0">,</span>
    <span class="s1">HalfBinomialLoss</span><span class="s0">,</span>
    <span class="s1">HalfGammaLoss</span><span class="s0">,</span>
    <span class="s1">HalfMultinomialLoss</span><span class="s0">,</span>
    <span class="s1">HalfPoissonLoss</span><span class="s0">,</span>
    <span class="s1">HalfSquaredError</span><span class="s0">,</span>
    <span class="s1">HalfTweedieLoss</span><span class="s0">,</span>
    <span class="s1">HalfTweedieLossIdentity</span><span class="s0">,</span>
    <span class="s1">HuberLoss</span><span class="s0">,</span>
    <span class="s1">PinballLoss</span><span class="s0">,</span>
<span class="s1">)</span>
<span class="s0">from </span><span class="s1">sklearn.utils </span><span class="s0">import </span><span class="s1">assert_all_finite</span>
<span class="s0">from </span><span class="s1">sklearn.utils._testing </span><span class="s0">import </span><span class="s1">create_memmap_backed_data</span><span class="s0">, </span><span class="s1">skip_if_32bit</span>

<span class="s1">ALL_LOSSES = list(_LOSSES.values())</span>

<span class="s1">LOSS_INSTANCES = [loss() </span><span class="s0">for </span><span class="s1">loss </span><span class="s0">in </span><span class="s1">ALL_LOSSES]</span>
<span class="s2"># HalfTweedieLoss(power=1.5) is already there as default</span>
<span class="s1">LOSS_INSTANCES += [</span>
    <span class="s1">PinballLoss(quantile=</span><span class="s3">0.25</span><span class="s1">)</span><span class="s0">,</span>
    <span class="s1">HuberLoss(quantile=</span><span class="s3">0.75</span><span class="s1">)</span><span class="s0">,</span>
    <span class="s1">HalfTweedieLoss(power=-</span><span class="s3">1.5</span><span class="s1">)</span><span class="s0">,</span>
    <span class="s1">HalfTweedieLoss(power=</span><span class="s3">0</span><span class="s1">)</span><span class="s0">,</span>
    <span class="s1">HalfTweedieLoss(power=</span><span class="s3">1</span><span class="s1">)</span><span class="s0">,</span>
    <span class="s1">HalfTweedieLoss(power=</span><span class="s3">2</span><span class="s1">)</span><span class="s0">,</span>
    <span class="s1">HalfTweedieLoss(power=</span><span class="s3">3.0</span><span class="s1">)</span><span class="s0">,</span>
    <span class="s1">HalfTweedieLossIdentity(power=</span><span class="s3">0</span><span class="s1">)</span><span class="s0">,</span>
    <span class="s1">HalfTweedieLossIdentity(power=</span><span class="s3">1</span><span class="s1">)</span><span class="s0">,</span>
    <span class="s1">HalfTweedieLossIdentity(power=</span><span class="s3">2</span><span class="s1">)</span><span class="s0">,</span>
    <span class="s1">HalfTweedieLossIdentity(power=</span><span class="s3">3.0</span><span class="s1">)</span><span class="s0">,</span>
<span class="s1">]</span>


<span class="s0">def </span><span class="s1">loss_instance_name(param):</span>
    <span class="s0">if </span><span class="s1">isinstance(param</span><span class="s0">, </span><span class="s1">BaseLoss):</span>
        <span class="s1">loss = param</span>
        <span class="s1">name = loss.__class__.__name__</span>
        <span class="s0">if </span><span class="s1">isinstance(loss</span><span class="s0">, </span><span class="s1">PinballLoss):</span>
            <span class="s1">name += </span><span class="s4">f&quot;(quantile=</span><span class="s0">{</span><span class="s1">loss.closs.quantile</span><span class="s0">}</span><span class="s4">)&quot;</span>
        <span class="s0">elif </span><span class="s1">isinstance(loss</span><span class="s0">, </span><span class="s1">HuberLoss):</span>
            <span class="s1">name += </span><span class="s4">f&quot;(quantile=</span><span class="s0">{</span><span class="s1">loss.quantile</span><span class="s0">}</span><span class="s4">&quot;</span>
        <span class="s0">elif </span><span class="s1">hasattr(loss</span><span class="s0">, </span><span class="s4">&quot;closs&quot;</span><span class="s1">) </span><span class="s0">and </span><span class="s1">hasattr(loss.closs</span><span class="s0">, </span><span class="s4">&quot;power&quot;</span><span class="s1">):</span>
            <span class="s1">name += </span><span class="s4">f&quot;(power=</span><span class="s0">{</span><span class="s1">loss.closs.power</span><span class="s0">}</span><span class="s4">)&quot;</span>
        <span class="s0">return </span><span class="s1">name</span>
    <span class="s0">else</span><span class="s1">:</span>
        <span class="s0">return </span><span class="s1">str(param)</span>


<span class="s0">def </span><span class="s1">random_y_true_raw_prediction(</span>
    <span class="s1">loss</span><span class="s0">, </span><span class="s1">n_samples</span><span class="s0">, </span><span class="s1">y_bound=(-</span><span class="s3">100</span><span class="s0">, </span><span class="s3">100</span><span class="s1">)</span><span class="s0">, </span><span class="s1">raw_bound=(-</span><span class="s3">5</span><span class="s0">, </span><span class="s3">5</span><span class="s1">)</span><span class="s0">, </span><span class="s1">seed=</span><span class="s3">42</span>
<span class="s1">):</span>
    <span class="s5">&quot;&quot;&quot;Random generate y_true and raw_prediction in valid range.&quot;&quot;&quot;</span>
    <span class="s1">rng = np.random.RandomState(seed)</span>
    <span class="s0">if </span><span class="s1">loss.is_multiclass:</span>
        <span class="s1">raw_prediction = np.empty((n_samples</span><span class="s0">, </span><span class="s1">loss.n_classes))</span>
        <span class="s1">raw_prediction.flat[:] = rng.uniform(</span>
            <span class="s1">low=raw_bound[</span><span class="s3">0</span><span class="s1">]</span><span class="s0">,</span>
            <span class="s1">high=raw_bound[</span><span class="s3">1</span><span class="s1">]</span><span class="s0">,</span>
            <span class="s1">size=n_samples * loss.n_classes</span><span class="s0">,</span>
        <span class="s1">)</span>
        <span class="s1">y_true = np.arange(n_samples).astype(float) % loss.n_classes</span>
    <span class="s0">else</span><span class="s1">:</span>
        <span class="s2"># If link is identity, we must respect the interval of y_pred:</span>
        <span class="s0">if </span><span class="s1">isinstance(loss.link</span><span class="s0">, </span><span class="s1">IdentityLink):</span>
            <span class="s1">low</span><span class="s0">, </span><span class="s1">high = _inclusive_low_high(loss.interval_y_pred)</span>
            <span class="s1">low = np.amax([low</span><span class="s0">, </span><span class="s1">raw_bound[</span><span class="s3">0</span><span class="s1">]])</span>
            <span class="s1">high = np.amin([high</span><span class="s0">, </span><span class="s1">raw_bound[</span><span class="s3">1</span><span class="s1">]])</span>
            <span class="s1">raw_bound = (low</span><span class="s0">, </span><span class="s1">high)</span>
        <span class="s1">raw_prediction = rng.uniform(</span>
            <span class="s1">low=raw_bound[</span><span class="s3">0</span><span class="s1">]</span><span class="s0">, </span><span class="s1">high=raw_bound[</span><span class="s3">1</span><span class="s1">]</span><span class="s0">, </span><span class="s1">size=n_samples</span>
        <span class="s1">)</span>
        <span class="s2"># generate a y_true in valid range</span>
        <span class="s1">low</span><span class="s0">, </span><span class="s1">high = _inclusive_low_high(loss.interval_y_true)</span>
        <span class="s1">low = max(low</span><span class="s0">, </span><span class="s1">y_bound[</span><span class="s3">0</span><span class="s1">])</span>
        <span class="s1">high = min(high</span><span class="s0">, </span><span class="s1">y_bound[</span><span class="s3">1</span><span class="s1">])</span>
        <span class="s1">y_true = rng.uniform(low</span><span class="s0">, </span><span class="s1">high</span><span class="s0">, </span><span class="s1">size=n_samples)</span>
        <span class="s2"># set some values at special boundaries</span>
        <span class="s0">if </span><span class="s1">loss.interval_y_true.low == </span><span class="s3">0 </span><span class="s0">and </span><span class="s1">loss.interval_y_true.low_inclusive:</span>
            <span class="s1">y_true[:: (n_samples // </span><span class="s3">3</span><span class="s1">)] = </span><span class="s3">0</span>
        <span class="s0">if </span><span class="s1">loss.interval_y_true.high == </span><span class="s3">1 </span><span class="s0">and </span><span class="s1">loss.interval_y_true.high_inclusive:</span>
            <span class="s1">y_true[</span><span class="s3">1 </span><span class="s1">:: (n_samples // </span><span class="s3">3</span><span class="s1">)] = </span><span class="s3">1</span>

    <span class="s0">return </span><span class="s1">y_true</span><span class="s0">, </span><span class="s1">raw_prediction</span>


<span class="s0">def </span><span class="s1">numerical_derivative(func</span><span class="s0">, </span><span class="s1">x</span><span class="s0">, </span><span class="s1">eps):</span>
    <span class="s5">&quot;&quot;&quot;Helper function for numerical (first) derivatives.&quot;&quot;&quot;</span>
    <span class="s2"># For numerical derivatives, see</span>
    <span class="s2"># https://en.wikipedia.org/wiki/Numerical_differentiation</span>
    <span class="s2"># https://en.wikipedia.org/wiki/Finite_difference_coefficient</span>
    <span class="s2"># We use central finite differences of accuracy 4.</span>
    <span class="s1">h = np.full_like(x</span><span class="s0">, </span><span class="s1">fill_value=eps)</span>
    <span class="s1">f_minus_2h = func(x - </span><span class="s3">2 </span><span class="s1">* h)</span>
    <span class="s1">f_minus_1h = func(x - h)</span>
    <span class="s1">f_plus_1h = func(x + h)</span>
    <span class="s1">f_plus_2h = func(x + </span><span class="s3">2 </span><span class="s1">* h)</span>
    <span class="s0">return </span><span class="s1">(-f_plus_2h + </span><span class="s3">8 </span><span class="s1">* f_plus_1h - </span><span class="s3">8 </span><span class="s1">* f_minus_1h + f_minus_2h) / (</span><span class="s3">12.0 </span><span class="s1">* eps)</span>


<span class="s1">@pytest.mark.parametrize(</span><span class="s4">&quot;loss&quot;</span><span class="s0">, </span><span class="s1">LOSS_INSTANCES</span><span class="s0">, </span><span class="s1">ids=loss_instance_name)</span>
<span class="s0">def </span><span class="s1">test_loss_boundary(loss):</span>
    <span class="s5">&quot;&quot;&quot;Test interval ranges of y_true and y_pred in losses.&quot;&quot;&quot;</span>
    <span class="s2"># make sure low and high are always within the interval, used for linspace</span>
    <span class="s0">if </span><span class="s1">loss.is_multiclass:</span>
        <span class="s1">y_true = np.linspace(</span><span class="s3">0</span><span class="s0">, </span><span class="s3">9</span><span class="s0">, </span><span class="s1">num=</span><span class="s3">10</span><span class="s1">)</span>
    <span class="s0">else</span><span class="s1">:</span>
        <span class="s1">low</span><span class="s0">, </span><span class="s1">high = _inclusive_low_high(loss.interval_y_true)</span>
        <span class="s1">y_true = np.linspace(low</span><span class="s0">, </span><span class="s1">high</span><span class="s0">, </span><span class="s1">num=</span><span class="s3">10</span><span class="s1">)</span>

    <span class="s2"># add boundaries if they are included</span>
    <span class="s0">if </span><span class="s1">loss.interval_y_true.low_inclusive:</span>
        <span class="s1">y_true = np.r_[y_true</span><span class="s0">, </span><span class="s1">loss.interval_y_true.low]</span>
    <span class="s0">if </span><span class="s1">loss.interval_y_true.high_inclusive:</span>
        <span class="s1">y_true = np.r_[y_true</span><span class="s0">, </span><span class="s1">loss.interval_y_true.high]</span>

    <span class="s0">assert </span><span class="s1">loss.in_y_true_range(y_true)</span>

    <span class="s1">n = y_true.shape[</span><span class="s3">0</span><span class="s1">]</span>
    <span class="s1">low</span><span class="s0">, </span><span class="s1">high = _inclusive_low_high(loss.interval_y_pred)</span>
    <span class="s0">if </span><span class="s1">loss.is_multiclass:</span>
        <span class="s1">y_pred = np.empty((n</span><span class="s0">, </span><span class="s3">3</span><span class="s1">))</span>
        <span class="s1">y_pred[:</span><span class="s0">, </span><span class="s3">0</span><span class="s1">] = np.linspace(low</span><span class="s0">, </span><span class="s1">high</span><span class="s0">, </span><span class="s1">num=n)</span>
        <span class="s1">y_pred[:</span><span class="s0">, </span><span class="s3">1</span><span class="s1">] = </span><span class="s3">0.5 </span><span class="s1">* (</span><span class="s3">1 </span><span class="s1">- y_pred[:</span><span class="s0">, </span><span class="s3">0</span><span class="s1">])</span>
        <span class="s1">y_pred[:</span><span class="s0">, </span><span class="s3">2</span><span class="s1">] = </span><span class="s3">0.5 </span><span class="s1">* (</span><span class="s3">1 </span><span class="s1">- y_pred[:</span><span class="s0">, </span><span class="s3">0</span><span class="s1">])</span>
    <span class="s0">else</span><span class="s1">:</span>
        <span class="s1">y_pred = np.linspace(low</span><span class="s0">, </span><span class="s1">high</span><span class="s0">, </span><span class="s1">num=n)</span>

    <span class="s0">assert </span><span class="s1">loss.in_y_pred_range(y_pred)</span>

    <span class="s2"># calculating losses should not fail</span>
    <span class="s1">raw_prediction = loss.link.link(y_pred)</span>
    <span class="s1">loss.loss(y_true=y_true</span><span class="s0">, </span><span class="s1">raw_prediction=raw_prediction)</span>


<span class="s2"># Fixture to test valid value ranges.</span>
<span class="s1">Y_COMMON_PARAMS = [</span>
    <span class="s2"># (loss, [y success], [y fail])</span>
    <span class="s1">(HalfSquaredError()</span><span class="s0">, </span><span class="s1">[-</span><span class="s3">100</span><span class="s0">, </span><span class="s3">0</span><span class="s0">, </span><span class="s3">0.1</span><span class="s0">, </span><span class="s3">100</span><span class="s1">]</span><span class="s0">, </span><span class="s1">[-np.inf</span><span class="s0">, </span><span class="s1">np.inf])</span><span class="s0">,</span>
    <span class="s1">(AbsoluteError()</span><span class="s0">, </span><span class="s1">[-</span><span class="s3">100</span><span class="s0">, </span><span class="s3">0</span><span class="s0">, </span><span class="s3">0.1</span><span class="s0">, </span><span class="s3">100</span><span class="s1">]</span><span class="s0">, </span><span class="s1">[-np.inf</span><span class="s0">, </span><span class="s1">np.inf])</span><span class="s0">,</span>
    <span class="s1">(PinballLoss()</span><span class="s0">, </span><span class="s1">[-</span><span class="s3">100</span><span class="s0">, </span><span class="s3">0</span><span class="s0">, </span><span class="s3">0.1</span><span class="s0">, </span><span class="s3">100</span><span class="s1">]</span><span class="s0">, </span><span class="s1">[-np.inf</span><span class="s0">, </span><span class="s1">np.inf])</span><span class="s0">,</span>
    <span class="s1">(HuberLoss()</span><span class="s0">, </span><span class="s1">[-</span><span class="s3">100</span><span class="s0">, </span><span class="s3">0</span><span class="s0">, </span><span class="s3">0.1</span><span class="s0">, </span><span class="s3">100</span><span class="s1">]</span><span class="s0">, </span><span class="s1">[-np.inf</span><span class="s0">, </span><span class="s1">np.inf])</span><span class="s0">,</span>
    <span class="s1">(HalfPoissonLoss()</span><span class="s0">, </span><span class="s1">[</span><span class="s3">0.1</span><span class="s0">, </span><span class="s3">100</span><span class="s1">]</span><span class="s0">, </span><span class="s1">[-np.inf</span><span class="s0">, </span><span class="s1">-</span><span class="s3">3</span><span class="s0">, </span><span class="s1">-</span><span class="s3">0.1</span><span class="s0">, </span><span class="s1">np.inf])</span><span class="s0">,</span>
    <span class="s1">(HalfGammaLoss()</span><span class="s0">, </span><span class="s1">[</span><span class="s3">0.1</span><span class="s0">, </span><span class="s3">100</span><span class="s1">]</span><span class="s0">, </span><span class="s1">[-np.inf</span><span class="s0">, </span><span class="s1">-</span><span class="s3">3</span><span class="s0">, </span><span class="s1">-</span><span class="s3">0.1</span><span class="s0">, </span><span class="s3">0</span><span class="s0">, </span><span class="s1">np.inf])</span><span class="s0">,</span>
    <span class="s1">(HalfTweedieLoss(power=-</span><span class="s3">3</span><span class="s1">)</span><span class="s0">, </span><span class="s1">[</span><span class="s3">0.1</span><span class="s0">, </span><span class="s3">100</span><span class="s1">]</span><span class="s0">, </span><span class="s1">[-np.inf</span><span class="s0">, </span><span class="s1">np.inf])</span><span class="s0">,</span>
    <span class="s1">(HalfTweedieLoss(power=</span><span class="s3">0</span><span class="s1">)</span><span class="s0">, </span><span class="s1">[</span><span class="s3">0.1</span><span class="s0">, </span><span class="s3">100</span><span class="s1">]</span><span class="s0">, </span><span class="s1">[-np.inf</span><span class="s0">, </span><span class="s1">np.inf])</span><span class="s0">,</span>
    <span class="s1">(HalfTweedieLoss(power=</span><span class="s3">1.5</span><span class="s1">)</span><span class="s0">, </span><span class="s1">[</span><span class="s3">0.1</span><span class="s0">, </span><span class="s3">100</span><span class="s1">]</span><span class="s0">, </span><span class="s1">[-np.inf</span><span class="s0">, </span><span class="s1">-</span><span class="s3">3</span><span class="s0">, </span><span class="s1">-</span><span class="s3">0.1</span><span class="s0">, </span><span class="s1">np.inf])</span><span class="s0">,</span>
    <span class="s1">(HalfTweedieLoss(power=</span><span class="s3">2</span><span class="s1">)</span><span class="s0">, </span><span class="s1">[</span><span class="s3">0.1</span><span class="s0">, </span><span class="s3">100</span><span class="s1">]</span><span class="s0">, </span><span class="s1">[-np.inf</span><span class="s0">, </span><span class="s1">-</span><span class="s3">3</span><span class="s0">, </span><span class="s1">-</span><span class="s3">0.1</span><span class="s0">, </span><span class="s3">0</span><span class="s0">, </span><span class="s1">np.inf])</span><span class="s0">,</span>
    <span class="s1">(HalfTweedieLoss(power=</span><span class="s3">3</span><span class="s1">)</span><span class="s0">, </span><span class="s1">[</span><span class="s3">0.1</span><span class="s0">, </span><span class="s3">100</span><span class="s1">]</span><span class="s0">, </span><span class="s1">[-np.inf</span><span class="s0">, </span><span class="s1">-</span><span class="s3">3</span><span class="s0">, </span><span class="s1">-</span><span class="s3">0.1</span><span class="s0">, </span><span class="s3">0</span><span class="s0">, </span><span class="s1">np.inf])</span><span class="s0">,</span>
    <span class="s1">(HalfTweedieLossIdentity(power=-</span><span class="s3">3</span><span class="s1">)</span><span class="s0">, </span><span class="s1">[</span><span class="s3">0.1</span><span class="s0">, </span><span class="s3">100</span><span class="s1">]</span><span class="s0">, </span><span class="s1">[-np.inf</span><span class="s0">, </span><span class="s1">np.inf])</span><span class="s0">,</span>
    <span class="s1">(HalfTweedieLossIdentity(power=</span><span class="s3">0</span><span class="s1">)</span><span class="s0">, </span><span class="s1">[-</span><span class="s3">3</span><span class="s0">, </span><span class="s1">-</span><span class="s3">0.1</span><span class="s0">, </span><span class="s3">0</span><span class="s0">, </span><span class="s3">0.1</span><span class="s0">, </span><span class="s3">100</span><span class="s1">]</span><span class="s0">, </span><span class="s1">[-np.inf</span><span class="s0">, </span><span class="s1">np.inf])</span><span class="s0">,</span>
    <span class="s1">(HalfTweedieLossIdentity(power=</span><span class="s3">1.5</span><span class="s1">)</span><span class="s0">, </span><span class="s1">[</span><span class="s3">0.1</span><span class="s0">, </span><span class="s3">100</span><span class="s1">]</span><span class="s0">, </span><span class="s1">[-np.inf</span><span class="s0">, </span><span class="s1">-</span><span class="s3">3</span><span class="s0">, </span><span class="s1">-</span><span class="s3">0.1</span><span class="s0">, </span><span class="s1">np.inf])</span><span class="s0">,</span>
    <span class="s1">(HalfTweedieLossIdentity(power=</span><span class="s3">2</span><span class="s1">)</span><span class="s0">, </span><span class="s1">[</span><span class="s3">0.1</span><span class="s0">, </span><span class="s3">100</span><span class="s1">]</span><span class="s0">, </span><span class="s1">[-np.inf</span><span class="s0">, </span><span class="s1">-</span><span class="s3">3</span><span class="s0">, </span><span class="s1">-</span><span class="s3">0.1</span><span class="s0">, </span><span class="s3">0</span><span class="s0">, </span><span class="s1">np.inf])</span><span class="s0">,</span>
    <span class="s1">(HalfTweedieLossIdentity(power=</span><span class="s3">3</span><span class="s1">)</span><span class="s0">, </span><span class="s1">[</span><span class="s3">0.1</span><span class="s0">, </span><span class="s3">100</span><span class="s1">]</span><span class="s0">, </span><span class="s1">[-np.inf</span><span class="s0">, </span><span class="s1">-</span><span class="s3">3</span><span class="s0">, </span><span class="s1">-</span><span class="s3">0.1</span><span class="s0">, </span><span class="s3">0</span><span class="s0">, </span><span class="s1">np.inf])</span><span class="s0">,</span>
    <span class="s1">(HalfBinomialLoss()</span><span class="s0">, </span><span class="s1">[</span><span class="s3">0.1</span><span class="s0">, </span><span class="s3">0.5</span><span class="s0">, </span><span class="s3">0.9</span><span class="s1">]</span><span class="s0">, </span><span class="s1">[-np.inf</span><span class="s0">, </span><span class="s1">-</span><span class="s3">1</span><span class="s0">, </span><span class="s3">2</span><span class="s0">, </span><span class="s1">np.inf])</span><span class="s0">,</span>
    <span class="s1">(HalfMultinomialLoss()</span><span class="s0">, </span><span class="s1">[]</span><span class="s0">, </span><span class="s1">[-np.inf</span><span class="s0">, </span><span class="s1">-</span><span class="s3">1</span><span class="s0">, </span><span class="s3">1.1</span><span class="s0">, </span><span class="s1">np.inf])</span><span class="s0">,</span>
<span class="s1">]</span>
<span class="s2"># y_pred and y_true do not always have the same domain (valid value range).</span>
<span class="s2"># Hence, we define extra sets of parameters for each of them.</span>
<span class="s1">Y_TRUE_PARAMS = [  </span><span class="s2"># type: ignore</span>
    <span class="s2"># (loss, [y success], [y fail])</span>
    <span class="s1">(HalfPoissonLoss()</span><span class="s0">, </span><span class="s1">[</span><span class="s3">0</span><span class="s1">]</span><span class="s0">, </span><span class="s1">[])</span><span class="s0">,</span>
    <span class="s1">(HuberLoss()</span><span class="s0">, </span><span class="s1">[</span><span class="s3">0</span><span class="s1">]</span><span class="s0">, </span><span class="s1">[])</span><span class="s0">,</span>
    <span class="s1">(HalfTweedieLoss(power=-</span><span class="s3">3</span><span class="s1">)</span><span class="s0">, </span><span class="s1">[-</span><span class="s3">100</span><span class="s0">, </span><span class="s1">-</span><span class="s3">0.1</span><span class="s0">, </span><span class="s3">0</span><span class="s1">]</span><span class="s0">, </span><span class="s1">[])</span><span class="s0">,</span>
    <span class="s1">(HalfTweedieLoss(power=</span><span class="s3">0</span><span class="s1">)</span><span class="s0">, </span><span class="s1">[-</span><span class="s3">100</span><span class="s0">, </span><span class="s3">0</span><span class="s1">]</span><span class="s0">, </span><span class="s1">[])</span><span class="s0">,</span>
    <span class="s1">(HalfTweedieLoss(power=</span><span class="s3">1.5</span><span class="s1">)</span><span class="s0">, </span><span class="s1">[</span><span class="s3">0</span><span class="s1">]</span><span class="s0">, </span><span class="s1">[])</span><span class="s0">,</span>
    <span class="s1">(HalfTweedieLossIdentity(power=-</span><span class="s3">3</span><span class="s1">)</span><span class="s0">, </span><span class="s1">[-</span><span class="s3">100</span><span class="s0">, </span><span class="s1">-</span><span class="s3">0.1</span><span class="s0">, </span><span class="s3">0</span><span class="s1">]</span><span class="s0">, </span><span class="s1">[])</span><span class="s0">,</span>
    <span class="s1">(HalfTweedieLossIdentity(power=</span><span class="s3">0</span><span class="s1">)</span><span class="s0">, </span><span class="s1">[-</span><span class="s3">100</span><span class="s0">, </span><span class="s3">0</span><span class="s1">]</span><span class="s0">, </span><span class="s1">[])</span><span class="s0">,</span>
    <span class="s1">(HalfTweedieLossIdentity(power=</span><span class="s3">1.5</span><span class="s1">)</span><span class="s0">, </span><span class="s1">[</span><span class="s3">0</span><span class="s1">]</span><span class="s0">, </span><span class="s1">[])</span><span class="s0">,</span>
    <span class="s1">(HalfBinomialLoss()</span><span class="s0">, </span><span class="s1">[</span><span class="s3">0</span><span class="s0">, </span><span class="s3">1</span><span class="s1">]</span><span class="s0">, </span><span class="s1">[])</span><span class="s0">,</span>
    <span class="s1">(HalfMultinomialLoss()</span><span class="s0">, </span><span class="s1">[</span><span class="s3">0.0</span><span class="s0">, </span><span class="s3">1.0</span><span class="s0">, </span><span class="s3">2</span><span class="s1">]</span><span class="s0">, </span><span class="s1">[])</span><span class="s0">,</span>
<span class="s1">]</span>
<span class="s1">Y_PRED_PARAMS = [</span>
    <span class="s2"># (loss, [y success], [y fail])</span>
    <span class="s1">(HalfPoissonLoss()</span><span class="s0">, </span><span class="s1">[]</span><span class="s0">, </span><span class="s1">[</span><span class="s3">0</span><span class="s1">])</span><span class="s0">,</span>
    <span class="s1">(HalfTweedieLoss(power=-</span><span class="s3">3</span><span class="s1">)</span><span class="s0">, </span><span class="s1">[]</span><span class="s0">, </span><span class="s1">[-</span><span class="s3">3</span><span class="s0">, </span><span class="s1">-</span><span class="s3">0.1</span><span class="s0">, </span><span class="s3">0</span><span class="s1">])</span><span class="s0">,</span>
    <span class="s1">(HalfTweedieLoss(power=</span><span class="s3">0</span><span class="s1">)</span><span class="s0">, </span><span class="s1">[]</span><span class="s0">, </span><span class="s1">[-</span><span class="s3">3</span><span class="s0">, </span><span class="s1">-</span><span class="s3">0.1</span><span class="s0">, </span><span class="s3">0</span><span class="s1">])</span><span class="s0">,</span>
    <span class="s1">(HalfTweedieLoss(power=</span><span class="s3">1.5</span><span class="s1">)</span><span class="s0">, </span><span class="s1">[]</span><span class="s0">, </span><span class="s1">[</span><span class="s3">0</span><span class="s1">])</span><span class="s0">,</span>
    <span class="s1">(HalfTweedieLossIdentity(power=-</span><span class="s3">3</span><span class="s1">)</span><span class="s0">, </span><span class="s1">[]</span><span class="s0">, </span><span class="s1">[-</span><span class="s3">3</span><span class="s0">, </span><span class="s1">-</span><span class="s3">0.1</span><span class="s0">, </span><span class="s3">0</span><span class="s1">])</span><span class="s0">,</span>
    <span class="s1">(HalfTweedieLossIdentity(power=</span><span class="s3">0</span><span class="s1">)</span><span class="s0">, </span><span class="s1">[-</span><span class="s3">3</span><span class="s0">, </span><span class="s1">-</span><span class="s3">0.1</span><span class="s0">, </span><span class="s3">0</span><span class="s1">]</span><span class="s0">, </span><span class="s1">[])</span><span class="s0">,</span>
    <span class="s1">(HalfTweedieLossIdentity(power=</span><span class="s3">1.5</span><span class="s1">)</span><span class="s0">, </span><span class="s1">[]</span><span class="s0">, </span><span class="s1">[</span><span class="s3">0</span><span class="s1">])</span><span class="s0">,</span>
    <span class="s1">(HalfBinomialLoss()</span><span class="s0">, </span><span class="s1">[]</span><span class="s0">, </span><span class="s1">[</span><span class="s3">0</span><span class="s0">, </span><span class="s3">1</span><span class="s1">])</span><span class="s0">,</span>
    <span class="s1">(HalfMultinomialLoss()</span><span class="s0">, </span><span class="s1">[</span><span class="s3">0.1</span><span class="s0">, </span><span class="s3">0.5</span><span class="s1">]</span><span class="s0">, </span><span class="s1">[</span><span class="s3">0</span><span class="s0">, </span><span class="s3">1</span><span class="s1">])</span><span class="s0">,</span>
<span class="s1">]</span>


<span class="s1">@pytest.mark.parametrize(</span>
    <span class="s4">&quot;loss, y_true_success, y_true_fail&quot;</span><span class="s0">, </span><span class="s1">Y_COMMON_PARAMS + Y_TRUE_PARAMS</span>
<span class="s1">)</span>
<span class="s0">def </span><span class="s1">test_loss_boundary_y_true(loss</span><span class="s0">, </span><span class="s1">y_true_success</span><span class="s0">, </span><span class="s1">y_true_fail):</span>
    <span class="s5">&quot;&quot;&quot;Test boundaries of y_true for loss functions.&quot;&quot;&quot;</span>
    <span class="s0">for </span><span class="s1">y </span><span class="s0">in </span><span class="s1">y_true_success:</span>
        <span class="s0">assert </span><span class="s1">loss.in_y_true_range(np.array([y]))</span>
    <span class="s0">for </span><span class="s1">y </span><span class="s0">in </span><span class="s1">y_true_fail:</span>
        <span class="s0">assert not </span><span class="s1">loss.in_y_true_range(np.array([y]))</span>


<span class="s1">@pytest.mark.parametrize(</span>
    <span class="s4">&quot;loss, y_pred_success, y_pred_fail&quot;</span><span class="s0">, </span><span class="s1">Y_COMMON_PARAMS + Y_PRED_PARAMS  </span><span class="s2"># type: ignore</span>
<span class="s1">)</span>
<span class="s0">def </span><span class="s1">test_loss_boundary_y_pred(loss</span><span class="s0">, </span><span class="s1">y_pred_success</span><span class="s0">, </span><span class="s1">y_pred_fail):</span>
    <span class="s5">&quot;&quot;&quot;Test boundaries of y_pred for loss functions.&quot;&quot;&quot;</span>
    <span class="s0">for </span><span class="s1">y </span><span class="s0">in </span><span class="s1">y_pred_success:</span>
        <span class="s0">assert </span><span class="s1">loss.in_y_pred_range(np.array([y]))</span>
    <span class="s0">for </span><span class="s1">y </span><span class="s0">in </span><span class="s1">y_pred_fail:</span>
        <span class="s0">assert not </span><span class="s1">loss.in_y_pred_range(np.array([y]))</span>


<span class="s1">@pytest.mark.parametrize(</span>
    <span class="s4">&quot;loss, y_true, raw_prediction, loss_true&quot;</span><span class="s0">,</span>
    <span class="s1">[</span>
        <span class="s1">(HalfSquaredError()</span><span class="s0">, </span><span class="s3">1.0</span><span class="s0">, </span><span class="s3">5.0</span><span class="s0">, </span><span class="s3">8</span><span class="s1">)</span><span class="s0">,</span>
        <span class="s1">(AbsoluteError()</span><span class="s0">, </span><span class="s3">1.0</span><span class="s0">, </span><span class="s3">5.0</span><span class="s0">, </span><span class="s3">4</span><span class="s1">)</span><span class="s0">,</span>
        <span class="s1">(PinballLoss(quantile=</span><span class="s3">0.5</span><span class="s1">)</span><span class="s0">, </span><span class="s3">1.0</span><span class="s0">, </span><span class="s3">5.0</span><span class="s0">, </span><span class="s3">2</span><span class="s1">)</span><span class="s0">,</span>
        <span class="s1">(PinballLoss(quantile=</span><span class="s3">0.25</span><span class="s1">)</span><span class="s0">, </span><span class="s3">1.0</span><span class="s0">, </span><span class="s3">5.0</span><span class="s0">, </span><span class="s3">4 </span><span class="s1">* (</span><span class="s3">1 </span><span class="s1">- </span><span class="s3">0.25</span><span class="s1">))</span><span class="s0">,</span>
        <span class="s1">(PinballLoss(quantile=</span><span class="s3">0.25</span><span class="s1">)</span><span class="s0">, </span><span class="s3">5.0</span><span class="s0">, </span><span class="s3">1.0</span><span class="s0">, </span><span class="s3">4 </span><span class="s1">* </span><span class="s3">0.25</span><span class="s1">)</span><span class="s0">,</span>
        <span class="s1">(HuberLoss(quantile=</span><span class="s3">0.5</span><span class="s0">, </span><span class="s1">delta=</span><span class="s3">3</span><span class="s1">)</span><span class="s0">, </span><span class="s3">1.0</span><span class="s0">, </span><span class="s3">5.0</span><span class="s0">, </span><span class="s3">3 </span><span class="s1">* (</span><span class="s3">4 </span><span class="s1">- </span><span class="s3">3 </span><span class="s1">/ </span><span class="s3">2</span><span class="s1">))</span><span class="s0">,</span>
        <span class="s1">(HuberLoss(quantile=</span><span class="s3">0.5</span><span class="s0">, </span><span class="s1">delta=</span><span class="s3">3</span><span class="s1">)</span><span class="s0">, </span><span class="s3">1.0</span><span class="s0">, </span><span class="s3">3.0</span><span class="s0">, </span><span class="s3">0.5 </span><span class="s1">* </span><span class="s3">2</span><span class="s1">**</span><span class="s3">2</span><span class="s1">)</span><span class="s0">,</span>
        <span class="s1">(HalfPoissonLoss()</span><span class="s0">, </span><span class="s3">2.0</span><span class="s0">, </span><span class="s1">np.log(</span><span class="s3">4</span><span class="s1">)</span><span class="s0">, </span><span class="s3">4 </span><span class="s1">- </span><span class="s3">2 </span><span class="s1">* np.log(</span><span class="s3">4</span><span class="s1">))</span><span class="s0">,</span>
        <span class="s1">(HalfGammaLoss()</span><span class="s0">, </span><span class="s3">2.0</span><span class="s0">, </span><span class="s1">np.log(</span><span class="s3">4</span><span class="s1">)</span><span class="s0">, </span><span class="s1">np.log(</span><span class="s3">4</span><span class="s1">) + </span><span class="s3">2 </span><span class="s1">/ </span><span class="s3">4</span><span class="s1">)</span><span class="s0">,</span>
        <span class="s1">(HalfTweedieLoss(power=</span><span class="s3">3</span><span class="s1">)</span><span class="s0">, </span><span class="s3">2.0</span><span class="s0">, </span><span class="s1">np.log(</span><span class="s3">4</span><span class="s1">)</span><span class="s0">, </span><span class="s1">-</span><span class="s3">1 </span><span class="s1">/ </span><span class="s3">4 </span><span class="s1">+ </span><span class="s3">1 </span><span class="s1">/ </span><span class="s3">4</span><span class="s1">**</span><span class="s3">2</span><span class="s1">)</span><span class="s0">,</span>
        <span class="s1">(HalfTweedieLossIdentity(power=</span><span class="s3">1</span><span class="s1">)</span><span class="s0">, </span><span class="s3">2.0</span><span class="s0">, </span><span class="s3">4.0</span><span class="s0">, </span><span class="s3">2 </span><span class="s1">- </span><span class="s3">2 </span><span class="s1">* np.log(</span><span class="s3">2</span><span class="s1">))</span><span class="s0">,</span>
        <span class="s1">(HalfTweedieLossIdentity(power=</span><span class="s3">2</span><span class="s1">)</span><span class="s0">, </span><span class="s3">2.0</span><span class="s0">, </span><span class="s3">4.0</span><span class="s0">, </span><span class="s1">np.log(</span><span class="s3">2</span><span class="s1">) - </span><span class="s3">1 </span><span class="s1">/ </span><span class="s3">2</span><span class="s1">)</span><span class="s0">,</span>
        <span class="s1">(HalfTweedieLossIdentity(power=</span><span class="s3">3</span><span class="s1">)</span><span class="s0">, </span><span class="s3">2.0</span><span class="s0">, </span><span class="s3">4.0</span><span class="s0">, </span><span class="s1">-</span><span class="s3">1 </span><span class="s1">/ </span><span class="s3">4 </span><span class="s1">+ </span><span class="s3">1 </span><span class="s1">/ </span><span class="s3">4</span><span class="s1">**</span><span class="s3">2 </span><span class="s1">+ </span><span class="s3">1 </span><span class="s1">/ </span><span class="s3">2 </span><span class="s1">/ </span><span class="s3">2</span><span class="s1">)</span><span class="s0">,</span>
        <span class="s1">(HalfBinomialLoss()</span><span class="s0">, </span><span class="s3">0.25</span><span class="s0">, </span><span class="s1">np.log(</span><span class="s3">4</span><span class="s1">)</span><span class="s0">, </span><span class="s1">np.log(</span><span class="s3">5</span><span class="s1">) - </span><span class="s3">0.25 </span><span class="s1">* np.log(</span><span class="s3">4</span><span class="s1">))</span><span class="s0">,</span>
        <span class="s1">(</span>
            <span class="s1">HalfMultinomialLoss(n_classes=</span><span class="s3">3</span><span class="s1">)</span><span class="s0">,</span>
            <span class="s3">0.0</span><span class="s0">,</span>
            <span class="s1">[</span><span class="s3">0.2</span><span class="s0">, </span><span class="s3">0.5</span><span class="s0">, </span><span class="s3">0.3</span><span class="s1">]</span><span class="s0">,</span>
            <span class="s1">logsumexp([</span><span class="s3">0.2</span><span class="s0">, </span><span class="s3">0.5</span><span class="s0">, </span><span class="s3">0.3</span><span class="s1">]) - </span><span class="s3">0.2</span><span class="s0">,</span>
        <span class="s1">)</span><span class="s0">,</span>
        <span class="s1">(</span>
            <span class="s1">HalfMultinomialLoss(n_classes=</span><span class="s3">3</span><span class="s1">)</span><span class="s0">,</span>
            <span class="s3">1.0</span><span class="s0">,</span>
            <span class="s1">[</span><span class="s3">0.2</span><span class="s0">, </span><span class="s3">0.5</span><span class="s0">, </span><span class="s3">0.3</span><span class="s1">]</span><span class="s0">,</span>
            <span class="s1">logsumexp([</span><span class="s3">0.2</span><span class="s0">, </span><span class="s3">0.5</span><span class="s0">, </span><span class="s3">0.3</span><span class="s1">]) - </span><span class="s3">0.5</span><span class="s0">,</span>
        <span class="s1">)</span><span class="s0">,</span>
        <span class="s1">(</span>
            <span class="s1">HalfMultinomialLoss(n_classes=</span><span class="s3">3</span><span class="s1">)</span><span class="s0">,</span>
            <span class="s3">2.0</span><span class="s0">,</span>
            <span class="s1">[</span><span class="s3">0.2</span><span class="s0">, </span><span class="s3">0.5</span><span class="s0">, </span><span class="s3">0.3</span><span class="s1">]</span><span class="s0">,</span>
            <span class="s1">logsumexp([</span><span class="s3">0.2</span><span class="s0">, </span><span class="s3">0.5</span><span class="s0">, </span><span class="s3">0.3</span><span class="s1">]) - </span><span class="s3">0.3</span><span class="s0">,</span>
        <span class="s1">)</span><span class="s0">,</span>
    <span class="s1">]</span><span class="s0">,</span>
    <span class="s1">ids=loss_instance_name</span><span class="s0">,</span>
<span class="s1">)</span>
<span class="s0">def </span><span class="s1">test_loss_on_specific_values(loss</span><span class="s0">, </span><span class="s1">y_true</span><span class="s0">, </span><span class="s1">raw_prediction</span><span class="s0">, </span><span class="s1">loss_true):</span>
    <span class="s5">&quot;&quot;&quot;Test losses at specific values.&quot;&quot;&quot;</span>
    <span class="s0">assert </span><span class="s1">loss(</span>
        <span class="s1">y_true=np.array([y_true])</span><span class="s0">, </span><span class="s1">raw_prediction=np.array([raw_prediction])</span>
    <span class="s1">) == approx(loss_true</span><span class="s0">, </span><span class="s1">rel=</span><span class="s3">1e-11</span><span class="s0">, </span><span class="s1">abs=</span><span class="s3">1e-12</span><span class="s1">)</span>


<span class="s1">@pytest.mark.parametrize(</span><span class="s4">&quot;loss&quot;</span><span class="s0">, </span><span class="s1">ALL_LOSSES)</span>
<span class="s1">@pytest.mark.parametrize(</span><span class="s4">&quot;readonly_memmap&quot;</span><span class="s0">, </span><span class="s1">[</span><span class="s0">False, True</span><span class="s1">])</span>
<span class="s1">@pytest.mark.parametrize(</span><span class="s4">&quot;dtype_in&quot;</span><span class="s0">, </span><span class="s1">[np.float32</span><span class="s0">, </span><span class="s1">np.float64])</span>
<span class="s1">@pytest.mark.parametrize(</span><span class="s4">&quot;dtype_out&quot;</span><span class="s0">, </span><span class="s1">[np.float32</span><span class="s0">, </span><span class="s1">np.float64])</span>
<span class="s1">@pytest.mark.parametrize(</span><span class="s4">&quot;sample_weight&quot;</span><span class="s0">, </span><span class="s1">[</span><span class="s0">None, </span><span class="s3">1</span><span class="s1">])</span>
<span class="s1">@pytest.mark.parametrize(</span><span class="s4">&quot;out1&quot;</span><span class="s0">, </span><span class="s1">[</span><span class="s0">None, </span><span class="s3">1</span><span class="s1">])</span>
<span class="s1">@pytest.mark.parametrize(</span><span class="s4">&quot;out2&quot;</span><span class="s0">, </span><span class="s1">[</span><span class="s0">None, </span><span class="s3">1</span><span class="s1">])</span>
<span class="s1">@pytest.mark.parametrize(</span><span class="s4">&quot;n_threads&quot;</span><span class="s0">, </span><span class="s1">[</span><span class="s3">1</span><span class="s0">, </span><span class="s3">2</span><span class="s1">])</span>
<span class="s0">def </span><span class="s1">test_loss_dtype(</span>
    <span class="s1">loss</span><span class="s0">, </span><span class="s1">readonly_memmap</span><span class="s0">, </span><span class="s1">dtype_in</span><span class="s0">, </span><span class="s1">dtype_out</span><span class="s0">, </span><span class="s1">sample_weight</span><span class="s0">, </span><span class="s1">out1</span><span class="s0">, </span><span class="s1">out2</span><span class="s0">, </span><span class="s1">n_threads</span>
<span class="s1">):</span>
    <span class="s5">&quot;&quot;&quot;Test acceptance of dtypes, readonly and writeable arrays in loss functions. 
 
    Check that loss accepts if all input arrays are either all float32 or all 
    float64, and all output arrays are either all float32 or all float64. 
 
    Also check that input arrays can be readonly, e.g. memory mapped. 
    &quot;&quot;&quot;</span>
    <span class="s1">loss = loss()</span>
    <span class="s2"># generate a y_true and raw_prediction in valid range</span>
    <span class="s1">n_samples = </span><span class="s3">5</span>
    <span class="s1">y_true</span><span class="s0">, </span><span class="s1">raw_prediction = random_y_true_raw_prediction(</span>
        <span class="s1">loss=loss</span><span class="s0">,</span>
        <span class="s1">n_samples=n_samples</span><span class="s0">,</span>
        <span class="s1">y_bound=(-</span><span class="s3">100</span><span class="s0">, </span><span class="s3">100</span><span class="s1">)</span><span class="s0">,</span>
        <span class="s1">raw_bound=(-</span><span class="s3">10</span><span class="s0">, </span><span class="s3">10</span><span class="s1">)</span><span class="s0">,</span>
        <span class="s1">seed=</span><span class="s3">42</span><span class="s0">,</span>
    <span class="s1">)</span>
    <span class="s1">y_true = y_true.astype(dtype_in)</span>
    <span class="s1">raw_prediction = raw_prediction.astype(dtype_in)</span>

    <span class="s0">if </span><span class="s1">sample_weight </span><span class="s0">is not None</span><span class="s1">:</span>
        <span class="s1">sample_weight = np.array([</span><span class="s3">2.0</span><span class="s1">] * n_samples</span><span class="s0">, </span><span class="s1">dtype=dtype_in)</span>
    <span class="s0">if </span><span class="s1">out1 </span><span class="s0">is not None</span><span class="s1">:</span>
        <span class="s1">out1 = np.empty_like(y_true</span><span class="s0">, </span><span class="s1">dtype=dtype_out)</span>
    <span class="s0">if </span><span class="s1">out2 </span><span class="s0">is not None</span><span class="s1">:</span>
        <span class="s1">out2 = np.empty_like(raw_prediction</span><span class="s0">, </span><span class="s1">dtype=dtype_out)</span>

    <span class="s0">if </span><span class="s1">readonly_memmap:</span>
        <span class="s1">y_true = create_memmap_backed_data(y_true</span><span class="s0">, </span><span class="s1">aligned=</span><span class="s0">True</span><span class="s1">)</span>
        <span class="s1">raw_prediction = create_memmap_backed_data(raw_prediction</span><span class="s0">, </span><span class="s1">aligned=</span><span class="s0">True</span><span class="s1">)</span>
        <span class="s0">if </span><span class="s1">sample_weight </span><span class="s0">is not None</span><span class="s1">:</span>
            <span class="s1">sample_weight = create_memmap_backed_data(sample_weight</span><span class="s0">, </span><span class="s1">aligned=</span><span class="s0">True</span><span class="s1">)</span>

    <span class="s1">loss.loss(</span>
        <span class="s1">y_true=y_true</span><span class="s0">,</span>
        <span class="s1">raw_prediction=raw_prediction</span><span class="s0">,</span>
        <span class="s1">sample_weight=sample_weight</span><span class="s0">,</span>
        <span class="s1">loss_out=out1</span><span class="s0">,</span>
        <span class="s1">n_threads=n_threads</span><span class="s0">,</span>
    <span class="s1">)</span>
    <span class="s1">loss.gradient(</span>
        <span class="s1">y_true=y_true</span><span class="s0">,</span>
        <span class="s1">raw_prediction=raw_prediction</span><span class="s0">,</span>
        <span class="s1">sample_weight=sample_weight</span><span class="s0">,</span>
        <span class="s1">gradient_out=out2</span><span class="s0">,</span>
        <span class="s1">n_threads=n_threads</span><span class="s0">,</span>
    <span class="s1">)</span>
    <span class="s1">loss.loss_gradient(</span>
        <span class="s1">y_true=y_true</span><span class="s0">,</span>
        <span class="s1">raw_prediction=raw_prediction</span><span class="s0">,</span>
        <span class="s1">sample_weight=sample_weight</span><span class="s0">,</span>
        <span class="s1">loss_out=out1</span><span class="s0">,</span>
        <span class="s1">gradient_out=out2</span><span class="s0">,</span>
        <span class="s1">n_threads=n_threads</span><span class="s0">,</span>
    <span class="s1">)</span>
    <span class="s0">if </span><span class="s1">out1 </span><span class="s0">is not None and </span><span class="s1">loss.is_multiclass:</span>
        <span class="s1">out1 = np.empty_like(raw_prediction</span><span class="s0">, </span><span class="s1">dtype=dtype_out)</span>
    <span class="s1">loss.gradient_hessian(</span>
        <span class="s1">y_true=y_true</span><span class="s0">,</span>
        <span class="s1">raw_prediction=raw_prediction</span><span class="s0">,</span>
        <span class="s1">sample_weight=sample_weight</span><span class="s0">,</span>
        <span class="s1">gradient_out=out1</span><span class="s0">,</span>
        <span class="s1">hessian_out=out2</span><span class="s0">,</span>
        <span class="s1">n_threads=n_threads</span><span class="s0">,</span>
    <span class="s1">)</span>
    <span class="s1">loss(y_true=y_true</span><span class="s0">, </span><span class="s1">raw_prediction=raw_prediction</span><span class="s0">, </span><span class="s1">sample_weight=sample_weight)</span>
    <span class="s1">loss.fit_intercept_only(y_true=y_true</span><span class="s0">, </span><span class="s1">sample_weight=sample_weight)</span>
    <span class="s1">loss.constant_to_optimal_zero(y_true=y_true</span><span class="s0">, </span><span class="s1">sample_weight=sample_weight)</span>
    <span class="s0">if </span><span class="s1">hasattr(loss</span><span class="s0">, </span><span class="s4">&quot;predict_proba&quot;</span><span class="s1">):</span>
        <span class="s1">loss.predict_proba(raw_prediction=raw_prediction)</span>
    <span class="s0">if </span><span class="s1">hasattr(loss</span><span class="s0">, </span><span class="s4">&quot;gradient_proba&quot;</span><span class="s1">):</span>
        <span class="s1">loss.gradient_proba(</span>
            <span class="s1">y_true=y_true</span><span class="s0">,</span>
            <span class="s1">raw_prediction=raw_prediction</span><span class="s0">,</span>
            <span class="s1">sample_weight=sample_weight</span><span class="s0">,</span>
            <span class="s1">gradient_out=out1</span><span class="s0">,</span>
            <span class="s1">proba_out=out2</span><span class="s0">,</span>
            <span class="s1">n_threads=n_threads</span><span class="s0">,</span>
        <span class="s1">)</span>


<span class="s1">@pytest.mark.parametrize(</span><span class="s4">&quot;loss&quot;</span><span class="s0">, </span><span class="s1">LOSS_INSTANCES</span><span class="s0">, </span><span class="s1">ids=loss_instance_name)</span>
<span class="s1">@pytest.mark.parametrize(</span><span class="s4">&quot;sample_weight&quot;</span><span class="s0">, </span><span class="s1">[</span><span class="s0">None, </span><span class="s4">&quot;range&quot;</span><span class="s1">])</span>
<span class="s0">def </span><span class="s1">test_loss_same_as_C_functions(loss</span><span class="s0">, </span><span class="s1">sample_weight):</span>
    <span class="s5">&quot;&quot;&quot;Test that Python and Cython functions return same results.&quot;&quot;&quot;</span>
    <span class="s1">y_true</span><span class="s0">, </span><span class="s1">raw_prediction = random_y_true_raw_prediction(</span>
        <span class="s1">loss=loss</span><span class="s0">,</span>
        <span class="s1">n_samples=</span><span class="s3">20</span><span class="s0">,</span>
        <span class="s1">y_bound=(-</span><span class="s3">100</span><span class="s0">, </span><span class="s3">100</span><span class="s1">)</span><span class="s0">,</span>
        <span class="s1">raw_bound=(-</span><span class="s3">10</span><span class="s0">, </span><span class="s3">10</span><span class="s1">)</span><span class="s0">,</span>
        <span class="s1">seed=</span><span class="s3">42</span><span class="s0">,</span>
    <span class="s1">)</span>
    <span class="s0">if </span><span class="s1">sample_weight == </span><span class="s4">&quot;range&quot;</span><span class="s1">:</span>
        <span class="s1">sample_weight = np.linspace(</span><span class="s3">1</span><span class="s0">, </span><span class="s1">y_true.shape[</span><span class="s3">0</span><span class="s1">]</span><span class="s0">, </span><span class="s1">num=y_true.shape[</span><span class="s3">0</span><span class="s1">])</span>

    <span class="s1">out_l1 = np.empty_like(y_true)</span>
    <span class="s1">out_l2 = np.empty_like(y_true)</span>
    <span class="s1">out_g1 = np.empty_like(raw_prediction)</span>
    <span class="s1">out_g2 = np.empty_like(raw_prediction)</span>
    <span class="s1">out_h1 = np.empty_like(raw_prediction)</span>
    <span class="s1">out_h2 = np.empty_like(raw_prediction)</span>
    <span class="s1">assert_allclose(</span>
        <span class="s1">loss.loss(</span>
            <span class="s1">y_true=y_true</span><span class="s0">,</span>
            <span class="s1">raw_prediction=raw_prediction</span><span class="s0">,</span>
            <span class="s1">sample_weight=sample_weight</span><span class="s0">,</span>
            <span class="s1">loss_out=out_l1</span><span class="s0">,</span>
        <span class="s1">)</span><span class="s0">,</span>
        <span class="s1">loss.closs.loss(</span>
            <span class="s1">y_true=y_true</span><span class="s0">,</span>
            <span class="s1">raw_prediction=raw_prediction</span><span class="s0">,</span>
            <span class="s1">sample_weight=sample_weight</span><span class="s0">,</span>
            <span class="s1">loss_out=out_l2</span><span class="s0">,</span>
        <span class="s1">)</span><span class="s0">,</span>
    <span class="s1">)</span>
    <span class="s1">assert_allclose(</span>
        <span class="s1">loss.gradient(</span>
            <span class="s1">y_true=y_true</span><span class="s0">,</span>
            <span class="s1">raw_prediction=raw_prediction</span><span class="s0">,</span>
            <span class="s1">sample_weight=sample_weight</span><span class="s0">,</span>
            <span class="s1">gradient_out=out_g1</span><span class="s0">,</span>
        <span class="s1">)</span><span class="s0">,</span>
        <span class="s1">loss.closs.gradient(</span>
            <span class="s1">y_true=y_true</span><span class="s0">,</span>
            <span class="s1">raw_prediction=raw_prediction</span><span class="s0">,</span>
            <span class="s1">sample_weight=sample_weight</span><span class="s0">,</span>
            <span class="s1">gradient_out=out_g2</span><span class="s0">,</span>
        <span class="s1">)</span><span class="s0">,</span>
    <span class="s1">)</span>
    <span class="s1">loss.closs.loss_gradient(</span>
        <span class="s1">y_true=y_true</span><span class="s0">,</span>
        <span class="s1">raw_prediction=raw_prediction</span><span class="s0">,</span>
        <span class="s1">sample_weight=sample_weight</span><span class="s0">,</span>
        <span class="s1">loss_out=out_l1</span><span class="s0">,</span>
        <span class="s1">gradient_out=out_g1</span><span class="s0">,</span>
    <span class="s1">)</span>
    <span class="s1">loss.closs.loss_gradient(</span>
        <span class="s1">y_true=y_true</span><span class="s0">,</span>
        <span class="s1">raw_prediction=raw_prediction</span><span class="s0">,</span>
        <span class="s1">sample_weight=sample_weight</span><span class="s0">,</span>
        <span class="s1">loss_out=out_l2</span><span class="s0">,</span>
        <span class="s1">gradient_out=out_g2</span><span class="s0">,</span>
    <span class="s1">)</span>
    <span class="s1">assert_allclose(out_l1</span><span class="s0">, </span><span class="s1">out_l2)</span>
    <span class="s1">assert_allclose(out_g1</span><span class="s0">, </span><span class="s1">out_g2)</span>
    <span class="s1">loss.gradient_hessian(</span>
        <span class="s1">y_true=y_true</span><span class="s0">,</span>
        <span class="s1">raw_prediction=raw_prediction</span><span class="s0">,</span>
        <span class="s1">sample_weight=sample_weight</span><span class="s0">,</span>
        <span class="s1">gradient_out=out_g1</span><span class="s0">,</span>
        <span class="s1">hessian_out=out_h1</span><span class="s0">,</span>
    <span class="s1">)</span>
    <span class="s1">loss.closs.gradient_hessian(</span>
        <span class="s1">y_true=y_true</span><span class="s0">,</span>
        <span class="s1">raw_prediction=raw_prediction</span><span class="s0">,</span>
        <span class="s1">sample_weight=sample_weight</span><span class="s0">,</span>
        <span class="s1">gradient_out=out_g2</span><span class="s0">,</span>
        <span class="s1">hessian_out=out_h2</span><span class="s0">,</span>
    <span class="s1">)</span>
    <span class="s1">assert_allclose(out_g1</span><span class="s0">, </span><span class="s1">out_g2)</span>
    <span class="s1">assert_allclose(out_h1</span><span class="s0">, </span><span class="s1">out_h2)</span>


<span class="s1">@pytest.mark.parametrize(</span><span class="s4">&quot;loss&quot;</span><span class="s0">, </span><span class="s1">LOSS_INSTANCES</span><span class="s0">, </span><span class="s1">ids=loss_instance_name)</span>
<span class="s1">@pytest.mark.parametrize(</span><span class="s4">&quot;sample_weight&quot;</span><span class="s0">, </span><span class="s1">[</span><span class="s0">None, </span><span class="s4">&quot;range&quot;</span><span class="s1">])</span>
<span class="s0">def </span><span class="s1">test_loss_gradients_are_the_same(loss</span><span class="s0">, </span><span class="s1">sample_weight</span><span class="s0">, </span><span class="s1">global_random_seed):</span>
    <span class="s5">&quot;&quot;&quot;Test that loss and gradient are the same across different functions. 
 
    Also test that output arguments contain correct results. 
    &quot;&quot;&quot;</span>
    <span class="s1">y_true</span><span class="s0">, </span><span class="s1">raw_prediction = random_y_true_raw_prediction(</span>
        <span class="s1">loss=loss</span><span class="s0">,</span>
        <span class="s1">n_samples=</span><span class="s3">20</span><span class="s0">,</span>
        <span class="s1">y_bound=(-</span><span class="s3">100</span><span class="s0">, </span><span class="s3">100</span><span class="s1">)</span><span class="s0">,</span>
        <span class="s1">raw_bound=(-</span><span class="s3">10</span><span class="s0">, </span><span class="s3">10</span><span class="s1">)</span><span class="s0">,</span>
        <span class="s1">seed=global_random_seed</span><span class="s0">,</span>
    <span class="s1">)</span>
    <span class="s0">if </span><span class="s1">sample_weight == </span><span class="s4">&quot;range&quot;</span><span class="s1">:</span>
        <span class="s1">sample_weight = np.linspace(</span><span class="s3">1</span><span class="s0">, </span><span class="s1">y_true.shape[</span><span class="s3">0</span><span class="s1">]</span><span class="s0">, </span><span class="s1">num=y_true.shape[</span><span class="s3">0</span><span class="s1">])</span>

    <span class="s1">out_l1 = np.empty_like(y_true)</span>
    <span class="s1">out_l2 = np.empty_like(y_true)</span>
    <span class="s1">out_g1 = np.empty_like(raw_prediction)</span>
    <span class="s1">out_g2 = np.empty_like(raw_prediction)</span>
    <span class="s1">out_g3 = np.empty_like(raw_prediction)</span>
    <span class="s1">out_h3 = np.empty_like(raw_prediction)</span>

    <span class="s1">l1 = loss.loss(</span>
        <span class="s1">y_true=y_true</span><span class="s0">,</span>
        <span class="s1">raw_prediction=raw_prediction</span><span class="s0">,</span>
        <span class="s1">sample_weight=sample_weight</span><span class="s0">,</span>
        <span class="s1">loss_out=out_l1</span><span class="s0">,</span>
    <span class="s1">)</span>
    <span class="s1">g1 = loss.gradient(</span>
        <span class="s1">y_true=y_true</span><span class="s0">,</span>
        <span class="s1">raw_prediction=raw_prediction</span><span class="s0">,</span>
        <span class="s1">sample_weight=sample_weight</span><span class="s0">,</span>
        <span class="s1">gradient_out=out_g1</span><span class="s0">,</span>
    <span class="s1">)</span>
    <span class="s1">l2</span><span class="s0">, </span><span class="s1">g2 = loss.loss_gradient(</span>
        <span class="s1">y_true=y_true</span><span class="s0">,</span>
        <span class="s1">raw_prediction=raw_prediction</span><span class="s0">,</span>
        <span class="s1">sample_weight=sample_weight</span><span class="s0">,</span>
        <span class="s1">loss_out=out_l2</span><span class="s0">,</span>
        <span class="s1">gradient_out=out_g2</span><span class="s0">,</span>
    <span class="s1">)</span>
    <span class="s1">g3</span><span class="s0">, </span><span class="s1">h3 = loss.gradient_hessian(</span>
        <span class="s1">y_true=y_true</span><span class="s0">,</span>
        <span class="s1">raw_prediction=raw_prediction</span><span class="s0">,</span>
        <span class="s1">sample_weight=sample_weight</span><span class="s0">,</span>
        <span class="s1">gradient_out=out_g3</span><span class="s0">,</span>
        <span class="s1">hessian_out=out_h3</span><span class="s0">,</span>
    <span class="s1">)</span>
    <span class="s1">assert_allclose(l1</span><span class="s0">, </span><span class="s1">l2)</span>
    <span class="s1">assert_array_equal(l1</span><span class="s0">, </span><span class="s1">out_l1)</span>
    <span class="s0">assert </span><span class="s1">np.shares_memory(l1</span><span class="s0">, </span><span class="s1">out_l1)</span>
    <span class="s1">assert_array_equal(l2</span><span class="s0">, </span><span class="s1">out_l2)</span>
    <span class="s0">assert </span><span class="s1">np.shares_memory(l2</span><span class="s0">, </span><span class="s1">out_l2)</span>
    <span class="s1">assert_allclose(g1</span><span class="s0">, </span><span class="s1">g2)</span>
    <span class="s1">assert_allclose(g1</span><span class="s0">, </span><span class="s1">g3)</span>
    <span class="s1">assert_array_equal(g1</span><span class="s0">, </span><span class="s1">out_g1)</span>
    <span class="s0">assert </span><span class="s1">np.shares_memory(g1</span><span class="s0">, </span><span class="s1">out_g1)</span>
    <span class="s1">assert_array_equal(g2</span><span class="s0">, </span><span class="s1">out_g2)</span>
    <span class="s0">assert </span><span class="s1">np.shares_memory(g2</span><span class="s0">, </span><span class="s1">out_g2)</span>
    <span class="s1">assert_array_equal(g3</span><span class="s0">, </span><span class="s1">out_g3)</span>
    <span class="s0">assert </span><span class="s1">np.shares_memory(g3</span><span class="s0">, </span><span class="s1">out_g3)</span>

    <span class="s0">if </span><span class="s1">hasattr(loss</span><span class="s0">, </span><span class="s4">&quot;gradient_proba&quot;</span><span class="s1">):</span>
        <span class="s0">assert </span><span class="s1">loss.is_multiclass  </span><span class="s2"># only for HalfMultinomialLoss</span>
        <span class="s1">out_g4 = np.empty_like(raw_prediction)</span>
        <span class="s1">out_proba = np.empty_like(raw_prediction)</span>
        <span class="s1">g4</span><span class="s0">, </span><span class="s1">proba = loss.gradient_proba(</span>
            <span class="s1">y_true=y_true</span><span class="s0">,</span>
            <span class="s1">raw_prediction=raw_prediction</span><span class="s0">,</span>
            <span class="s1">sample_weight=sample_weight</span><span class="s0">,</span>
            <span class="s1">gradient_out=out_g4</span><span class="s0">,</span>
            <span class="s1">proba_out=out_proba</span><span class="s0">,</span>
        <span class="s1">)</span>
        <span class="s1">assert_allclose(g1</span><span class="s0">, </span><span class="s1">out_g4)</span>
        <span class="s1">assert_allclose(g1</span><span class="s0">, </span><span class="s1">g4)</span>
        <span class="s1">assert_allclose(proba</span><span class="s0">, </span><span class="s1">out_proba)</span>
        <span class="s1">assert_allclose(np.sum(proba</span><span class="s0">, </span><span class="s1">axis=</span><span class="s3">1</span><span class="s1">)</span><span class="s0">, </span><span class="s3">1</span><span class="s0">, </span><span class="s1">rtol=</span><span class="s3">1e-11</span><span class="s1">)</span>


<span class="s1">@pytest.mark.parametrize(</span><span class="s4">&quot;loss&quot;</span><span class="s0">, </span><span class="s1">LOSS_INSTANCES</span><span class="s0">, </span><span class="s1">ids=loss_instance_name)</span>
<span class="s1">@pytest.mark.parametrize(</span><span class="s4">&quot;sample_weight&quot;</span><span class="s0">, </span><span class="s1">[</span><span class="s4">&quot;ones&quot;</span><span class="s0">, </span><span class="s4">&quot;random&quot;</span><span class="s1">])</span>
<span class="s0">def </span><span class="s1">test_sample_weight_multiplies(loss</span><span class="s0">, </span><span class="s1">sample_weight</span><span class="s0">, </span><span class="s1">global_random_seed):</span>
    <span class="s5">&quot;&quot;&quot;Test sample weights in loss, gradients and hessians. 
 
    Make sure that passing sample weights to loss, gradient and hessian 
    computation methods is equivalent to multiplying by the weights. 
    &quot;&quot;&quot;</span>
    <span class="s1">n_samples = </span><span class="s3">100</span>
    <span class="s1">y_true</span><span class="s0">, </span><span class="s1">raw_prediction = random_y_true_raw_prediction(</span>
        <span class="s1">loss=loss</span><span class="s0">,</span>
        <span class="s1">n_samples=n_samples</span><span class="s0">,</span>
        <span class="s1">y_bound=(-</span><span class="s3">100</span><span class="s0">, </span><span class="s3">100</span><span class="s1">)</span><span class="s0">,</span>
        <span class="s1">raw_bound=(-</span><span class="s3">5</span><span class="s0">, </span><span class="s3">5</span><span class="s1">)</span><span class="s0">,</span>
        <span class="s1">seed=global_random_seed</span><span class="s0">,</span>
    <span class="s1">)</span>

    <span class="s0">if </span><span class="s1">sample_weight == </span><span class="s4">&quot;ones&quot;</span><span class="s1">:</span>
        <span class="s1">sample_weight = np.ones(shape=n_samples</span><span class="s0">, </span><span class="s1">dtype=np.float64)</span>
    <span class="s0">else</span><span class="s1">:</span>
        <span class="s1">rng = np.random.RandomState(global_random_seed)</span>
        <span class="s1">sample_weight = rng.normal(size=n_samples).astype(np.float64)</span>

    <span class="s1">assert_allclose(</span>
        <span class="s1">loss.loss(</span>
            <span class="s1">y_true=y_true</span><span class="s0">,</span>
            <span class="s1">raw_prediction=raw_prediction</span><span class="s0">,</span>
            <span class="s1">sample_weight=sample_weight</span><span class="s0">,</span>
        <span class="s1">)</span><span class="s0">,</span>
        <span class="s1">sample_weight</span>
        <span class="s1">* loss.loss(</span>
            <span class="s1">y_true=y_true</span><span class="s0">,</span>
            <span class="s1">raw_prediction=raw_prediction</span><span class="s0">,</span>
            <span class="s1">sample_weight=</span><span class="s0">None,</span>
        <span class="s1">)</span><span class="s0">,</span>
    <span class="s1">)</span>

    <span class="s1">losses</span><span class="s0">, </span><span class="s1">gradient = loss.loss_gradient(</span>
        <span class="s1">y_true=y_true</span><span class="s0">,</span>
        <span class="s1">raw_prediction=raw_prediction</span><span class="s0">,</span>
        <span class="s1">sample_weight=</span><span class="s0">None,</span>
    <span class="s1">)</span>
    <span class="s1">losses_sw</span><span class="s0">, </span><span class="s1">gradient_sw = loss.loss_gradient(</span>
        <span class="s1">y_true=y_true</span><span class="s0">,</span>
        <span class="s1">raw_prediction=raw_prediction</span><span class="s0">,</span>
        <span class="s1">sample_weight=sample_weight</span><span class="s0">,</span>
    <span class="s1">)</span>
    <span class="s1">assert_allclose(losses * sample_weight</span><span class="s0">, </span><span class="s1">losses_sw)</span>
    <span class="s0">if not </span><span class="s1">loss.is_multiclass:</span>
        <span class="s1">assert_allclose(gradient * sample_weight</span><span class="s0">, </span><span class="s1">gradient_sw)</span>
    <span class="s0">else</span><span class="s1">:</span>
        <span class="s1">assert_allclose(gradient * sample_weight[:</span><span class="s0">, None</span><span class="s1">]</span><span class="s0">, </span><span class="s1">gradient_sw)</span>

    <span class="s1">gradient</span><span class="s0">, </span><span class="s1">hessian = loss.gradient_hessian(</span>
        <span class="s1">y_true=y_true</span><span class="s0">,</span>
        <span class="s1">raw_prediction=raw_prediction</span><span class="s0">,</span>
        <span class="s1">sample_weight=</span><span class="s0">None,</span>
    <span class="s1">)</span>
    <span class="s1">gradient_sw</span><span class="s0">, </span><span class="s1">hessian_sw = loss.gradient_hessian(</span>
        <span class="s1">y_true=y_true</span><span class="s0">,</span>
        <span class="s1">raw_prediction=raw_prediction</span><span class="s0">,</span>
        <span class="s1">sample_weight=sample_weight</span><span class="s0">,</span>
    <span class="s1">)</span>
    <span class="s0">if not </span><span class="s1">loss.is_multiclass:</span>
        <span class="s1">assert_allclose(gradient * sample_weight</span><span class="s0">, </span><span class="s1">gradient_sw)</span>
        <span class="s1">assert_allclose(hessian * sample_weight</span><span class="s0">, </span><span class="s1">hessian_sw)</span>
    <span class="s0">else</span><span class="s1">:</span>
        <span class="s1">assert_allclose(gradient * sample_weight[:</span><span class="s0">, None</span><span class="s1">]</span><span class="s0">, </span><span class="s1">gradient_sw)</span>
        <span class="s1">assert_allclose(hessian * sample_weight[:</span><span class="s0">, None</span><span class="s1">]</span><span class="s0">, </span><span class="s1">hessian_sw)</span>


<span class="s1">@pytest.mark.parametrize(</span><span class="s4">&quot;loss&quot;</span><span class="s0">, </span><span class="s1">LOSS_INSTANCES</span><span class="s0">, </span><span class="s1">ids=loss_instance_name)</span>
<span class="s0">def </span><span class="s1">test_graceful_squeezing(loss):</span>
    <span class="s5">&quot;&quot;&quot;Test that reshaped raw_prediction gives same results.&quot;&quot;&quot;</span>
    <span class="s1">y_true</span><span class="s0">, </span><span class="s1">raw_prediction = random_y_true_raw_prediction(</span>
        <span class="s1">loss=loss</span><span class="s0">,</span>
        <span class="s1">n_samples=</span><span class="s3">20</span><span class="s0">,</span>
        <span class="s1">y_bound=(-</span><span class="s3">100</span><span class="s0">, </span><span class="s3">100</span><span class="s1">)</span><span class="s0">,</span>
        <span class="s1">raw_bound=(-</span><span class="s3">10</span><span class="s0">, </span><span class="s3">10</span><span class="s1">)</span><span class="s0">,</span>
        <span class="s1">seed=</span><span class="s3">42</span><span class="s0">,</span>
    <span class="s1">)</span>

    <span class="s0">if </span><span class="s1">raw_prediction.ndim == </span><span class="s3">1</span><span class="s1">:</span>
        <span class="s1">raw_prediction_2d = raw_prediction[:</span><span class="s0">, None</span><span class="s1">]</span>
        <span class="s1">assert_allclose(</span>
            <span class="s1">loss.loss(y_true=y_true</span><span class="s0">, </span><span class="s1">raw_prediction=raw_prediction_2d)</span><span class="s0">,</span>
            <span class="s1">loss.loss(y_true=y_true</span><span class="s0">, </span><span class="s1">raw_prediction=raw_prediction)</span><span class="s0">,</span>
        <span class="s1">)</span>
        <span class="s1">assert_allclose(</span>
            <span class="s1">loss.loss_gradient(y_true=y_true</span><span class="s0">, </span><span class="s1">raw_prediction=raw_prediction_2d)</span><span class="s0">,</span>
            <span class="s1">loss.loss_gradient(y_true=y_true</span><span class="s0">, </span><span class="s1">raw_prediction=raw_prediction)</span><span class="s0">,</span>
        <span class="s1">)</span>
        <span class="s1">assert_allclose(</span>
            <span class="s1">loss.gradient(y_true=y_true</span><span class="s0">, </span><span class="s1">raw_prediction=raw_prediction_2d)</span><span class="s0">,</span>
            <span class="s1">loss.gradient(y_true=y_true</span><span class="s0">, </span><span class="s1">raw_prediction=raw_prediction)</span><span class="s0">,</span>
        <span class="s1">)</span>
        <span class="s1">assert_allclose(</span>
            <span class="s1">loss.gradient_hessian(y_true=y_true</span><span class="s0">, </span><span class="s1">raw_prediction=raw_prediction_2d)</span><span class="s0">,</span>
            <span class="s1">loss.gradient_hessian(y_true=y_true</span><span class="s0">, </span><span class="s1">raw_prediction=raw_prediction)</span><span class="s0">,</span>
        <span class="s1">)</span>


<span class="s1">@pytest.mark.parametrize(</span><span class="s4">&quot;loss&quot;</span><span class="s0">, </span><span class="s1">LOSS_INSTANCES</span><span class="s0">, </span><span class="s1">ids=loss_instance_name)</span>
<span class="s1">@pytest.mark.parametrize(</span><span class="s4">&quot;sample_weight&quot;</span><span class="s0">, </span><span class="s1">[</span><span class="s0">None, </span><span class="s4">&quot;range&quot;</span><span class="s1">])</span>
<span class="s0">def </span><span class="s1">test_loss_of_perfect_prediction(loss</span><span class="s0">, </span><span class="s1">sample_weight):</span>
    <span class="s5">&quot;&quot;&quot;Test value of perfect predictions. 
 
    Loss of y_pred = y_true plus constant_to_optimal_zero should sums up to 
    zero. 
    &quot;&quot;&quot;</span>
    <span class="s0">if not </span><span class="s1">loss.is_multiclass:</span>
        <span class="s2"># Use small values such that exp(value) is not nan.</span>
        <span class="s1">raw_prediction = np.array([-</span><span class="s3">10</span><span class="s0">, </span><span class="s1">-</span><span class="s3">0.1</span><span class="s0">, </span><span class="s3">0</span><span class="s0">, </span><span class="s3">0.1</span><span class="s0">, </span><span class="s3">3</span><span class="s0">, </span><span class="s3">10</span><span class="s1">])</span>
        <span class="s2"># If link is identity, we must respect the interval of y_pred:</span>
        <span class="s0">if </span><span class="s1">isinstance(loss.link</span><span class="s0">, </span><span class="s1">IdentityLink):</span>
            <span class="s1">eps = </span><span class="s3">1e-10</span>
            <span class="s1">low = loss.interval_y_pred.low</span>
            <span class="s0">if not </span><span class="s1">loss.interval_y_pred.low_inclusive:</span>
                <span class="s1">low = low + eps</span>
            <span class="s1">high = loss.interval_y_pred.high</span>
            <span class="s0">if not </span><span class="s1">loss.interval_y_pred.high_inclusive:</span>
                <span class="s1">high = high - eps</span>
            <span class="s1">raw_prediction = np.clip(raw_prediction</span><span class="s0">, </span><span class="s1">low</span><span class="s0">, </span><span class="s1">high)</span>
        <span class="s1">y_true = loss.link.inverse(raw_prediction)</span>
    <span class="s0">else</span><span class="s1">:</span>
        <span class="s2"># HalfMultinomialLoss</span>
        <span class="s1">y_true = np.arange(loss.n_classes).astype(float)</span>
        <span class="s2"># raw_prediction with entries -exp(10), but +exp(10) on the diagonal</span>
        <span class="s2"># this is close enough to np.inf which would produce nan</span>
        <span class="s1">raw_prediction = np.full(</span>
            <span class="s1">shape=(loss.n_classes</span><span class="s0">, </span><span class="s1">loss.n_classes)</span><span class="s0">,</span>
            <span class="s1">fill_value=-np.exp(</span><span class="s3">10</span><span class="s1">)</span><span class="s0">,</span>
            <span class="s1">dtype=float</span><span class="s0">,</span>
        <span class="s1">)</span>
        <span class="s1">raw_prediction.flat[:: loss.n_classes + </span><span class="s3">1</span><span class="s1">] = np.exp(</span><span class="s3">10</span><span class="s1">)</span>

    <span class="s0">if </span><span class="s1">sample_weight == </span><span class="s4">&quot;range&quot;</span><span class="s1">:</span>
        <span class="s1">sample_weight = np.linspace(</span><span class="s3">1</span><span class="s0">, </span><span class="s1">y_true.shape[</span><span class="s3">0</span><span class="s1">]</span><span class="s0">, </span><span class="s1">num=y_true.shape[</span><span class="s3">0</span><span class="s1">])</span>

    <span class="s1">loss_value = loss.loss(</span>
        <span class="s1">y_true=y_true</span><span class="s0">,</span>
        <span class="s1">raw_prediction=raw_prediction</span><span class="s0">,</span>
        <span class="s1">sample_weight=sample_weight</span><span class="s0">,</span>
    <span class="s1">)</span>
    <span class="s1">constant_term = loss.constant_to_optimal_zero(</span>
        <span class="s1">y_true=y_true</span><span class="s0">, </span><span class="s1">sample_weight=sample_weight</span>
    <span class="s1">)</span>
    <span class="s2"># Comparing loss_value + constant_term to zero would result in large</span>
    <span class="s2"># round-off errors.</span>
    <span class="s1">assert_allclose(loss_value</span><span class="s0">, </span><span class="s1">-constant_term</span><span class="s0">, </span><span class="s1">atol=</span><span class="s3">1e-14</span><span class="s0">, </span><span class="s1">rtol=</span><span class="s3">1e-15</span><span class="s1">)</span>


<span class="s1">@pytest.mark.parametrize(</span><span class="s4">&quot;loss&quot;</span><span class="s0">, </span><span class="s1">LOSS_INSTANCES</span><span class="s0">, </span><span class="s1">ids=loss_instance_name)</span>
<span class="s1">@pytest.mark.parametrize(</span><span class="s4">&quot;sample_weight&quot;</span><span class="s0">, </span><span class="s1">[</span><span class="s0">None, </span><span class="s4">&quot;range&quot;</span><span class="s1">])</span>
<span class="s0">def </span><span class="s1">test_gradients_hessians_numerically(loss</span><span class="s0">, </span><span class="s1">sample_weight</span><span class="s0">, </span><span class="s1">global_random_seed):</span>
    <span class="s5">&quot;&quot;&quot;Test gradients and hessians with numerical derivatives. 
 
    Gradient should equal the numerical derivatives of the loss function. 
    Hessians should equal the numerical derivatives of gradients. 
    &quot;&quot;&quot;</span>
    <span class="s1">n_samples = </span><span class="s3">20</span>
    <span class="s1">y_true</span><span class="s0">, </span><span class="s1">raw_prediction = random_y_true_raw_prediction(</span>
        <span class="s1">loss=loss</span><span class="s0">,</span>
        <span class="s1">n_samples=n_samples</span><span class="s0">,</span>
        <span class="s1">y_bound=(-</span><span class="s3">100</span><span class="s0">, </span><span class="s3">100</span><span class="s1">)</span><span class="s0">,</span>
        <span class="s1">raw_bound=(-</span><span class="s3">5</span><span class="s0">, </span><span class="s3">5</span><span class="s1">)</span><span class="s0">,</span>
        <span class="s1">seed=global_random_seed</span><span class="s0">,</span>
    <span class="s1">)</span>

    <span class="s0">if </span><span class="s1">sample_weight == </span><span class="s4">&quot;range&quot;</span><span class="s1">:</span>
        <span class="s1">sample_weight = np.linspace(</span><span class="s3">1</span><span class="s0">, </span><span class="s1">y_true.shape[</span><span class="s3">0</span><span class="s1">]</span><span class="s0">, </span><span class="s1">num=y_true.shape[</span><span class="s3">0</span><span class="s1">])</span>

    <span class="s1">g</span><span class="s0">, </span><span class="s1">h = loss.gradient_hessian(</span>
        <span class="s1">y_true=y_true</span><span class="s0">,</span>
        <span class="s1">raw_prediction=raw_prediction</span><span class="s0">,</span>
        <span class="s1">sample_weight=sample_weight</span><span class="s0">,</span>
    <span class="s1">)</span>

    <span class="s0">assert </span><span class="s1">g.shape == raw_prediction.shape</span>
    <span class="s0">assert </span><span class="s1">h.shape == raw_prediction.shape</span>

    <span class="s0">if not </span><span class="s1">loss.is_multiclass:</span>

        <span class="s0">def </span><span class="s1">loss_func(x):</span>
            <span class="s0">return </span><span class="s1">loss.loss(</span>
                <span class="s1">y_true=y_true</span><span class="s0">,</span>
                <span class="s1">raw_prediction=x</span><span class="s0">,</span>
                <span class="s1">sample_weight=sample_weight</span><span class="s0">,</span>
            <span class="s1">)</span>

        <span class="s1">g_numeric = numerical_derivative(loss_func</span><span class="s0">, </span><span class="s1">raw_prediction</span><span class="s0">, </span><span class="s1">eps=</span><span class="s3">1e-6</span><span class="s1">)</span>
        <span class="s1">assert_allclose(g</span><span class="s0">, </span><span class="s1">g_numeric</span><span class="s0">, </span><span class="s1">rtol=</span><span class="s3">5e-6</span><span class="s0">, </span><span class="s1">atol=</span><span class="s3">1e-10</span><span class="s1">)</span>

        <span class="s0">def </span><span class="s1">grad_func(x):</span>
            <span class="s0">return </span><span class="s1">loss.gradient(</span>
                <span class="s1">y_true=y_true</span><span class="s0">,</span>
                <span class="s1">raw_prediction=x</span><span class="s0">,</span>
                <span class="s1">sample_weight=sample_weight</span><span class="s0">,</span>
            <span class="s1">)</span>

        <span class="s1">h_numeric = numerical_derivative(grad_func</span><span class="s0">, </span><span class="s1">raw_prediction</span><span class="s0">, </span><span class="s1">eps=</span><span class="s3">1e-6</span><span class="s1">)</span>
        <span class="s0">if </span><span class="s1">loss.approx_hessian:</span>
            <span class="s2"># TODO: What could we test if loss.approx_hessian?</span>
            <span class="s0">pass</span>
        <span class="s0">else</span><span class="s1">:</span>
            <span class="s1">assert_allclose(h</span><span class="s0">, </span><span class="s1">h_numeric</span><span class="s0">, </span><span class="s1">rtol=</span><span class="s3">5e-6</span><span class="s0">, </span><span class="s1">atol=</span><span class="s3">1e-10</span><span class="s1">)</span>
    <span class="s0">else</span><span class="s1">:</span>
        <span class="s2"># For multiclass loss, we should only change the predictions of the</span>
        <span class="s2"># class for which the derivative is taken for, e.g. offset[:, k] = eps</span>
        <span class="s2"># for class k.</span>
        <span class="s2"># As a softmax is computed, offsetting the whole array by a constant</span>
        <span class="s2"># would have no effect on the probabilities, and thus on the loss.</span>
        <span class="s0">for </span><span class="s1">k </span><span class="s0">in </span><span class="s1">range(loss.n_classes):</span>

            <span class="s0">def </span><span class="s1">loss_func(x):</span>
                <span class="s1">raw = raw_prediction.copy()</span>
                <span class="s1">raw[:</span><span class="s0">, </span><span class="s1">k] = x</span>
                <span class="s0">return </span><span class="s1">loss.loss(</span>
                    <span class="s1">y_true=y_true</span><span class="s0">,</span>
                    <span class="s1">raw_prediction=raw</span><span class="s0">,</span>
                    <span class="s1">sample_weight=sample_weight</span><span class="s0">,</span>
                <span class="s1">)</span>

            <span class="s1">g_numeric = numerical_derivative(loss_func</span><span class="s0">, </span><span class="s1">raw_prediction[:</span><span class="s0">, </span><span class="s1">k]</span><span class="s0">, </span><span class="s1">eps=</span><span class="s3">1e-5</span><span class="s1">)</span>
            <span class="s1">assert_allclose(g[:</span><span class="s0">, </span><span class="s1">k]</span><span class="s0">, </span><span class="s1">g_numeric</span><span class="s0">, </span><span class="s1">rtol=</span><span class="s3">5e-6</span><span class="s0">, </span><span class="s1">atol=</span><span class="s3">1e-10</span><span class="s1">)</span>

            <span class="s0">def </span><span class="s1">grad_func(x):</span>
                <span class="s1">raw = raw_prediction.copy()</span>
                <span class="s1">raw[:</span><span class="s0">, </span><span class="s1">k] = x</span>
                <span class="s0">return </span><span class="s1">loss.gradient(</span>
                    <span class="s1">y_true=y_true</span><span class="s0">,</span>
                    <span class="s1">raw_prediction=raw</span><span class="s0">,</span>
                    <span class="s1">sample_weight=sample_weight</span><span class="s0">,</span>
                <span class="s1">)[:</span><span class="s0">, </span><span class="s1">k]</span>

            <span class="s1">h_numeric = numerical_derivative(grad_func</span><span class="s0">, </span><span class="s1">raw_prediction[:</span><span class="s0">, </span><span class="s1">k]</span><span class="s0">, </span><span class="s1">eps=</span><span class="s3">1e-6</span><span class="s1">)</span>
            <span class="s0">if </span><span class="s1">loss.approx_hessian:</span>
                <span class="s2"># TODO: What could we test if loss.approx_hessian?</span>
                <span class="s0">pass</span>
            <span class="s0">else</span><span class="s1">:</span>
                <span class="s1">assert_allclose(h[:</span><span class="s0">, </span><span class="s1">k]</span><span class="s0">, </span><span class="s1">h_numeric</span><span class="s0">, </span><span class="s1">rtol=</span><span class="s3">5e-6</span><span class="s0">, </span><span class="s1">atol=</span><span class="s3">1e-10</span><span class="s1">)</span>


<span class="s1">@pytest.mark.parametrize(</span>
    <span class="s4">&quot;loss, x0, y_true&quot;</span><span class="s0">,</span>
    <span class="s1">[</span>
        <span class="s1">(</span><span class="s4">&quot;squared_error&quot;</span><span class="s0">, </span><span class="s1">-</span><span class="s3">2.0</span><span class="s0">, </span><span class="s3">42</span><span class="s1">)</span><span class="s0">,</span>
        <span class="s1">(</span><span class="s4">&quot;squared_error&quot;</span><span class="s0">, </span><span class="s3">117.0</span><span class="s0">, </span><span class="s3">1.05</span><span class="s1">)</span><span class="s0">,</span>
        <span class="s1">(</span><span class="s4">&quot;squared_error&quot;</span><span class="s0">, </span><span class="s3">0.0</span><span class="s0">, </span><span class="s3">0.0</span><span class="s1">)</span><span class="s0">,</span>
        <span class="s2"># The argmin of binomial_loss for y_true=0 and y_true=1 is resp.</span>
        <span class="s2"># -inf and +inf due to logit, cf. &quot;complete separation&quot;. Therefore, we</span>
        <span class="s2"># use 0 &lt; y_true &lt; 1.</span>
        <span class="s1">(</span><span class="s4">&quot;binomial_loss&quot;</span><span class="s0">, </span><span class="s3">0.3</span><span class="s0">, </span><span class="s3">0.1</span><span class="s1">)</span><span class="s0">,</span>
        <span class="s1">(</span><span class="s4">&quot;binomial_loss&quot;</span><span class="s0">, </span><span class="s1">-</span><span class="s3">12</span><span class="s0">, </span><span class="s3">0.2</span><span class="s1">)</span><span class="s0">,</span>
        <span class="s1">(</span><span class="s4">&quot;binomial_loss&quot;</span><span class="s0">, </span><span class="s3">30</span><span class="s0">, </span><span class="s3">0.9</span><span class="s1">)</span><span class="s0">,</span>
        <span class="s1">(</span><span class="s4">&quot;poisson_loss&quot;</span><span class="s0">, </span><span class="s3">12.0</span><span class="s0">, </span><span class="s3">1.0</span><span class="s1">)</span><span class="s0">,</span>
        <span class="s1">(</span><span class="s4">&quot;poisson_loss&quot;</span><span class="s0">, </span><span class="s3">0.0</span><span class="s0">, </span><span class="s3">2.0</span><span class="s1">)</span><span class="s0">,</span>
        <span class="s1">(</span><span class="s4">&quot;poisson_loss&quot;</span><span class="s0">, </span><span class="s1">-</span><span class="s3">22.0</span><span class="s0">, </span><span class="s3">10.0</span><span class="s1">)</span><span class="s0">,</span>
    <span class="s1">]</span><span class="s0">,</span>
<span class="s1">)</span>
<span class="s1">@skip_if_32bit</span>
<span class="s0">def </span><span class="s1">test_derivatives(loss</span><span class="s0">, </span><span class="s1">x0</span><span class="s0">, </span><span class="s1">y_true):</span>
    <span class="s5">&quot;&quot;&quot;Test that gradients are zero at the minimum of the loss. 
 
    We check this on a single value/sample using Halley's method with the 
    first and second order derivatives computed by the Loss instance. 
    Note that methods of Loss instances operate on arrays while the newton 
    root finder expects a scalar or a one-element array for this purpose. 
    &quot;&quot;&quot;</span>
    <span class="s1">loss = _LOSSES[loss](sample_weight=</span><span class="s0">None</span><span class="s1">)</span>
    <span class="s1">y_true = np.array([y_true]</span><span class="s0">, </span><span class="s1">dtype=np.float64)</span>
    <span class="s1">x0 = np.array([x0]</span><span class="s0">, </span><span class="s1">dtype=np.float64)</span>

    <span class="s0">def </span><span class="s1">func(x: np.ndarray) -&gt; np.ndarray:</span>
        <span class="s5">&quot;&quot;&quot;Compute loss plus constant term. 
 
        The constant term is such that the minimum function value is zero, 
        which is required by the Newton method. 
        &quot;&quot;&quot;</span>
        <span class="s0">return </span><span class="s1">loss.loss(</span>
            <span class="s1">y_true=y_true</span><span class="s0">, </span><span class="s1">raw_prediction=x</span>
        <span class="s1">) + loss.constant_to_optimal_zero(y_true=y_true)</span>

    <span class="s0">def </span><span class="s1">fprime(x: np.ndarray) -&gt; np.ndarray:</span>
        <span class="s0">return </span><span class="s1">loss.gradient(y_true=y_true</span><span class="s0">, </span><span class="s1">raw_prediction=x)</span>

    <span class="s0">def </span><span class="s1">fprime2(x: np.ndarray) -&gt; np.ndarray:</span>
        <span class="s0">return </span><span class="s1">loss.gradient_hessian(y_true=y_true</span><span class="s0">, </span><span class="s1">raw_prediction=x)[</span><span class="s3">1</span><span class="s1">]</span>

    <span class="s1">optimum = newton(</span>
        <span class="s1">func</span><span class="s0">,</span>
        <span class="s1">x0=x0</span><span class="s0">,</span>
        <span class="s1">fprime=fprime</span><span class="s0">,</span>
        <span class="s1">fprime2=fprime2</span><span class="s0">,</span>
        <span class="s1">maxiter=</span><span class="s3">100</span><span class="s0">,</span>
        <span class="s1">tol=</span><span class="s3">5e-8</span><span class="s0">,</span>
    <span class="s1">)</span>

    <span class="s2"># Need to ravel arrays because assert_allclose requires matching</span>
    <span class="s2"># dimensions.</span>
    <span class="s1">y_true = y_true.ravel()</span>
    <span class="s1">optimum = optimum.ravel()</span>
    <span class="s1">assert_allclose(loss.link.inverse(optimum)</span><span class="s0">, </span><span class="s1">y_true)</span>
    <span class="s1">assert_allclose(func(optimum)</span><span class="s0">, </span><span class="s3">0</span><span class="s0">, </span><span class="s1">atol=</span><span class="s3">1e-14</span><span class="s1">)</span>
    <span class="s1">assert_allclose(loss.gradient(y_true=y_true</span><span class="s0">, </span><span class="s1">raw_prediction=optimum)</span><span class="s0">, </span><span class="s3">0</span><span class="s0">, </span><span class="s1">atol=</span><span class="s3">5e-7</span><span class="s1">)</span>


<span class="s1">@pytest.mark.parametrize(</span><span class="s4">&quot;loss&quot;</span><span class="s0">, </span><span class="s1">LOSS_INSTANCES</span><span class="s0">, </span><span class="s1">ids=loss_instance_name)</span>
<span class="s1">@pytest.mark.parametrize(</span><span class="s4">&quot;sample_weight&quot;</span><span class="s0">, </span><span class="s1">[</span><span class="s0">None, </span><span class="s4">&quot;range&quot;</span><span class="s1">])</span>
<span class="s0">def </span><span class="s1">test_loss_intercept_only(loss</span><span class="s0">, </span><span class="s1">sample_weight):</span>
    <span class="s5">&quot;&quot;&quot;Test that fit_intercept_only returns the argmin of the loss. 
 
    Also test that the gradient is zero at the minimum. 
    &quot;&quot;&quot;</span>
    <span class="s1">n_samples = </span><span class="s3">50</span>
    <span class="s0">if not </span><span class="s1">loss.is_multiclass:</span>
        <span class="s1">y_true = loss.link.inverse(np.linspace(-</span><span class="s3">4</span><span class="s0">, </span><span class="s3">4</span><span class="s0">, </span><span class="s1">num=n_samples))</span>
    <span class="s0">else</span><span class="s1">:</span>
        <span class="s1">y_true = np.arange(n_samples).astype(np.float64) % loss.n_classes</span>
        <span class="s1">y_true[::</span><span class="s3">5</span><span class="s1">] = </span><span class="s3">0  </span><span class="s2"># exceedance of class 0</span>

    <span class="s0">if </span><span class="s1">sample_weight == </span><span class="s4">&quot;range&quot;</span><span class="s1">:</span>
        <span class="s1">sample_weight = np.linspace(</span><span class="s3">0.1</span><span class="s0">, </span><span class="s3">2</span><span class="s0">, </span><span class="s1">num=n_samples)</span>

    <span class="s1">a = loss.fit_intercept_only(y_true=y_true</span><span class="s0">, </span><span class="s1">sample_weight=sample_weight)</span>

    <span class="s2"># find minimum by optimization</span>
    <span class="s0">def </span><span class="s1">fun(x):</span>
        <span class="s0">if not </span><span class="s1">loss.is_multiclass:</span>
            <span class="s1">raw_prediction = np.full(shape=(n_samples)</span><span class="s0">, </span><span class="s1">fill_value=x)</span>
        <span class="s0">else</span><span class="s1">:</span>
            <span class="s1">raw_prediction = np.ascontiguousarray(</span>
                <span class="s1">np.broadcast_to(x</span><span class="s0">, </span><span class="s1">shape=(n_samples</span><span class="s0">, </span><span class="s1">loss.n_classes))</span>
            <span class="s1">)</span>
        <span class="s0">return </span><span class="s1">loss(</span>
            <span class="s1">y_true=y_true</span><span class="s0">,</span>
            <span class="s1">raw_prediction=raw_prediction</span><span class="s0">,</span>
            <span class="s1">sample_weight=sample_weight</span><span class="s0">,</span>
        <span class="s1">)</span>

    <span class="s0">if not </span><span class="s1">loss.is_multiclass:</span>
        <span class="s1">opt = minimize_scalar(fun</span><span class="s0">, </span><span class="s1">tol=</span><span class="s3">1e-7</span><span class="s0">, </span><span class="s1">options={</span><span class="s4">&quot;maxiter&quot;</span><span class="s1">: </span><span class="s3">100</span><span class="s1">})</span>
        <span class="s1">grad = loss.gradient(</span>
            <span class="s1">y_true=y_true</span><span class="s0">,</span>
            <span class="s1">raw_prediction=np.full_like(y_true</span><span class="s0">, </span><span class="s1">a)</span><span class="s0">,</span>
            <span class="s1">sample_weight=sample_weight</span><span class="s0">,</span>
        <span class="s1">)</span>
        <span class="s0">assert </span><span class="s1">a.shape == tuple()  </span><span class="s2"># scalar</span>
        <span class="s0">assert </span><span class="s1">a.dtype == y_true.dtype</span>
        <span class="s1">assert_all_finite(a)</span>
        <span class="s1">a == approx(opt.x</span><span class="s0">, </span><span class="s1">rel=</span><span class="s3">1e-7</span><span class="s1">)</span>
        <span class="s1">grad.sum() == approx(</span><span class="s3">0</span><span class="s0">, </span><span class="s1">abs=</span><span class="s3">1e-12</span><span class="s1">)</span>
    <span class="s0">else</span><span class="s1">:</span>
        <span class="s2"># The constraint corresponds to sum(raw_prediction) = 0. Without it, we would</span>
        <span class="s2"># need to apply loss.symmetrize_raw_prediction to opt.x before comparing.</span>
        <span class="s1">opt = minimize(</span>
            <span class="s1">fun</span><span class="s0">,</span>
            <span class="s1">np.zeros((loss.n_classes))</span><span class="s0">,</span>
            <span class="s1">tol=</span><span class="s3">1e-13</span><span class="s0">,</span>
            <span class="s1">options={</span><span class="s4">&quot;maxiter&quot;</span><span class="s1">: </span><span class="s3">100</span><span class="s1">}</span><span class="s0">,</span>
            <span class="s1">method=</span><span class="s4">&quot;SLSQP&quot;</span><span class="s0">,</span>
            <span class="s1">constraints=LinearConstraint(np.ones((</span><span class="s3">1</span><span class="s0">, </span><span class="s1">loss.n_classes))</span><span class="s0">, </span><span class="s3">0</span><span class="s0">, </span><span class="s3">0</span><span class="s1">)</span><span class="s0">,</span>
        <span class="s1">)</span>
        <span class="s1">grad = loss.gradient(</span>
            <span class="s1">y_true=y_true</span><span class="s0">,</span>
            <span class="s1">raw_prediction=np.tile(a</span><span class="s0">, </span><span class="s1">(n_samples</span><span class="s0">, </span><span class="s3">1</span><span class="s1">))</span><span class="s0">,</span>
            <span class="s1">sample_weight=sample_weight</span><span class="s0">,</span>
        <span class="s1">)</span>
        <span class="s0">assert </span><span class="s1">a.dtype == y_true.dtype</span>
        <span class="s1">assert_all_finite(a)</span>
        <span class="s1">assert_allclose(a</span><span class="s0">, </span><span class="s1">opt.x</span><span class="s0">, </span><span class="s1">rtol=</span><span class="s3">5e-6</span><span class="s0">, </span><span class="s1">atol=</span><span class="s3">1e-12</span><span class="s1">)</span>
        <span class="s1">assert_allclose(grad.sum(axis=</span><span class="s3">0</span><span class="s1">)</span><span class="s0">, </span><span class="s3">0</span><span class="s0">, </span><span class="s1">atol=</span><span class="s3">1e-12</span><span class="s1">)</span>


<span class="s1">@pytest.mark.parametrize(</span>
    <span class="s4">&quot;loss, func, random_dist&quot;</span><span class="s0">,</span>
    <span class="s1">[</span>
        <span class="s1">(HalfSquaredError()</span><span class="s0">, </span><span class="s1">np.mean</span><span class="s0">, </span><span class="s4">&quot;normal&quot;</span><span class="s1">)</span><span class="s0">,</span>
        <span class="s1">(AbsoluteError()</span><span class="s0">, </span><span class="s1">np.median</span><span class="s0">, </span><span class="s4">&quot;normal&quot;</span><span class="s1">)</span><span class="s0">,</span>
        <span class="s1">(PinballLoss(quantile=</span><span class="s3">0.25</span><span class="s1">)</span><span class="s0">, lambda </span><span class="s1">x: np.percentile(x</span><span class="s0">, </span><span class="s1">q=</span><span class="s3">25</span><span class="s1">)</span><span class="s0">, </span><span class="s4">&quot;normal&quot;</span><span class="s1">)</span><span class="s0">,</span>
        <span class="s1">(HalfPoissonLoss()</span><span class="s0">, </span><span class="s1">np.mean</span><span class="s0">, </span><span class="s4">&quot;poisson&quot;</span><span class="s1">)</span><span class="s0">,</span>
        <span class="s1">(HalfGammaLoss()</span><span class="s0">, </span><span class="s1">np.mean</span><span class="s0">, </span><span class="s4">&quot;exponential&quot;</span><span class="s1">)</span><span class="s0">,</span>
        <span class="s1">(HalfTweedieLoss()</span><span class="s0">, </span><span class="s1">np.mean</span><span class="s0">, </span><span class="s4">&quot;exponential&quot;</span><span class="s1">)</span><span class="s0">,</span>
        <span class="s1">(HalfBinomialLoss()</span><span class="s0">, </span><span class="s1">np.mean</span><span class="s0">, </span><span class="s4">&quot;binomial&quot;</span><span class="s1">)</span><span class="s0">,</span>
    <span class="s1">]</span><span class="s0">,</span>
<span class="s1">)</span>
<span class="s0">def </span><span class="s1">test_specific_fit_intercept_only(loss</span><span class="s0">, </span><span class="s1">func</span><span class="s0">, </span><span class="s1">random_dist</span><span class="s0">, </span><span class="s1">global_random_seed):</span>
    <span class="s5">&quot;&quot;&quot;Test that fit_intercept_only returns the correct functional. 
 
    We test the functional for specific, meaningful distributions, e.g. 
    squared error estimates the expectation of a probability distribution. 
    &quot;&quot;&quot;</span>
    <span class="s1">rng = np.random.RandomState(global_random_seed)</span>
    <span class="s0">if </span><span class="s1">random_dist == </span><span class="s4">&quot;binomial&quot;</span><span class="s1">:</span>
        <span class="s1">y_train = rng.binomial(</span><span class="s3">1</span><span class="s0">, </span><span class="s3">0.5</span><span class="s0">, </span><span class="s1">size=</span><span class="s3">100</span><span class="s1">)</span>
    <span class="s0">else</span><span class="s1">:</span>
        <span class="s1">y_train = getattr(rng</span><span class="s0">, </span><span class="s1">random_dist)(size=</span><span class="s3">100</span><span class="s1">)</span>
    <span class="s1">baseline_prediction = loss.fit_intercept_only(y_true=y_train)</span>
    <span class="s2"># Make sure baseline prediction is the expected functional=func, e.g. mean</span>
    <span class="s2"># or median.</span>
    <span class="s1">assert_all_finite(baseline_prediction)</span>
    <span class="s0">assert </span><span class="s1">baseline_prediction == approx(loss.link.link(func(y_train)))</span>
    <span class="s0">assert </span><span class="s1">loss.link.inverse(baseline_prediction) == approx(func(y_train))</span>
    <span class="s0">if </span><span class="s1">isinstance(loss</span><span class="s0">, </span><span class="s1">IdentityLink):</span>
        <span class="s1">assert_allclose(loss.link.inverse(baseline_prediction)</span><span class="s0">, </span><span class="s1">baseline_prediction)</span>

    <span class="s2"># Test baseline at boundary</span>
    <span class="s0">if </span><span class="s1">loss.interval_y_true.low_inclusive:</span>
        <span class="s1">y_train.fill(loss.interval_y_true.low)</span>
        <span class="s1">baseline_prediction = loss.fit_intercept_only(y_true=y_train)</span>
        <span class="s1">assert_all_finite(baseline_prediction)</span>
    <span class="s0">if </span><span class="s1">loss.interval_y_true.high_inclusive:</span>
        <span class="s1">y_train.fill(loss.interval_y_true.high)</span>
        <span class="s1">baseline_prediction = loss.fit_intercept_only(y_true=y_train)</span>
        <span class="s1">assert_all_finite(baseline_prediction)</span>


<span class="s0">def </span><span class="s1">test_multinomial_loss_fit_intercept_only():</span>
    <span class="s5">&quot;&quot;&quot;Test that fit_intercept_only returns the mean functional for CCE.&quot;&quot;&quot;</span>
    <span class="s1">rng = np.random.RandomState(</span><span class="s3">0</span><span class="s1">)</span>
    <span class="s1">n_classes = </span><span class="s3">4</span>
    <span class="s1">loss = HalfMultinomialLoss(n_classes=n_classes)</span>
    <span class="s2"># Same logic as test_specific_fit_intercept_only. Here inverse link</span>
    <span class="s2"># function = softmax and link function = log - symmetry term.</span>
    <span class="s1">y_train = rng.randint(</span><span class="s3">0</span><span class="s0">, </span><span class="s1">n_classes + </span><span class="s3">1</span><span class="s0">, </span><span class="s1">size=</span><span class="s3">100</span><span class="s1">).astype(np.float64)</span>
    <span class="s1">baseline_prediction = loss.fit_intercept_only(y_true=y_train)</span>
    <span class="s0">assert </span><span class="s1">baseline_prediction.shape == (n_classes</span><span class="s0">,</span><span class="s1">)</span>
    <span class="s1">p = np.zeros(n_classes</span><span class="s0">, </span><span class="s1">dtype=y_train.dtype)</span>
    <span class="s0">for </span><span class="s1">k </span><span class="s0">in </span><span class="s1">range(n_classes):</span>
        <span class="s1">p[k] = (y_train == k).mean()</span>
    <span class="s1">assert_allclose(baseline_prediction</span><span class="s0">, </span><span class="s1">np.log(p) - np.mean(np.log(p)))</span>
    <span class="s1">assert_allclose(baseline_prediction[</span><span class="s0">None, </span><span class="s1">:]</span><span class="s0">, </span><span class="s1">loss.link.link(p[</span><span class="s0">None, </span><span class="s1">:]))</span>

    <span class="s0">for </span><span class="s1">y_train </span><span class="s0">in </span><span class="s1">(np.zeros(shape=</span><span class="s3">10</span><span class="s1">)</span><span class="s0">, </span><span class="s1">np.ones(shape=</span><span class="s3">10</span><span class="s1">)):</span>
        <span class="s1">y_train = y_train.astype(np.float64)</span>
        <span class="s1">baseline_prediction = loss.fit_intercept_only(y_true=y_train)</span>
        <span class="s0">assert </span><span class="s1">baseline_prediction.dtype == y_train.dtype</span>
        <span class="s1">assert_all_finite(baseline_prediction)</span>


<span class="s0">def </span><span class="s1">test_binomial_and_multinomial_loss(global_random_seed):</span>
    <span class="s5">&quot;&quot;&quot;Test that multinomial loss with n_classes = 2 is the same as binomial loss.&quot;&quot;&quot;</span>
    <span class="s1">rng = np.random.RandomState(global_random_seed)</span>
    <span class="s1">n_samples = </span><span class="s3">20</span>
    <span class="s1">binom = HalfBinomialLoss()</span>
    <span class="s1">multinom = HalfMultinomialLoss(n_classes=</span><span class="s3">2</span><span class="s1">)</span>
    <span class="s1">y_train = rng.randint(</span><span class="s3">0</span><span class="s0">, </span><span class="s3">2</span><span class="s0">, </span><span class="s1">size=n_samples).astype(np.float64)</span>
    <span class="s1">raw_prediction = rng.normal(size=n_samples)</span>
    <span class="s1">raw_multinom = np.empty((n_samples</span><span class="s0">, </span><span class="s3">2</span><span class="s1">))</span>
    <span class="s1">raw_multinom[:</span><span class="s0">, </span><span class="s3">0</span><span class="s1">] = -</span><span class="s3">0.5 </span><span class="s1">* raw_prediction</span>
    <span class="s1">raw_multinom[:</span><span class="s0">, </span><span class="s3">1</span><span class="s1">] = </span><span class="s3">0.5 </span><span class="s1">* raw_prediction</span>
    <span class="s1">assert_allclose(</span>
        <span class="s1">binom.loss(y_true=y_train</span><span class="s0">, </span><span class="s1">raw_prediction=raw_prediction)</span><span class="s0">,</span>
        <span class="s1">multinom.loss(y_true=y_train</span><span class="s0">, </span><span class="s1">raw_prediction=raw_multinom)</span><span class="s0">,</span>
    <span class="s1">)</span>


<span class="s1">@pytest.mark.parametrize(</span><span class="s4">&quot;y_true&quot;</span><span class="s0">, </span><span class="s1">(np.array([</span><span class="s3">0.0</span><span class="s0">, </span><span class="s3">0</span><span class="s0">, </span><span class="s3">0</span><span class="s1">])</span><span class="s0">, </span><span class="s1">np.array([</span><span class="s3">1.0</span><span class="s0">, </span><span class="s3">1</span><span class="s0">, </span><span class="s3">1</span><span class="s1">])))</span>
<span class="s1">@pytest.mark.parametrize(</span><span class="s4">&quot;y_pred&quot;</span><span class="s0">, </span><span class="s1">(np.array([-</span><span class="s3">5.0</span><span class="s0">, </span><span class="s1">-</span><span class="s3">5</span><span class="s0">, </span><span class="s1">-</span><span class="s3">5</span><span class="s1">])</span><span class="s0">, </span><span class="s1">np.array([</span><span class="s3">3.0</span><span class="s0">, </span><span class="s3">3</span><span class="s0">, </span><span class="s3">3</span><span class="s1">])))</span>
<span class="s0">def </span><span class="s1">test_binomial_vs_alternative_formulation(y_true</span><span class="s0">, </span><span class="s1">y_pred</span><span class="s0">, </span><span class="s1">global_dtype):</span>
    <span class="s5">&quot;&quot;&quot;Test that both formulations of the binomial deviance agree. 
 
    Often, the binomial deviance or log loss is written in terms of a variable 
    z in {-1, +1}, but we use y in {0, 1}, hence z = 2 * y - 1. 
    ESL II Eq. (10.18): 
 
        -loglike(z, f) = log(1 + exp(-2 * z * f)) 
 
    Note: 
        - ESL 2*f = raw_prediction, hence the factor 2 of ESL disappears. 
        - Deviance = -2*loglike + .., but HalfBinomialLoss is half of the 
          deviance, hence the factor of 2 cancels in the comparison. 
    &quot;&quot;&quot;</span>

    <span class="s0">def </span><span class="s1">alt_loss(y</span><span class="s0">, </span><span class="s1">raw_pred):</span>
        <span class="s1">z = </span><span class="s3">2 </span><span class="s1">* y - </span><span class="s3">1</span>
        <span class="s0">return </span><span class="s1">np.mean(np.log(</span><span class="s3">1 </span><span class="s1">+ np.exp(-z * raw_pred)))</span>

    <span class="s0">def </span><span class="s1">alt_gradient(y</span><span class="s0">, </span><span class="s1">raw_pred):</span>
        <span class="s2"># alternative gradient formula according to ESL</span>
        <span class="s1">z = </span><span class="s3">2 </span><span class="s1">* y - </span><span class="s3">1</span>
        <span class="s0">return </span><span class="s1">-z / (</span><span class="s3">1 </span><span class="s1">+ np.exp(z * raw_pred))</span>

    <span class="s1">bin_loss = HalfBinomialLoss()</span>

    <span class="s1">y_true = y_true.astype(global_dtype)</span>
    <span class="s1">y_pred = y_pred.astype(global_dtype)</span>
    <span class="s1">datum = (y_true</span><span class="s0">, </span><span class="s1">y_pred)</span>

    <span class="s0">assert </span><span class="s1">bin_loss(*datum) == approx(alt_loss(*datum))</span>
    <span class="s1">assert_allclose(bin_loss.gradient(*datum)</span><span class="s0">, </span><span class="s1">alt_gradient(*datum))</span>


<span class="s1">@pytest.mark.parametrize(</span><span class="s4">&quot;loss&quot;</span><span class="s0">, </span><span class="s1">LOSS_INSTANCES</span><span class="s0">, </span><span class="s1">ids=loss_instance_name)</span>
<span class="s0">def </span><span class="s1">test_predict_proba(loss</span><span class="s0">, </span><span class="s1">global_random_seed):</span>
    <span class="s5">&quot;&quot;&quot;Test that predict_proba and gradient_proba work as expected.&quot;&quot;&quot;</span>
    <span class="s1">n_samples = </span><span class="s3">20</span>
    <span class="s1">y_true</span><span class="s0">, </span><span class="s1">raw_prediction = random_y_true_raw_prediction(</span>
        <span class="s1">loss=loss</span><span class="s0">,</span>
        <span class="s1">n_samples=n_samples</span><span class="s0">,</span>
        <span class="s1">y_bound=(-</span><span class="s3">100</span><span class="s0">, </span><span class="s3">100</span><span class="s1">)</span><span class="s0">,</span>
        <span class="s1">raw_bound=(-</span><span class="s3">5</span><span class="s0">, </span><span class="s3">5</span><span class="s1">)</span><span class="s0">,</span>
        <span class="s1">seed=global_random_seed</span><span class="s0">,</span>
    <span class="s1">)</span>

    <span class="s0">if </span><span class="s1">hasattr(loss</span><span class="s0">, </span><span class="s4">&quot;predict_proba&quot;</span><span class="s1">):</span>
        <span class="s1">proba = loss.predict_proba(raw_prediction)</span>
        <span class="s0">assert </span><span class="s1">proba.shape == (n_samples</span><span class="s0">, </span><span class="s1">loss.n_classes)</span>
        <span class="s0">assert </span><span class="s1">np.sum(proba</span><span class="s0">, </span><span class="s1">axis=</span><span class="s3">1</span><span class="s1">) == approx(</span><span class="s3">1</span><span class="s0">, </span><span class="s1">rel=</span><span class="s3">1e-11</span><span class="s1">)</span>

    <span class="s0">if </span><span class="s1">hasattr(loss</span><span class="s0">, </span><span class="s4">&quot;gradient_proba&quot;</span><span class="s1">):</span>
        <span class="s0">for </span><span class="s1">grad</span><span class="s0">, </span><span class="s1">proba </span><span class="s0">in </span><span class="s1">(</span>
            <span class="s1">(</span><span class="s0">None, None</span><span class="s1">)</span><span class="s0">,</span>
            <span class="s1">(</span><span class="s0">None, </span><span class="s1">np.empty_like(raw_prediction))</span><span class="s0">,</span>
            <span class="s1">(np.empty_like(raw_prediction)</span><span class="s0">, None</span><span class="s1">)</span><span class="s0">,</span>
            <span class="s1">(np.empty_like(raw_prediction)</span><span class="s0">, </span><span class="s1">np.empty_like(raw_prediction))</span><span class="s0">,</span>
        <span class="s1">):</span>
            <span class="s1">grad</span><span class="s0">, </span><span class="s1">proba = loss.gradient_proba(</span>
                <span class="s1">y_true=y_true</span><span class="s0">,</span>
                <span class="s1">raw_prediction=raw_prediction</span><span class="s0">,</span>
                <span class="s1">sample_weight=</span><span class="s0">None,</span>
                <span class="s1">gradient_out=grad</span><span class="s0">,</span>
                <span class="s1">proba_out=proba</span><span class="s0">,</span>
            <span class="s1">)</span>
            <span class="s0">assert </span><span class="s1">proba.shape == (n_samples</span><span class="s0">, </span><span class="s1">loss.n_classes)</span>
            <span class="s0">assert </span><span class="s1">np.sum(proba</span><span class="s0">, </span><span class="s1">axis=</span><span class="s3">1</span><span class="s1">) == approx(</span><span class="s3">1</span><span class="s0">, </span><span class="s1">rel=</span><span class="s3">1e-11</span><span class="s1">)</span>
            <span class="s1">assert_allclose(</span>
                <span class="s1">grad</span><span class="s0">,</span>
                <span class="s1">loss.gradient(</span>
                    <span class="s1">y_true=y_true</span><span class="s0">,</span>
                    <span class="s1">raw_prediction=raw_prediction</span><span class="s0">,</span>
                    <span class="s1">sample_weight=</span><span class="s0">None,</span>
                    <span class="s1">gradient_out=</span><span class="s0">None,</span>
                <span class="s1">)</span><span class="s0">,</span>
            <span class="s1">)</span>


<span class="s1">@pytest.mark.parametrize(</span><span class="s4">&quot;loss&quot;</span><span class="s0">, </span><span class="s1">ALL_LOSSES)</span>
<span class="s1">@pytest.mark.parametrize(</span><span class="s4">&quot;sample_weight&quot;</span><span class="s0">, </span><span class="s1">[</span><span class="s0">None, </span><span class="s4">&quot;range&quot;</span><span class="s1">])</span>
<span class="s1">@pytest.mark.parametrize(</span><span class="s4">&quot;dtype&quot;</span><span class="s0">, </span><span class="s1">(np.float32</span><span class="s0">, </span><span class="s1">np.float64))</span>
<span class="s1">@pytest.mark.parametrize(</span><span class="s4">&quot;order&quot;</span><span class="s0">, </span><span class="s1">(</span><span class="s4">&quot;C&quot;</span><span class="s0">, </span><span class="s4">&quot;F&quot;</span><span class="s1">))</span>
<span class="s0">def </span><span class="s1">test_init_gradient_and_hessians(loss</span><span class="s0">, </span><span class="s1">sample_weight</span><span class="s0">, </span><span class="s1">dtype</span><span class="s0">, </span><span class="s1">order):</span>
    <span class="s5">&quot;&quot;&quot;Test that init_gradient_and_hessian works as expected. 
 
    passing sample_weight to a loss correctly influences the constant_hessian 
    attribute, and consequently the shape of the hessian array. 
    &quot;&quot;&quot;</span>
    <span class="s1">n_samples = </span><span class="s3">5</span>
    <span class="s0">if </span><span class="s1">sample_weight == </span><span class="s4">&quot;range&quot;</span><span class="s1">:</span>
        <span class="s1">sample_weight = np.ones(n_samples)</span>
    <span class="s1">loss = loss(sample_weight=sample_weight)</span>
    <span class="s1">gradient</span><span class="s0">, </span><span class="s1">hessian = loss.init_gradient_and_hessian(</span>
        <span class="s1">n_samples=n_samples</span><span class="s0">,</span>
        <span class="s1">dtype=dtype</span><span class="s0">,</span>
        <span class="s1">order=order</span><span class="s0">,</span>
    <span class="s1">)</span>
    <span class="s0">if </span><span class="s1">loss.constant_hessian:</span>
        <span class="s0">assert </span><span class="s1">gradient.shape == (n_samples</span><span class="s0">,</span><span class="s1">)</span>
        <span class="s0">assert </span><span class="s1">hessian.shape == (</span><span class="s3">1</span><span class="s0">,</span><span class="s1">)</span>
    <span class="s0">elif </span><span class="s1">loss.is_multiclass:</span>
        <span class="s0">assert </span><span class="s1">gradient.shape == (n_samples</span><span class="s0">, </span><span class="s1">loss.n_classes)</span>
        <span class="s0">assert </span><span class="s1">hessian.shape == (n_samples</span><span class="s0">, </span><span class="s1">loss.n_classes)</span>
    <span class="s0">else</span><span class="s1">:</span>
        <span class="s0">assert </span><span class="s1">hessian.shape == (n_samples</span><span class="s0">,</span><span class="s1">)</span>
        <span class="s0">assert </span><span class="s1">hessian.shape == (n_samples</span><span class="s0">,</span><span class="s1">)</span>

    <span class="s0">assert </span><span class="s1">gradient.dtype == dtype</span>
    <span class="s0">assert </span><span class="s1">hessian.dtype == dtype</span>

    <span class="s0">if </span><span class="s1">order == </span><span class="s4">&quot;C&quot;</span><span class="s1">:</span>
        <span class="s0">assert </span><span class="s1">gradient.flags.c_contiguous</span>
        <span class="s0">assert </span><span class="s1">hessian.flags.c_contiguous</span>
    <span class="s0">else</span><span class="s1">:</span>
        <span class="s0">assert </span><span class="s1">gradient.flags.f_contiguous</span>
        <span class="s0">assert </span><span class="s1">hessian.flags.f_contiguous</span>


<span class="s1">@pytest.mark.parametrize(</span><span class="s4">&quot;loss&quot;</span><span class="s0">, </span><span class="s1">ALL_LOSSES)</span>
<span class="s1">@pytest.mark.parametrize(</span>
    <span class="s4">&quot;params, err_msg&quot;</span><span class="s0">,</span>
    <span class="s1">[</span>
        <span class="s1">(</span>
            <span class="s1">{</span><span class="s4">&quot;dtype&quot;</span><span class="s1">: np.int64}</span><span class="s0">,</span>
            <span class="s4">f&quot;Valid options for 'dtype' are .* Got dtype=</span><span class="s0">{</span><span class="s1">np.int64</span><span class="s0">} </span><span class="s4">instead.&quot;</span><span class="s0">,</span>
        <span class="s1">)</span><span class="s0">,</span>
    <span class="s1">]</span><span class="s0">,</span>
<span class="s1">)</span>
<span class="s0">def </span><span class="s1">test_init_gradient_and_hessian_raises(loss</span><span class="s0">, </span><span class="s1">params</span><span class="s0">, </span><span class="s1">err_msg):</span>
    <span class="s5">&quot;&quot;&quot;Test that init_gradient_and_hessian raises errors for invalid input.&quot;&quot;&quot;</span>
    <span class="s1">loss = loss()</span>
    <span class="s0">with </span><span class="s1">pytest.raises((ValueError</span><span class="s0">, </span><span class="s1">TypeError)</span><span class="s0">, </span><span class="s1">match=err_msg):</span>
        <span class="s1">gradient</span><span class="s0">, </span><span class="s1">hessian = loss.init_gradient_and_hessian(n_samples=</span><span class="s3">5</span><span class="s0">, </span><span class="s1">**params)</span>


<span class="s1">@pytest.mark.parametrize(</span>
    <span class="s4">&quot;loss, params, err_type, err_msg&quot;</span><span class="s0">,</span>
    <span class="s1">[</span>
        <span class="s1">(</span>
            <span class="s1">PinballLoss</span><span class="s0">,</span>
            <span class="s1">{</span><span class="s4">&quot;quantile&quot;</span><span class="s1">: </span><span class="s0">None</span><span class="s1">}</span><span class="s0">,</span>
            <span class="s1">TypeError</span><span class="s0">,</span>
            <span class="s4">&quot;quantile must be an instance of float, not NoneType.&quot;</span><span class="s0">,</span>
        <span class="s1">)</span><span class="s0">,</span>
        <span class="s1">(</span>
            <span class="s1">PinballLoss</span><span class="s0">,</span>
            <span class="s1">{</span><span class="s4">&quot;quantile&quot;</span><span class="s1">: </span><span class="s3">0</span><span class="s1">}</span><span class="s0">,</span>
            <span class="s1">ValueError</span><span class="s0">,</span>
            <span class="s4">&quot;quantile == 0, must be &gt; 0.&quot;</span><span class="s0">,</span>
        <span class="s1">)</span><span class="s0">,</span>
        <span class="s1">(PinballLoss</span><span class="s0">, </span><span class="s1">{</span><span class="s4">&quot;quantile&quot;</span><span class="s1">: </span><span class="s3">1.1</span><span class="s1">}</span><span class="s0">, </span><span class="s1">ValueError</span><span class="s0">, </span><span class="s4">&quot;quantile == 1.1, must be &lt; 1.&quot;</span><span class="s1">)</span><span class="s0">,</span>
        <span class="s1">(</span>
            <span class="s1">HuberLoss</span><span class="s0">,</span>
            <span class="s1">{</span><span class="s4">&quot;quantile&quot;</span><span class="s1">: </span><span class="s0">None</span><span class="s1">}</span><span class="s0">,</span>
            <span class="s1">TypeError</span><span class="s0">,</span>
            <span class="s4">&quot;quantile must be an instance of float, not NoneType.&quot;</span><span class="s0">,</span>
        <span class="s1">)</span><span class="s0">,</span>
        <span class="s1">(</span>
            <span class="s1">HuberLoss</span><span class="s0">,</span>
            <span class="s1">{</span><span class="s4">&quot;quantile&quot;</span><span class="s1">: </span><span class="s3">0</span><span class="s1">}</span><span class="s0">,</span>
            <span class="s1">ValueError</span><span class="s0">,</span>
            <span class="s4">&quot;quantile == 0, must be &gt; 0.&quot;</span><span class="s0">,</span>
        <span class="s1">)</span><span class="s0">,</span>
        <span class="s1">(HuberLoss</span><span class="s0">, </span><span class="s1">{</span><span class="s4">&quot;quantile&quot;</span><span class="s1">: </span><span class="s3">1.1</span><span class="s1">}</span><span class="s0">, </span><span class="s1">ValueError</span><span class="s0">, </span><span class="s4">&quot;quantile == 1.1, must be &lt; 1.&quot;</span><span class="s1">)</span><span class="s0">,</span>
    <span class="s1">]</span><span class="s0">,</span>
<span class="s1">)</span>
<span class="s0">def </span><span class="s1">test_loss_init_parameter_validation(loss</span><span class="s0">, </span><span class="s1">params</span><span class="s0">, </span><span class="s1">err_type</span><span class="s0">, </span><span class="s1">err_msg):</span>
    <span class="s5">&quot;&quot;&quot;Test that loss raises errors for invalid input.&quot;&quot;&quot;</span>
    <span class="s0">with </span><span class="s1">pytest.raises(err_type</span><span class="s0">, </span><span class="s1">match=err_msg):</span>
        <span class="s1">loss(**params)</span>


<span class="s1">@pytest.mark.parametrize(</span><span class="s4">&quot;loss&quot;</span><span class="s0">, </span><span class="s1">LOSS_INSTANCES</span><span class="s0">, </span><span class="s1">ids=loss_instance_name)</span>
<span class="s0">def </span><span class="s1">test_loss_pickle(loss):</span>
    <span class="s5">&quot;&quot;&quot;Test that losses can be pickled.&quot;&quot;&quot;</span>
    <span class="s1">n_samples = </span><span class="s3">20</span>
    <span class="s1">y_true</span><span class="s0">, </span><span class="s1">raw_prediction = random_y_true_raw_prediction(</span>
        <span class="s1">loss=loss</span><span class="s0">,</span>
        <span class="s1">n_samples=n_samples</span><span class="s0">,</span>
        <span class="s1">y_bound=(-</span><span class="s3">100</span><span class="s0">, </span><span class="s3">100</span><span class="s1">)</span><span class="s0">,</span>
        <span class="s1">raw_bound=(-</span><span class="s3">5</span><span class="s0">, </span><span class="s3">5</span><span class="s1">)</span><span class="s0">,</span>
        <span class="s1">seed=</span><span class="s3">42</span><span class="s0">,</span>
    <span class="s1">)</span>
    <span class="s1">pickled_loss = pickle.dumps(loss)</span>
    <span class="s1">unpickled_loss = pickle.loads(pickled_loss)</span>
    <span class="s0">assert </span><span class="s1">loss(y_true=y_true</span><span class="s0">, </span><span class="s1">raw_prediction=raw_prediction) == approx(</span>
        <span class="s1">unpickled_loss(y_true=y_true</span><span class="s0">, </span><span class="s1">raw_prediction=raw_prediction)</span>
    <span class="s1">)</span>


<span class="s1">@pytest.mark.parametrize(</span><span class="s4">&quot;p&quot;</span><span class="s0">, </span><span class="s1">[-</span><span class="s3">1.5</span><span class="s0">, </span><span class="s3">0</span><span class="s0">, </span><span class="s3">1</span><span class="s0">, </span><span class="s3">1.5</span><span class="s0">, </span><span class="s3">2</span><span class="s0">, </span><span class="s3">3</span><span class="s1">])</span>
<span class="s0">def </span><span class="s1">test_tweedie_log_identity_consistency(p):</span>
    <span class="s5">&quot;&quot;&quot;Test for identical losses when only the link function is different.&quot;&quot;&quot;</span>
    <span class="s1">half_tweedie_log = HalfTweedieLoss(power=p)</span>
    <span class="s1">half_tweedie_identity = HalfTweedieLossIdentity(power=p)</span>
    <span class="s1">n_samples = </span><span class="s3">10</span>
    <span class="s1">y_true</span><span class="s0">, </span><span class="s1">raw_prediction = random_y_true_raw_prediction(</span>
        <span class="s1">loss=half_tweedie_log</span><span class="s0">, </span><span class="s1">n_samples=n_samples</span><span class="s0">, </span><span class="s1">seed=</span><span class="s3">42</span>
    <span class="s1">)</span>
    <span class="s1">y_pred = half_tweedie_log.link.inverse(raw_prediction)  </span><span class="s2"># exp(raw_prediction)</span>

    <span class="s2"># Let's compare the loss values, up to some constant term that is dropped</span>
    <span class="s2"># in HalfTweedieLoss but not in HalfTweedieLossIdentity.</span>
    <span class="s1">loss_log = half_tweedie_log.loss(</span>
        <span class="s1">y_true=y_true</span><span class="s0">, </span><span class="s1">raw_prediction=raw_prediction</span>
    <span class="s1">) + half_tweedie_log.constant_to_optimal_zero(y_true)</span>
    <span class="s1">loss_identity = half_tweedie_identity.loss(</span>
        <span class="s1">y_true=y_true</span><span class="s0">, </span><span class="s1">raw_prediction=y_pred</span>
    <span class="s1">) + half_tweedie_identity.constant_to_optimal_zero(y_true)</span>
    <span class="s2"># Note that HalfTweedieLoss ignores different constant terms than</span>
    <span class="s2"># HalfTweedieLossIdentity. Constant terms means terms not depending on</span>
    <span class="s2"># raw_prediction. By adding these terms, `constant_to_optimal_zero`, both losses</span>
    <span class="s2"># give the same values.</span>
    <span class="s1">assert_allclose(loss_log</span><span class="s0">, </span><span class="s1">loss_identity)</span>

    <span class="s2"># For gradients and hessians, the constant terms do not matter. We have, however,</span>
    <span class="s2"># to account for the chain rule, i.e. with x=raw_prediction</span>
    <span class="s2">#     gradient_log(x) = d/dx loss_log(x)</span>
    <span class="s2">#                     = d/dx loss_identity(exp(x))</span>
    <span class="s2">#                     = exp(x) * gradient_identity(exp(x))</span>
    <span class="s2"># Similarly,</span>
    <span class="s2">#     hessian_log(x) = exp(x) * gradient_identity(exp(x))</span>
    <span class="s2">#                    + exp(x)**2 * hessian_identity(x)</span>
    <span class="s1">gradient_log</span><span class="s0">, </span><span class="s1">hessian_log = half_tweedie_log.gradient_hessian(</span>
        <span class="s1">y_true=y_true</span><span class="s0">, </span><span class="s1">raw_prediction=raw_prediction</span>
    <span class="s1">)</span>
    <span class="s1">gradient_identity</span><span class="s0">, </span><span class="s1">hessian_identity = half_tweedie_identity.gradient_hessian(</span>
        <span class="s1">y_true=y_true</span><span class="s0">, </span><span class="s1">raw_prediction=y_pred</span>
    <span class="s1">)</span>
    <span class="s1">assert_allclose(gradient_log</span><span class="s0">, </span><span class="s1">y_pred * gradient_identity)</span>
    <span class="s1">assert_allclose(</span>
        <span class="s1">hessian_log</span><span class="s0">, </span><span class="s1">y_pred * gradient_identity + y_pred**</span><span class="s3">2 </span><span class="s1">* hessian_identity</span>
    <span class="s1">)</span>
</pre>
</body>
</html>