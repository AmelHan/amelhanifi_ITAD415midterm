<html>
<head>
<title>_logistic.py</title>
<meta http-equiv="Content-Type" content="text/html; charset=utf-8">
<style type="text/css">
.s0 { color: #629755; font-style: italic;}
.s1 { color: #a9b7c6;}
.s2 { color: #808080;}
.s3 { color: #cc7832;}
.s4 { color: #6a8759;}
.s5 { color: #6897bb;}
</style>
</head>
<body bgcolor="#2b2b2b">
<table CELLSPACING=0 CELLPADDING=5 COLS=1 WIDTH="100%" BGCOLOR="#606060" >
<tr><td><center>
<font face="Arial, Helvetica" color="#000000">
_logistic.py</font>
</center></td></tr></table>
<pre><span class="s0">&quot;&quot;&quot; 
Logistic Regression 
&quot;&quot;&quot;</span>

<span class="s2"># Author: Gael Varoquaux &lt;gael.varoquaux@normalesup.org&gt;</span>
<span class="s2">#         Fabian Pedregosa &lt;f@bianp.net&gt;</span>
<span class="s2">#         Alexandre Gramfort &lt;alexandre.gramfort@telecom-paristech.fr&gt;</span>
<span class="s2">#         Manoj Kumar &lt;manojkumarsivaraj334@gmail.com&gt;</span>
<span class="s2">#         Lars Buitinck</span>
<span class="s2">#         Simon Wu &lt;s8wu@uwaterloo.ca&gt;</span>
<span class="s2">#         Arthur Mensch &lt;arthur.mensch@m4x.org</span>

<span class="s3">import </span><span class="s1">numbers</span>
<span class="s3">import </span><span class="s1">warnings</span>
<span class="s3">from </span><span class="s1">numbers </span><span class="s3">import </span><span class="s1">Integral</span><span class="s3">, </span><span class="s1">Real</span>

<span class="s3">import </span><span class="s1">numpy </span><span class="s3">as </span><span class="s1">np</span>
<span class="s3">from </span><span class="s1">joblib </span><span class="s3">import </span><span class="s1">effective_n_jobs</span>
<span class="s3">from </span><span class="s1">scipy </span><span class="s3">import </span><span class="s1">optimize</span>

<span class="s3">from </span><span class="s1">sklearn.metrics </span><span class="s3">import </span><span class="s1">get_scorer_names</span>

<span class="s3">from </span><span class="s1">.._loss.loss </span><span class="s3">import </span><span class="s1">HalfBinomialLoss</span><span class="s3">, </span><span class="s1">HalfMultinomialLoss</span>
<span class="s3">from </span><span class="s1">..base </span><span class="s3">import </span><span class="s1">_fit_context</span>
<span class="s3">from </span><span class="s1">..metrics </span><span class="s3">import </span><span class="s1">get_scorer</span>
<span class="s3">from </span><span class="s1">..model_selection </span><span class="s3">import </span><span class="s1">check_cv</span>
<span class="s3">from </span><span class="s1">..preprocessing </span><span class="s3">import </span><span class="s1">LabelBinarizer</span><span class="s3">, </span><span class="s1">LabelEncoder</span>
<span class="s3">from </span><span class="s1">..svm._base </span><span class="s3">import </span><span class="s1">_fit_liblinear</span>
<span class="s3">from </span><span class="s1">..utils </span><span class="s3">import </span><span class="s1">(</span>
    <span class="s1">check_array</span><span class="s3">,</span>
    <span class="s1">check_consistent_length</span><span class="s3">,</span>
    <span class="s1">check_random_state</span><span class="s3">,</span>
    <span class="s1">compute_class_weight</span><span class="s3">,</span>
<span class="s1">)</span>
<span class="s3">from </span><span class="s1">..utils._param_validation </span><span class="s3">import </span><span class="s1">Interval</span><span class="s3">, </span><span class="s1">StrOptions</span>
<span class="s3">from </span><span class="s1">..utils.extmath </span><span class="s3">import </span><span class="s1">row_norms</span><span class="s3">, </span><span class="s1">softmax</span>
<span class="s3">from </span><span class="s1">..utils.multiclass </span><span class="s3">import </span><span class="s1">check_classification_targets</span>
<span class="s3">from </span><span class="s1">..utils.optimize </span><span class="s3">import </span><span class="s1">_check_optimize_result</span><span class="s3">, </span><span class="s1">_newton_cg</span>
<span class="s3">from </span><span class="s1">..utils.parallel </span><span class="s3">import </span><span class="s1">Parallel</span><span class="s3">, </span><span class="s1">delayed</span>
<span class="s3">from </span><span class="s1">..utils.validation </span><span class="s3">import </span><span class="s1">_check_sample_weight</span><span class="s3">, </span><span class="s1">check_is_fitted</span>
<span class="s3">from </span><span class="s1">._base </span><span class="s3">import </span><span class="s1">BaseEstimator</span><span class="s3">, </span><span class="s1">LinearClassifierMixin</span><span class="s3">, </span><span class="s1">SparseCoefMixin</span>
<span class="s3">from </span><span class="s1">._glm.glm </span><span class="s3">import </span><span class="s1">NewtonCholeskySolver</span>
<span class="s3">from </span><span class="s1">._linear_loss </span><span class="s3">import </span><span class="s1">LinearModelLoss</span>
<span class="s3">from </span><span class="s1">._sag </span><span class="s3">import </span><span class="s1">sag_solver</span>

<span class="s1">_LOGISTIC_SOLVER_CONVERGENCE_MSG = (</span>
    <span class="s4">&quot;Please also refer to the documentation for alternative solver options:</span><span class="s3">\n</span><span class="s4">&quot;</span>
    <span class="s4">&quot;    https://scikit-learn.org/stable/modules/linear_model.html&quot;</span>
    <span class="s4">&quot;#logistic-regression&quot;</span>
<span class="s1">)</span>


<span class="s3">def </span><span class="s1">_check_solver(solver</span><span class="s3">, </span><span class="s1">penalty</span><span class="s3">, </span><span class="s1">dual):</span>
    <span class="s2"># TODO(1.4): Remove &quot;none&quot; option</span>
    <span class="s3">if </span><span class="s1">solver </span><span class="s3">not in </span><span class="s1">[</span><span class="s4">&quot;liblinear&quot;</span><span class="s3">, </span><span class="s4">&quot;saga&quot;</span><span class="s1">] </span><span class="s3">and </span><span class="s1">penalty </span><span class="s3">not in </span><span class="s1">(</span><span class="s4">&quot;l2&quot;</span><span class="s3">, </span><span class="s4">&quot;none&quot;</span><span class="s3">, None</span><span class="s1">):</span>
        <span class="s3">raise </span><span class="s1">ValueError(</span>
            <span class="s4">&quot;Solver %s supports only 'l2' or 'none' penalties, got %s penalty.&quot;</span>
            <span class="s1">% (solver</span><span class="s3">, </span><span class="s1">penalty)</span>
        <span class="s1">)</span>
    <span class="s3">if </span><span class="s1">solver != </span><span class="s4">&quot;liblinear&quot; </span><span class="s3">and </span><span class="s1">dual:</span>
        <span class="s3">raise </span><span class="s1">ValueError(</span>
            <span class="s4">&quot;Solver %s supports only dual=False, got dual=%s&quot; </span><span class="s1">% (solver</span><span class="s3">, </span><span class="s1">dual)</span>
        <span class="s1">)</span>

    <span class="s3">if </span><span class="s1">penalty == </span><span class="s4">&quot;elasticnet&quot; </span><span class="s3">and </span><span class="s1">solver != </span><span class="s4">&quot;saga&quot;</span><span class="s1">:</span>
        <span class="s3">raise </span><span class="s1">ValueError(</span>
            <span class="s4">&quot;Only 'saga' solver supports elasticnet penalty, got solver={}.&quot;</span><span class="s1">.format(</span>
                <span class="s1">solver</span>
            <span class="s1">)</span>
        <span class="s1">)</span>

    <span class="s3">if </span><span class="s1">solver == </span><span class="s4">&quot;liblinear&quot; </span><span class="s3">and </span><span class="s1">penalty == </span><span class="s4">&quot;none&quot;</span><span class="s1">:</span>
        <span class="s3">raise </span><span class="s1">ValueError(</span><span class="s4">&quot;penalty='none' is not supported for the liblinear solver&quot;</span><span class="s1">)</span>

    <span class="s3">return </span><span class="s1">solver</span>


<span class="s3">def </span><span class="s1">_check_multi_class(multi_class</span><span class="s3">, </span><span class="s1">solver</span><span class="s3">, </span><span class="s1">n_classes):</span>
    <span class="s0">&quot;&quot;&quot;Computes the multi class type, either &quot;multinomial&quot; or &quot;ovr&quot;. 
 
    For `n_classes` &gt; 2 and a solver that supports it, returns &quot;multinomial&quot;. 
    For all other cases, in particular binary classification, return &quot;ovr&quot;. 
    &quot;&quot;&quot;</span>
    <span class="s3">if </span><span class="s1">multi_class == </span><span class="s4">&quot;auto&quot;</span><span class="s1">:</span>
        <span class="s3">if </span><span class="s1">solver </span><span class="s3">in </span><span class="s1">(</span><span class="s4">&quot;liblinear&quot;</span><span class="s3">, </span><span class="s4">&quot;newton-cholesky&quot;</span><span class="s1">):</span>
            <span class="s1">multi_class = </span><span class="s4">&quot;ovr&quot;</span>
        <span class="s3">elif </span><span class="s1">n_classes &gt; </span><span class="s5">2</span><span class="s1">:</span>
            <span class="s1">multi_class = </span><span class="s4">&quot;multinomial&quot;</span>
        <span class="s3">else</span><span class="s1">:</span>
            <span class="s1">multi_class = </span><span class="s4">&quot;ovr&quot;</span>
    <span class="s3">if </span><span class="s1">multi_class == </span><span class="s4">&quot;multinomial&quot; </span><span class="s3">and </span><span class="s1">solver </span><span class="s3">in </span><span class="s1">(</span><span class="s4">&quot;liblinear&quot;</span><span class="s3">, </span><span class="s4">&quot;newton-cholesky&quot;</span><span class="s1">):</span>
        <span class="s3">raise </span><span class="s1">ValueError(</span><span class="s4">&quot;Solver %s does not support a multinomial backend.&quot; </span><span class="s1">% solver)</span>
    <span class="s3">return </span><span class="s1">multi_class</span>


<span class="s3">def </span><span class="s1">_logistic_regression_path(</span>
    <span class="s1">X</span><span class="s3">,</span>
    <span class="s1">y</span><span class="s3">,</span>
    <span class="s1">pos_class=</span><span class="s3">None,</span>
    <span class="s1">Cs=</span><span class="s5">10</span><span class="s3">,</span>
    <span class="s1">fit_intercept=</span><span class="s3">True,</span>
    <span class="s1">max_iter=</span><span class="s5">100</span><span class="s3">,</span>
    <span class="s1">tol=</span><span class="s5">1e-4</span><span class="s3">,</span>
    <span class="s1">verbose=</span><span class="s5">0</span><span class="s3">,</span>
    <span class="s1">solver=</span><span class="s4">&quot;lbfgs&quot;</span><span class="s3">,</span>
    <span class="s1">coef=</span><span class="s3">None,</span>
    <span class="s1">class_weight=</span><span class="s3">None,</span>
    <span class="s1">dual=</span><span class="s3">False,</span>
    <span class="s1">penalty=</span><span class="s4">&quot;l2&quot;</span><span class="s3">,</span>
    <span class="s1">intercept_scaling=</span><span class="s5">1.0</span><span class="s3">,</span>
    <span class="s1">multi_class=</span><span class="s4">&quot;auto&quot;</span><span class="s3">,</span>
    <span class="s1">random_state=</span><span class="s3">None,</span>
    <span class="s1">check_input=</span><span class="s3">True,</span>
    <span class="s1">max_squared_sum=</span><span class="s3">None,</span>
    <span class="s1">sample_weight=</span><span class="s3">None,</span>
    <span class="s1">l1_ratio=</span><span class="s3">None,</span>
    <span class="s1">n_threads=</span><span class="s5">1</span><span class="s3">,</span>
<span class="s1">):</span>
    <span class="s0">&quot;&quot;&quot;Compute a Logistic Regression model for a list of regularization 
    parameters. 
 
    This is an implementation that uses the result of the previous model 
    to speed up computations along the set of solutions, making it faster 
    than sequentially calling LogisticRegression for the different parameters. 
    Note that there will be no speedup with liblinear solver, since it does 
    not handle warm-starting. 
 
    Read more in the :ref:`User Guide &lt;logistic_regression&gt;`. 
 
    Parameters 
    ---------- 
    X : {array-like, sparse matrix} of shape (n_samples, n_features) 
        Input data. 
 
    y : array-like of shape (n_samples,) or (n_samples, n_targets) 
        Input data, target values. 
 
    pos_class : int, default=None 
        The class with respect to which we perform a one-vs-all fit. 
        If None, then it is assumed that the given problem is binary. 
 
    Cs : int or array-like of shape (n_cs,), default=10 
        List of values for the regularization parameter or integer specifying 
        the number of regularization parameters that should be used. In this 
        case, the parameters will be chosen in a logarithmic scale between 
        1e-4 and 1e4. 
 
    fit_intercept : bool, default=True 
        Whether to fit an intercept for the model. In this case the shape of 
        the returned array is (n_cs, n_features + 1). 
 
    max_iter : int, default=100 
        Maximum number of iterations for the solver. 
 
    tol : float, default=1e-4 
        Stopping criterion. For the newton-cg and lbfgs solvers, the iteration 
        will stop when ``max{|g_i | i = 1, ..., n} &lt;= tol`` 
        where ``g_i`` is the i-th component of the gradient. 
 
    verbose : int, default=0 
        For the liblinear and lbfgs solvers set verbose to any positive 
        number for verbosity. 
 
    solver : {'lbfgs', 'liblinear', 'newton-cg', 'newton-cholesky', 'sag', 'saga'}, \ 
            default='lbfgs' 
        Numerical solver to use. 
 
    coef : array-like of shape (n_features,), default=None 
        Initialization value for coefficients of logistic regression. 
        Useless for liblinear solver. 
 
    class_weight : dict or 'balanced', default=None 
        Weights associated with classes in the form ``{class_label: weight}``. 
        If not given, all classes are supposed to have weight one. 
 
        The &quot;balanced&quot; mode uses the values of y to automatically adjust 
        weights inversely proportional to class frequencies in the input data 
        as ``n_samples / (n_classes * np.bincount(y))``. 
 
        Note that these weights will be multiplied with sample_weight (passed 
        through the fit method) if sample_weight is specified. 
 
    dual : bool, default=False 
        Dual or primal formulation. Dual formulation is only implemented for 
        l2 penalty with liblinear solver. Prefer dual=False when 
        n_samples &gt; n_features. 
 
    penalty : {'l1', 'l2', 'elasticnet'}, default='l2' 
        Used to specify the norm used in the penalization. The 'newton-cg', 
        'sag' and 'lbfgs' solvers support only l2 penalties. 'elasticnet' is 
        only supported by the 'saga' solver. 
 
    intercept_scaling : float, default=1. 
        Useful only when the solver 'liblinear' is used 
        and self.fit_intercept is set to True. In this case, x becomes 
        [x, self.intercept_scaling], 
        i.e. a &quot;synthetic&quot; feature with constant value equal to 
        intercept_scaling is appended to the instance vector. 
        The intercept becomes ``intercept_scaling * synthetic_feature_weight``. 
 
        Note! the synthetic feature weight is subject to l1/l2 regularization 
        as all other features. 
        To lessen the effect of regularization on synthetic feature weight 
        (and therefore on the intercept) intercept_scaling has to be increased. 
 
    multi_class : {'ovr', 'multinomial', 'auto'}, default='auto' 
        If the option chosen is 'ovr', then a binary problem is fit for each 
        label. For 'multinomial' the loss minimised is the multinomial loss fit 
        across the entire probability distribution, *even when the data is 
        binary*. 'multinomial' is unavailable when solver='liblinear'. 
        'auto' selects 'ovr' if the data is binary, or if solver='liblinear', 
        and otherwise selects 'multinomial'. 
 
        .. versionadded:: 0.18 
           Stochastic Average Gradient descent solver for 'multinomial' case. 
        .. versionchanged:: 0.22 
            Default changed from 'ovr' to 'auto' in 0.22. 
 
    random_state : int, RandomState instance, default=None 
        Used when ``solver`` == 'sag', 'saga' or 'liblinear' to shuffle the 
        data. See :term:`Glossary &lt;random_state&gt;` for details. 
 
    check_input : bool, default=True 
        If False, the input arrays X and y will not be checked. 
 
    max_squared_sum : float, default=None 
        Maximum squared sum of X over samples. Used only in SAG solver. 
        If None, it will be computed, going through all the samples. 
        The value should be precomputed to speed up cross validation. 
 
    sample_weight : array-like of shape(n_samples,), default=None 
        Array of weights that are assigned to individual samples. 
        If not provided, then each sample is given unit weight. 
 
    l1_ratio : float, default=None 
        The Elastic-Net mixing parameter, with ``0 &lt;= l1_ratio &lt;= 1``. Only 
        used if ``penalty='elasticnet'``. Setting ``l1_ratio=0`` is equivalent 
        to using ``penalty='l2'``, while setting ``l1_ratio=1`` is equivalent 
        to using ``penalty='l1'``. For ``0 &lt; l1_ratio &lt;1``, the penalty is a 
        combination of L1 and L2. 
 
    n_threads : int, default=1 
       Number of OpenMP threads to use. 
 
    Returns 
    ------- 
    coefs : ndarray of shape (n_cs, n_features) or (n_cs, n_features + 1) 
        List of coefficients for the Logistic Regression model. If 
        fit_intercept is set to True then the second dimension will be 
        n_features + 1, where the last item represents the intercept. For 
        ``multiclass='multinomial'``, the shape is (n_classes, n_cs, 
        n_features) or (n_classes, n_cs, n_features + 1). 
 
    Cs : ndarray 
        Grid of Cs used for cross-validation. 
 
    n_iter : array of shape (n_cs,) 
        Actual number of iteration for each Cs. 
 
    Notes 
    ----- 
    You might get slightly different results with the solver liblinear than 
    with the others since this uses LIBLINEAR which penalizes the intercept. 
 
    .. versionchanged:: 0.19 
        The &quot;copy&quot; parameter was removed. 
    &quot;&quot;&quot;</span>
    <span class="s3">if </span><span class="s1">isinstance(Cs</span><span class="s3">, </span><span class="s1">numbers.Integral):</span>
        <span class="s1">Cs = np.logspace(-</span><span class="s5">4</span><span class="s3">, </span><span class="s5">4</span><span class="s3">, </span><span class="s1">Cs)</span>

    <span class="s1">solver = _check_solver(solver</span><span class="s3">, </span><span class="s1">penalty</span><span class="s3">, </span><span class="s1">dual)</span>

    <span class="s2"># Preprocessing.</span>
    <span class="s3">if </span><span class="s1">check_input:</span>
        <span class="s1">X = check_array(</span>
            <span class="s1">X</span><span class="s3">,</span>
            <span class="s1">accept_sparse=</span><span class="s4">&quot;csr&quot;</span><span class="s3">,</span>
            <span class="s1">dtype=np.float64</span><span class="s3">,</span>
            <span class="s1">accept_large_sparse=solver </span><span class="s3">not in </span><span class="s1">[</span><span class="s4">&quot;liblinear&quot;</span><span class="s3">, </span><span class="s4">&quot;sag&quot;</span><span class="s3">, </span><span class="s4">&quot;saga&quot;</span><span class="s1">]</span><span class="s3">,</span>
        <span class="s1">)</span>
        <span class="s1">y = check_array(y</span><span class="s3">, </span><span class="s1">ensure_2d=</span><span class="s3">False, </span><span class="s1">dtype=</span><span class="s3">None</span><span class="s1">)</span>
        <span class="s1">check_consistent_length(X</span><span class="s3">, </span><span class="s1">y)</span>
    <span class="s1">n_samples</span><span class="s3">, </span><span class="s1">n_features = X.shape</span>

    <span class="s1">classes = np.unique(y)</span>
    <span class="s1">random_state = check_random_state(random_state)</span>

    <span class="s1">multi_class = _check_multi_class(multi_class</span><span class="s3">, </span><span class="s1">solver</span><span class="s3">, </span><span class="s1">len(classes))</span>
    <span class="s3">if </span><span class="s1">pos_class </span><span class="s3">is None and </span><span class="s1">multi_class != </span><span class="s4">&quot;multinomial&quot;</span><span class="s1">:</span>
        <span class="s3">if </span><span class="s1">classes.size &gt; </span><span class="s5">2</span><span class="s1">:</span>
            <span class="s3">raise </span><span class="s1">ValueError(</span><span class="s4">&quot;To fit OvR, use the pos_class argument&quot;</span><span class="s1">)</span>
        <span class="s2"># np.unique(y) gives labels in sorted order.</span>
        <span class="s1">pos_class = classes[</span><span class="s5">1</span><span class="s1">]</span>

    <span class="s2"># If sample weights exist, convert them to array (support for lists)</span>
    <span class="s2"># and check length</span>
    <span class="s2"># Otherwise set them to 1 for all examples</span>
    <span class="s1">sample_weight = _check_sample_weight(sample_weight</span><span class="s3">, </span><span class="s1">X</span><span class="s3">, </span><span class="s1">dtype=X.dtype</span><span class="s3">, </span><span class="s1">copy=</span><span class="s3">True</span><span class="s1">)</span>

    <span class="s3">if </span><span class="s1">solver == </span><span class="s4">&quot;newton-cholesky&quot;</span><span class="s1">:</span>
        <span class="s2"># IMPORTANT NOTE: Rescaling of sample_weight:</span>
        <span class="s2"># Same as in _GeneralizedLinearRegressor.fit().</span>
        <span class="s2"># We want to minimize</span>
        <span class="s2">#     obj = 1/(2*sum(sample_weight)) * sum(sample_weight * deviance)</span>
        <span class="s2">#         + 1/2 * alpha * L2,</span>
        <span class="s2"># with</span>
        <span class="s2">#     deviance = 2 * log_loss.</span>
        <span class="s2"># The objective is invariant to multiplying sample_weight by a constant. We</span>
        <span class="s2"># choose this constant such that sum(sample_weight) = 1. Thus, we end up with</span>
        <span class="s2">#     obj = sum(sample_weight * loss) + 1/2 * alpha * L2.</span>
        <span class="s2"># Note that LinearModelLoss.loss() computes sum(sample_weight * loss).</span>
        <span class="s2">#</span>
        <span class="s2"># This rescaling has to be done before multiplying by class_weights.</span>
        <span class="s1">sw_sum = sample_weight.sum()  </span><span class="s2"># needed to rescale penalty, nasty matter!</span>
        <span class="s1">sample_weight = sample_weight / sw_sum</span>

    <span class="s2"># If class_weights is a dict (provided by the user), the weights</span>
    <span class="s2"># are assigned to the original labels. If it is &quot;balanced&quot;, then</span>
    <span class="s2"># the class_weights are assigned after masking the labels with a OvR.</span>
    <span class="s1">le = LabelEncoder()</span>
    <span class="s3">if </span><span class="s1">isinstance(class_weight</span><span class="s3">, </span><span class="s1">dict) </span><span class="s3">or </span><span class="s1">multi_class == </span><span class="s4">&quot;multinomial&quot;</span><span class="s1">:</span>
        <span class="s1">class_weight_ = compute_class_weight(class_weight</span><span class="s3">, </span><span class="s1">classes=classes</span><span class="s3">, </span><span class="s1">y=y)</span>
        <span class="s1">sample_weight *= class_weight_[le.fit_transform(y)]</span>

    <span class="s2"># For doing a ovr, we need to mask the labels first. For the</span>
    <span class="s2"># multinomial case this is not necessary.</span>
    <span class="s3">if </span><span class="s1">multi_class == </span><span class="s4">&quot;ovr&quot;</span><span class="s1">:</span>
        <span class="s1">w0 = np.zeros(n_features + int(fit_intercept)</span><span class="s3">, </span><span class="s1">dtype=X.dtype)</span>
        <span class="s1">mask = y == pos_class</span>
        <span class="s1">y_bin = np.ones(y.shape</span><span class="s3">, </span><span class="s1">dtype=X.dtype)</span>
        <span class="s3">if </span><span class="s1">solver </span><span class="s3">in </span><span class="s1">[</span><span class="s4">&quot;lbfgs&quot;</span><span class="s3">, </span><span class="s4">&quot;newton-cg&quot;</span><span class="s3">, </span><span class="s4">&quot;newton-cholesky&quot;</span><span class="s1">]:</span>
            <span class="s2"># HalfBinomialLoss, used for those solvers, represents y in [0, 1] instead</span>
            <span class="s2"># of in [-1, 1].</span>
            <span class="s1">mask_classes = np.array([</span><span class="s5">0</span><span class="s3">, </span><span class="s5">1</span><span class="s1">])</span>
            <span class="s1">y_bin[~mask] = </span><span class="s5">0.0</span>
        <span class="s3">else</span><span class="s1">:</span>
            <span class="s1">mask_classes = np.array([-</span><span class="s5">1</span><span class="s3">, </span><span class="s5">1</span><span class="s1">])</span>
            <span class="s1">y_bin[~mask] = -</span><span class="s5">1.0</span>

        <span class="s2"># for compute_class_weight</span>
        <span class="s3">if </span><span class="s1">class_weight == </span><span class="s4">&quot;balanced&quot;</span><span class="s1">:</span>
            <span class="s1">class_weight_ = compute_class_weight(</span>
                <span class="s1">class_weight</span><span class="s3">, </span><span class="s1">classes=mask_classes</span><span class="s3">, </span><span class="s1">y=y_bin</span>
            <span class="s1">)</span>
            <span class="s1">sample_weight *= class_weight_[le.fit_transform(y_bin)]</span>

    <span class="s3">else</span><span class="s1">:</span>
        <span class="s3">if </span><span class="s1">solver </span><span class="s3">in </span><span class="s1">[</span><span class="s4">&quot;sag&quot;</span><span class="s3">, </span><span class="s4">&quot;saga&quot;</span><span class="s3">, </span><span class="s4">&quot;lbfgs&quot;</span><span class="s3">, </span><span class="s4">&quot;newton-cg&quot;</span><span class="s1">]:</span>
            <span class="s2"># SAG, lbfgs and newton-cg multinomial solvers need LabelEncoder,</span>
            <span class="s2"># not LabelBinarizer, i.e. y as a 1d-array of integers.</span>
            <span class="s2"># LabelEncoder also saves memory compared to LabelBinarizer, especially</span>
            <span class="s2"># when n_classes is large.</span>
            <span class="s1">le = LabelEncoder()</span>
            <span class="s1">Y_multi = le.fit_transform(y).astype(X.dtype</span><span class="s3">, </span><span class="s1">copy=</span><span class="s3">False</span><span class="s1">)</span>
        <span class="s3">else</span><span class="s1">:</span>
            <span class="s2"># For liblinear solver, apply LabelBinarizer, i.e. y is one-hot encoded.</span>
            <span class="s1">lbin = LabelBinarizer()</span>
            <span class="s1">Y_multi = lbin.fit_transform(y)</span>
            <span class="s3">if </span><span class="s1">Y_multi.shape[</span><span class="s5">1</span><span class="s1">] == </span><span class="s5">1</span><span class="s1">:</span>
                <span class="s1">Y_multi = np.hstack([</span><span class="s5">1 </span><span class="s1">- Y_multi</span><span class="s3">, </span><span class="s1">Y_multi])</span>

        <span class="s1">w0 = np.zeros(</span>
            <span class="s1">(classes.size</span><span class="s3">, </span><span class="s1">n_features + int(fit_intercept))</span><span class="s3">, </span><span class="s1">order=</span><span class="s4">&quot;F&quot;</span><span class="s3">, </span><span class="s1">dtype=X.dtype</span>
        <span class="s1">)</span>

    <span class="s3">if </span><span class="s1">coef </span><span class="s3">is not None</span><span class="s1">:</span>
        <span class="s2"># it must work both giving the bias term and not</span>
        <span class="s3">if </span><span class="s1">multi_class == </span><span class="s4">&quot;ovr&quot;</span><span class="s1">:</span>
            <span class="s3">if </span><span class="s1">coef.size </span><span class="s3">not in </span><span class="s1">(n_features</span><span class="s3">, </span><span class="s1">w0.size):</span>
                <span class="s3">raise </span><span class="s1">ValueError(</span>
                    <span class="s4">&quot;Initialization coef is of shape %d, expected shape %d or %d&quot;</span>
                    <span class="s1">% (coef.size</span><span class="s3">, </span><span class="s1">n_features</span><span class="s3">, </span><span class="s1">w0.size)</span>
                <span class="s1">)</span>
            <span class="s1">w0[: coef.size] = coef</span>
        <span class="s3">else</span><span class="s1">:</span>
            <span class="s2"># For binary problems coef.shape[0] should be 1, otherwise it</span>
            <span class="s2"># should be classes.size.</span>
            <span class="s1">n_classes = classes.size</span>
            <span class="s3">if </span><span class="s1">n_classes == </span><span class="s5">2</span><span class="s1">:</span>
                <span class="s1">n_classes = </span><span class="s5">1</span>

            <span class="s3">if </span><span class="s1">coef.shape[</span><span class="s5">0</span><span class="s1">] != n_classes </span><span class="s3">or </span><span class="s1">coef.shape[</span><span class="s5">1</span><span class="s1">] </span><span class="s3">not in </span><span class="s1">(</span>
                <span class="s1">n_features</span><span class="s3">,</span>
                <span class="s1">n_features + </span><span class="s5">1</span><span class="s3">,</span>
            <span class="s1">):</span>
                <span class="s3">raise </span><span class="s1">ValueError(</span>
                    <span class="s4">&quot;Initialization coef is of shape (%d, %d), expected &quot;</span>
                    <span class="s4">&quot;shape (%d, %d) or (%d, %d)&quot;</span>
                    <span class="s1">% (</span>
                        <span class="s1">coef.shape[</span><span class="s5">0</span><span class="s1">]</span><span class="s3">,</span>
                        <span class="s1">coef.shape[</span><span class="s5">1</span><span class="s1">]</span><span class="s3">,</span>
                        <span class="s1">classes.size</span><span class="s3">,</span>
                        <span class="s1">n_features</span><span class="s3">,</span>
                        <span class="s1">classes.size</span><span class="s3">,</span>
                        <span class="s1">n_features + </span><span class="s5">1</span><span class="s3">,</span>
                    <span class="s1">)</span>
                <span class="s1">)</span>

            <span class="s3">if </span><span class="s1">n_classes == </span><span class="s5">1</span><span class="s1">:</span>
                <span class="s1">w0[</span><span class="s5">0</span><span class="s3">, </span><span class="s1">: coef.shape[</span><span class="s5">1</span><span class="s1">]] = -coef</span>
                <span class="s1">w0[</span><span class="s5">1</span><span class="s3">, </span><span class="s1">: coef.shape[</span><span class="s5">1</span><span class="s1">]] = coef</span>
            <span class="s3">else</span><span class="s1">:</span>
                <span class="s1">w0[:</span><span class="s3">, </span><span class="s1">: coef.shape[</span><span class="s5">1</span><span class="s1">]] = coef</span>

    <span class="s3">if </span><span class="s1">multi_class == </span><span class="s4">&quot;multinomial&quot;</span><span class="s1">:</span>
        <span class="s3">if </span><span class="s1">solver </span><span class="s3">in </span><span class="s1">[</span><span class="s4">&quot;lbfgs&quot;</span><span class="s3">, </span><span class="s4">&quot;newton-cg&quot;</span><span class="s1">]:</span>
            <span class="s2"># scipy.optimize.minimize and newton-cg accept only ravelled parameters,</span>
            <span class="s2"># i.e. 1d-arrays. LinearModelLoss expects classes to be contiguous and</span>
            <span class="s2"># reconstructs the 2d-array via w0.reshape((n_classes, -1), order=&quot;F&quot;).</span>
            <span class="s2"># As w0 is F-contiguous, ravel(order=&quot;F&quot;) also avoids a copy.</span>
            <span class="s1">w0 = w0.ravel(order=</span><span class="s4">&quot;F&quot;</span><span class="s1">)</span>
            <span class="s1">loss = LinearModelLoss(</span>
                <span class="s1">base_loss=HalfMultinomialLoss(n_classes=classes.size)</span><span class="s3">,</span>
                <span class="s1">fit_intercept=fit_intercept</span><span class="s3">,</span>
            <span class="s1">)</span>
        <span class="s1">target = Y_multi</span>
        <span class="s3">if </span><span class="s1">solver </span><span class="s3">in </span><span class="s4">&quot;lbfgs&quot;</span><span class="s1">:</span>
            <span class="s1">func = loss.loss_gradient</span>
        <span class="s3">elif </span><span class="s1">solver == </span><span class="s4">&quot;newton-cg&quot;</span><span class="s1">:</span>
            <span class="s1">func = loss.loss</span>
            <span class="s1">grad = loss.gradient</span>
            <span class="s1">hess = loss.gradient_hessian_product  </span><span class="s2"># hess = [gradient, hessp]</span>
        <span class="s1">warm_start_sag = {</span><span class="s4">&quot;coef&quot;</span><span class="s1">: w0.T}</span>
    <span class="s3">else</span><span class="s1">:</span>
        <span class="s1">target = y_bin</span>
        <span class="s3">if </span><span class="s1">solver == </span><span class="s4">&quot;lbfgs&quot;</span><span class="s1">:</span>
            <span class="s1">loss = LinearModelLoss(</span>
                <span class="s1">base_loss=HalfBinomialLoss()</span><span class="s3">, </span><span class="s1">fit_intercept=fit_intercept</span>
            <span class="s1">)</span>
            <span class="s1">func = loss.loss_gradient</span>
        <span class="s3">elif </span><span class="s1">solver == </span><span class="s4">&quot;newton-cg&quot;</span><span class="s1">:</span>
            <span class="s1">loss = LinearModelLoss(</span>
                <span class="s1">base_loss=HalfBinomialLoss()</span><span class="s3">, </span><span class="s1">fit_intercept=fit_intercept</span>
            <span class="s1">)</span>
            <span class="s1">func = loss.loss</span>
            <span class="s1">grad = loss.gradient</span>
            <span class="s1">hess = loss.gradient_hessian_product  </span><span class="s2"># hess = [gradient, hessp]</span>
        <span class="s3">elif </span><span class="s1">solver == </span><span class="s4">&quot;newton-cholesky&quot;</span><span class="s1">:</span>
            <span class="s1">loss = LinearModelLoss(</span>
                <span class="s1">base_loss=HalfBinomialLoss()</span><span class="s3">, </span><span class="s1">fit_intercept=fit_intercept</span>
            <span class="s1">)</span>
        <span class="s1">warm_start_sag = {</span><span class="s4">&quot;coef&quot;</span><span class="s1">: np.expand_dims(w0</span><span class="s3">, </span><span class="s1">axis=</span><span class="s5">1</span><span class="s1">)}</span>

    <span class="s1">coefs = list()</span>
    <span class="s1">n_iter = np.zeros(len(Cs)</span><span class="s3">, </span><span class="s1">dtype=np.int32)</span>
    <span class="s3">for </span><span class="s1">i</span><span class="s3">, </span><span class="s1">C </span><span class="s3">in </span><span class="s1">enumerate(Cs):</span>
        <span class="s3">if </span><span class="s1">solver == </span><span class="s4">&quot;lbfgs&quot;</span><span class="s1">:</span>
            <span class="s1">l2_reg_strength = </span><span class="s5">1.0 </span><span class="s1">/ C</span>
            <span class="s1">iprint = [-</span><span class="s5">1</span><span class="s3">, </span><span class="s5">50</span><span class="s3">, </span><span class="s5">1</span><span class="s3">, </span><span class="s5">100</span><span class="s3">, </span><span class="s5">101</span><span class="s1">][</span>
                <span class="s1">np.searchsorted(np.array([</span><span class="s5">0</span><span class="s3">, </span><span class="s5">1</span><span class="s3">, </span><span class="s5">2</span><span class="s3">, </span><span class="s5">3</span><span class="s1">])</span><span class="s3">, </span><span class="s1">verbose)</span>
            <span class="s1">]</span>
            <span class="s1">opt_res = optimize.minimize(</span>
                <span class="s1">func</span><span class="s3">,</span>
                <span class="s1">w0</span><span class="s3">,</span>
                <span class="s1">method=</span><span class="s4">&quot;L-BFGS-B&quot;</span><span class="s3">,</span>
                <span class="s1">jac=</span><span class="s3">True,</span>
                <span class="s1">args=(X</span><span class="s3">, </span><span class="s1">target</span><span class="s3">, </span><span class="s1">sample_weight</span><span class="s3">, </span><span class="s1">l2_reg_strength</span><span class="s3">, </span><span class="s1">n_threads)</span><span class="s3">,</span>
                <span class="s1">options={</span><span class="s4">&quot;iprint&quot;</span><span class="s1">: iprint</span><span class="s3">, </span><span class="s4">&quot;gtol&quot;</span><span class="s1">: tol</span><span class="s3">, </span><span class="s4">&quot;maxiter&quot;</span><span class="s1">: max_iter}</span><span class="s3">,</span>
            <span class="s1">)</span>
            <span class="s1">n_iter_i = _check_optimize_result(</span>
                <span class="s1">solver</span><span class="s3">,</span>
                <span class="s1">opt_res</span><span class="s3">,</span>
                <span class="s1">max_iter</span><span class="s3">,</span>
                <span class="s1">extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG</span><span class="s3">,</span>
            <span class="s1">)</span>
            <span class="s1">w0</span><span class="s3">, </span><span class="s1">loss = opt_res.x</span><span class="s3">, </span><span class="s1">opt_res.fun</span>
        <span class="s3">elif </span><span class="s1">solver == </span><span class="s4">&quot;newton-cg&quot;</span><span class="s1">:</span>
            <span class="s1">l2_reg_strength = </span><span class="s5">1.0 </span><span class="s1">/ C</span>
            <span class="s1">args = (X</span><span class="s3">, </span><span class="s1">target</span><span class="s3">, </span><span class="s1">sample_weight</span><span class="s3">, </span><span class="s1">l2_reg_strength</span><span class="s3">, </span><span class="s1">n_threads)</span>
            <span class="s1">w0</span><span class="s3">, </span><span class="s1">n_iter_i = _newton_cg(</span>
                <span class="s1">hess</span><span class="s3">, </span><span class="s1">func</span><span class="s3">, </span><span class="s1">grad</span><span class="s3">, </span><span class="s1">w0</span><span class="s3">, </span><span class="s1">args=args</span><span class="s3">, </span><span class="s1">maxiter=max_iter</span><span class="s3">, </span><span class="s1">tol=tol</span>
            <span class="s1">)</span>
        <span class="s3">elif </span><span class="s1">solver == </span><span class="s4">&quot;newton-cholesky&quot;</span><span class="s1">:</span>
            <span class="s2"># The division by sw_sum is a consequence of the rescaling of</span>
            <span class="s2"># sample_weight, see comment above.</span>
            <span class="s1">l2_reg_strength = </span><span class="s5">1.0 </span><span class="s1">/ C / sw_sum</span>
            <span class="s1">sol = NewtonCholeskySolver(</span>
                <span class="s1">coef=w0</span><span class="s3">,</span>
                <span class="s1">linear_loss=loss</span><span class="s3">,</span>
                <span class="s1">l2_reg_strength=l2_reg_strength</span><span class="s3">,</span>
                <span class="s1">tol=tol</span><span class="s3">,</span>
                <span class="s1">max_iter=max_iter</span><span class="s3">,</span>
                <span class="s1">n_threads=n_threads</span><span class="s3">,</span>
                <span class="s1">verbose=verbose</span><span class="s3">,</span>
            <span class="s1">)</span>
            <span class="s1">w0 = sol.solve(X=X</span><span class="s3">, </span><span class="s1">y=target</span><span class="s3">, </span><span class="s1">sample_weight=sample_weight)</span>
            <span class="s1">n_iter_i = sol.iteration</span>
        <span class="s3">elif </span><span class="s1">solver == </span><span class="s4">&quot;liblinear&quot;</span><span class="s1">:</span>
            <span class="s1">(</span>
                <span class="s1">coef_</span><span class="s3">,</span>
                <span class="s1">intercept_</span><span class="s3">,</span>
                <span class="s1">n_iter_i</span><span class="s3">,</span>
            <span class="s1">) = _fit_liblinear(</span>
                <span class="s1">X</span><span class="s3">,</span>
                <span class="s1">target</span><span class="s3">,</span>
                <span class="s1">C</span><span class="s3">,</span>
                <span class="s1">fit_intercept</span><span class="s3">,</span>
                <span class="s1">intercept_scaling</span><span class="s3">,</span>
                <span class="s3">None,</span>
                <span class="s1">penalty</span><span class="s3">,</span>
                <span class="s1">dual</span><span class="s3">,</span>
                <span class="s1">verbose</span><span class="s3">,</span>
                <span class="s1">max_iter</span><span class="s3">,</span>
                <span class="s1">tol</span><span class="s3">,</span>
                <span class="s1">random_state</span><span class="s3">,</span>
                <span class="s1">sample_weight=sample_weight</span><span class="s3">,</span>
            <span class="s1">)</span>
            <span class="s3">if </span><span class="s1">fit_intercept:</span>
                <span class="s1">w0 = np.concatenate([coef_.ravel()</span><span class="s3">, </span><span class="s1">intercept_])</span>
            <span class="s3">else</span><span class="s1">:</span>
                <span class="s1">w0 = coef_.ravel()</span>
            <span class="s2"># n_iter_i is an array for each class. However, `target` is always encoded</span>
            <span class="s2"># in {-1, 1}, so we only take the first element of n_iter_i.</span>
            <span class="s1">n_iter_i = n_iter_i.item()</span>

        <span class="s3">elif </span><span class="s1">solver </span><span class="s3">in </span><span class="s1">[</span><span class="s4">&quot;sag&quot;</span><span class="s3">, </span><span class="s4">&quot;saga&quot;</span><span class="s1">]:</span>
            <span class="s3">if </span><span class="s1">multi_class == </span><span class="s4">&quot;multinomial&quot;</span><span class="s1">:</span>
                <span class="s1">target = target.astype(X.dtype</span><span class="s3">, </span><span class="s1">copy=</span><span class="s3">False</span><span class="s1">)</span>
                <span class="s1">loss = </span><span class="s4">&quot;multinomial&quot;</span>
            <span class="s3">else</span><span class="s1">:</span>
                <span class="s1">loss = </span><span class="s4">&quot;log&quot;</span>
            <span class="s2"># alpha is for L2-norm, beta is for L1-norm</span>
            <span class="s3">if </span><span class="s1">penalty == </span><span class="s4">&quot;l1&quot;</span><span class="s1">:</span>
                <span class="s1">alpha = </span><span class="s5">0.0</span>
                <span class="s1">beta = </span><span class="s5">1.0 </span><span class="s1">/ C</span>
            <span class="s3">elif </span><span class="s1">penalty == </span><span class="s4">&quot;l2&quot;</span><span class="s1">:</span>
                <span class="s1">alpha = </span><span class="s5">1.0 </span><span class="s1">/ C</span>
                <span class="s1">beta = </span><span class="s5">0.0</span>
            <span class="s3">else</span><span class="s1">:  </span><span class="s2"># Elastic-Net penalty</span>
                <span class="s1">alpha = (</span><span class="s5">1.0 </span><span class="s1">/ C) * (</span><span class="s5">1 </span><span class="s1">- l1_ratio)</span>
                <span class="s1">beta = (</span><span class="s5">1.0 </span><span class="s1">/ C) * l1_ratio</span>

            <span class="s1">w0</span><span class="s3">, </span><span class="s1">n_iter_i</span><span class="s3">, </span><span class="s1">warm_start_sag = sag_solver(</span>
                <span class="s1">X</span><span class="s3">,</span>
                <span class="s1">target</span><span class="s3">,</span>
                <span class="s1">sample_weight</span><span class="s3">,</span>
                <span class="s1">loss</span><span class="s3">,</span>
                <span class="s1">alpha</span><span class="s3">,</span>
                <span class="s1">beta</span><span class="s3">,</span>
                <span class="s1">max_iter</span><span class="s3">,</span>
                <span class="s1">tol</span><span class="s3">,</span>
                <span class="s1">verbose</span><span class="s3">,</span>
                <span class="s1">random_state</span><span class="s3">,</span>
                <span class="s3">False,</span>
                <span class="s1">max_squared_sum</span><span class="s3">,</span>
                <span class="s1">warm_start_sag</span><span class="s3">,</span>
                <span class="s1">is_saga=(solver == </span><span class="s4">&quot;saga&quot;</span><span class="s1">)</span><span class="s3">,</span>
            <span class="s1">)</span>

        <span class="s3">else</span><span class="s1">:</span>
            <span class="s3">raise </span><span class="s1">ValueError(</span>
                <span class="s4">&quot;solver must be one of {'liblinear', 'lbfgs', &quot;</span>
                <span class="s4">&quot;'newton-cg', 'sag'}, got '%s' instead&quot; </span><span class="s1">% solver</span>
            <span class="s1">)</span>

        <span class="s3">if </span><span class="s1">multi_class == </span><span class="s4">&quot;multinomial&quot;</span><span class="s1">:</span>
            <span class="s1">n_classes = max(</span><span class="s5">2</span><span class="s3">, </span><span class="s1">classes.size)</span>
            <span class="s3">if </span><span class="s1">solver </span><span class="s3">in </span><span class="s1">[</span><span class="s4">&quot;lbfgs&quot;</span><span class="s3">, </span><span class="s4">&quot;newton-cg&quot;</span><span class="s1">]:</span>
                <span class="s1">multi_w0 = np.reshape(w0</span><span class="s3">, </span><span class="s1">(n_classes</span><span class="s3">, </span><span class="s1">-</span><span class="s5">1</span><span class="s1">)</span><span class="s3">, </span><span class="s1">order=</span><span class="s4">&quot;F&quot;</span><span class="s1">)</span>
            <span class="s3">else</span><span class="s1">:</span>
                <span class="s1">multi_w0 = w0</span>
            <span class="s3">if </span><span class="s1">n_classes == </span><span class="s5">2</span><span class="s1">:</span>
                <span class="s1">multi_w0 = multi_w0[</span><span class="s5">1</span><span class="s1">][np.newaxis</span><span class="s3">, </span><span class="s1">:]</span>
            <span class="s1">coefs.append(multi_w0.copy())</span>
        <span class="s3">else</span><span class="s1">:</span>
            <span class="s1">coefs.append(w0.copy())</span>

        <span class="s1">n_iter[i] = n_iter_i</span>

    <span class="s3">return </span><span class="s1">np.array(coefs)</span><span class="s3">, </span><span class="s1">np.array(Cs)</span><span class="s3">, </span><span class="s1">n_iter</span>


<span class="s2"># helper function for LogisticCV</span>
<span class="s3">def </span><span class="s1">_log_reg_scoring_path(</span>
    <span class="s1">X</span><span class="s3">,</span>
    <span class="s1">y</span><span class="s3">,</span>
    <span class="s1">train</span><span class="s3">,</span>
    <span class="s1">test</span><span class="s3">,</span>
    <span class="s1">pos_class=</span><span class="s3">None,</span>
    <span class="s1">Cs=</span><span class="s5">10</span><span class="s3">,</span>
    <span class="s1">scoring=</span><span class="s3">None,</span>
    <span class="s1">fit_intercept=</span><span class="s3">False,</span>
    <span class="s1">max_iter=</span><span class="s5">100</span><span class="s3">,</span>
    <span class="s1">tol=</span><span class="s5">1e-4</span><span class="s3">,</span>
    <span class="s1">class_weight=</span><span class="s3">None,</span>
    <span class="s1">verbose=</span><span class="s5">0</span><span class="s3">,</span>
    <span class="s1">solver=</span><span class="s4">&quot;lbfgs&quot;</span><span class="s3">,</span>
    <span class="s1">penalty=</span><span class="s4">&quot;l2&quot;</span><span class="s3">,</span>
    <span class="s1">dual=</span><span class="s3">False,</span>
    <span class="s1">intercept_scaling=</span><span class="s5">1.0</span><span class="s3">,</span>
    <span class="s1">multi_class=</span><span class="s4">&quot;auto&quot;</span><span class="s3">,</span>
    <span class="s1">random_state=</span><span class="s3">None,</span>
    <span class="s1">max_squared_sum=</span><span class="s3">None,</span>
    <span class="s1">sample_weight=</span><span class="s3">None,</span>
    <span class="s1">l1_ratio=</span><span class="s3">None,</span>
<span class="s1">):</span>
    <span class="s0">&quot;&quot;&quot;Computes scores across logistic_regression_path 
 
    Parameters 
    ---------- 
    X : {array-like, sparse matrix} of shape (n_samples, n_features) 
        Training data. 
 
    y : array-like of shape (n_samples,) or (n_samples, n_targets) 
        Target labels. 
 
    train : list of indices 
        The indices of the train set. 
 
    test : list of indices 
        The indices of the test set. 
 
    pos_class : int, default=None 
        The class with respect to which we perform a one-vs-all fit. 
        If None, then it is assumed that the given problem is binary. 
 
    Cs : int or list of floats, default=10 
        Each of the values in Cs describes the inverse of 
        regularization strength. If Cs is as an int, then a grid of Cs 
        values are chosen in a logarithmic scale between 1e-4 and 1e4. 
        If not provided, then a fixed set of values for Cs are used. 
 
    scoring : callable, default=None 
        A string (see model evaluation documentation) or 
        a scorer callable object / function with signature 
        ``scorer(estimator, X, y)``. For a list of scoring functions 
        that can be used, look at :mod:`sklearn.metrics`. The 
        default scoring option used is accuracy_score. 
 
    fit_intercept : bool, default=False 
        If False, then the bias term is set to zero. Else the last 
        term of each coef_ gives us the intercept. 
 
    max_iter : int, default=100 
        Maximum number of iterations for the solver. 
 
    tol : float, default=1e-4 
        Tolerance for stopping criteria. 
 
    class_weight : dict or 'balanced', default=None 
        Weights associated with classes in the form ``{class_label: weight}``. 
        If not given, all classes are supposed to have weight one. 
 
        The &quot;balanced&quot; mode uses the values of y to automatically adjust 
        weights inversely proportional to class frequencies in the input data 
        as ``n_samples / (n_classes * np.bincount(y))`` 
 
        Note that these weights will be multiplied with sample_weight (passed 
        through the fit method) if sample_weight is specified. 
 
    verbose : int, default=0 
        For the liblinear and lbfgs solvers set verbose to any positive 
        number for verbosity. 
 
    solver : {'lbfgs', 'liblinear', 'newton-cg', 'newton-cholesky', 'sag', 'saga'}, \ 
            default='lbfgs' 
        Decides which solver to use. 
 
    penalty : {'l1', 'l2', 'elasticnet'}, default='l2' 
        Used to specify the norm used in the penalization. The 'newton-cg', 
        'sag' and 'lbfgs' solvers support only l2 penalties. 'elasticnet' is 
        only supported by the 'saga' solver. 
 
    dual : bool, default=False 
        Dual or primal formulation. Dual formulation is only implemented for 
        l2 penalty with liblinear solver. Prefer dual=False when 
        n_samples &gt; n_features. 
 
    intercept_scaling : float, default=1. 
        Useful only when the solver 'liblinear' is used 
        and self.fit_intercept is set to True. In this case, x becomes 
        [x, self.intercept_scaling], 
        i.e. a &quot;synthetic&quot; feature with constant value equals to 
        intercept_scaling is appended to the instance vector. 
        The intercept becomes intercept_scaling * synthetic feature weight 
        Note! the synthetic feature weight is subject to l1/l2 regularization 
        as all other features. 
        To lessen the effect of regularization on synthetic feature weight 
        (and therefore on the intercept) intercept_scaling has to be increased. 
 
    multi_class : {'auto', 'ovr', 'multinomial'}, default='auto' 
        If the option chosen is 'ovr', then a binary problem is fit for each 
        label. For 'multinomial' the loss minimised is the multinomial loss fit 
        across the entire probability distribution, *even when the data is 
        binary*. 'multinomial' is unavailable when solver='liblinear'. 
 
    random_state : int, RandomState instance, default=None 
        Used when ``solver`` == 'sag', 'saga' or 'liblinear' to shuffle the 
        data. See :term:`Glossary &lt;random_state&gt;` for details. 
 
    max_squared_sum : float, default=None 
        Maximum squared sum of X over samples. Used only in SAG solver. 
        If None, it will be computed, going through all the samples. 
        The value should be precomputed to speed up cross validation. 
 
    sample_weight : array-like of shape(n_samples,), default=None 
        Array of weights that are assigned to individual samples. 
        If not provided, then each sample is given unit weight. 
 
    l1_ratio : float, default=None 
        The Elastic-Net mixing parameter, with ``0 &lt;= l1_ratio &lt;= 1``. Only 
        used if ``penalty='elasticnet'``. Setting ``l1_ratio=0`` is equivalent 
        to using ``penalty='l2'``, while setting ``l1_ratio=1`` is equivalent 
        to using ``penalty='l1'``. For ``0 &lt; l1_ratio &lt;1``, the penalty is a 
        combination of L1 and L2. 
 
    Returns 
    ------- 
    coefs : ndarray of shape (n_cs, n_features) or (n_cs, n_features + 1) 
        List of coefficients for the Logistic Regression model. If 
        fit_intercept is set to True then the second dimension will be 
        n_features + 1, where the last item represents the intercept. 
 
    Cs : ndarray 
        Grid of Cs used for cross-validation. 
 
    scores : ndarray of shape (n_cs,) 
        Scores obtained for each Cs. 
 
    n_iter : ndarray of shape(n_cs,) 
        Actual number of iteration for each Cs. 
    &quot;&quot;&quot;</span>
    <span class="s1">X_train = X[train]</span>
    <span class="s1">X_test = X[test]</span>
    <span class="s1">y_train = y[train]</span>
    <span class="s1">y_test = y[test]</span>

    <span class="s3">if </span><span class="s1">sample_weight </span><span class="s3">is not None</span><span class="s1">:</span>
        <span class="s1">sample_weight = _check_sample_weight(sample_weight</span><span class="s3">, </span><span class="s1">X)</span>
        <span class="s1">sample_weight = sample_weight[train]</span>

    <span class="s1">coefs</span><span class="s3">, </span><span class="s1">Cs</span><span class="s3">, </span><span class="s1">n_iter = _logistic_regression_path(</span>
        <span class="s1">X_train</span><span class="s3">,</span>
        <span class="s1">y_train</span><span class="s3">,</span>
        <span class="s1">Cs=Cs</span><span class="s3">,</span>
        <span class="s1">l1_ratio=l1_ratio</span><span class="s3">,</span>
        <span class="s1">fit_intercept=fit_intercept</span><span class="s3">,</span>
        <span class="s1">solver=solver</span><span class="s3">,</span>
        <span class="s1">max_iter=max_iter</span><span class="s3">,</span>
        <span class="s1">class_weight=class_weight</span><span class="s3">,</span>
        <span class="s1">pos_class=pos_class</span><span class="s3">,</span>
        <span class="s1">multi_class=multi_class</span><span class="s3">,</span>
        <span class="s1">tol=tol</span><span class="s3">,</span>
        <span class="s1">verbose=verbose</span><span class="s3">,</span>
        <span class="s1">dual=dual</span><span class="s3">,</span>
        <span class="s1">penalty=penalty</span><span class="s3">,</span>
        <span class="s1">intercept_scaling=intercept_scaling</span><span class="s3">,</span>
        <span class="s1">random_state=random_state</span><span class="s3">,</span>
        <span class="s1">check_input=</span><span class="s3">False,</span>
        <span class="s1">max_squared_sum=max_squared_sum</span><span class="s3">,</span>
        <span class="s1">sample_weight=sample_weight</span><span class="s3">,</span>
    <span class="s1">)</span>

    <span class="s1">log_reg = LogisticRegression(solver=solver</span><span class="s3">, </span><span class="s1">multi_class=multi_class)</span>

    <span class="s2"># The score method of Logistic Regression has a classes_ attribute.</span>
    <span class="s3">if </span><span class="s1">multi_class == </span><span class="s4">&quot;ovr&quot;</span><span class="s1">:</span>
        <span class="s1">log_reg.classes_ = np.array([-</span><span class="s5">1</span><span class="s3">, </span><span class="s5">1</span><span class="s1">])</span>
    <span class="s3">elif </span><span class="s1">multi_class == </span><span class="s4">&quot;multinomial&quot;</span><span class="s1">:</span>
        <span class="s1">log_reg.classes_ = np.unique(y_train)</span>
    <span class="s3">else</span><span class="s1">:</span>
        <span class="s3">raise </span><span class="s1">ValueError(</span>
            <span class="s4">&quot;multi_class should be either multinomial or ovr, got %d&quot; </span><span class="s1">% multi_class</span>
        <span class="s1">)</span>

    <span class="s3">if </span><span class="s1">pos_class </span><span class="s3">is not None</span><span class="s1">:</span>
        <span class="s1">mask = y_test == pos_class</span>
        <span class="s1">y_test = np.ones(y_test.shape</span><span class="s3">, </span><span class="s1">dtype=np.float64)</span>
        <span class="s1">y_test[~mask] = -</span><span class="s5">1.0</span>

    <span class="s1">scores = list()</span>

    <span class="s1">scoring = get_scorer(scoring)</span>
    <span class="s3">for </span><span class="s1">w </span><span class="s3">in </span><span class="s1">coefs:</span>
        <span class="s3">if </span><span class="s1">multi_class == </span><span class="s4">&quot;ovr&quot;</span><span class="s1">:</span>
            <span class="s1">w = w[np.newaxis</span><span class="s3">, </span><span class="s1">:]</span>
        <span class="s3">if </span><span class="s1">fit_intercept:</span>
            <span class="s1">log_reg.coef_ = w[:</span><span class="s3">, </span><span class="s1">:-</span><span class="s5">1</span><span class="s1">]</span>
            <span class="s1">log_reg.intercept_ = w[:</span><span class="s3">, </span><span class="s1">-</span><span class="s5">1</span><span class="s1">]</span>
        <span class="s3">else</span><span class="s1">:</span>
            <span class="s1">log_reg.coef_ = w</span>
            <span class="s1">log_reg.intercept_ = </span><span class="s5">0.0</span>

        <span class="s3">if </span><span class="s1">scoring </span><span class="s3">is None</span><span class="s1">:</span>
            <span class="s1">scores.append(log_reg.score(X_test</span><span class="s3">, </span><span class="s1">y_test))</span>
        <span class="s3">else</span><span class="s1">:</span>
            <span class="s1">scores.append(scoring(log_reg</span><span class="s3">, </span><span class="s1">X_test</span><span class="s3">, </span><span class="s1">y_test))</span>

    <span class="s3">return </span><span class="s1">coefs</span><span class="s3">, </span><span class="s1">Cs</span><span class="s3">, </span><span class="s1">np.array(scores)</span><span class="s3">, </span><span class="s1">n_iter</span>


<span class="s3">class </span><span class="s1">LogisticRegression(LinearClassifierMixin</span><span class="s3">, </span><span class="s1">SparseCoefMixin</span><span class="s3">, </span><span class="s1">BaseEstimator):</span>
    <span class="s0">&quot;&quot;&quot; 
    Logistic Regression (aka logit, MaxEnt) classifier. 
 
    In the multiclass case, the training algorithm uses the one-vs-rest (OvR) 
    scheme if the 'multi_class' option is set to 'ovr', and uses the 
    cross-entropy loss if the 'multi_class' option is set to 'multinomial'. 
    (Currently the 'multinomial' option is supported only by the 'lbfgs', 
    'sag', 'saga' and 'newton-cg' solvers.) 
 
    This class implements regularized logistic regression using the 
    'liblinear' library, 'newton-cg', 'sag', 'saga' and 'lbfgs' solvers. **Note 
    that regularization is applied by default**. It can handle both dense 
    and sparse input. Use C-ordered arrays or CSR matrices containing 64-bit 
    floats for optimal performance; any other input format will be converted 
    (and copied). 
 
    The 'newton-cg', 'sag', and 'lbfgs' solvers support only L2 regularization 
    with primal formulation, or no regularization. The 'liblinear' solver 
    supports both L1 and L2 regularization, with a dual formulation only for 
    the L2 penalty. The Elastic-Net regularization is only supported by the 
    'saga' solver. 
 
    Read more in the :ref:`User Guide &lt;logistic_regression&gt;`. 
 
    Parameters 
    ---------- 
    penalty : {'l1', 'l2', 'elasticnet', None}, default='l2' 
        Specify the norm of the penalty: 
 
        - `None`: no penalty is added; 
        - `'l2'`: add a L2 penalty term and it is the default choice; 
        - `'l1'`: add a L1 penalty term; 
        - `'elasticnet'`: both L1 and L2 penalty terms are added. 
 
        .. warning:: 
           Some penalties may not work with some solvers. See the parameter 
           `solver` below, to know the compatibility between the penalty and 
           solver. 
 
        .. versionadded:: 0.19 
           l1 penalty with SAGA solver (allowing 'multinomial' + L1) 
 
        .. deprecated:: 1.2 
           The 'none' option was deprecated in version 1.2, and will be removed 
           in 1.4. Use `None` instead. 
 
    dual : bool, default=False 
        Dual (constrained) or primal (regularized, see also 
        :ref:`this equation &lt;regularized-logistic-loss&gt;`) formulation. Dual formulation 
        is only implemented for l2 penalty with liblinear solver. Prefer dual=False when 
        n_samples &gt; n_features. 
 
    tol : float, default=1e-4 
        Tolerance for stopping criteria. 
 
    C : float, default=1.0 
        Inverse of regularization strength; must be a positive float. 
        Like in support vector machines, smaller values specify stronger 
        regularization. 
 
    fit_intercept : bool, default=True 
        Specifies if a constant (a.k.a. bias or intercept) should be 
        added to the decision function. 
 
    intercept_scaling : float, default=1 
        Useful only when the solver 'liblinear' is used 
        and self.fit_intercept is set to True. In this case, x becomes 
        [x, self.intercept_scaling], 
        i.e. a &quot;synthetic&quot; feature with constant value equal to 
        intercept_scaling is appended to the instance vector. 
        The intercept becomes ``intercept_scaling * synthetic_feature_weight``. 
 
        Note! the synthetic feature weight is subject to l1/l2 regularization 
        as all other features. 
        To lessen the effect of regularization on synthetic feature weight 
        (and therefore on the intercept) intercept_scaling has to be increased. 
 
    class_weight : dict or 'balanced', default=None 
        Weights associated with classes in the form ``{class_label: weight}``. 
        If not given, all classes are supposed to have weight one. 
 
        The &quot;balanced&quot; mode uses the values of y to automatically adjust 
        weights inversely proportional to class frequencies in the input data 
        as ``n_samples / (n_classes * np.bincount(y))``. 
 
        Note that these weights will be multiplied with sample_weight (passed 
        through the fit method) if sample_weight is specified. 
 
        .. versionadded:: 0.17 
           *class_weight='balanced'* 
 
    random_state : int, RandomState instance, default=None 
        Used when ``solver`` == 'sag', 'saga' or 'liblinear' to shuffle the 
        data. See :term:`Glossary &lt;random_state&gt;` for details. 
 
    solver : {'lbfgs', 'liblinear', 'newton-cg', 'newton-cholesky', 'sag', 'saga'}, \ 
            default='lbfgs' 
 
        Algorithm to use in the optimization problem. Default is 'lbfgs'. 
        To choose a solver, you might want to consider the following aspects: 
 
            - For small datasets, 'liblinear' is a good choice, whereas 'sag' 
              and 'saga' are faster for large ones; 
            - For multiclass problems, only 'newton-cg', 'sag', 'saga' and 
              'lbfgs' handle multinomial loss; 
            - 'liblinear' is limited to one-versus-rest schemes. 
            - 'newton-cholesky' is a good choice for `n_samples` &gt;&gt; `n_features`, 
              especially with one-hot encoded categorical features with rare 
              categories. Note that it is limited to binary classification and the 
              one-versus-rest reduction for multiclass classification. Be aware that 
              the memory usage of this solver has a quadratic dependency on 
              `n_features` because it explicitly computes the Hessian matrix. 
 
        .. warning:: 
           The choice of the algorithm depends on the penalty chosen. 
           Supported penalties by solver: 
 
           - 'lbfgs'           -   ['l2', None] 
           - 'liblinear'       -   ['l1', 'l2'] 
           - 'newton-cg'       -   ['l2', None] 
           - 'newton-cholesky' -   ['l2', None] 
           - 'sag'             -   ['l2', None] 
           - 'saga'            -   ['elasticnet', 'l1', 'l2', None] 
 
        .. note:: 
           'sag' and 'saga' fast convergence is only guaranteed on features 
           with approximately the same scale. You can preprocess the data with 
           a scaler from :mod:`sklearn.preprocessing`. 
 
        .. seealso:: 
           Refer to the User Guide for more information regarding 
           :class:`LogisticRegression` and more specifically the 
           :ref:`Table &lt;Logistic_regression&gt;` 
           summarizing solver/penalty supports. 
 
        .. versionadded:: 0.17 
           Stochastic Average Gradient descent solver. 
        .. versionadded:: 0.19 
           SAGA solver. 
        .. versionchanged:: 0.22 
            The default solver changed from 'liblinear' to 'lbfgs' in 0.22. 
        .. versionadded:: 1.2 
           newton-cholesky solver. 
 
    max_iter : int, default=100 
        Maximum number of iterations taken for the solvers to converge. 
 
    multi_class : {'auto', 'ovr', 'multinomial'}, default='auto' 
        If the option chosen is 'ovr', then a binary problem is fit for each 
        label. For 'multinomial' the loss minimised is the multinomial loss fit 
        across the entire probability distribution, *even when the data is 
        binary*. 'multinomial' is unavailable when solver='liblinear'. 
        'auto' selects 'ovr' if the data is binary, or if solver='liblinear', 
        and otherwise selects 'multinomial'. 
 
        .. versionadded:: 0.18 
           Stochastic Average Gradient descent solver for 'multinomial' case. 
        .. versionchanged:: 0.22 
            Default changed from 'ovr' to 'auto' in 0.22. 
 
    verbose : int, default=0 
        For the liblinear and lbfgs solvers set verbose to any positive 
        number for verbosity. 
 
    warm_start : bool, default=False 
        When set to True, reuse the solution of the previous call to fit as 
        initialization, otherwise, just erase the previous solution. 
        Useless for liblinear solver. See :term:`the Glossary &lt;warm_start&gt;`. 
 
        .. versionadded:: 0.17 
           *warm_start* to support *lbfgs*, *newton-cg*, *sag*, *saga* solvers. 
 
    n_jobs : int, default=None 
        Number of CPU cores used when parallelizing over classes if 
        multi_class='ovr'&quot;. This parameter is ignored when the ``solver`` is 
        set to 'liblinear' regardless of whether 'multi_class' is specified or 
        not. ``None`` means 1 unless in a :obj:`joblib.parallel_backend` 
        context. ``-1`` means using all processors. 
        See :term:`Glossary &lt;n_jobs&gt;` for more details. 
 
    l1_ratio : float, default=None 
        The Elastic-Net mixing parameter, with ``0 &lt;= l1_ratio &lt;= 1``. Only 
        used if ``penalty='elasticnet'``. Setting ``l1_ratio=0`` is equivalent 
        to using ``penalty='l2'``, while setting ``l1_ratio=1`` is equivalent 
        to using ``penalty='l1'``. For ``0 &lt; l1_ratio &lt;1``, the penalty is a 
        combination of L1 and L2. 
 
    Attributes 
    ---------- 
 
    classes_ : ndarray of shape (n_classes, ) 
        A list of class labels known to the classifier. 
 
    coef_ : ndarray of shape (1, n_features) or (n_classes, n_features) 
        Coefficient of the features in the decision function. 
 
        `coef_` is of shape (1, n_features) when the given problem is binary. 
        In particular, when `multi_class='multinomial'`, `coef_` corresponds 
        to outcome 1 (True) and `-coef_` corresponds to outcome 0 (False). 
 
    intercept_ : ndarray of shape (1,) or (n_classes,) 
        Intercept (a.k.a. bias) added to the decision function. 
 
        If `fit_intercept` is set to False, the intercept is set to zero. 
        `intercept_` is of shape (1,) when the given problem is binary. 
        In particular, when `multi_class='multinomial'`, `intercept_` 
        corresponds to outcome 1 (True) and `-intercept_` corresponds to 
        outcome 0 (False). 
 
    n_features_in_ : int 
        Number of features seen during :term:`fit`. 
 
        .. versionadded:: 0.24 
 
    feature_names_in_ : ndarray of shape (`n_features_in_`,) 
        Names of features seen during :term:`fit`. Defined only when `X` 
        has feature names that are all strings. 
 
        .. versionadded:: 1.0 
 
    n_iter_ : ndarray of shape (n_classes,) or (1, ) 
        Actual number of iterations for all classes. If binary or multinomial, 
        it returns only 1 element. For liblinear solver, only the maximum 
        number of iteration across all classes is given. 
 
        .. versionchanged:: 0.20 
 
            In SciPy &lt;= 1.0.0 the number of lbfgs iterations may exceed 
            ``max_iter``. ``n_iter_`` will now report at most ``max_iter``. 
 
    See Also 
    -------- 
    SGDClassifier : Incrementally trained logistic regression (when given 
        the parameter ``loss=&quot;log_loss&quot;``). 
    LogisticRegressionCV : Logistic regression with built-in cross validation. 
 
    Notes 
    ----- 
    The underlying C implementation uses a random number generator to 
    select features when fitting the model. It is thus not uncommon, 
    to have slightly different results for the same input data. If 
    that happens, try with a smaller tol parameter. 
 
    Predict output may not match that of standalone liblinear in certain 
    cases. See :ref:`differences from liblinear &lt;liblinear_differences&gt;` 
    in the narrative documentation. 
 
    References 
    ---------- 
 
    L-BFGS-B -- Software for Large-scale Bound-constrained Optimization 
        Ciyou Zhu, Richard Byrd, Jorge Nocedal and Jose Luis Morales. 
        http://users.iems.northwestern.edu/~nocedal/lbfgsb.html 
 
    LIBLINEAR -- A Library for Large Linear Classification 
        https://www.csie.ntu.edu.tw/~cjlin/liblinear/ 
 
    SAG -- Mark Schmidt, Nicolas Le Roux, and Francis Bach 
        Minimizing Finite Sums with the Stochastic Average Gradient 
        https://hal.inria.fr/hal-00860051/document 
 
    SAGA -- Defazio, A., Bach F. &amp; Lacoste-Julien S. (2014). 
            :arxiv:`&quot;SAGA: A Fast Incremental Gradient Method With Support 
            for Non-Strongly Convex Composite Objectives&quot; &lt;1407.0202&gt;` 
 
    Hsiang-Fu Yu, Fang-Lan Huang, Chih-Jen Lin (2011). Dual coordinate descent 
        methods for logistic regression and maximum entropy models. 
        Machine Learning 85(1-2):41-75. 
        https://www.csie.ntu.edu.tw/~cjlin/papers/maxent_dual.pdf 
 
    Examples 
    -------- 
    &gt;&gt;&gt; from sklearn.datasets import load_iris 
    &gt;&gt;&gt; from sklearn.linear_model import LogisticRegression 
    &gt;&gt;&gt; X, y = load_iris(return_X_y=True) 
    &gt;&gt;&gt; clf = LogisticRegression(random_state=0).fit(X, y) 
    &gt;&gt;&gt; clf.predict(X[:2, :]) 
    array([0, 0]) 
    &gt;&gt;&gt; clf.predict_proba(X[:2, :]) 
    array([[9.8...e-01, 1.8...e-02, 1.4...e-08], 
           [9.7...e-01, 2.8...e-02, ...e-08]]) 
    &gt;&gt;&gt; clf.score(X, y) 
    0.97... 
    &quot;&quot;&quot;</span>

    <span class="s1">_parameter_constraints: dict = {</span>
        <span class="s2"># TODO(1.4): Remove &quot;none&quot; option</span>
        <span class="s4">&quot;penalty&quot;</span><span class="s1">: [</span>
            <span class="s1">StrOptions({</span><span class="s4">&quot;l1&quot;</span><span class="s3">, </span><span class="s4">&quot;l2&quot;</span><span class="s3">, </span><span class="s4">&quot;elasticnet&quot;</span><span class="s3">, </span><span class="s4">&quot;none&quot;</span><span class="s1">}</span><span class="s3">, </span><span class="s1">deprecated={</span><span class="s4">&quot;none&quot;</span><span class="s1">})</span><span class="s3">,</span>
            <span class="s3">None,</span>
        <span class="s1">]</span><span class="s3">,</span>
        <span class="s4">&quot;dual&quot;</span><span class="s1">: [</span><span class="s4">&quot;boolean&quot;</span><span class="s1">]</span><span class="s3">,</span>
        <span class="s4">&quot;tol&quot;</span><span class="s1">: [Interval(Real</span><span class="s3">, </span><span class="s5">0</span><span class="s3">, None, </span><span class="s1">closed=</span><span class="s4">&quot;left&quot;</span><span class="s1">)]</span><span class="s3">,</span>
        <span class="s4">&quot;C&quot;</span><span class="s1">: [Interval(Real</span><span class="s3">, </span><span class="s5">0</span><span class="s3">, None, </span><span class="s1">closed=</span><span class="s4">&quot;right&quot;</span><span class="s1">)]</span><span class="s3">,</span>
        <span class="s4">&quot;fit_intercept&quot;</span><span class="s1">: [</span><span class="s4">&quot;boolean&quot;</span><span class="s1">]</span><span class="s3">,</span>
        <span class="s4">&quot;intercept_scaling&quot;</span><span class="s1">: [Interval(Real</span><span class="s3">, </span><span class="s5">0</span><span class="s3">, None, </span><span class="s1">closed=</span><span class="s4">&quot;neither&quot;</span><span class="s1">)]</span><span class="s3">,</span>
        <span class="s4">&quot;class_weight&quot;</span><span class="s1">: [dict</span><span class="s3">, </span><span class="s1">StrOptions({</span><span class="s4">&quot;balanced&quot;</span><span class="s1">})</span><span class="s3">, None</span><span class="s1">]</span><span class="s3">,</span>
        <span class="s4">&quot;random_state&quot;</span><span class="s1">: [</span><span class="s4">&quot;random_state&quot;</span><span class="s1">]</span><span class="s3">,</span>
        <span class="s4">&quot;solver&quot;</span><span class="s1">: [</span>
            <span class="s1">StrOptions(</span>
                <span class="s1">{</span><span class="s4">&quot;lbfgs&quot;</span><span class="s3">, </span><span class="s4">&quot;liblinear&quot;</span><span class="s3">, </span><span class="s4">&quot;newton-cg&quot;</span><span class="s3">, </span><span class="s4">&quot;newton-cholesky&quot;</span><span class="s3">, </span><span class="s4">&quot;sag&quot;</span><span class="s3">, </span><span class="s4">&quot;saga&quot;</span><span class="s1">}</span>
            <span class="s1">)</span>
        <span class="s1">]</span><span class="s3">,</span>
        <span class="s4">&quot;max_iter&quot;</span><span class="s1">: [Interval(Integral</span><span class="s3">, </span><span class="s5">0</span><span class="s3">, None, </span><span class="s1">closed=</span><span class="s4">&quot;left&quot;</span><span class="s1">)]</span><span class="s3">,</span>
        <span class="s4">&quot;multi_class&quot;</span><span class="s1">: [StrOptions({</span><span class="s4">&quot;auto&quot;</span><span class="s3">, </span><span class="s4">&quot;ovr&quot;</span><span class="s3">, </span><span class="s4">&quot;multinomial&quot;</span><span class="s1">})]</span><span class="s3">,</span>
        <span class="s4">&quot;verbose&quot;</span><span class="s1">: [</span><span class="s4">&quot;verbose&quot;</span><span class="s1">]</span><span class="s3">,</span>
        <span class="s4">&quot;warm_start&quot;</span><span class="s1">: [</span><span class="s4">&quot;boolean&quot;</span><span class="s1">]</span><span class="s3">,</span>
        <span class="s4">&quot;n_jobs&quot;</span><span class="s1">: [</span><span class="s3">None, </span><span class="s1">Integral]</span><span class="s3">,</span>
        <span class="s4">&quot;l1_ratio&quot;</span><span class="s1">: [Interval(Real</span><span class="s3">, </span><span class="s5">0</span><span class="s3">, </span><span class="s5">1</span><span class="s3">, </span><span class="s1">closed=</span><span class="s4">&quot;both&quot;</span><span class="s1">)</span><span class="s3">, None</span><span class="s1">]</span><span class="s3">,</span>
    <span class="s1">}</span>

    <span class="s3">def </span><span class="s1">__init__(</span>
        <span class="s1">self</span><span class="s3">,</span>
        <span class="s1">penalty=</span><span class="s4">&quot;l2&quot;</span><span class="s3">,</span>
        <span class="s1">*</span><span class="s3">,</span>
        <span class="s1">dual=</span><span class="s3">False,</span>
        <span class="s1">tol=</span><span class="s5">1e-4</span><span class="s3">,</span>
        <span class="s1">C=</span><span class="s5">1.0</span><span class="s3">,</span>
        <span class="s1">fit_intercept=</span><span class="s3">True,</span>
        <span class="s1">intercept_scaling=</span><span class="s5">1</span><span class="s3">,</span>
        <span class="s1">class_weight=</span><span class="s3">None,</span>
        <span class="s1">random_state=</span><span class="s3">None,</span>
        <span class="s1">solver=</span><span class="s4">&quot;lbfgs&quot;</span><span class="s3">,</span>
        <span class="s1">max_iter=</span><span class="s5">100</span><span class="s3">,</span>
        <span class="s1">multi_class=</span><span class="s4">&quot;auto&quot;</span><span class="s3">,</span>
        <span class="s1">verbose=</span><span class="s5">0</span><span class="s3">,</span>
        <span class="s1">warm_start=</span><span class="s3">False,</span>
        <span class="s1">n_jobs=</span><span class="s3">None,</span>
        <span class="s1">l1_ratio=</span><span class="s3">None,</span>
    <span class="s1">):</span>
        <span class="s1">self.penalty = penalty</span>
        <span class="s1">self.dual = dual</span>
        <span class="s1">self.tol = tol</span>
        <span class="s1">self.C = C</span>
        <span class="s1">self.fit_intercept = fit_intercept</span>
        <span class="s1">self.intercept_scaling = intercept_scaling</span>
        <span class="s1">self.class_weight = class_weight</span>
        <span class="s1">self.random_state = random_state</span>
        <span class="s1">self.solver = solver</span>
        <span class="s1">self.max_iter = max_iter</span>
        <span class="s1">self.multi_class = multi_class</span>
        <span class="s1">self.verbose = verbose</span>
        <span class="s1">self.warm_start = warm_start</span>
        <span class="s1">self.n_jobs = n_jobs</span>
        <span class="s1">self.l1_ratio = l1_ratio</span>

    <span class="s1">@_fit_context(prefer_skip_nested_validation=</span><span class="s3">True</span><span class="s1">)</span>
    <span class="s3">def </span><span class="s1">fit(self</span><span class="s3">, </span><span class="s1">X</span><span class="s3">, </span><span class="s1">y</span><span class="s3">, </span><span class="s1">sample_weight=</span><span class="s3">None</span><span class="s1">):</span>
        <span class="s0">&quot;&quot;&quot; 
        Fit the model according to the given training data. 
 
        Parameters 
        ---------- 
        X : {array-like, sparse matrix} of shape (n_samples, n_features) 
            Training vector, where `n_samples` is the number of samples and 
            `n_features` is the number of features. 
 
        y : array-like of shape (n_samples,) 
            Target vector relative to X. 
 
        sample_weight : array-like of shape (n_samples,) default=None 
            Array of weights that are assigned to individual samples. 
            If not provided, then each sample is given unit weight. 
 
            .. versionadded:: 0.17 
               *sample_weight* support to LogisticRegression. 
 
        Returns 
        ------- 
        self 
            Fitted estimator. 
 
        Notes 
        ----- 
        The SAGA solver supports both float64 and float32 bit arrays. 
        &quot;&quot;&quot;</span>
        <span class="s1">solver = _check_solver(self.solver</span><span class="s3">, </span><span class="s1">self.penalty</span><span class="s3">, </span><span class="s1">self.dual)</span>

        <span class="s3">if </span><span class="s1">self.penalty != </span><span class="s4">&quot;elasticnet&quot; </span><span class="s3">and </span><span class="s1">self.l1_ratio </span><span class="s3">is not None</span><span class="s1">:</span>
            <span class="s1">warnings.warn(</span>
                <span class="s4">&quot;l1_ratio parameter is only used when penalty is &quot;</span>
                <span class="s4">&quot;'elasticnet'. Got &quot;</span>
                <span class="s4">&quot;(penalty={})&quot;</span><span class="s1">.format(self.penalty)</span>
            <span class="s1">)</span>

        <span class="s3">if </span><span class="s1">self.penalty == </span><span class="s4">&quot;elasticnet&quot; </span><span class="s3">and </span><span class="s1">self.l1_ratio </span><span class="s3">is None</span><span class="s1">:</span>
            <span class="s3">raise </span><span class="s1">ValueError(</span><span class="s4">&quot;l1_ratio must be specified when penalty is elasticnet.&quot;</span><span class="s1">)</span>

        <span class="s2"># TODO(1.4): Remove &quot;none&quot; option</span>
        <span class="s3">if </span><span class="s1">self.penalty == </span><span class="s4">&quot;none&quot;</span><span class="s1">:</span>
            <span class="s1">warnings.warn(</span>
                <span class="s1">(</span>
                    <span class="s4">&quot;`penalty='none'`has been deprecated in 1.2 and will be removed in&quot;</span>
                    <span class="s4">&quot; 1.4. To keep the past behaviour, set `penalty=None`.&quot;</span>
                <span class="s1">)</span><span class="s3">,</span>
                <span class="s1">FutureWarning</span><span class="s3">,</span>
            <span class="s1">)</span>

        <span class="s3">if </span><span class="s1">self.penalty </span><span class="s3">is None or </span><span class="s1">self.penalty == </span><span class="s4">&quot;none&quot;</span><span class="s1">:</span>
            <span class="s3">if </span><span class="s1">self.C != </span><span class="s5">1.0</span><span class="s1">:  </span><span class="s2"># default values</span>
                <span class="s1">warnings.warn(</span>
                    <span class="s4">&quot;Setting penalty=None will ignore the C and l1_ratio parameters&quot;</span>
                <span class="s1">)</span>
                <span class="s2"># Note that check for l1_ratio is done right above</span>
            <span class="s1">C_ = np.inf</span>
            <span class="s1">penalty = </span><span class="s4">&quot;l2&quot;</span>
        <span class="s3">else</span><span class="s1">:</span>
            <span class="s1">C_ = self.C</span>
            <span class="s1">penalty = self.penalty</span>

        <span class="s3">if </span><span class="s1">solver == </span><span class="s4">&quot;lbfgs&quot;</span><span class="s1">:</span>
            <span class="s1">_dtype = np.float64</span>
        <span class="s3">else</span><span class="s1">:</span>
            <span class="s1">_dtype = [np.float64</span><span class="s3">, </span><span class="s1">np.float32]</span>

        <span class="s1">X</span><span class="s3">, </span><span class="s1">y = self._validate_data(</span>
            <span class="s1">X</span><span class="s3">,</span>
            <span class="s1">y</span><span class="s3">,</span>
            <span class="s1">accept_sparse=</span><span class="s4">&quot;csr&quot;</span><span class="s3">,</span>
            <span class="s1">dtype=_dtype</span><span class="s3">,</span>
            <span class="s1">order=</span><span class="s4">&quot;C&quot;</span><span class="s3">,</span>
            <span class="s1">accept_large_sparse=solver </span><span class="s3">not in </span><span class="s1">[</span><span class="s4">&quot;liblinear&quot;</span><span class="s3">, </span><span class="s4">&quot;sag&quot;</span><span class="s3">, </span><span class="s4">&quot;saga&quot;</span><span class="s1">]</span><span class="s3">,</span>
        <span class="s1">)</span>
        <span class="s1">check_classification_targets(y)</span>
        <span class="s1">self.classes_ = np.unique(y)</span>

        <span class="s1">multi_class = _check_multi_class(self.multi_class</span><span class="s3">, </span><span class="s1">solver</span><span class="s3">, </span><span class="s1">len(self.classes_))</span>

        <span class="s3">if </span><span class="s1">solver == </span><span class="s4">&quot;liblinear&quot;</span><span class="s1">:</span>
            <span class="s3">if </span><span class="s1">effective_n_jobs(self.n_jobs) != </span><span class="s5">1</span><span class="s1">:</span>
                <span class="s1">warnings.warn(</span>
                    <span class="s4">&quot;'n_jobs' &gt; 1 does not have any effect when&quot;</span>
                    <span class="s4">&quot; 'solver' is set to 'liblinear'. Got 'n_jobs'&quot;</span>
                    <span class="s4">&quot; = {}.&quot;</span><span class="s1">.format(effective_n_jobs(self.n_jobs))</span>
                <span class="s1">)</span>
            <span class="s1">self.coef_</span><span class="s3">, </span><span class="s1">self.intercept_</span><span class="s3">, </span><span class="s1">self.n_iter_ = _fit_liblinear(</span>
                <span class="s1">X</span><span class="s3">,</span>
                <span class="s1">y</span><span class="s3">,</span>
                <span class="s1">self.C</span><span class="s3">,</span>
                <span class="s1">self.fit_intercept</span><span class="s3">,</span>
                <span class="s1">self.intercept_scaling</span><span class="s3">,</span>
                <span class="s1">self.class_weight</span><span class="s3">,</span>
                <span class="s1">self.penalty</span><span class="s3">,</span>
                <span class="s1">self.dual</span><span class="s3">,</span>
                <span class="s1">self.verbose</span><span class="s3">,</span>
                <span class="s1">self.max_iter</span><span class="s3">,</span>
                <span class="s1">self.tol</span><span class="s3">,</span>
                <span class="s1">self.random_state</span><span class="s3">,</span>
                <span class="s1">sample_weight=sample_weight</span><span class="s3">,</span>
            <span class="s1">)</span>
            <span class="s3">return </span><span class="s1">self</span>

        <span class="s3">if </span><span class="s1">solver </span><span class="s3">in </span><span class="s1">[</span><span class="s4">&quot;sag&quot;</span><span class="s3">, </span><span class="s4">&quot;saga&quot;</span><span class="s1">]:</span>
            <span class="s1">max_squared_sum = row_norms(X</span><span class="s3">, </span><span class="s1">squared=</span><span class="s3">True</span><span class="s1">).max()</span>
        <span class="s3">else</span><span class="s1">:</span>
            <span class="s1">max_squared_sum = </span><span class="s3">None</span>

        <span class="s1">n_classes = len(self.classes_)</span>
        <span class="s1">classes_ = self.classes_</span>
        <span class="s3">if </span><span class="s1">n_classes &lt; </span><span class="s5">2</span><span class="s1">:</span>
            <span class="s3">raise </span><span class="s1">ValueError(</span>
                <span class="s4">&quot;This solver needs samples of at least 2 classes&quot;</span>
                <span class="s4">&quot; in the data, but the data contains only one&quot;</span>
                <span class="s4">&quot; class: %r&quot;</span>
                <span class="s1">% classes_[</span><span class="s5">0</span><span class="s1">]</span>
            <span class="s1">)</span>

        <span class="s3">if </span><span class="s1">len(self.classes_) == </span><span class="s5">2</span><span class="s1">:</span>
            <span class="s1">n_classes = </span><span class="s5">1</span>
            <span class="s1">classes_ = classes_[</span><span class="s5">1</span><span class="s1">:]</span>

        <span class="s3">if </span><span class="s1">self.warm_start:</span>
            <span class="s1">warm_start_coef = getattr(self</span><span class="s3">, </span><span class="s4">&quot;coef_&quot;</span><span class="s3">, None</span><span class="s1">)</span>
        <span class="s3">else</span><span class="s1">:</span>
            <span class="s1">warm_start_coef = </span><span class="s3">None</span>
        <span class="s3">if </span><span class="s1">warm_start_coef </span><span class="s3">is not None and </span><span class="s1">self.fit_intercept:</span>
            <span class="s1">warm_start_coef = np.append(</span>
                <span class="s1">warm_start_coef</span><span class="s3">, </span><span class="s1">self.intercept_[:</span><span class="s3">, </span><span class="s1">np.newaxis]</span><span class="s3">, </span><span class="s1">axis=</span><span class="s5">1</span>
            <span class="s1">)</span>

        <span class="s2"># Hack so that we iterate only once for the multinomial case.</span>
        <span class="s3">if </span><span class="s1">multi_class == </span><span class="s4">&quot;multinomial&quot;</span><span class="s1">:</span>
            <span class="s1">classes_ = [</span><span class="s3">None</span><span class="s1">]</span>
            <span class="s1">warm_start_coef = [warm_start_coef]</span>
        <span class="s3">if </span><span class="s1">warm_start_coef </span><span class="s3">is None</span><span class="s1">:</span>
            <span class="s1">warm_start_coef = [</span><span class="s3">None</span><span class="s1">] * n_classes</span>

        <span class="s1">path_func = delayed(_logistic_regression_path)</span>

        <span class="s2"># The SAG solver releases the GIL so it's more efficient to use</span>
        <span class="s2"># threads for this solver.</span>
        <span class="s3">if </span><span class="s1">solver </span><span class="s3">in </span><span class="s1">[</span><span class="s4">&quot;sag&quot;</span><span class="s3">, </span><span class="s4">&quot;saga&quot;</span><span class="s1">]:</span>
            <span class="s1">prefer = </span><span class="s4">&quot;threads&quot;</span>
        <span class="s3">else</span><span class="s1">:</span>
            <span class="s1">prefer = </span><span class="s4">&quot;processes&quot;</span>

        <span class="s2"># TODO: Refactor this to avoid joblib parallelism entirely when doing binary</span>
        <span class="s2"># and multinomial multiclass classification and use joblib only for the</span>
        <span class="s2"># one-vs-rest multiclass case.</span>
        <span class="s3">if </span><span class="s1">(</span>
            <span class="s1">solver </span><span class="s3">in </span><span class="s1">[</span><span class="s4">&quot;lbfgs&quot;</span><span class="s3">, </span><span class="s4">&quot;newton-cg&quot;</span><span class="s3">, </span><span class="s4">&quot;newton-cholesky&quot;</span><span class="s1">]</span>
            <span class="s3">and </span><span class="s1">len(classes_) == </span><span class="s5">1</span>
            <span class="s3">and </span><span class="s1">effective_n_jobs(self.n_jobs) == </span><span class="s5">1</span>
        <span class="s1">):</span>
            <span class="s2"># In the future, we would like n_threads = _openmp_effective_n_threads()</span>
            <span class="s2"># For the time being, we just do</span>
            <span class="s1">n_threads = </span><span class="s5">1</span>
        <span class="s3">else</span><span class="s1">:</span>
            <span class="s1">n_threads = </span><span class="s5">1</span>

        <span class="s1">fold_coefs_ = Parallel(n_jobs=self.n_jobs</span><span class="s3">, </span><span class="s1">verbose=self.verbose</span><span class="s3">, </span><span class="s1">prefer=prefer)(</span>
            <span class="s1">path_func(</span>
                <span class="s1">X</span><span class="s3">,</span>
                <span class="s1">y</span><span class="s3">,</span>
                <span class="s1">pos_class=class_</span><span class="s3">,</span>
                <span class="s1">Cs=[C_]</span><span class="s3">,</span>
                <span class="s1">l1_ratio=self.l1_ratio</span><span class="s3">,</span>
                <span class="s1">fit_intercept=self.fit_intercept</span><span class="s3">,</span>
                <span class="s1">tol=self.tol</span><span class="s3">,</span>
                <span class="s1">verbose=self.verbose</span><span class="s3">,</span>
                <span class="s1">solver=solver</span><span class="s3">,</span>
                <span class="s1">multi_class=multi_class</span><span class="s3">,</span>
                <span class="s1">max_iter=self.max_iter</span><span class="s3">,</span>
                <span class="s1">class_weight=self.class_weight</span><span class="s3">,</span>
                <span class="s1">check_input=</span><span class="s3">False,</span>
                <span class="s1">random_state=self.random_state</span><span class="s3">,</span>
                <span class="s1">coef=warm_start_coef_</span><span class="s3">,</span>
                <span class="s1">penalty=penalty</span><span class="s3">,</span>
                <span class="s1">max_squared_sum=max_squared_sum</span><span class="s3">,</span>
                <span class="s1">sample_weight=sample_weight</span><span class="s3">,</span>
                <span class="s1">n_threads=n_threads</span><span class="s3">,</span>
            <span class="s1">)</span>
            <span class="s3">for </span><span class="s1">class_</span><span class="s3">, </span><span class="s1">warm_start_coef_ </span><span class="s3">in </span><span class="s1">zip(classes_</span><span class="s3">, </span><span class="s1">warm_start_coef)</span>
        <span class="s1">)</span>

        <span class="s1">fold_coefs_</span><span class="s3">, </span><span class="s1">_</span><span class="s3">, </span><span class="s1">n_iter_ = zip(*fold_coefs_)</span>
        <span class="s1">self.n_iter_ = np.asarray(n_iter_</span><span class="s3">, </span><span class="s1">dtype=np.int32)[:</span><span class="s3">, </span><span class="s5">0</span><span class="s1">]</span>

        <span class="s1">n_features = X.shape[</span><span class="s5">1</span><span class="s1">]</span>
        <span class="s3">if </span><span class="s1">multi_class == </span><span class="s4">&quot;multinomial&quot;</span><span class="s1">:</span>
            <span class="s1">self.coef_ = fold_coefs_[</span><span class="s5">0</span><span class="s1">][</span><span class="s5">0</span><span class="s1">]</span>
        <span class="s3">else</span><span class="s1">:</span>
            <span class="s1">self.coef_ = np.asarray(fold_coefs_)</span>
            <span class="s1">self.coef_ = self.coef_.reshape(</span>
                <span class="s1">n_classes</span><span class="s3">, </span><span class="s1">n_features + int(self.fit_intercept)</span>
            <span class="s1">)</span>

        <span class="s3">if </span><span class="s1">self.fit_intercept:</span>
            <span class="s1">self.intercept_ = self.coef_[:</span><span class="s3">, </span><span class="s1">-</span><span class="s5">1</span><span class="s1">]</span>
            <span class="s1">self.coef_ = self.coef_[:</span><span class="s3">, </span><span class="s1">:-</span><span class="s5">1</span><span class="s1">]</span>
        <span class="s3">else</span><span class="s1">:</span>
            <span class="s1">self.intercept_ = np.zeros(n_classes)</span>

        <span class="s3">return </span><span class="s1">self</span>

    <span class="s3">def </span><span class="s1">predict_proba(self</span><span class="s3">, </span><span class="s1">X):</span>
        <span class="s0">&quot;&quot;&quot; 
        Probability estimates. 
 
        The returned estimates for all classes are ordered by the 
        label of classes. 
 
        For a multi_class problem, if multi_class is set to be &quot;multinomial&quot; 
        the softmax function is used to find the predicted probability of 
        each class. 
        Else use a one-vs-rest approach, i.e calculate the probability 
        of each class assuming it to be positive using the logistic function. 
        and normalize these values across all the classes. 
 
        Parameters 
        ---------- 
        X : array-like of shape (n_samples, n_features) 
            Vector to be scored, where `n_samples` is the number of samples and 
            `n_features` is the number of features. 
 
        Returns 
        ------- 
        T : array-like of shape (n_samples, n_classes) 
            Returns the probability of the sample for each class in the model, 
            where classes are ordered as they are in ``self.classes_``. 
        &quot;&quot;&quot;</span>
        <span class="s1">check_is_fitted(self)</span>

        <span class="s1">ovr = self.multi_class </span><span class="s3">in </span><span class="s1">[</span><span class="s4">&quot;ovr&quot;</span><span class="s3">, </span><span class="s4">&quot;warn&quot;</span><span class="s1">] </span><span class="s3">or </span><span class="s1">(</span>
            <span class="s1">self.multi_class == </span><span class="s4">&quot;auto&quot;</span>
            <span class="s3">and </span><span class="s1">(</span>
                <span class="s1">self.classes_.size &lt;= </span><span class="s5">2</span>
                <span class="s3">or </span><span class="s1">self.solver </span><span class="s3">in </span><span class="s1">(</span><span class="s4">&quot;liblinear&quot;</span><span class="s3">, </span><span class="s4">&quot;newton-cholesky&quot;</span><span class="s1">)</span>
            <span class="s1">)</span>
        <span class="s1">)</span>
        <span class="s3">if </span><span class="s1">ovr:</span>
            <span class="s3">return </span><span class="s1">super()._predict_proba_lr(X)</span>
        <span class="s3">else</span><span class="s1">:</span>
            <span class="s1">decision = self.decision_function(X)</span>
            <span class="s3">if </span><span class="s1">decision.ndim == </span><span class="s5">1</span><span class="s1">:</span>
                <span class="s2"># Workaround for multi_class=&quot;multinomial&quot; and binary outcomes</span>
                <span class="s2"># which requires softmax prediction with only a 1D decision.</span>
                <span class="s1">decision_2d = np.c_[-decision</span><span class="s3">, </span><span class="s1">decision]</span>
            <span class="s3">else</span><span class="s1">:</span>
                <span class="s1">decision_2d = decision</span>
            <span class="s3">return </span><span class="s1">softmax(decision_2d</span><span class="s3">, </span><span class="s1">copy=</span><span class="s3">False</span><span class="s1">)</span>

    <span class="s3">def </span><span class="s1">predict_log_proba(self</span><span class="s3">, </span><span class="s1">X):</span>
        <span class="s0">&quot;&quot;&quot; 
        Predict logarithm of probability estimates. 
 
        The returned estimates for all classes are ordered by the 
        label of classes. 
 
        Parameters 
        ---------- 
        X : array-like of shape (n_samples, n_features) 
            Vector to be scored, where `n_samples` is the number of samples and 
            `n_features` is the number of features. 
 
        Returns 
        ------- 
        T : array-like of shape (n_samples, n_classes) 
            Returns the log-probability of the sample for each class in the 
            model, where classes are ordered as they are in ``self.classes_``. 
        &quot;&quot;&quot;</span>
        <span class="s3">return </span><span class="s1">np.log(self.predict_proba(X))</span>


<span class="s3">class </span><span class="s1">LogisticRegressionCV(LogisticRegression</span><span class="s3">, </span><span class="s1">LinearClassifierMixin</span><span class="s3">, </span><span class="s1">BaseEstimator):</span>
    <span class="s0">&quot;&quot;&quot;Logistic Regression CV (aka logit, MaxEnt) classifier. 
 
    See glossary entry for :term:`cross-validation estimator`. 
 
    This class implements logistic regression using liblinear, newton-cg, sag 
    of lbfgs optimizer. The newton-cg, sag and lbfgs solvers support only L2 
    regularization with primal formulation. The liblinear solver supports both 
    L1 and L2 regularization, with a dual formulation only for the L2 penalty. 
    Elastic-Net penalty is only supported by the saga solver. 
 
    For the grid of `Cs` values and `l1_ratios` values, the best hyperparameter 
    is selected by the cross-validator 
    :class:`~sklearn.model_selection.StratifiedKFold`, but it can be changed 
    using the :term:`cv` parameter. The 'newton-cg', 'sag', 'saga' and 'lbfgs' 
    solvers can warm-start the coefficients (see :term:`Glossary&lt;warm_start&gt;`). 
 
    Read more in the :ref:`User Guide &lt;logistic_regression&gt;`. 
 
    Parameters 
    ---------- 
    Cs : int or list of floats, default=10 
        Each of the values in Cs describes the inverse of regularization 
        strength. If Cs is as an int, then a grid of Cs values are chosen 
        in a logarithmic scale between 1e-4 and 1e4. 
        Like in support vector machines, smaller values specify stronger 
        regularization. 
 
    fit_intercept : bool, default=True 
        Specifies if a constant (a.k.a. bias or intercept) should be 
        added to the decision function. 
 
    cv : int or cross-validation generator, default=None 
        The default cross-validation generator used is Stratified K-Folds. 
        If an integer is provided, then it is the number of folds used. 
        See the module :mod:`sklearn.model_selection` module for the 
        list of possible cross-validation objects. 
 
        .. versionchanged:: 0.22 
            ``cv`` default value if None changed from 3-fold to 5-fold. 
 
    dual : bool, default=False 
        Dual (constrained) or primal (regularized, see also 
        :ref:`this equation &lt;regularized-logistic-loss&gt;`) formulation. Dual formulation 
        is only implemented for l2 penalty with liblinear solver. Prefer dual=False when 
        n_samples &gt; n_features. 
 
    penalty : {'l1', 'l2', 'elasticnet'}, default='l2' 
        Specify the norm of the penalty: 
 
        - `'l2'`: add a L2 penalty term (used by default); 
        - `'l1'`: add a L1 penalty term; 
        - `'elasticnet'`: both L1 and L2 penalty terms are added. 
 
        .. warning:: 
           Some penalties may not work with some solvers. See the parameter 
           `solver` below, to know the compatibility between the penalty and 
           solver. 
 
    scoring : str or callable, default=None 
        A string (see model evaluation documentation) or 
        a scorer callable object / function with signature 
        ``scorer(estimator, X, y)``. For a list of scoring functions 
        that can be used, look at :mod:`sklearn.metrics`. The 
        default scoring option used is 'accuracy'. 
 
    solver : {'lbfgs', 'liblinear', 'newton-cg', 'newton-cholesky', 'sag', 'saga'}, \ 
            default='lbfgs' 
 
        Algorithm to use in the optimization problem. Default is 'lbfgs'. 
        To choose a solver, you might want to consider the following aspects: 
 
            - For small datasets, 'liblinear' is a good choice, whereas 'sag' 
              and 'saga' are faster for large ones; 
            - For multiclass problems, only 'newton-cg', 'sag', 'saga' and 
              'lbfgs' handle multinomial loss; 
            - 'liblinear' might be slower in :class:`LogisticRegressionCV` 
              because it does not handle warm-starting. 'liblinear' is 
              limited to one-versus-rest schemes. 
            - 'newton-cholesky' is a good choice for `n_samples` &gt;&gt; `n_features`, 
              especially with one-hot encoded categorical features with rare 
              categories. Note that it is limited to binary classification and the 
              one-versus-rest reduction for multiclass classification. Be aware that 
              the memory usage of this solver has a quadratic dependency on 
              `n_features` because it explicitly computes the Hessian matrix. 
 
        .. warning:: 
           The choice of the algorithm depends on the penalty chosen. 
           Supported penalties by solver: 
 
           - 'lbfgs'           -   ['l2'] 
           - 'liblinear'       -   ['l1', 'l2'] 
           - 'newton-cg'       -   ['l2'] 
           - 'newton-cholesky' -   ['l2'] 
           - 'sag'             -   ['l2'] 
           - 'saga'            -   ['elasticnet', 'l1', 'l2'] 
 
        .. note:: 
           'sag' and 'saga' fast convergence is only guaranteed on features 
           with approximately the same scale. You can preprocess the data with 
           a scaler from :mod:`sklearn.preprocessing`. 
 
        .. versionadded:: 0.17 
           Stochastic Average Gradient descent solver. 
        .. versionadded:: 0.19 
           SAGA solver. 
        .. versionadded:: 1.2 
           newton-cholesky solver. 
 
    tol : float, default=1e-4 
        Tolerance for stopping criteria. 
 
    max_iter : int, default=100 
        Maximum number of iterations of the optimization algorithm. 
 
    class_weight : dict or 'balanced', default=None 
        Weights associated with classes in the form ``{class_label: weight}``. 
        If not given, all classes are supposed to have weight one. 
 
        The &quot;balanced&quot; mode uses the values of y to automatically adjust 
        weights inversely proportional to class frequencies in the input data 
        as ``n_samples / (n_classes * np.bincount(y))``. 
 
        Note that these weights will be multiplied with sample_weight (passed 
        through the fit method) if sample_weight is specified. 
 
        .. versionadded:: 0.17 
           class_weight == 'balanced' 
 
    n_jobs : int, default=None 
        Number of CPU cores used during the cross-validation loop. 
        ``None`` means 1 unless in a :obj:`joblib.parallel_backend` context. 
        ``-1`` means using all processors. See :term:`Glossary &lt;n_jobs&gt;` 
        for more details. 
 
    verbose : int, default=0 
        For the 'liblinear', 'sag' and 'lbfgs' solvers set verbose to any 
        positive number for verbosity. 
 
    refit : bool, default=True 
        If set to True, the scores are averaged across all folds, and the 
        coefs and the C that corresponds to the best score is taken, and a 
        final refit is done using these parameters. 
        Otherwise the coefs, intercepts and C that correspond to the 
        best scores across folds are averaged. 
 
    intercept_scaling : float, default=1 
        Useful only when the solver 'liblinear' is used 
        and self.fit_intercept is set to True. In this case, x becomes 
        [x, self.intercept_scaling], 
        i.e. a &quot;synthetic&quot; feature with constant value equal to 
        intercept_scaling is appended to the instance vector. 
        The intercept becomes ``intercept_scaling * synthetic_feature_weight``. 
 
        Note! the synthetic feature weight is subject to l1/l2 regularization 
        as all other features. 
        To lessen the effect of regularization on synthetic feature weight 
        (and therefore on the intercept) intercept_scaling has to be increased. 
 
    multi_class : {'auto, 'ovr', 'multinomial'}, default='auto' 
        If the option chosen is 'ovr', then a binary problem is fit for each 
        label. For 'multinomial' the loss minimised is the multinomial loss fit 
        across the entire probability distribution, *even when the data is 
        binary*. 'multinomial' is unavailable when solver='liblinear'. 
        'auto' selects 'ovr' if the data is binary, or if solver='liblinear', 
        and otherwise selects 'multinomial'. 
 
        .. versionadded:: 0.18 
           Stochastic Average Gradient descent solver for 'multinomial' case. 
        .. versionchanged:: 0.22 
            Default changed from 'ovr' to 'auto' in 0.22. 
 
    random_state : int, RandomState instance, default=None 
        Used when `solver='sag'`, 'saga' or 'liblinear' to shuffle the data. 
        Note that this only applies to the solver and not the cross-validation 
        generator. See :term:`Glossary &lt;random_state&gt;` for details. 
 
    l1_ratios : list of float, default=None 
        The list of Elastic-Net mixing parameter, with ``0 &lt;= l1_ratio &lt;= 1``. 
        Only used if ``penalty='elasticnet'``. A value of 0 is equivalent to 
        using ``penalty='l2'``, while 1 is equivalent to using 
        ``penalty='l1'``. For ``0 &lt; l1_ratio &lt;1``, the penalty is a combination 
        of L1 and L2. 
 
    Attributes 
    ---------- 
    classes_ : ndarray of shape (n_classes, ) 
        A list of class labels known to the classifier. 
 
    coef_ : ndarray of shape (1, n_features) or (n_classes, n_features) 
        Coefficient of the features in the decision function. 
 
        `coef_` is of shape (1, n_features) when the given problem 
        is binary. 
 
    intercept_ : ndarray of shape (1,) or (n_classes,) 
        Intercept (a.k.a. bias) added to the decision function. 
 
        If `fit_intercept` is set to False, the intercept is set to zero. 
        `intercept_` is of shape(1,) when the problem is binary. 
 
    Cs_ : ndarray of shape (n_cs) 
        Array of C i.e. inverse of regularization parameter values used 
        for cross-validation. 
 
    l1_ratios_ : ndarray of shape (n_l1_ratios) 
        Array of l1_ratios used for cross-validation. If no l1_ratio is used 
        (i.e. penalty is not 'elasticnet'), this is set to ``[None]`` 
 
    coefs_paths_ : ndarray of shape (n_folds, n_cs, n_features) or \ 
                   (n_folds, n_cs, n_features + 1) 
        dict with classes as the keys, and the path of coefficients obtained 
        during cross-validating across each fold and then across each Cs 
        after doing an OvR for the corresponding class as values. 
        If the 'multi_class' option is set to 'multinomial', then 
        the coefs_paths are the coefficients corresponding to each class. 
        Each dict value has shape ``(n_folds, n_cs, n_features)`` or 
        ``(n_folds, n_cs, n_features + 1)`` depending on whether the 
        intercept is fit or not. If ``penalty='elasticnet'``, the shape is 
        ``(n_folds, n_cs, n_l1_ratios_, n_features)`` or 
        ``(n_folds, n_cs, n_l1_ratios_, n_features + 1)``. 
 
    scores_ : dict 
        dict with classes as the keys, and the values as the 
        grid of scores obtained during cross-validating each fold, after doing 
        an OvR for the corresponding class. If the 'multi_class' option 
        given is 'multinomial' then the same scores are repeated across 
        all classes, since this is the multinomial class. Each dict value 
        has shape ``(n_folds, n_cs)`` or ``(n_folds, n_cs, n_l1_ratios)`` if 
        ``penalty='elasticnet'``. 
 
    C_ : ndarray of shape (n_classes,) or (n_classes - 1,) 
        Array of C that maps to the best scores across every class. If refit is 
        set to False, then for each class, the best C is the average of the 
        C's that correspond to the best scores for each fold. 
        `C_` is of shape(n_classes,) when the problem is binary. 
 
    l1_ratio_ : ndarray of shape (n_classes,) or (n_classes - 1,) 
        Array of l1_ratio that maps to the best scores across every class. If 
        refit is set to False, then for each class, the best l1_ratio is the 
        average of the l1_ratio's that correspond to the best scores for each 
        fold.  `l1_ratio_` is of shape(n_classes,) when the problem is binary. 
 
    n_iter_ : ndarray of shape (n_classes, n_folds, n_cs) or (1, n_folds, n_cs) 
        Actual number of iterations for all classes, folds and Cs. 
        In the binary or multinomial cases, the first dimension is equal to 1. 
        If ``penalty='elasticnet'``, the shape is ``(n_classes, n_folds, 
        n_cs, n_l1_ratios)`` or ``(1, n_folds, n_cs, n_l1_ratios)``. 
 
    n_features_in_ : int 
        Number of features seen during :term:`fit`. 
 
        .. versionadded:: 0.24 
 
    feature_names_in_ : ndarray of shape (`n_features_in_`,) 
        Names of features seen during :term:`fit`. Defined only when `X` 
        has feature names that are all strings. 
 
        .. versionadded:: 1.0 
 
    See Also 
    -------- 
    LogisticRegression : Logistic regression without tuning the 
        hyperparameter `C`. 
 
    Examples 
    -------- 
    &gt;&gt;&gt; from sklearn.datasets import load_iris 
    &gt;&gt;&gt; from sklearn.linear_model import LogisticRegressionCV 
    &gt;&gt;&gt; X, y = load_iris(return_X_y=True) 
    &gt;&gt;&gt; clf = LogisticRegressionCV(cv=5, random_state=0).fit(X, y) 
    &gt;&gt;&gt; clf.predict(X[:2, :]) 
    array([0, 0]) 
    &gt;&gt;&gt; clf.predict_proba(X[:2, :]).shape 
    (2, 3) 
    &gt;&gt;&gt; clf.score(X, y) 
    0.98... 
    &quot;&quot;&quot;</span>

    <span class="s1">_parameter_constraints: dict = {**LogisticRegression._parameter_constraints}</span>

    <span class="s3">for </span><span class="s1">param </span><span class="s3">in </span><span class="s1">[</span><span class="s4">&quot;C&quot;</span><span class="s3">, </span><span class="s4">&quot;warm_start&quot;</span><span class="s3">, </span><span class="s4">&quot;l1_ratio&quot;</span><span class="s1">]:</span>
        <span class="s1">_parameter_constraints.pop(param)</span>

    <span class="s1">_parameter_constraints.update(</span>
        <span class="s1">{</span>
            <span class="s4">&quot;Cs&quot;</span><span class="s1">: [Interval(Integral</span><span class="s3">, </span><span class="s5">1</span><span class="s3">, None, </span><span class="s1">closed=</span><span class="s4">&quot;left&quot;</span><span class="s1">)</span><span class="s3">, </span><span class="s4">&quot;array-like&quot;</span><span class="s1">]</span><span class="s3">,</span>
            <span class="s4">&quot;cv&quot;</span><span class="s1">: [</span><span class="s4">&quot;cv_object&quot;</span><span class="s1">]</span><span class="s3">,</span>
            <span class="s4">&quot;scoring&quot;</span><span class="s1">: [StrOptions(set(get_scorer_names()))</span><span class="s3">, </span><span class="s1">callable</span><span class="s3">, None</span><span class="s1">]</span><span class="s3">,</span>
            <span class="s4">&quot;l1_ratios&quot;</span><span class="s1">: [</span><span class="s4">&quot;array-like&quot;</span><span class="s3">, None</span><span class="s1">]</span><span class="s3">,</span>
            <span class="s4">&quot;refit&quot;</span><span class="s1">: [</span><span class="s4">&quot;boolean&quot;</span><span class="s1">]</span><span class="s3">,</span>
            <span class="s4">&quot;penalty&quot;</span><span class="s1">: [StrOptions({</span><span class="s4">&quot;l1&quot;</span><span class="s3">, </span><span class="s4">&quot;l2&quot;</span><span class="s3">, </span><span class="s4">&quot;elasticnet&quot;</span><span class="s1">})]</span><span class="s3">,</span>
        <span class="s1">}</span>
    <span class="s1">)</span>

    <span class="s3">def </span><span class="s1">__init__(</span>
        <span class="s1">self</span><span class="s3">,</span>
        <span class="s1">*</span><span class="s3">,</span>
        <span class="s1">Cs=</span><span class="s5">10</span><span class="s3">,</span>
        <span class="s1">fit_intercept=</span><span class="s3">True,</span>
        <span class="s1">cv=</span><span class="s3">None,</span>
        <span class="s1">dual=</span><span class="s3">False,</span>
        <span class="s1">penalty=</span><span class="s4">&quot;l2&quot;</span><span class="s3">,</span>
        <span class="s1">scoring=</span><span class="s3">None,</span>
        <span class="s1">solver=</span><span class="s4">&quot;lbfgs&quot;</span><span class="s3">,</span>
        <span class="s1">tol=</span><span class="s5">1e-4</span><span class="s3">,</span>
        <span class="s1">max_iter=</span><span class="s5">100</span><span class="s3">,</span>
        <span class="s1">class_weight=</span><span class="s3">None,</span>
        <span class="s1">n_jobs=</span><span class="s3">None,</span>
        <span class="s1">verbose=</span><span class="s5">0</span><span class="s3">,</span>
        <span class="s1">refit=</span><span class="s3">True,</span>
        <span class="s1">intercept_scaling=</span><span class="s5">1.0</span><span class="s3">,</span>
        <span class="s1">multi_class=</span><span class="s4">&quot;auto&quot;</span><span class="s3">,</span>
        <span class="s1">random_state=</span><span class="s3">None,</span>
        <span class="s1">l1_ratios=</span><span class="s3">None,</span>
    <span class="s1">):</span>
        <span class="s1">self.Cs = Cs</span>
        <span class="s1">self.fit_intercept = fit_intercept</span>
        <span class="s1">self.cv = cv</span>
        <span class="s1">self.dual = dual</span>
        <span class="s1">self.penalty = penalty</span>
        <span class="s1">self.scoring = scoring</span>
        <span class="s1">self.tol = tol</span>
        <span class="s1">self.max_iter = max_iter</span>
        <span class="s1">self.class_weight = class_weight</span>
        <span class="s1">self.n_jobs = n_jobs</span>
        <span class="s1">self.verbose = verbose</span>
        <span class="s1">self.solver = solver</span>
        <span class="s1">self.refit = refit</span>
        <span class="s1">self.intercept_scaling = intercept_scaling</span>
        <span class="s1">self.multi_class = multi_class</span>
        <span class="s1">self.random_state = random_state</span>
        <span class="s1">self.l1_ratios = l1_ratios</span>

    <span class="s1">@_fit_context(prefer_skip_nested_validation=</span><span class="s3">True</span><span class="s1">)</span>
    <span class="s3">def </span><span class="s1">fit(self</span><span class="s3">, </span><span class="s1">X</span><span class="s3">, </span><span class="s1">y</span><span class="s3">, </span><span class="s1">sample_weight=</span><span class="s3">None</span><span class="s1">):</span>
        <span class="s0">&quot;&quot;&quot;Fit the model according to the given training data. 
 
        Parameters 
        ---------- 
        X : {array-like, sparse matrix} of shape (n_samples, n_features) 
            Training vector, where `n_samples` is the number of samples and 
            `n_features` is the number of features. 
 
        y : array-like of shape (n_samples,) 
            Target vector relative to X. 
 
        sample_weight : array-like of shape (n_samples,) default=None 
            Array of weights that are assigned to individual samples. 
            If not provided, then each sample is given unit weight. 
 
        Returns 
        ------- 
        self : object 
            Fitted LogisticRegressionCV estimator. 
        &quot;&quot;&quot;</span>
        <span class="s1">solver = _check_solver(self.solver</span><span class="s3">, </span><span class="s1">self.penalty</span><span class="s3">, </span><span class="s1">self.dual)</span>

        <span class="s3">if </span><span class="s1">self.penalty == </span><span class="s4">&quot;elasticnet&quot;</span><span class="s1">:</span>
            <span class="s3">if </span><span class="s1">(</span>
                <span class="s1">self.l1_ratios </span><span class="s3">is None</span>
                <span class="s3">or </span><span class="s1">len(self.l1_ratios) == </span><span class="s5">0</span>
                <span class="s3">or </span><span class="s1">any(</span>
                    <span class="s1">(</span>
                        <span class="s3">not </span><span class="s1">isinstance(l1_ratio</span><span class="s3">, </span><span class="s1">numbers.Number)</span>
                        <span class="s3">or </span><span class="s1">l1_ratio &lt; </span><span class="s5">0</span>
                        <span class="s3">or </span><span class="s1">l1_ratio &gt; </span><span class="s5">1</span>
                    <span class="s1">)</span>
                    <span class="s3">for </span><span class="s1">l1_ratio </span><span class="s3">in </span><span class="s1">self.l1_ratios</span>
                <span class="s1">)</span>
            <span class="s1">):</span>
                <span class="s3">raise </span><span class="s1">ValueError(</span>
                    <span class="s4">&quot;l1_ratios must be a list of numbers between &quot;</span>
                    <span class="s4">&quot;0 and 1; got (l1_ratios=%r)&quot;</span>
                    <span class="s1">% self.l1_ratios</span>
                <span class="s1">)</span>
            <span class="s1">l1_ratios_ = self.l1_ratios</span>
        <span class="s3">else</span><span class="s1">:</span>
            <span class="s3">if </span><span class="s1">self.l1_ratios </span><span class="s3">is not None</span><span class="s1">:</span>
                <span class="s1">warnings.warn(</span>
                    <span class="s4">&quot;l1_ratios parameter is only used when penalty &quot;</span>
                    <span class="s4">&quot;is 'elasticnet'. Got (penalty={})&quot;</span><span class="s1">.format(self.penalty)</span>
                <span class="s1">)</span>

            <span class="s1">l1_ratios_ = [</span><span class="s3">None</span><span class="s1">]</span>

        <span class="s1">X</span><span class="s3">, </span><span class="s1">y = self._validate_data(</span>
            <span class="s1">X</span><span class="s3">,</span>
            <span class="s1">y</span><span class="s3">,</span>
            <span class="s1">accept_sparse=</span><span class="s4">&quot;csr&quot;</span><span class="s3">,</span>
            <span class="s1">dtype=np.float64</span><span class="s3">,</span>
            <span class="s1">order=</span><span class="s4">&quot;C&quot;</span><span class="s3">,</span>
            <span class="s1">accept_large_sparse=solver </span><span class="s3">not in </span><span class="s1">[</span><span class="s4">&quot;liblinear&quot;</span><span class="s3">, </span><span class="s4">&quot;sag&quot;</span><span class="s3">, </span><span class="s4">&quot;saga&quot;</span><span class="s1">]</span><span class="s3">,</span>
        <span class="s1">)</span>
        <span class="s1">check_classification_targets(y)</span>

        <span class="s1">class_weight = self.class_weight</span>

        <span class="s2"># Encode for string labels</span>
        <span class="s1">label_encoder = LabelEncoder().fit(y)</span>
        <span class="s1">y = label_encoder.transform(y)</span>
        <span class="s3">if </span><span class="s1">isinstance(class_weight</span><span class="s3">, </span><span class="s1">dict):</span>
            <span class="s1">class_weight = {</span>
                <span class="s1">label_encoder.transform([cls])[</span><span class="s5">0</span><span class="s1">]: v </span><span class="s3">for </span><span class="s1">cls</span><span class="s3">, </span><span class="s1">v </span><span class="s3">in </span><span class="s1">class_weight.items()</span>
            <span class="s1">}</span>

        <span class="s2"># The original class labels</span>
        <span class="s1">classes = self.classes_ = label_encoder.classes_</span>
        <span class="s1">encoded_labels = label_encoder.transform(label_encoder.classes_)</span>

        <span class="s1">multi_class = _check_multi_class(self.multi_class</span><span class="s3">, </span><span class="s1">solver</span><span class="s3">, </span><span class="s1">len(classes))</span>

        <span class="s3">if </span><span class="s1">solver </span><span class="s3">in </span><span class="s1">[</span><span class="s4">&quot;sag&quot;</span><span class="s3">, </span><span class="s4">&quot;saga&quot;</span><span class="s1">]:</span>
            <span class="s1">max_squared_sum = row_norms(X</span><span class="s3">, </span><span class="s1">squared=</span><span class="s3">True</span><span class="s1">).max()</span>
        <span class="s3">else</span><span class="s1">:</span>
            <span class="s1">max_squared_sum = </span><span class="s3">None</span>

        <span class="s2"># init cross-validation generator</span>
        <span class="s1">cv = check_cv(self.cv</span><span class="s3">, </span><span class="s1">y</span><span class="s3">, </span><span class="s1">classifier=</span><span class="s3">True</span><span class="s1">)</span>
        <span class="s1">folds = list(cv.split(X</span><span class="s3">, </span><span class="s1">y))</span>

        <span class="s2"># Use the label encoded classes</span>
        <span class="s1">n_classes = len(encoded_labels)</span>

        <span class="s3">if </span><span class="s1">n_classes &lt; </span><span class="s5">2</span><span class="s1">:</span>
            <span class="s3">raise </span><span class="s1">ValueError(</span>
                <span class="s4">&quot;This solver needs samples of at least 2 classes&quot;</span>
                <span class="s4">&quot; in the data, but the data contains only one&quot;</span>
                <span class="s4">&quot; class: %r&quot;</span>
                <span class="s1">% classes[</span><span class="s5">0</span><span class="s1">]</span>
            <span class="s1">)</span>

        <span class="s3">if </span><span class="s1">n_classes == </span><span class="s5">2</span><span class="s1">:</span>
            <span class="s2"># OvR in case of binary problems is as good as fitting</span>
            <span class="s2"># the higher label</span>
            <span class="s1">n_classes = </span><span class="s5">1</span>
            <span class="s1">encoded_labels = encoded_labels[</span><span class="s5">1</span><span class="s1">:]</span>
            <span class="s1">classes = classes[</span><span class="s5">1</span><span class="s1">:]</span>

        <span class="s2"># We need this hack to iterate only once over labels, in the case of</span>
        <span class="s2"># multi_class = multinomial, without changing the value of the labels.</span>
        <span class="s3">if </span><span class="s1">multi_class == </span><span class="s4">&quot;multinomial&quot;</span><span class="s1">:</span>
            <span class="s1">iter_encoded_labels = iter_classes = [</span><span class="s3">None</span><span class="s1">]</span>
        <span class="s3">else</span><span class="s1">:</span>
            <span class="s1">iter_encoded_labels = encoded_labels</span>
            <span class="s1">iter_classes = classes</span>

        <span class="s2"># compute the class weights for the entire dataset y</span>
        <span class="s3">if </span><span class="s1">class_weight == </span><span class="s4">&quot;balanced&quot;</span><span class="s1">:</span>
            <span class="s1">class_weight = compute_class_weight(</span>
                <span class="s1">class_weight</span><span class="s3">, </span><span class="s1">classes=np.arange(len(self.classes_))</span><span class="s3">, </span><span class="s1">y=y</span>
            <span class="s1">)</span>
            <span class="s1">class_weight = dict(enumerate(class_weight))</span>

        <span class="s1">path_func = delayed(_log_reg_scoring_path)</span>

        <span class="s2"># The SAG solver releases the GIL so it's more efficient to use</span>
        <span class="s2"># threads for this solver.</span>
        <span class="s3">if </span><span class="s1">self.solver </span><span class="s3">in </span><span class="s1">[</span><span class="s4">&quot;sag&quot;</span><span class="s3">, </span><span class="s4">&quot;saga&quot;</span><span class="s1">]:</span>
            <span class="s1">prefer = </span><span class="s4">&quot;threads&quot;</span>
        <span class="s3">else</span><span class="s1">:</span>
            <span class="s1">prefer = </span><span class="s4">&quot;processes&quot;</span>

        <span class="s1">fold_coefs_ = Parallel(n_jobs=self.n_jobs</span><span class="s3">, </span><span class="s1">verbose=self.verbose</span><span class="s3">, </span><span class="s1">prefer=prefer)(</span>
            <span class="s1">path_func(</span>
                <span class="s1">X</span><span class="s3">,</span>
                <span class="s1">y</span><span class="s3">,</span>
                <span class="s1">train</span><span class="s3">,</span>
                <span class="s1">test</span><span class="s3">,</span>
                <span class="s1">pos_class=label</span><span class="s3">,</span>
                <span class="s1">Cs=self.Cs</span><span class="s3">,</span>
                <span class="s1">fit_intercept=self.fit_intercept</span><span class="s3">,</span>
                <span class="s1">penalty=self.penalty</span><span class="s3">,</span>
                <span class="s1">dual=self.dual</span><span class="s3">,</span>
                <span class="s1">solver=solver</span><span class="s3">,</span>
                <span class="s1">tol=self.tol</span><span class="s3">,</span>
                <span class="s1">max_iter=self.max_iter</span><span class="s3">,</span>
                <span class="s1">verbose=self.verbose</span><span class="s3">,</span>
                <span class="s1">class_weight=class_weight</span><span class="s3">,</span>
                <span class="s1">scoring=self.scoring</span><span class="s3">,</span>
                <span class="s1">multi_class=multi_class</span><span class="s3">,</span>
                <span class="s1">intercept_scaling=self.intercept_scaling</span><span class="s3">,</span>
                <span class="s1">random_state=self.random_state</span><span class="s3">,</span>
                <span class="s1">max_squared_sum=max_squared_sum</span><span class="s3">,</span>
                <span class="s1">sample_weight=sample_weight</span><span class="s3">,</span>
                <span class="s1">l1_ratio=l1_ratio</span><span class="s3">,</span>
            <span class="s1">)</span>
            <span class="s3">for </span><span class="s1">label </span><span class="s3">in </span><span class="s1">iter_encoded_labels</span>
            <span class="s3">for </span><span class="s1">train</span><span class="s3">, </span><span class="s1">test </span><span class="s3">in </span><span class="s1">folds</span>
            <span class="s3">for </span><span class="s1">l1_ratio </span><span class="s3">in </span><span class="s1">l1_ratios_</span>
        <span class="s1">)</span>

        <span class="s2"># _log_reg_scoring_path will output different shapes depending on the</span>
        <span class="s2"># multi_class param, so we need to reshape the outputs accordingly.</span>
        <span class="s2"># Cs is of shape (n_classes . n_folds . n_l1_ratios, n_Cs) and all the</span>
        <span class="s2"># rows are equal, so we just take the first one.</span>
        <span class="s2"># After reshaping,</span>
        <span class="s2"># - scores is of shape (n_classes, n_folds, n_Cs . n_l1_ratios)</span>
        <span class="s2"># - coefs_paths is of shape</span>
        <span class="s2">#  (n_classes, n_folds, n_Cs . n_l1_ratios, n_features)</span>
        <span class="s2"># - n_iter is of shape</span>
        <span class="s2">#  (n_classes, n_folds, n_Cs . n_l1_ratios) or</span>
        <span class="s2">#  (1, n_folds, n_Cs . n_l1_ratios)</span>
        <span class="s1">coefs_paths</span><span class="s3">, </span><span class="s1">Cs</span><span class="s3">, </span><span class="s1">scores</span><span class="s3">, </span><span class="s1">n_iter_ = zip(*fold_coefs_)</span>
        <span class="s1">self.Cs_ = Cs[</span><span class="s5">0</span><span class="s1">]</span>
        <span class="s3">if </span><span class="s1">multi_class == </span><span class="s4">&quot;multinomial&quot;</span><span class="s1">:</span>
            <span class="s1">coefs_paths = np.reshape(</span>
                <span class="s1">coefs_paths</span><span class="s3">,</span>
                <span class="s1">(len(folds)</span><span class="s3">, </span><span class="s1">len(l1_ratios_) * len(self.Cs_)</span><span class="s3">, </span><span class="s1">n_classes</span><span class="s3">, </span><span class="s1">-</span><span class="s5">1</span><span class="s1">)</span><span class="s3">,</span>
            <span class="s1">)</span>
            <span class="s2"># equiv to coefs_paths = np.moveaxis(coefs_paths, (0, 1, 2, 3),</span>
            <span class="s2">#                                                 (1, 2, 0, 3))</span>
            <span class="s1">coefs_paths = np.swapaxes(coefs_paths</span><span class="s3">, </span><span class="s5">0</span><span class="s3">, </span><span class="s5">1</span><span class="s1">)</span>
            <span class="s1">coefs_paths = np.swapaxes(coefs_paths</span><span class="s3">, </span><span class="s5">0</span><span class="s3">, </span><span class="s5">2</span><span class="s1">)</span>
            <span class="s1">self.n_iter_ = np.reshape(</span>
                <span class="s1">n_iter_</span><span class="s3">, </span><span class="s1">(</span><span class="s5">1</span><span class="s3">, </span><span class="s1">len(folds)</span><span class="s3">, </span><span class="s1">len(self.Cs_) * len(l1_ratios_))</span>
            <span class="s1">)</span>
            <span class="s2"># repeat same scores across all classes</span>
            <span class="s1">scores = np.tile(scores</span><span class="s3">, </span><span class="s1">(n_classes</span><span class="s3">, </span><span class="s5">1</span><span class="s3">, </span><span class="s5">1</span><span class="s1">))</span>
        <span class="s3">else</span><span class="s1">:</span>
            <span class="s1">coefs_paths = np.reshape(</span>
                <span class="s1">coefs_paths</span><span class="s3">,</span>
                <span class="s1">(n_classes</span><span class="s3">, </span><span class="s1">len(folds)</span><span class="s3">, </span><span class="s1">len(self.Cs_) * len(l1_ratios_)</span><span class="s3">, </span><span class="s1">-</span><span class="s5">1</span><span class="s1">)</span><span class="s3">,</span>
            <span class="s1">)</span>
            <span class="s1">self.n_iter_ = np.reshape(</span>
                <span class="s1">n_iter_</span><span class="s3">, </span><span class="s1">(n_classes</span><span class="s3">, </span><span class="s1">len(folds)</span><span class="s3">, </span><span class="s1">len(self.Cs_) * len(l1_ratios_))</span>
            <span class="s1">)</span>
        <span class="s1">scores = np.reshape(scores</span><span class="s3">, </span><span class="s1">(n_classes</span><span class="s3">, </span><span class="s1">len(folds)</span><span class="s3">, </span><span class="s1">-</span><span class="s5">1</span><span class="s1">))</span>
        <span class="s1">self.scores_ = dict(zip(classes</span><span class="s3">, </span><span class="s1">scores))</span>
        <span class="s1">self.coefs_paths_ = dict(zip(classes</span><span class="s3">, </span><span class="s1">coefs_paths))</span>

        <span class="s1">self.C_ = list()</span>
        <span class="s1">self.l1_ratio_ = list()</span>
        <span class="s1">self.coef_ = np.empty((n_classes</span><span class="s3">, </span><span class="s1">X.shape[</span><span class="s5">1</span><span class="s1">]))</span>
        <span class="s1">self.intercept_ = np.zeros(n_classes)</span>
        <span class="s3">for </span><span class="s1">index</span><span class="s3">, </span><span class="s1">(cls</span><span class="s3">, </span><span class="s1">encoded_label) </span><span class="s3">in </span><span class="s1">enumerate(</span>
            <span class="s1">zip(iter_classes</span><span class="s3">, </span><span class="s1">iter_encoded_labels)</span>
        <span class="s1">):</span>
            <span class="s3">if </span><span class="s1">multi_class == </span><span class="s4">&quot;ovr&quot;</span><span class="s1">:</span>
                <span class="s1">scores = self.scores_[cls]</span>
                <span class="s1">coefs_paths = self.coefs_paths_[cls]</span>
            <span class="s3">else</span><span class="s1">:</span>
                <span class="s2"># For multinomial, all scores are the same across classes</span>
                <span class="s1">scores = scores[</span><span class="s5">0</span><span class="s1">]</span>
                <span class="s2"># coefs_paths will keep its original shape because</span>
                <span class="s2"># logistic_regression_path expects it this way</span>

            <span class="s3">if </span><span class="s1">self.refit:</span>
                <span class="s2"># best_index is between 0 and (n_Cs . n_l1_ratios - 1)</span>
                <span class="s2"># for example, with n_cs=2 and n_l1_ratios=3</span>
                <span class="s2"># the layout of scores is</span>
                <span class="s2"># [c1, c2, c1, c2, c1, c2]</span>
                <span class="s2">#   l1_1 ,  l1_2 ,  l1_3</span>
                <span class="s1">best_index = scores.sum(axis=</span><span class="s5">0</span><span class="s1">).argmax()</span>

                <span class="s1">best_index_C = best_index % len(self.Cs_)</span>
                <span class="s1">C_ = self.Cs_[best_index_C]</span>
                <span class="s1">self.C_.append(C_)</span>

                <span class="s1">best_index_l1 = best_index // len(self.Cs_)</span>
                <span class="s1">l1_ratio_ = l1_ratios_[best_index_l1]</span>
                <span class="s1">self.l1_ratio_.append(l1_ratio_)</span>

                <span class="s3">if </span><span class="s1">multi_class == </span><span class="s4">&quot;multinomial&quot;</span><span class="s1">:</span>
                    <span class="s1">coef_init = np.mean(coefs_paths[:</span><span class="s3">, </span><span class="s1">:</span><span class="s3">, </span><span class="s1">best_index</span><span class="s3">, </span><span class="s1">:]</span><span class="s3">, </span><span class="s1">axis=</span><span class="s5">1</span><span class="s1">)</span>
                <span class="s3">else</span><span class="s1">:</span>
                    <span class="s1">coef_init = np.mean(coefs_paths[:</span><span class="s3">, </span><span class="s1">best_index</span><span class="s3">, </span><span class="s1">:]</span><span class="s3">, </span><span class="s1">axis=</span><span class="s5">0</span><span class="s1">)</span>

                <span class="s2"># Note that y is label encoded and hence pos_class must be</span>
                <span class="s2"># the encoded label / None (for 'multinomial')</span>
                <span class="s1">w</span><span class="s3">, </span><span class="s1">_</span><span class="s3">, </span><span class="s1">_ = _logistic_regression_path(</span>
                    <span class="s1">X</span><span class="s3">,</span>
                    <span class="s1">y</span><span class="s3">,</span>
                    <span class="s1">pos_class=encoded_label</span><span class="s3">,</span>
                    <span class="s1">Cs=[C_]</span><span class="s3">,</span>
                    <span class="s1">solver=solver</span><span class="s3">,</span>
                    <span class="s1">fit_intercept=self.fit_intercept</span><span class="s3">,</span>
                    <span class="s1">coef=coef_init</span><span class="s3">,</span>
                    <span class="s1">max_iter=self.max_iter</span><span class="s3">,</span>
                    <span class="s1">tol=self.tol</span><span class="s3">,</span>
                    <span class="s1">penalty=self.penalty</span><span class="s3">,</span>
                    <span class="s1">class_weight=class_weight</span><span class="s3">,</span>
                    <span class="s1">multi_class=multi_class</span><span class="s3">,</span>
                    <span class="s1">verbose=max(</span><span class="s5">0</span><span class="s3">, </span><span class="s1">self.verbose - </span><span class="s5">1</span><span class="s1">)</span><span class="s3">,</span>
                    <span class="s1">random_state=self.random_state</span><span class="s3">,</span>
                    <span class="s1">check_input=</span><span class="s3">False,</span>
                    <span class="s1">max_squared_sum=max_squared_sum</span><span class="s3">,</span>
                    <span class="s1">sample_weight=sample_weight</span><span class="s3">,</span>
                    <span class="s1">l1_ratio=l1_ratio_</span><span class="s3">,</span>
                <span class="s1">)</span>
                <span class="s1">w = w[</span><span class="s5">0</span><span class="s1">]</span>

            <span class="s3">else</span><span class="s1">:</span>
                <span class="s2"># Take the best scores across every fold and the average of</span>
                <span class="s2"># all coefficients corresponding to the best scores.</span>
                <span class="s1">best_indices = np.argmax(scores</span><span class="s3">, </span><span class="s1">axis=</span><span class="s5">1</span><span class="s1">)</span>
                <span class="s3">if </span><span class="s1">multi_class == </span><span class="s4">&quot;ovr&quot;</span><span class="s1">:</span>
                    <span class="s1">w = np.mean(</span>
                        <span class="s1">[coefs_paths[i</span><span class="s3">, </span><span class="s1">best_indices[i]</span><span class="s3">, </span><span class="s1">:] </span><span class="s3">for </span><span class="s1">i </span><span class="s3">in </span><span class="s1">range(len(folds))]</span><span class="s3">,</span>
                        <span class="s1">axis=</span><span class="s5">0</span><span class="s3">,</span>
                    <span class="s1">)</span>
                <span class="s3">else</span><span class="s1">:</span>
                    <span class="s1">w = np.mean(</span>
                        <span class="s1">[</span>
                            <span class="s1">coefs_paths[:</span><span class="s3">, </span><span class="s1">i</span><span class="s3">, </span><span class="s1">best_indices[i]</span><span class="s3">, </span><span class="s1">:]</span>
                            <span class="s3">for </span><span class="s1">i </span><span class="s3">in </span><span class="s1">range(len(folds))</span>
                        <span class="s1">]</span><span class="s3">,</span>
                        <span class="s1">axis=</span><span class="s5">0</span><span class="s3">,</span>
                    <span class="s1">)</span>

                <span class="s1">best_indices_C = best_indices % len(self.Cs_)</span>
                <span class="s1">self.C_.append(np.mean(self.Cs_[best_indices_C]))</span>

                <span class="s3">if </span><span class="s1">self.penalty == </span><span class="s4">&quot;elasticnet&quot;</span><span class="s1">:</span>
                    <span class="s1">best_indices_l1 = best_indices // len(self.Cs_)</span>
                    <span class="s1">self.l1_ratio_.append(np.mean(l1_ratios_[best_indices_l1]))</span>
                <span class="s3">else</span><span class="s1">:</span>
                    <span class="s1">self.l1_ratio_.append(</span><span class="s3">None</span><span class="s1">)</span>

            <span class="s3">if </span><span class="s1">multi_class == </span><span class="s4">&quot;multinomial&quot;</span><span class="s1">:</span>
                <span class="s1">self.C_ = np.tile(self.C_</span><span class="s3">, </span><span class="s1">n_classes)</span>
                <span class="s1">self.l1_ratio_ = np.tile(self.l1_ratio_</span><span class="s3">, </span><span class="s1">n_classes)</span>
                <span class="s1">self.coef_ = w[:</span><span class="s3">, </span><span class="s1">: X.shape[</span><span class="s5">1</span><span class="s1">]]</span>
                <span class="s3">if </span><span class="s1">self.fit_intercept:</span>
                    <span class="s1">self.intercept_ = w[:</span><span class="s3">, </span><span class="s1">-</span><span class="s5">1</span><span class="s1">]</span>
            <span class="s3">else</span><span class="s1">:</span>
                <span class="s1">self.coef_[index] = w[: X.shape[</span><span class="s5">1</span><span class="s1">]]</span>
                <span class="s3">if </span><span class="s1">self.fit_intercept:</span>
                    <span class="s1">self.intercept_[index] = w[-</span><span class="s5">1</span><span class="s1">]</span>

        <span class="s1">self.C_ = np.asarray(self.C_)</span>
        <span class="s1">self.l1_ratio_ = np.asarray(self.l1_ratio_)</span>
        <span class="s1">self.l1_ratios_ = np.asarray(l1_ratios_)</span>
        <span class="s2"># if elasticnet was used, add the l1_ratios dimension to some</span>
        <span class="s2"># attributes</span>
        <span class="s3">if </span><span class="s1">self.l1_ratios </span><span class="s3">is not None</span><span class="s1">:</span>
            <span class="s2"># with n_cs=2 and n_l1_ratios=3</span>
            <span class="s2"># the layout of scores is</span>
            <span class="s2"># [c1, c2, c1, c2, c1, c2]</span>
            <span class="s2">#   l1_1 ,  l1_2 ,  l1_3</span>
            <span class="s2"># To get a 2d array with the following layout</span>
            <span class="s2">#      l1_1, l1_2, l1_3</span>
            <span class="s2"># c1 [[ .  ,  .  ,  .  ],</span>
            <span class="s2"># c2  [ .  ,  .  ,  .  ]]</span>
            <span class="s2"># We need to first reshape and then transpose.</span>
            <span class="s2"># The same goes for the other arrays</span>
            <span class="s3">for </span><span class="s1">cls</span><span class="s3">, </span><span class="s1">coefs_path </span><span class="s3">in </span><span class="s1">self.coefs_paths_.items():</span>
                <span class="s1">self.coefs_paths_[cls] = coefs_path.reshape(</span>
                    <span class="s1">(len(folds)</span><span class="s3">, </span><span class="s1">self.l1_ratios_.size</span><span class="s3">, </span><span class="s1">self.Cs_.size</span><span class="s3">, </span><span class="s1">-</span><span class="s5">1</span><span class="s1">)</span>
                <span class="s1">)</span>
                <span class="s1">self.coefs_paths_[cls] = np.transpose(</span>
                    <span class="s1">self.coefs_paths_[cls]</span><span class="s3">, </span><span class="s1">(</span><span class="s5">0</span><span class="s3">, </span><span class="s5">2</span><span class="s3">, </span><span class="s5">1</span><span class="s3">, </span><span class="s5">3</span><span class="s1">)</span>
                <span class="s1">)</span>
            <span class="s3">for </span><span class="s1">cls</span><span class="s3">, </span><span class="s1">score </span><span class="s3">in </span><span class="s1">self.scores_.items():</span>
                <span class="s1">self.scores_[cls] = score.reshape(</span>
                    <span class="s1">(len(folds)</span><span class="s3">, </span><span class="s1">self.l1_ratios_.size</span><span class="s3">, </span><span class="s1">self.Cs_.size)</span>
                <span class="s1">)</span>
                <span class="s1">self.scores_[cls] = np.transpose(self.scores_[cls]</span><span class="s3">, </span><span class="s1">(</span><span class="s5">0</span><span class="s3">, </span><span class="s5">2</span><span class="s3">, </span><span class="s5">1</span><span class="s1">))</span>

            <span class="s1">self.n_iter_ = self.n_iter_.reshape(</span>
                <span class="s1">(-</span><span class="s5">1</span><span class="s3">, </span><span class="s1">len(folds)</span><span class="s3">, </span><span class="s1">self.l1_ratios_.size</span><span class="s3">, </span><span class="s1">self.Cs_.size)</span>
            <span class="s1">)</span>
            <span class="s1">self.n_iter_ = np.transpose(self.n_iter_</span><span class="s3">, </span><span class="s1">(</span><span class="s5">0</span><span class="s3">, </span><span class="s5">1</span><span class="s3">, </span><span class="s5">3</span><span class="s3">, </span><span class="s5">2</span><span class="s1">))</span>

        <span class="s3">return </span><span class="s1">self</span>

    <span class="s3">def </span><span class="s1">score(self</span><span class="s3">, </span><span class="s1">X</span><span class="s3">, </span><span class="s1">y</span><span class="s3">, </span><span class="s1">sample_weight=</span><span class="s3">None</span><span class="s1">):</span>
        <span class="s0">&quot;&quot;&quot;Score using the `scoring` option on the given test data and labels. 
 
        Parameters 
        ---------- 
        X : array-like of shape (n_samples, n_features) 
            Test samples. 
 
        y : array-like of shape (n_samples,) 
            True labels for X. 
 
        sample_weight : array-like of shape (n_samples,), default=None 
            Sample weights. 
 
        Returns 
        ------- 
        score : float 
            Score of self.predict(X) w.r.t. y. 
        &quot;&quot;&quot;</span>
        <span class="s1">scoring = self.scoring </span><span class="s3">or </span><span class="s4">&quot;accuracy&quot;</span>
        <span class="s1">scoring = get_scorer(scoring)</span>

        <span class="s3">return </span><span class="s1">scoring(self</span><span class="s3">, </span><span class="s1">X</span><span class="s3">, </span><span class="s1">y</span><span class="s3">, </span><span class="s1">sample_weight=sample_weight)</span>

    <span class="s3">def </span><span class="s1">_more_tags(self):</span>
        <span class="s3">return </span><span class="s1">{</span>
            <span class="s4">&quot;_xfail_checks&quot;</span><span class="s1">: {</span>
                <span class="s4">&quot;check_sample_weights_invariance&quot;</span><span class="s1">: (</span>
                    <span class="s4">&quot;zero sample_weight is not equivalent to removing samples&quot;</span>
                <span class="s1">)</span><span class="s3">,</span>
            <span class="s1">}</span>
        <span class="s1">}</span>
</pre>
</body>
</html>