<html>
<head>
<title>test_rfe.py</title>
<meta http-equiv="Content-Type" content="text/html; charset=utf-8">
<style type="text/css">
.s0 { color: #629755; font-style: italic;}
.s1 { color: #a9b7c6;}
.s2 { color: #cc7832;}
.s3 { color: #6897bb;}
.s4 { color: #6a8759;}
.s5 { color: #808080;}
</style>
</head>
<body bgcolor="#2b2b2b">
<table CELLSPACING=0 CELLPADDING=5 COLS=1 WIDTH="100%" BGCOLOR="#606060" >
<tr><td><center>
<font face="Arial, Helvetica" color="#000000">
test_rfe.py</font>
</center></td></tr></table>
<pre><span class="s0">&quot;&quot;&quot; 
Testing Recursive feature elimination 
&quot;&quot;&quot;</span>

<span class="s2">from </span><span class="s1">operator </span><span class="s2">import </span><span class="s1">attrgetter</span>

<span class="s2">import </span><span class="s1">numpy </span><span class="s2">as </span><span class="s1">np</span>
<span class="s2">import </span><span class="s1">pytest</span>
<span class="s2">from </span><span class="s1">numpy.testing </span><span class="s2">import </span><span class="s1">assert_allclose</span><span class="s2">, </span><span class="s1">assert_array_almost_equal</span><span class="s2">, </span><span class="s1">assert_array_equal</span>
<span class="s2">from </span><span class="s1">scipy </span><span class="s2">import </span><span class="s1">sparse</span>

<span class="s2">from </span><span class="s1">sklearn.base </span><span class="s2">import </span><span class="s1">BaseEstimator</span><span class="s2">, </span><span class="s1">ClassifierMixin</span>
<span class="s2">from </span><span class="s1">sklearn.compose </span><span class="s2">import </span><span class="s1">TransformedTargetRegressor</span>
<span class="s2">from </span><span class="s1">sklearn.cross_decomposition </span><span class="s2">import </span><span class="s1">CCA</span><span class="s2">, </span><span class="s1">PLSCanonical</span><span class="s2">, </span><span class="s1">PLSRegression</span>
<span class="s2">from </span><span class="s1">sklearn.datasets </span><span class="s2">import </span><span class="s1">load_iris</span><span class="s2">, </span><span class="s1">make_friedman1</span>
<span class="s2">from </span><span class="s1">sklearn.ensemble </span><span class="s2">import </span><span class="s1">RandomForestClassifier</span>
<span class="s2">from </span><span class="s1">sklearn.feature_selection </span><span class="s2">import </span><span class="s1">RFE</span><span class="s2">, </span><span class="s1">RFECV</span>
<span class="s2">from </span><span class="s1">sklearn.linear_model </span><span class="s2">import </span><span class="s1">LogisticRegression</span>
<span class="s2">from </span><span class="s1">sklearn.metrics </span><span class="s2">import </span><span class="s1">get_scorer</span><span class="s2">, </span><span class="s1">make_scorer</span><span class="s2">, </span><span class="s1">zero_one_loss</span>
<span class="s2">from </span><span class="s1">sklearn.model_selection </span><span class="s2">import </span><span class="s1">GroupKFold</span><span class="s2">, </span><span class="s1">cross_val_score</span>
<span class="s2">from </span><span class="s1">sklearn.pipeline </span><span class="s2">import </span><span class="s1">make_pipeline</span>
<span class="s2">from </span><span class="s1">sklearn.preprocessing </span><span class="s2">import </span><span class="s1">StandardScaler</span>
<span class="s2">from </span><span class="s1">sklearn.svm </span><span class="s2">import </span><span class="s1">SVC</span><span class="s2">, </span><span class="s1">SVR</span><span class="s2">, </span><span class="s1">LinearSVR</span>
<span class="s2">from </span><span class="s1">sklearn.utils </span><span class="s2">import </span><span class="s1">check_random_state</span>
<span class="s2">from </span><span class="s1">sklearn.utils._testing </span><span class="s2">import </span><span class="s1">ignore_warnings</span>


<span class="s2">class </span><span class="s1">MockClassifier:</span>
    <span class="s0">&quot;&quot;&quot; 
    Dummy classifier to test recursive feature elimination 
    &quot;&quot;&quot;</span>

    <span class="s2">def </span><span class="s1">__init__(self</span><span class="s2">, </span><span class="s1">foo_param=</span><span class="s3">0</span><span class="s1">):</span>
        <span class="s1">self.foo_param = foo_param</span>

    <span class="s2">def </span><span class="s1">fit(self</span><span class="s2">, </span><span class="s1">X</span><span class="s2">, </span><span class="s1">y):</span>
        <span class="s2">assert </span><span class="s1">len(X) == len(y)</span>
        <span class="s1">self.coef_ = np.ones(X.shape[</span><span class="s3">1</span><span class="s1">]</span><span class="s2">, </span><span class="s1">dtype=np.float64)</span>
        <span class="s2">return </span><span class="s1">self</span>

    <span class="s2">def </span><span class="s1">predict(self</span><span class="s2">, </span><span class="s1">T):</span>
        <span class="s2">return </span><span class="s1">T.shape[</span><span class="s3">0</span><span class="s1">]</span>

    <span class="s1">predict_proba = predict</span>
    <span class="s1">decision_function = predict</span>
    <span class="s1">transform = predict</span>

    <span class="s2">def </span><span class="s1">score(self</span><span class="s2">, </span><span class="s1">X=</span><span class="s2">None, </span><span class="s1">y=</span><span class="s2">None</span><span class="s1">):</span>
        <span class="s2">return </span><span class="s3">0.0</span>

    <span class="s2">def </span><span class="s1">get_params(self</span><span class="s2">, </span><span class="s1">deep=</span><span class="s2">True</span><span class="s1">):</span>
        <span class="s2">return </span><span class="s1">{</span><span class="s4">&quot;foo_param&quot;</span><span class="s1">: self.foo_param}</span>

    <span class="s2">def </span><span class="s1">set_params(self</span><span class="s2">, </span><span class="s1">**params):</span>
        <span class="s2">return </span><span class="s1">self</span>

    <span class="s2">def </span><span class="s1">_more_tags(self):</span>
        <span class="s2">return </span><span class="s1">{</span><span class="s4">&quot;allow_nan&quot;</span><span class="s1">: </span><span class="s2">True</span><span class="s1">}</span>


<span class="s2">def </span><span class="s1">test_rfe_features_importance():</span>
    <span class="s1">generator = check_random_state(</span><span class="s3">0</span><span class="s1">)</span>
    <span class="s1">iris = load_iris()</span>
    <span class="s5"># Add some irrelevant features. Random seed is set to make sure that</span>
    <span class="s5"># irrelevant features are always irrelevant.</span>
    <span class="s1">X = np.c_[iris.data</span><span class="s2">, </span><span class="s1">generator.normal(size=(len(iris.data)</span><span class="s2">, </span><span class="s3">6</span><span class="s1">))]</span>
    <span class="s1">y = iris.target</span>

    <span class="s1">clf = RandomForestClassifier(n_estimators=</span><span class="s3">20</span><span class="s2">, </span><span class="s1">random_state=generator</span><span class="s2">, </span><span class="s1">max_depth=</span><span class="s3">2</span><span class="s1">)</span>
    <span class="s1">rfe = RFE(estimator=clf</span><span class="s2">, </span><span class="s1">n_features_to_select=</span><span class="s3">4</span><span class="s2">, </span><span class="s1">step=</span><span class="s3">0.1</span><span class="s1">)</span>
    <span class="s1">rfe.fit(X</span><span class="s2">, </span><span class="s1">y)</span>
    <span class="s2">assert </span><span class="s1">len(rfe.ranking_) == X.shape[</span><span class="s3">1</span><span class="s1">]</span>

    <span class="s1">clf_svc = SVC(kernel=</span><span class="s4">&quot;linear&quot;</span><span class="s1">)</span>
    <span class="s1">rfe_svc = RFE(estimator=clf_svc</span><span class="s2">, </span><span class="s1">n_features_to_select=</span><span class="s3">4</span><span class="s2">, </span><span class="s1">step=</span><span class="s3">0.1</span><span class="s1">)</span>
    <span class="s1">rfe_svc.fit(X</span><span class="s2">, </span><span class="s1">y)</span>

    <span class="s5"># Check if the supports are equal</span>
    <span class="s1">assert_array_equal(rfe.get_support()</span><span class="s2">, </span><span class="s1">rfe_svc.get_support())</span>


<span class="s2">def </span><span class="s1">test_rfe():</span>
    <span class="s1">generator = check_random_state(</span><span class="s3">0</span><span class="s1">)</span>
    <span class="s1">iris = load_iris()</span>
    <span class="s5"># Add some irrelevant features. Random seed is set to make sure that</span>
    <span class="s5"># irrelevant features are always irrelevant.</span>
    <span class="s1">X = np.c_[iris.data</span><span class="s2">, </span><span class="s1">generator.normal(size=(len(iris.data)</span><span class="s2">, </span><span class="s3">6</span><span class="s1">))]</span>
    <span class="s1">X_sparse = sparse.csr_matrix(X)</span>
    <span class="s1">y = iris.target</span>

    <span class="s5"># dense model</span>
    <span class="s1">clf = SVC(kernel=</span><span class="s4">&quot;linear&quot;</span><span class="s1">)</span>
    <span class="s1">rfe = RFE(estimator=clf</span><span class="s2">, </span><span class="s1">n_features_to_select=</span><span class="s3">4</span><span class="s2">, </span><span class="s1">step=</span><span class="s3">0.1</span><span class="s1">)</span>
    <span class="s1">rfe.fit(X</span><span class="s2">, </span><span class="s1">y)</span>
    <span class="s1">X_r = rfe.transform(X)</span>
    <span class="s1">clf.fit(X_r</span><span class="s2">, </span><span class="s1">y)</span>
    <span class="s2">assert </span><span class="s1">len(rfe.ranking_) == X.shape[</span><span class="s3">1</span><span class="s1">]</span>

    <span class="s5"># sparse model</span>
    <span class="s1">clf_sparse = SVC(kernel=</span><span class="s4">&quot;linear&quot;</span><span class="s1">)</span>
    <span class="s1">rfe_sparse = RFE(estimator=clf_sparse</span><span class="s2">, </span><span class="s1">n_features_to_select=</span><span class="s3">4</span><span class="s2">, </span><span class="s1">step=</span><span class="s3">0.1</span><span class="s1">)</span>
    <span class="s1">rfe_sparse.fit(X_sparse</span><span class="s2">, </span><span class="s1">y)</span>
    <span class="s1">X_r_sparse = rfe_sparse.transform(X_sparse)</span>

    <span class="s2">assert </span><span class="s1">X_r.shape == iris.data.shape</span>
    <span class="s1">assert_array_almost_equal(X_r[:</span><span class="s3">10</span><span class="s1">]</span><span class="s2">, </span><span class="s1">iris.data[:</span><span class="s3">10</span><span class="s1">])</span>

    <span class="s1">assert_array_almost_equal(rfe.predict(X)</span><span class="s2">, </span><span class="s1">clf.predict(iris.data))</span>
    <span class="s2">assert </span><span class="s1">rfe.score(X</span><span class="s2">, </span><span class="s1">y) == clf.score(iris.data</span><span class="s2">, </span><span class="s1">iris.target)</span>
    <span class="s1">assert_array_almost_equal(X_r</span><span class="s2">, </span><span class="s1">X_r_sparse.toarray())</span>


<span class="s2">def </span><span class="s1">test_RFE_fit_score_params():</span>
    <span class="s5"># Make sure RFE passes the metadata down to fit and score methods of the</span>
    <span class="s5"># underlying estimator</span>
    <span class="s2">class </span><span class="s1">TestEstimator(BaseEstimator</span><span class="s2">, </span><span class="s1">ClassifierMixin):</span>
        <span class="s2">def </span><span class="s1">fit(self</span><span class="s2">, </span><span class="s1">X</span><span class="s2">, </span><span class="s1">y</span><span class="s2">, </span><span class="s1">prop=</span><span class="s2">None</span><span class="s1">):</span>
            <span class="s2">if </span><span class="s1">prop </span><span class="s2">is None</span><span class="s1">:</span>
                <span class="s2">raise </span><span class="s1">ValueError(</span><span class="s4">&quot;fit: prop cannot be None&quot;</span><span class="s1">)</span>
            <span class="s1">self.svc_ = SVC(kernel=</span><span class="s4">&quot;linear&quot;</span><span class="s1">).fit(X</span><span class="s2">, </span><span class="s1">y)</span>
            <span class="s1">self.coef_ = self.svc_.coef_</span>
            <span class="s2">return </span><span class="s1">self</span>

        <span class="s2">def </span><span class="s1">score(self</span><span class="s2">, </span><span class="s1">X</span><span class="s2">, </span><span class="s1">y</span><span class="s2">, </span><span class="s1">prop=</span><span class="s2">None</span><span class="s1">):</span>
            <span class="s2">if </span><span class="s1">prop </span><span class="s2">is None</span><span class="s1">:</span>
                <span class="s2">raise </span><span class="s1">ValueError(</span><span class="s4">&quot;score: prop cannot be None&quot;</span><span class="s1">)</span>
            <span class="s2">return </span><span class="s1">self.svc_.score(X</span><span class="s2">, </span><span class="s1">y)</span>

    <span class="s1">X</span><span class="s2">, </span><span class="s1">y = load_iris(return_X_y=</span><span class="s2">True</span><span class="s1">)</span>
    <span class="s2">with </span><span class="s1">pytest.raises(ValueError</span><span class="s2">, </span><span class="s1">match=</span><span class="s4">&quot;fit: prop cannot be None&quot;</span><span class="s1">):</span>
        <span class="s1">RFE(estimator=TestEstimator()).fit(X</span><span class="s2">, </span><span class="s1">y)</span>
    <span class="s2">with </span><span class="s1">pytest.raises(ValueError</span><span class="s2">, </span><span class="s1">match=</span><span class="s4">&quot;score: prop cannot be None&quot;</span><span class="s1">):</span>
        <span class="s1">RFE(estimator=TestEstimator()).fit(X</span><span class="s2">, </span><span class="s1">y</span><span class="s2">, </span><span class="s1">prop=</span><span class="s4">&quot;foo&quot;</span><span class="s1">).score(X</span><span class="s2">, </span><span class="s1">y)</span>

    <span class="s1">RFE(estimator=TestEstimator()).fit(X</span><span class="s2">, </span><span class="s1">y</span><span class="s2">, </span><span class="s1">prop=</span><span class="s4">&quot;foo&quot;</span><span class="s1">).score(X</span><span class="s2">, </span><span class="s1">y</span><span class="s2">, </span><span class="s1">prop=</span><span class="s4">&quot;foo&quot;</span><span class="s1">)</span>


<span class="s2">def </span><span class="s1">test_rfe_percent_n_features():</span>
    <span class="s5"># test that the results are the same</span>
    <span class="s1">generator = check_random_state(</span><span class="s3">0</span><span class="s1">)</span>
    <span class="s1">iris = load_iris()</span>
    <span class="s5"># Add some irrelevant features. Random seed is set to make sure that</span>
    <span class="s5"># irrelevant features are always irrelevant.</span>
    <span class="s1">X = np.c_[iris.data</span><span class="s2">, </span><span class="s1">generator.normal(size=(len(iris.data)</span><span class="s2">, </span><span class="s3">6</span><span class="s1">))]</span>
    <span class="s1">y = iris.target</span>
    <span class="s5"># there are 10 features in the data. We select 40%.</span>
    <span class="s1">clf = SVC(kernel=</span><span class="s4">&quot;linear&quot;</span><span class="s1">)</span>
    <span class="s1">rfe_num = RFE(estimator=clf</span><span class="s2">, </span><span class="s1">n_features_to_select=</span><span class="s3">4</span><span class="s2">, </span><span class="s1">step=</span><span class="s3">0.1</span><span class="s1">)</span>
    <span class="s1">rfe_num.fit(X</span><span class="s2">, </span><span class="s1">y)</span>

    <span class="s1">rfe_perc = RFE(estimator=clf</span><span class="s2">, </span><span class="s1">n_features_to_select=</span><span class="s3">0.4</span><span class="s2">, </span><span class="s1">step=</span><span class="s3">0.1</span><span class="s1">)</span>
    <span class="s1">rfe_perc.fit(X</span><span class="s2">, </span><span class="s1">y)</span>

    <span class="s1">assert_array_equal(rfe_perc.ranking_</span><span class="s2">, </span><span class="s1">rfe_num.ranking_)</span>
    <span class="s1">assert_array_equal(rfe_perc.support_</span><span class="s2">, </span><span class="s1">rfe_num.support_)</span>


<span class="s2">def </span><span class="s1">test_rfe_mockclassifier():</span>
    <span class="s1">generator = check_random_state(</span><span class="s3">0</span><span class="s1">)</span>
    <span class="s1">iris = load_iris()</span>
    <span class="s5"># Add some irrelevant features. Random seed is set to make sure that</span>
    <span class="s5"># irrelevant features are always irrelevant.</span>
    <span class="s1">X = np.c_[iris.data</span><span class="s2">, </span><span class="s1">generator.normal(size=(len(iris.data)</span><span class="s2">, </span><span class="s3">6</span><span class="s1">))]</span>
    <span class="s1">y = iris.target</span>

    <span class="s5"># dense model</span>
    <span class="s1">clf = MockClassifier()</span>
    <span class="s1">rfe = RFE(estimator=clf</span><span class="s2">, </span><span class="s1">n_features_to_select=</span><span class="s3">4</span><span class="s2">, </span><span class="s1">step=</span><span class="s3">0.1</span><span class="s1">)</span>
    <span class="s1">rfe.fit(X</span><span class="s2">, </span><span class="s1">y)</span>
    <span class="s1">X_r = rfe.transform(X)</span>
    <span class="s1">clf.fit(X_r</span><span class="s2">, </span><span class="s1">y)</span>
    <span class="s2">assert </span><span class="s1">len(rfe.ranking_) == X.shape[</span><span class="s3">1</span><span class="s1">]</span>
    <span class="s2">assert </span><span class="s1">X_r.shape == iris.data.shape</span>


<span class="s2">def </span><span class="s1">test_rfecv():</span>
    <span class="s1">generator = check_random_state(</span><span class="s3">0</span><span class="s1">)</span>
    <span class="s1">iris = load_iris()</span>
    <span class="s5"># Add some irrelevant features. Random seed is set to make sure that</span>
    <span class="s5"># irrelevant features are always irrelevant.</span>
    <span class="s1">X = np.c_[iris.data</span><span class="s2">, </span><span class="s1">generator.normal(size=(len(iris.data)</span><span class="s2">, </span><span class="s3">6</span><span class="s1">))]</span>
    <span class="s1">y = list(iris.target)  </span><span class="s5"># regression test: list should be supported</span>

    <span class="s5"># Test using the score function</span>
    <span class="s1">rfecv = RFECV(estimator=SVC(kernel=</span><span class="s4">&quot;linear&quot;</span><span class="s1">)</span><span class="s2">, </span><span class="s1">step=</span><span class="s3">1</span><span class="s1">)</span>
    <span class="s1">rfecv.fit(X</span><span class="s2">, </span><span class="s1">y)</span>
    <span class="s5"># non-regression test for missing worst feature:</span>

    <span class="s2">for </span><span class="s1">key </span><span class="s2">in </span><span class="s1">rfecv.cv_results_.keys():</span>
        <span class="s2">assert </span><span class="s1">len(rfecv.cv_results_[key]) == X.shape[</span><span class="s3">1</span><span class="s1">]</span>

    <span class="s2">assert </span><span class="s1">len(rfecv.ranking_) == X.shape[</span><span class="s3">1</span><span class="s1">]</span>
    <span class="s1">X_r = rfecv.transform(X)</span>

    <span class="s5"># All the noisy variable were filtered out</span>
    <span class="s1">assert_array_equal(X_r</span><span class="s2">, </span><span class="s1">iris.data)</span>

    <span class="s5"># same in sparse</span>
    <span class="s1">rfecv_sparse = RFECV(estimator=SVC(kernel=</span><span class="s4">&quot;linear&quot;</span><span class="s1">)</span><span class="s2">, </span><span class="s1">step=</span><span class="s3">1</span><span class="s1">)</span>
    <span class="s1">X_sparse = sparse.csr_matrix(X)</span>
    <span class="s1">rfecv_sparse.fit(X_sparse</span><span class="s2">, </span><span class="s1">y)</span>
    <span class="s1">X_r_sparse = rfecv_sparse.transform(X_sparse)</span>
    <span class="s1">assert_array_equal(X_r_sparse.toarray()</span><span class="s2">, </span><span class="s1">iris.data)</span>

    <span class="s5"># Test using a customized loss function</span>
    <span class="s1">scoring = make_scorer(zero_one_loss</span><span class="s2">, </span><span class="s1">greater_is_better=</span><span class="s2">False</span><span class="s1">)</span>
    <span class="s1">rfecv = RFECV(estimator=SVC(kernel=</span><span class="s4">&quot;linear&quot;</span><span class="s1">)</span><span class="s2">, </span><span class="s1">step=</span><span class="s3">1</span><span class="s2">, </span><span class="s1">scoring=scoring)</span>
    <span class="s1">ignore_warnings(rfecv.fit)(X</span><span class="s2">, </span><span class="s1">y)</span>
    <span class="s1">X_r = rfecv.transform(X)</span>
    <span class="s1">assert_array_equal(X_r</span><span class="s2">, </span><span class="s1">iris.data)</span>

    <span class="s5"># Test using a scorer</span>
    <span class="s1">scorer = get_scorer(</span><span class="s4">&quot;accuracy&quot;</span><span class="s1">)</span>
    <span class="s1">rfecv = RFECV(estimator=SVC(kernel=</span><span class="s4">&quot;linear&quot;</span><span class="s1">)</span><span class="s2">, </span><span class="s1">step=</span><span class="s3">1</span><span class="s2">, </span><span class="s1">scoring=scorer)</span>
    <span class="s1">rfecv.fit(X</span><span class="s2">, </span><span class="s1">y)</span>
    <span class="s1">X_r = rfecv.transform(X)</span>
    <span class="s1">assert_array_equal(X_r</span><span class="s2">, </span><span class="s1">iris.data)</span>

    <span class="s5"># Test fix on cv_results_</span>
    <span class="s2">def </span><span class="s1">test_scorer(estimator</span><span class="s2">, </span><span class="s1">X</span><span class="s2">, </span><span class="s1">y):</span>
        <span class="s2">return </span><span class="s3">1.0</span>

    <span class="s1">rfecv = RFECV(estimator=SVC(kernel=</span><span class="s4">&quot;linear&quot;</span><span class="s1">)</span><span class="s2">, </span><span class="s1">step=</span><span class="s3">1</span><span class="s2">, </span><span class="s1">scoring=test_scorer)</span>
    <span class="s1">rfecv.fit(X</span><span class="s2">, </span><span class="s1">y)</span>

    <span class="s5"># In the event of cross validation score ties, the expected behavior of</span>
    <span class="s5"># RFECV is to return the FEWEST features that maximize the CV score.</span>
    <span class="s5"># Because test_scorer always returns 1.0 in this example, RFECV should</span>
    <span class="s5"># reduce the dimensionality to a single feature (i.e. n_features_ = 1)</span>
    <span class="s2">assert </span><span class="s1">rfecv.n_features_ == </span><span class="s3">1</span>

    <span class="s5"># Same as the first two tests, but with step=2</span>
    <span class="s1">rfecv = RFECV(estimator=SVC(kernel=</span><span class="s4">&quot;linear&quot;</span><span class="s1">)</span><span class="s2">, </span><span class="s1">step=</span><span class="s3">2</span><span class="s1">)</span>
    <span class="s1">rfecv.fit(X</span><span class="s2">, </span><span class="s1">y)</span>

    <span class="s2">for </span><span class="s1">key </span><span class="s2">in </span><span class="s1">rfecv.cv_results_.keys():</span>
        <span class="s2">assert </span><span class="s1">len(rfecv.cv_results_[key]) == </span><span class="s3">6</span>

    <span class="s2">assert </span><span class="s1">len(rfecv.ranking_) == X.shape[</span><span class="s3">1</span><span class="s1">]</span>
    <span class="s1">X_r = rfecv.transform(X)</span>
    <span class="s1">assert_array_equal(X_r</span><span class="s2">, </span><span class="s1">iris.data)</span>

    <span class="s1">rfecv_sparse = RFECV(estimator=SVC(kernel=</span><span class="s4">&quot;linear&quot;</span><span class="s1">)</span><span class="s2">, </span><span class="s1">step=</span><span class="s3">2</span><span class="s1">)</span>
    <span class="s1">X_sparse = sparse.csr_matrix(X)</span>
    <span class="s1">rfecv_sparse.fit(X_sparse</span><span class="s2">, </span><span class="s1">y)</span>
    <span class="s1">X_r_sparse = rfecv_sparse.transform(X_sparse)</span>
    <span class="s1">assert_array_equal(X_r_sparse.toarray()</span><span class="s2">, </span><span class="s1">iris.data)</span>

    <span class="s5"># Verifying that steps &lt; 1 don't blow up.</span>
    <span class="s1">rfecv_sparse = RFECV(estimator=SVC(kernel=</span><span class="s4">&quot;linear&quot;</span><span class="s1">)</span><span class="s2">, </span><span class="s1">step=</span><span class="s3">0.2</span><span class="s1">)</span>
    <span class="s1">X_sparse = sparse.csr_matrix(X)</span>
    <span class="s1">rfecv_sparse.fit(X_sparse</span><span class="s2">, </span><span class="s1">y)</span>
    <span class="s1">X_r_sparse = rfecv_sparse.transform(X_sparse)</span>
    <span class="s1">assert_array_equal(X_r_sparse.toarray()</span><span class="s2">, </span><span class="s1">iris.data)</span>


<span class="s2">def </span><span class="s1">test_rfecv_mockclassifier():</span>
    <span class="s1">generator = check_random_state(</span><span class="s3">0</span><span class="s1">)</span>
    <span class="s1">iris = load_iris()</span>
    <span class="s1">X = np.c_[iris.data</span><span class="s2">, </span><span class="s1">generator.normal(size=(len(iris.data)</span><span class="s2">, </span><span class="s3">6</span><span class="s1">))]</span>
    <span class="s1">y = list(iris.target)  </span><span class="s5"># regression test: list should be supported</span>

    <span class="s5"># Test using the score function</span>
    <span class="s1">rfecv = RFECV(estimator=MockClassifier()</span><span class="s2">, </span><span class="s1">step=</span><span class="s3">1</span><span class="s1">)</span>
    <span class="s1">rfecv.fit(X</span><span class="s2">, </span><span class="s1">y)</span>
    <span class="s5"># non-regression test for missing worst feature:</span>

    <span class="s2">for </span><span class="s1">key </span><span class="s2">in </span><span class="s1">rfecv.cv_results_.keys():</span>
        <span class="s2">assert </span><span class="s1">len(rfecv.cv_results_[key]) == X.shape[</span><span class="s3">1</span><span class="s1">]</span>

    <span class="s2">assert </span><span class="s1">len(rfecv.ranking_) == X.shape[</span><span class="s3">1</span><span class="s1">]</span>


<span class="s2">def </span><span class="s1">test_rfecv_verbose_output():</span>
    <span class="s5"># Check verbose=1 is producing an output.</span>
    <span class="s2">import </span><span class="s1">sys</span>
    <span class="s2">from </span><span class="s1">io </span><span class="s2">import </span><span class="s1">StringIO</span>

    <span class="s1">sys.stdout = StringIO()</span>

    <span class="s1">generator = check_random_state(</span><span class="s3">0</span><span class="s1">)</span>
    <span class="s1">iris = load_iris()</span>
    <span class="s1">X = np.c_[iris.data</span><span class="s2">, </span><span class="s1">generator.normal(size=(len(iris.data)</span><span class="s2">, </span><span class="s3">6</span><span class="s1">))]</span>
    <span class="s1">y = list(iris.target)</span>

    <span class="s1">rfecv = RFECV(estimator=SVC(kernel=</span><span class="s4">&quot;linear&quot;</span><span class="s1">)</span><span class="s2">, </span><span class="s1">step=</span><span class="s3">1</span><span class="s2">, </span><span class="s1">verbose=</span><span class="s3">1</span><span class="s1">)</span>
    <span class="s1">rfecv.fit(X</span><span class="s2">, </span><span class="s1">y)</span>

    <span class="s1">verbose_output = sys.stdout</span>
    <span class="s1">verbose_output.seek(</span><span class="s3">0</span><span class="s1">)</span>
    <span class="s2">assert </span><span class="s1">len(verbose_output.readline()) &gt; </span><span class="s3">0</span>


<span class="s2">def </span><span class="s1">test_rfecv_cv_results_size(global_random_seed):</span>
    <span class="s1">generator = check_random_state(global_random_seed)</span>
    <span class="s1">iris = load_iris()</span>
    <span class="s1">X = np.c_[iris.data</span><span class="s2">, </span><span class="s1">generator.normal(size=(len(iris.data)</span><span class="s2">, </span><span class="s3">6</span><span class="s1">))]</span>
    <span class="s1">y = list(iris.target)  </span><span class="s5"># regression test: list should be supported</span>

    <span class="s5"># Non-regression test for varying combinations of step and</span>
    <span class="s5"># min_features_to_select.</span>
    <span class="s2">for </span><span class="s1">step</span><span class="s2">, </span><span class="s1">min_features_to_select </span><span class="s2">in </span><span class="s1">[[</span><span class="s3">2</span><span class="s2">, </span><span class="s3">1</span><span class="s1">]</span><span class="s2">, </span><span class="s1">[</span><span class="s3">2</span><span class="s2">, </span><span class="s3">2</span><span class="s1">]</span><span class="s2">, </span><span class="s1">[</span><span class="s3">3</span><span class="s2">, </span><span class="s3">3</span><span class="s1">]]:</span>
        <span class="s1">rfecv = RFECV(</span>
            <span class="s1">estimator=MockClassifier()</span><span class="s2">,</span>
            <span class="s1">step=step</span><span class="s2">,</span>
            <span class="s1">min_features_to_select=min_features_to_select</span><span class="s2">,</span>
        <span class="s1">)</span>
        <span class="s1">rfecv.fit(X</span><span class="s2">, </span><span class="s1">y)</span>

        <span class="s1">score_len = np.ceil((X.shape[</span><span class="s3">1</span><span class="s1">] - min_features_to_select) / step) + </span><span class="s3">1</span>

        <span class="s2">for </span><span class="s1">key </span><span class="s2">in </span><span class="s1">rfecv.cv_results_.keys():</span>
            <span class="s2">assert </span><span class="s1">len(rfecv.cv_results_[key]) == score_len</span>

        <span class="s2">assert </span><span class="s1">len(rfecv.ranking_) == X.shape[</span><span class="s3">1</span><span class="s1">]</span>
        <span class="s2">assert </span><span class="s1">rfecv.n_features_ &gt;= min_features_to_select</span>


<span class="s2">def </span><span class="s1">test_rfe_estimator_tags():</span>
    <span class="s1">rfe = RFE(SVC(kernel=</span><span class="s4">&quot;linear&quot;</span><span class="s1">))</span>
    <span class="s2">assert </span><span class="s1">rfe._estimator_type == </span><span class="s4">&quot;classifier&quot;</span>
    <span class="s5"># make sure that cross-validation is stratified</span>
    <span class="s1">iris = load_iris()</span>
    <span class="s1">score = cross_val_score(rfe</span><span class="s2">, </span><span class="s1">iris.data</span><span class="s2">, </span><span class="s1">iris.target)</span>
    <span class="s2">assert </span><span class="s1">score.min() &gt; </span><span class="s3">0.7</span>


<span class="s2">def </span><span class="s1">test_rfe_min_step(global_random_seed):</span>
    <span class="s1">n_features = </span><span class="s3">10</span>
    <span class="s1">X</span><span class="s2">, </span><span class="s1">y = make_friedman1(</span>
        <span class="s1">n_samples=</span><span class="s3">50</span><span class="s2">, </span><span class="s1">n_features=n_features</span><span class="s2">, </span><span class="s1">random_state=global_random_seed</span>
    <span class="s1">)</span>
    <span class="s1">n_samples</span><span class="s2">, </span><span class="s1">n_features = X.shape</span>
    <span class="s1">estimator = SVR(kernel=</span><span class="s4">&quot;linear&quot;</span><span class="s1">)</span>

    <span class="s5"># Test when floor(step * n_features) &lt;= 0</span>
    <span class="s1">selector = RFE(estimator</span><span class="s2">, </span><span class="s1">step=</span><span class="s3">0.01</span><span class="s1">)</span>
    <span class="s1">sel = selector.fit(X</span><span class="s2">, </span><span class="s1">y)</span>
    <span class="s2">assert </span><span class="s1">sel.support_.sum() == n_features // </span><span class="s3">2</span>

    <span class="s5"># Test when step is between (0,1) and floor(step * n_features) &gt; 0</span>
    <span class="s1">selector = RFE(estimator</span><span class="s2">, </span><span class="s1">step=</span><span class="s3">0.20</span><span class="s1">)</span>
    <span class="s1">sel = selector.fit(X</span><span class="s2">, </span><span class="s1">y)</span>
    <span class="s2">assert </span><span class="s1">sel.support_.sum() == n_features // </span><span class="s3">2</span>

    <span class="s5"># Test when step is an integer</span>
    <span class="s1">selector = RFE(estimator</span><span class="s2">, </span><span class="s1">step=</span><span class="s3">5</span><span class="s1">)</span>
    <span class="s1">sel = selector.fit(X</span><span class="s2">, </span><span class="s1">y)</span>
    <span class="s2">assert </span><span class="s1">sel.support_.sum() == n_features // </span><span class="s3">2</span>


<span class="s2">def </span><span class="s1">test_number_of_subsets_of_features(global_random_seed):</span>
    <span class="s5"># In RFE, 'number_of_subsets_of_features'</span>
    <span class="s5"># = the number of iterations in '_fit'</span>
    <span class="s5"># = max(ranking_)</span>
    <span class="s5"># = 1 + (n_features + step - n_features_to_select - 1) // step</span>
    <span class="s5"># After optimization #4534, this number</span>
    <span class="s5"># = 1 + np.ceil((n_features - n_features_to_select) / float(step))</span>
    <span class="s5"># This test case is to test their equivalence, refer to #4534 and #3824</span>

    <span class="s2">def </span><span class="s1">formula1(n_features</span><span class="s2">, </span><span class="s1">n_features_to_select</span><span class="s2">, </span><span class="s1">step):</span>
        <span class="s2">return </span><span class="s3">1 </span><span class="s1">+ ((n_features + step - n_features_to_select - </span><span class="s3">1</span><span class="s1">) // step)</span>

    <span class="s2">def </span><span class="s1">formula2(n_features</span><span class="s2">, </span><span class="s1">n_features_to_select</span><span class="s2">, </span><span class="s1">step):</span>
        <span class="s2">return </span><span class="s3">1 </span><span class="s1">+ np.ceil((n_features - n_features_to_select) / float(step))</span>

    <span class="s5"># RFE</span>
    <span class="s5"># Case 1, n_features - n_features_to_select is divisible by step</span>
    <span class="s5"># Case 2, n_features - n_features_to_select is not divisible by step</span>
    <span class="s1">n_features_list = [</span><span class="s3">11</span><span class="s2">, </span><span class="s3">11</span><span class="s1">]</span>
    <span class="s1">n_features_to_select_list = [</span><span class="s3">3</span><span class="s2">, </span><span class="s3">3</span><span class="s1">]</span>
    <span class="s1">step_list = [</span><span class="s3">2</span><span class="s2">, </span><span class="s3">3</span><span class="s1">]</span>
    <span class="s2">for </span><span class="s1">n_features</span><span class="s2">, </span><span class="s1">n_features_to_select</span><span class="s2">, </span><span class="s1">step </span><span class="s2">in </span><span class="s1">zip(</span>
        <span class="s1">n_features_list</span><span class="s2">, </span><span class="s1">n_features_to_select_list</span><span class="s2">, </span><span class="s1">step_list</span>
    <span class="s1">):</span>
        <span class="s1">generator = check_random_state(global_random_seed)</span>
        <span class="s1">X = generator.normal(size=(</span><span class="s3">100</span><span class="s2">, </span><span class="s1">n_features))</span>
        <span class="s1">y = generator.rand(</span><span class="s3">100</span><span class="s1">).round()</span>
        <span class="s1">rfe = RFE(</span>
            <span class="s1">estimator=SVC(kernel=</span><span class="s4">&quot;linear&quot;</span><span class="s1">)</span><span class="s2">,</span>
            <span class="s1">n_features_to_select=n_features_to_select</span><span class="s2">,</span>
            <span class="s1">step=step</span><span class="s2">,</span>
        <span class="s1">)</span>
        <span class="s1">rfe.fit(X</span><span class="s2">, </span><span class="s1">y)</span>
        <span class="s5"># this number also equals to the maximum of ranking_</span>
        <span class="s2">assert </span><span class="s1">np.max(rfe.ranking_) == formula1(n_features</span><span class="s2">, </span><span class="s1">n_features_to_select</span><span class="s2">, </span><span class="s1">step)</span>
        <span class="s2">assert </span><span class="s1">np.max(rfe.ranking_) == formula2(n_features</span><span class="s2">, </span><span class="s1">n_features_to_select</span><span class="s2">, </span><span class="s1">step)</span>

    <span class="s5"># In RFECV, 'fit' calls 'RFE._fit'</span>
    <span class="s5"># 'number_of_subsets_of_features' of RFE</span>
    <span class="s5"># = the size of each score in 'cv_results_' of RFECV</span>
    <span class="s5"># = the number of iterations of the for loop before optimization #4534</span>

    <span class="s5"># RFECV, n_features_to_select = 1</span>
    <span class="s5"># Case 1, n_features - 1 is divisible by step</span>
    <span class="s5"># Case 2, n_features - 1 is not divisible by step</span>

    <span class="s1">n_features_to_select = </span><span class="s3">1</span>
    <span class="s1">n_features_list = [</span><span class="s3">11</span><span class="s2">, </span><span class="s3">10</span><span class="s1">]</span>
    <span class="s1">step_list = [</span><span class="s3">2</span><span class="s2">, </span><span class="s3">2</span><span class="s1">]</span>
    <span class="s2">for </span><span class="s1">n_features</span><span class="s2">, </span><span class="s1">step </span><span class="s2">in </span><span class="s1">zip(n_features_list</span><span class="s2">, </span><span class="s1">step_list):</span>
        <span class="s1">generator = check_random_state(global_random_seed)</span>
        <span class="s1">X = generator.normal(size=(</span><span class="s3">100</span><span class="s2">, </span><span class="s1">n_features))</span>
        <span class="s1">y = generator.rand(</span><span class="s3">100</span><span class="s1">).round()</span>
        <span class="s1">rfecv = RFECV(estimator=SVC(kernel=</span><span class="s4">&quot;linear&quot;</span><span class="s1">)</span><span class="s2">, </span><span class="s1">step=step)</span>
        <span class="s1">rfecv.fit(X</span><span class="s2">, </span><span class="s1">y)</span>

        <span class="s2">for </span><span class="s1">key </span><span class="s2">in </span><span class="s1">rfecv.cv_results_.keys():</span>
            <span class="s2">assert </span><span class="s1">len(rfecv.cv_results_[key]) == formula1(</span>
                <span class="s1">n_features</span><span class="s2">, </span><span class="s1">n_features_to_select</span><span class="s2">, </span><span class="s1">step</span>
            <span class="s1">)</span>
            <span class="s2">assert </span><span class="s1">len(rfecv.cv_results_[key]) == formula2(</span>
                <span class="s1">n_features</span><span class="s2">, </span><span class="s1">n_features_to_select</span><span class="s2">, </span><span class="s1">step</span>
            <span class="s1">)</span>


<span class="s2">def </span><span class="s1">test_rfe_cv_n_jobs(global_random_seed):</span>
    <span class="s1">generator = check_random_state(global_random_seed)</span>
    <span class="s1">iris = load_iris()</span>
    <span class="s1">X = np.c_[iris.data</span><span class="s2">, </span><span class="s1">generator.normal(size=(len(iris.data)</span><span class="s2">, </span><span class="s3">6</span><span class="s1">))]</span>
    <span class="s1">y = iris.target</span>

    <span class="s1">rfecv = RFECV(estimator=SVC(kernel=</span><span class="s4">&quot;linear&quot;</span><span class="s1">))</span>
    <span class="s1">rfecv.fit(X</span><span class="s2">, </span><span class="s1">y)</span>
    <span class="s1">rfecv_ranking = rfecv.ranking_</span>

    <span class="s1">rfecv_cv_results_ = rfecv.cv_results_</span>

    <span class="s1">rfecv.set_params(n_jobs=</span><span class="s3">2</span><span class="s1">)</span>
    <span class="s1">rfecv.fit(X</span><span class="s2">, </span><span class="s1">y)</span>
    <span class="s1">assert_array_almost_equal(rfecv.ranking_</span><span class="s2">, </span><span class="s1">rfecv_ranking)</span>

    <span class="s2">assert </span><span class="s1">rfecv_cv_results_.keys() == rfecv.cv_results_.keys()</span>
    <span class="s2">for </span><span class="s1">key </span><span class="s2">in </span><span class="s1">rfecv_cv_results_.keys():</span>
        <span class="s2">assert </span><span class="s1">rfecv_cv_results_[key] == pytest.approx(rfecv.cv_results_[key])</span>


<span class="s2">def </span><span class="s1">test_rfe_cv_groups():</span>
    <span class="s1">generator = check_random_state(</span><span class="s3">0</span><span class="s1">)</span>
    <span class="s1">iris = load_iris()</span>
    <span class="s1">number_groups = </span><span class="s3">4</span>
    <span class="s1">groups = np.floor(np.linspace(</span><span class="s3">0</span><span class="s2">, </span><span class="s1">number_groups</span><span class="s2">, </span><span class="s1">len(iris.target)))</span>
    <span class="s1">X = iris.data</span>
    <span class="s1">y = (iris.target &gt; </span><span class="s3">0</span><span class="s1">).astype(int)</span>

    <span class="s1">est_groups = RFECV(</span>
        <span class="s1">estimator=RandomForestClassifier(random_state=generator)</span><span class="s2">,</span>
        <span class="s1">step=</span><span class="s3">1</span><span class="s2">,</span>
        <span class="s1">scoring=</span><span class="s4">&quot;accuracy&quot;</span><span class="s2">,</span>
        <span class="s1">cv=GroupKFold(n_splits=</span><span class="s3">2</span><span class="s1">)</span><span class="s2">,</span>
    <span class="s1">)</span>
    <span class="s1">est_groups.fit(X</span><span class="s2">, </span><span class="s1">y</span><span class="s2">, </span><span class="s1">groups=groups)</span>
    <span class="s2">assert </span><span class="s1">est_groups.n_features_ &gt; </span><span class="s3">0</span>


<span class="s1">@pytest.mark.parametrize(</span>
    <span class="s4">&quot;importance_getter&quot;</span><span class="s2">, </span><span class="s1">[attrgetter(</span><span class="s4">&quot;regressor_.coef_&quot;</span><span class="s1">)</span><span class="s2">, </span><span class="s4">&quot;regressor_.coef_&quot;</span><span class="s1">]</span>
<span class="s1">)</span>
<span class="s1">@pytest.mark.parametrize(</span><span class="s4">&quot;selector, expected_n_features&quot;</span><span class="s2">, </span><span class="s1">[(RFE</span><span class="s2">, </span><span class="s3">5</span><span class="s1">)</span><span class="s2">, </span><span class="s1">(RFECV</span><span class="s2">, </span><span class="s3">4</span><span class="s1">)])</span>
<span class="s2">def </span><span class="s1">test_rfe_wrapped_estimator(importance_getter</span><span class="s2">, </span><span class="s1">selector</span><span class="s2">, </span><span class="s1">expected_n_features):</span>
    <span class="s5"># Non-regression test for</span>
    <span class="s5"># https://github.com/scikit-learn/scikit-learn/issues/15312</span>
    <span class="s1">X</span><span class="s2">, </span><span class="s1">y = make_friedman1(n_samples=</span><span class="s3">50</span><span class="s2">, </span><span class="s1">n_features=</span><span class="s3">10</span><span class="s2">, </span><span class="s1">random_state=</span><span class="s3">0</span><span class="s1">)</span>
    <span class="s1">estimator = LinearSVR(dual=</span><span class="s4">&quot;auto&quot;</span><span class="s2">, </span><span class="s1">random_state=</span><span class="s3">0</span><span class="s1">)</span>

    <span class="s1">log_estimator = TransformedTargetRegressor(</span>
        <span class="s1">regressor=estimator</span><span class="s2">, </span><span class="s1">func=np.log</span><span class="s2">, </span><span class="s1">inverse_func=np.exp</span>
    <span class="s1">)</span>

    <span class="s1">selector = selector(log_estimator</span><span class="s2">, </span><span class="s1">importance_getter=importance_getter)</span>
    <span class="s1">sel = selector.fit(X</span><span class="s2">, </span><span class="s1">y)</span>
    <span class="s2">assert </span><span class="s1">sel.support_.sum() == expected_n_features</span>


<span class="s1">@pytest.mark.parametrize(</span>
    <span class="s4">&quot;importance_getter, err_type&quot;</span><span class="s2">,</span>
    <span class="s1">[</span>
        <span class="s1">(</span><span class="s4">&quot;auto&quot;</span><span class="s2">, </span><span class="s1">ValueError)</span><span class="s2">,</span>
        <span class="s1">(</span><span class="s4">&quot;random&quot;</span><span class="s2">, </span><span class="s1">AttributeError)</span><span class="s2">,</span>
        <span class="s1">(</span><span class="s2">lambda </span><span class="s1">x: x.importance</span><span class="s2">, </span><span class="s1">AttributeError)</span><span class="s2">,</span>
    <span class="s1">]</span><span class="s2">,</span>
<span class="s1">)</span>
<span class="s1">@pytest.mark.parametrize(</span><span class="s4">&quot;Selector&quot;</span><span class="s2">, </span><span class="s1">[RFE</span><span class="s2">, </span><span class="s1">RFECV])</span>
<span class="s2">def </span><span class="s1">test_rfe_importance_getter_validation(importance_getter</span><span class="s2">, </span><span class="s1">err_type</span><span class="s2">, </span><span class="s1">Selector):</span>
    <span class="s1">X</span><span class="s2">, </span><span class="s1">y = make_friedman1(n_samples=</span><span class="s3">50</span><span class="s2">, </span><span class="s1">n_features=</span><span class="s3">10</span><span class="s2">, </span><span class="s1">random_state=</span><span class="s3">42</span><span class="s1">)</span>
    <span class="s1">estimator = LinearSVR(dual=</span><span class="s4">&quot;auto&quot;</span><span class="s1">)</span>
    <span class="s1">log_estimator = TransformedTargetRegressor(</span>
        <span class="s1">regressor=estimator</span><span class="s2">, </span><span class="s1">func=np.log</span><span class="s2">, </span><span class="s1">inverse_func=np.exp</span>
    <span class="s1">)</span>

    <span class="s2">with </span><span class="s1">pytest.raises(err_type):</span>
        <span class="s1">model = Selector(log_estimator</span><span class="s2">, </span><span class="s1">importance_getter=importance_getter)</span>
        <span class="s1">model.fit(X</span><span class="s2">, </span><span class="s1">y)</span>


<span class="s1">@pytest.mark.parametrize(</span><span class="s4">&quot;cv&quot;</span><span class="s2">, </span><span class="s1">[</span><span class="s2">None, </span><span class="s3">5</span><span class="s1">])</span>
<span class="s2">def </span><span class="s1">test_rfe_allow_nan_inf_in_x(cv):</span>
    <span class="s1">iris = load_iris()</span>
    <span class="s1">X = iris.data</span>
    <span class="s1">y = iris.target</span>

    <span class="s5"># add nan and inf value to X</span>
    <span class="s1">X[</span><span class="s3">0</span><span class="s1">][</span><span class="s3">0</span><span class="s1">] = np.nan</span>
    <span class="s1">X[</span><span class="s3">0</span><span class="s1">][</span><span class="s3">1</span><span class="s1">] = np.inf</span>

    <span class="s1">clf = MockClassifier()</span>
    <span class="s2">if </span><span class="s1">cv </span><span class="s2">is not None</span><span class="s1">:</span>
        <span class="s1">rfe = RFECV(estimator=clf</span><span class="s2">, </span><span class="s1">cv=cv)</span>
    <span class="s2">else</span><span class="s1">:</span>
        <span class="s1">rfe = RFE(estimator=clf)</span>
    <span class="s1">rfe.fit(X</span><span class="s2">, </span><span class="s1">y)</span>
    <span class="s1">rfe.transform(X)</span>


<span class="s2">def </span><span class="s1">test_w_pipeline_2d_coef_():</span>
    <span class="s1">pipeline = make_pipeline(StandardScaler()</span><span class="s2">, </span><span class="s1">LogisticRegression())</span>

    <span class="s1">data</span><span class="s2">, </span><span class="s1">y = load_iris(return_X_y=</span><span class="s2">True</span><span class="s1">)</span>
    <span class="s1">sfm = RFE(</span>
        <span class="s1">pipeline</span><span class="s2">,</span>
        <span class="s1">n_features_to_select=</span><span class="s3">2</span><span class="s2">,</span>
        <span class="s1">importance_getter=</span><span class="s4">&quot;named_steps.logisticregression.coef_&quot;</span><span class="s2">,</span>
    <span class="s1">)</span>

    <span class="s1">sfm.fit(data</span><span class="s2">, </span><span class="s1">y)</span>
    <span class="s2">assert </span><span class="s1">sfm.transform(data).shape[</span><span class="s3">1</span><span class="s1">] == </span><span class="s3">2</span>


<span class="s2">def </span><span class="s1">test_rfecv_std_and_mean(global_random_seed):</span>
    <span class="s1">generator = check_random_state(global_random_seed)</span>
    <span class="s1">iris = load_iris()</span>
    <span class="s1">X = np.c_[iris.data</span><span class="s2">, </span><span class="s1">generator.normal(size=(len(iris.data)</span><span class="s2">, </span><span class="s3">6</span><span class="s1">))]</span>
    <span class="s1">y = iris.target</span>

    <span class="s1">rfecv = RFECV(estimator=SVC(kernel=</span><span class="s4">&quot;linear&quot;</span><span class="s1">))</span>
    <span class="s1">rfecv.fit(X</span><span class="s2">, </span><span class="s1">y)</span>
    <span class="s1">n_split_keys = len(rfecv.cv_results_) - </span><span class="s3">2</span>
    <span class="s1">split_keys = [</span><span class="s4">f&quot;split</span><span class="s2">{</span><span class="s1">i</span><span class="s2">}</span><span class="s4">_test_score&quot; </span><span class="s2">for </span><span class="s1">i </span><span class="s2">in </span><span class="s1">range(n_split_keys)]</span>

    <span class="s1">cv_scores = np.asarray([rfecv.cv_results_[key] </span><span class="s2">for </span><span class="s1">key </span><span class="s2">in </span><span class="s1">split_keys])</span>
    <span class="s1">expected_mean = np.mean(cv_scores</span><span class="s2">, </span><span class="s1">axis=</span><span class="s3">0</span><span class="s1">)</span>
    <span class="s1">expected_std = np.std(cv_scores</span><span class="s2">, </span><span class="s1">axis=</span><span class="s3">0</span><span class="s1">)</span>

    <span class="s1">assert_allclose(rfecv.cv_results_[</span><span class="s4">&quot;mean_test_score&quot;</span><span class="s1">]</span><span class="s2">, </span><span class="s1">expected_mean)</span>
    <span class="s1">assert_allclose(rfecv.cv_results_[</span><span class="s4">&quot;std_test_score&quot;</span><span class="s1">]</span><span class="s2">, </span><span class="s1">expected_std)</span>


<span class="s1">@pytest.mark.parametrize(</span><span class="s4">&quot;ClsRFE&quot;</span><span class="s2">, </span><span class="s1">[RFE</span><span class="s2">, </span><span class="s1">RFECV])</span>
<span class="s2">def </span><span class="s1">test_multioutput(ClsRFE):</span>
    <span class="s1">X = np.random.normal(size=(</span><span class="s3">10</span><span class="s2">, </span><span class="s3">3</span><span class="s1">))</span>
    <span class="s1">y = np.random.randint(</span><span class="s3">2</span><span class="s2">, </span><span class="s1">size=(</span><span class="s3">10</span><span class="s2">, </span><span class="s3">2</span><span class="s1">))</span>
    <span class="s1">clf = RandomForestClassifier(n_estimators=</span><span class="s3">5</span><span class="s1">)</span>
    <span class="s1">rfe_test = ClsRFE(clf)</span>
    <span class="s1">rfe_test.fit(X</span><span class="s2">, </span><span class="s1">y)</span>


<span class="s1">@pytest.mark.parametrize(</span><span class="s4">&quot;ClsRFE&quot;</span><span class="s2">, </span><span class="s1">[RFE</span><span class="s2">, </span><span class="s1">RFECV])</span>
<span class="s1">@pytest.mark.parametrize(</span><span class="s4">&quot;PLSEstimator&quot;</span><span class="s2">, </span><span class="s1">[CCA</span><span class="s2">, </span><span class="s1">PLSCanonical</span><span class="s2">, </span><span class="s1">PLSRegression])</span>
<span class="s2">def </span><span class="s1">test_rfe_pls(ClsRFE</span><span class="s2">, </span><span class="s1">PLSEstimator):</span>
    <span class="s0">&quot;&quot;&quot;Check the behaviour of RFE with PLS estimators. 
 
    Non-regression test for: 
    https://github.com/scikit-learn/scikit-learn/issues/12410 
    &quot;&quot;&quot;</span>
    <span class="s1">X</span><span class="s2">, </span><span class="s1">y = make_friedman1(n_samples=</span><span class="s3">50</span><span class="s2">, </span><span class="s1">n_features=</span><span class="s3">10</span><span class="s2">, </span><span class="s1">random_state=</span><span class="s3">0</span><span class="s1">)</span>
    <span class="s1">estimator = PLSEstimator(n_components=</span><span class="s3">1</span><span class="s1">)</span>
    <span class="s1">selector = ClsRFE(estimator</span><span class="s2">, </span><span class="s1">step=</span><span class="s3">1</span><span class="s1">).fit(X</span><span class="s2">, </span><span class="s1">y)</span>
    <span class="s2">assert </span><span class="s1">selector.score(X</span><span class="s2">, </span><span class="s1">y) &gt; </span><span class="s3">0.5</span>
</pre>
</body>
</html>