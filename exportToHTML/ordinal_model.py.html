<html>
<head>
<title>ordinal_model.py</title>
<meta http-equiv="Content-Type" content="text/html; charset=utf-8">
<style type="text/css">
.s0 { color: #808080;}
.s1 { color: #a9b7c6;}
.s2 { color: #629755; font-style: italic;}
.s3 { color: #cc7832;}
.s4 { color: #6a8759;}
.s5 { color: #6897bb;}
</style>
</head>
<body bgcolor="#2b2b2b">
<table CELLSPACING=0 CELLPADDING=5 COLS=1 WIDTH="100%" BGCOLOR="#606060" >
<tr><td><center>
<font face="Arial, Helvetica" color="#000000">
ordinal_model.py</font>
</center></td></tr></table>
<pre><span class="s0"># -*- coding: utf-8 -*-</span>
<span class="s2">&quot;&quot;&quot; 
Created on Sat Aug 22 20:24:42 2015 
 
Author: Josef Perktold 
License: BSD-3 
&quot;&quot;&quot;</span>

<span class="s3">import </span><span class="s1">warnings</span>

<span class="s3">from </span><span class="s1">statsmodels.compat.pandas </span><span class="s3">import </span><span class="s1">Appender</span>

<span class="s3">import </span><span class="s1">numpy </span><span class="s3">as </span><span class="s1">np</span>
<span class="s3">import </span><span class="s1">pandas </span><span class="s3">as </span><span class="s1">pd</span>
<span class="s3">from </span><span class="s1">pandas.api.types </span><span class="s3">import </span><span class="s1">CategoricalDtype</span>
<span class="s3">from </span><span class="s1">scipy </span><span class="s3">import </span><span class="s1">stats</span>

<span class="s3">from </span><span class="s1">statsmodels.base.model </span><span class="s3">import </span><span class="s1">(</span>
    <span class="s1">Model</span><span class="s3">,</span>
    <span class="s1">LikelihoodModel</span><span class="s3">,</span>
    <span class="s1">GenericLikelihoodModel</span><span class="s3">,</span>
    <span class="s1">GenericLikelihoodModelResults</span><span class="s3">,</span>
<span class="s1">)</span>
<span class="s3">import </span><span class="s1">statsmodels.base.wrapper </span><span class="s3">as </span><span class="s1">wrap</span>
<span class="s0"># for results wrapper:</span>
<span class="s3">import </span><span class="s1">statsmodels.regression.linear_model </span><span class="s3">as </span><span class="s1">lm</span>
<span class="s3">from </span><span class="s1">statsmodels.tools.decorators </span><span class="s3">import </span><span class="s1">cache_readonly</span>


<span class="s3">class </span><span class="s1">OrderedModel(GenericLikelihoodModel):</span>
    <span class="s2">&quot;&quot;&quot;Ordinal Model based on logistic or normal distribution 
 
    The parameterization corresponds to the proportional odds model in the 
    logistic case. 
    The model assumes that the endogenous variable is ordered but that the 
    labels have no numeric interpretation besides the ordering. 
 
    The model is based on a latent linear variable, where we observe only a 
    discretization. 
 
    y_latent = X beta + u 
 
    The observed variable is defined by the interval 
 
    y = {0 if y_latent &lt;= cut_0 
         1 of cut_0 &lt; y_latent &lt;= cut_1 
         ... 
         K if cut_K &lt; y_latent 
 
    The probability of observing y=k conditional on the explanatory variables 
    X is given by 
 
    prob(y = k | x) = Prob(cut_k &lt; y_latent &lt;= cut_k+1) 
                    = Prob(cut_k - x beta &lt; u &lt;= cut_k+1 - x beta 
                    = F(cut_k+1 - x beta) - F(cut_k - x beta) 
 
    Where F is the cumulative distribution of u which is either the normal 
    or the logistic distribution, but can be set to any other continuous 
    distribution. We use standardized distributions to avoid identifiability 
    problems. 
 
    Parameters 
    ---------- 
    endog : array_like 
        Endogenous or dependent ordered categorical variable with k levels. 
        Labels or values of endog will internally transformed to consecutive 
        integers, 0, 1, 2, ... 
        pd.Series with ordered Categorical as dtype should be preferred as it 
        gives the order relation between the levels. 
        If endog is not a pandas Categorical, then categories are 
        sorted in lexicographic order (by numpy.unique). 
    exog : array_like 
        Exogenous, explanatory variables. This should not include an intercept. 
        pd.DataFrame are also accepted. 
        see Notes about constant when using formulas 
    offset : array_like 
        Offset is added to the linear prediction with coefficient equal to 1. 
    distr : string 'probit' or 'logit', or a distribution instance 
        The default is currently 'probit' which uses the normal distribution 
        and corresponds to an ordered Probit model. The distribution is 
        assumed to have the main methods of scipy.stats distributions, mainly 
        cdf, pdf and ppf. The inverse cdf, ppf, is only use to calculate 
        starting values. 
 
    Notes 
    ----- 
    Status: experimental, core results are verified, still subclasses 
    `GenericLikelihoodModel` which will change in future versions. 
 
    The parameterization of OrderedModel requires that there is no constant in 
    the model, neither explicit nor implicit. The constant is equivalent to 
    shifting all thresholds and is therefore not separately identified. 
 
    Patsy's formula specification does not allow a design matrix without 
    explicit or implicit constant if there are categorical variables (or maybe 
    splines) among explanatory variables. As workaround, statsmodels removes an 
    explicit intercept. 
 
    Consequently, there are two valid cases to get a design matrix without 
    intercept when using formulas: 
 
    - specify a model without explicit and implicit intercept which is possible 
      if there are only numerical variables in the model. 
    - specify a model with an explicit intercept which statsmodels will remove. 
 
    Models with an implicit intercept will be overparameterized, the parameter 
    estimates will not be fully identified, cov_params will not be invertible 
    and standard errors might contain nans. The computed results will be 
    dominated by numerical imprecision coming mainly from convergence tolerance 
    and numerical derivatives. 
 
    The model will raise a ValueError if a remaining constant is detected. 
 
    &quot;&quot;&quot;</span>
    <span class="s1">_formula_max_endog = np.inf</span>

    <span class="s3">def </span><span class="s1">__init__(self</span><span class="s3">, </span><span class="s1">endog</span><span class="s3">, </span><span class="s1">exog</span><span class="s3">, </span><span class="s1">offset=</span><span class="s3">None, </span><span class="s1">distr=</span><span class="s4">'probit'</span><span class="s3">, </span><span class="s1">**kwds):</span>

        <span class="s3">if </span><span class="s1">distr == </span><span class="s4">'probit'</span><span class="s1">:</span>
            <span class="s1">self.distr = stats.norm</span>
        <span class="s3">elif </span><span class="s1">distr == </span><span class="s4">'logit'</span><span class="s1">:</span>
            <span class="s1">self.distr = stats.logistic</span>
        <span class="s3">else</span><span class="s1">:</span>
            <span class="s1">self.distr = distr</span>

        <span class="s3">if </span><span class="s1">offset </span><span class="s3">is not None</span><span class="s1">:</span>
            <span class="s1">offset = np.asarray(offset)</span>

        <span class="s1">self.offset = offset</span>

        <span class="s1">endog</span><span class="s3">, </span><span class="s1">labels</span><span class="s3">, </span><span class="s1">is_pandas = self._check_inputs(endog</span><span class="s3">, </span><span class="s1">exog)</span>

        <span class="s1">super(OrderedModel</span><span class="s3">, </span><span class="s1">self).__init__(endog</span><span class="s3">, </span><span class="s1">exog</span><span class="s3">, </span><span class="s1">**kwds)</span>
        <span class="s1">k_levels = </span><span class="s3">None  </span><span class="s0"># initialize</span>
        <span class="s3">if not </span><span class="s1">is_pandas:</span>
            <span class="s3">if </span><span class="s1">self.endog.ndim == </span><span class="s5">1</span><span class="s1">:</span>
                <span class="s1">unique</span><span class="s3">, </span><span class="s1">index = np.unique(self.endog</span><span class="s3">, </span><span class="s1">return_inverse=</span><span class="s3">True</span><span class="s1">)</span>
                <span class="s1">self.endog = index</span>
                <span class="s1">labels = unique</span>
                <span class="s3">if </span><span class="s1">np.isnan(labels).any():</span>
                    <span class="s1">msg = (</span><span class="s4">&quot;NaN in dependent variable detected. &quot;</span>
                           <span class="s4">&quot;Missing values need to be removed.&quot;</span><span class="s1">)</span>
                    <span class="s3">raise </span><span class="s1">ValueError(msg)</span>
            <span class="s3">elif </span><span class="s1">self.endog.ndim == </span><span class="s5">2</span><span class="s1">:</span>
                <span class="s3">if not </span><span class="s1">hasattr(self</span><span class="s3">, </span><span class="s4">&quot;design_info&quot;</span><span class="s1">):</span>
                    <span class="s3">raise </span><span class="s1">ValueError(</span><span class="s4">&quot;2-dim endog not supported&quot;</span><span class="s1">)</span>
                <span class="s0"># this branch is currently only in support of from_formula</span>
                <span class="s0"># we need to initialize k_levels correctly for df_resid</span>
                <span class="s1">k_levels = self.endog.shape[</span><span class="s5">1</span><span class="s1">]</span>
                <span class="s1">labels = []</span>
                <span class="s0"># Note: Doing the following here would break from_formula</span>
                <span class="s0"># self.endog = self.endog.argmax(1)</span>

        <span class="s3">if </span><span class="s1">self.k_constant &gt; </span><span class="s5">0</span><span class="s1">:</span>
            <span class="s3">raise </span><span class="s1">ValueError(</span><span class="s4">&quot;There should not be a constant in the model&quot;</span><span class="s1">)</span>

        <span class="s1">self._initialize_labels(labels</span><span class="s3">, </span><span class="s1">k_levels=k_levels)</span>

        <span class="s0"># adjust df</span>
        <span class="s1">self.k_extra = self.k_levels - </span><span class="s5">1</span>
        <span class="s1">self.df_model = self.k_vars</span>
        <span class="s1">self.df_resid = self.nobs - (self.k_vars + self.k_extra)</span>

        <span class="s1">self.results_class = OrderedResults</span>

    <span class="s3">def </span><span class="s1">_check_inputs(self</span><span class="s3">, </span><span class="s1">endog</span><span class="s3">, </span><span class="s1">exog):</span>
        <span class="s2">&quot;&quot;&quot;Handle endog that is pandas Categorical. 
 
        Checks if self.distrib is legal and provides Pandas ordered Categorical 
        support for endog. 
 
        Parameters 
        ---------- 
        endog : array_like 
            Endogenous, dependent variable, 1-D. 
        exog : array_like 
            Exogenous, explanatory variables. 
            Currently not used. 
 
        Returns 
        ------- 
        endog : array_like or pandas Series 
            If the original endog is a pandas ordered Categorical Series, 
            then the returned endog are the ``codes``, i.e. integer 
            representation of ordere categorical variable 
        labels : None or list 
            If original endog is pandas ordered Categorical Series, then the 
            categories are returned. Otherwise ``labels`` is None. 
        is_pandas : bool 
            This is True if original endog is a pandas ordered Categorical 
            Series and False otherwise. 
 
        &quot;&quot;&quot;</span>

        <span class="s3">if not </span><span class="s1">isinstance(self.distr</span><span class="s3">, </span><span class="s1">stats.rv_continuous):</span>
            <span class="s1">msg = (</span>
                <span class="s4">f&quot;</span><span class="s3">{</span><span class="s1">self.distr.name</span><span class="s3">} </span><span class="s4">is not a scipy.stats distribution.&quot;</span>
            <span class="s1">)</span>
            <span class="s1">warnings.warn(msg)</span>

        <span class="s1">labels = </span><span class="s3">None</span>
        <span class="s1">is_pandas = </span><span class="s3">False</span>
        <span class="s3">if </span><span class="s1">isinstance(endog</span><span class="s3">, </span><span class="s1">pd.Series):</span>
            <span class="s3">if </span><span class="s1">isinstance(endog.dtypes</span><span class="s3">, </span><span class="s1">CategoricalDtype):</span>
                <span class="s3">if not </span><span class="s1">endog.dtype.ordered:</span>
                    <span class="s1">warnings.warn(</span><span class="s4">&quot;the endog has ordered == False, &quot;</span>
                                  <span class="s4">&quot;risk of capturing a wrong order for the &quot;</span>
                                  <span class="s4">&quot;categories. ordered == True preferred.&quot;</span><span class="s3">,</span>
                                  <span class="s1">Warning)</span>

                <span class="s1">endog_name = endog.name</span>
                <span class="s1">labels = endog.values.categories</span>
                <span class="s1">endog = endog.cat.codes</span>
                <span class="s3">if </span><span class="s1">endog.min() == -</span><span class="s5">1</span><span class="s1">:  </span><span class="s0"># means there is a missing value</span>
                    <span class="s3">raise </span><span class="s1">ValueError(</span><span class="s4">&quot;missing values in categorical endog are &quot;</span>
                                     <span class="s4">&quot;not supported&quot;</span><span class="s1">)</span>
                <span class="s1">endog.name = endog_name</span>
                <span class="s1">is_pandas = </span><span class="s3">True</span>

        <span class="s3">return </span><span class="s1">endog</span><span class="s3">, </span><span class="s1">labels</span><span class="s3">, </span><span class="s1">is_pandas</span>

    <span class="s3">def </span><span class="s1">_initialize_labels(self</span><span class="s3">, </span><span class="s1">labels</span><span class="s3">, </span><span class="s1">k_levels=</span><span class="s3">None</span><span class="s1">):</span>
        <span class="s1">self.labels = labels</span>
        <span class="s3">if </span><span class="s1">k_levels </span><span class="s3">is None</span><span class="s1">:</span>
            <span class="s1">self.k_levels = len(labels)</span>
        <span class="s3">else</span><span class="s1">:</span>
            <span class="s1">self.k_levels = k_levels</span>

        <span class="s3">if </span><span class="s1">self.exog </span><span class="s3">is not None</span><span class="s1">:</span>
            <span class="s1">self.nobs</span><span class="s3">, </span><span class="s1">self.k_vars = self.exog.shape</span>
        <span class="s3">else</span><span class="s1">:  </span><span class="s0"># no exog in model</span>
            <span class="s1">self.nobs</span><span class="s3">, </span><span class="s1">self.k_vars = self.endog.shape[</span><span class="s5">0</span><span class="s1">]</span><span class="s3">, </span><span class="s5">0</span>

        <span class="s1">threshold_names = [str(x) + </span><span class="s4">'/' </span><span class="s1">+ str(y)</span>
                           <span class="s3">for </span><span class="s1">x</span><span class="s3">, </span><span class="s1">y </span><span class="s3">in </span><span class="s1">zip(labels[:-</span><span class="s5">1</span><span class="s1">]</span><span class="s3">, </span><span class="s1">labels[</span><span class="s5">1</span><span class="s1">:])]</span>

        <span class="s0"># from GenericLikelihoodModel.fit</span>
        <span class="s3">if </span><span class="s1">self.exog </span><span class="s3">is not None</span><span class="s1">:</span>
            <span class="s0"># avoid extending several times</span>
            <span class="s3">if </span><span class="s1">len(self.exog_names) &gt; self.k_vars:</span>
                <span class="s3">raise </span><span class="s1">RuntimeError(</span><span class="s4">&quot;something wrong with exog_names, too long&quot;</span><span class="s1">)</span>
            <span class="s1">self.exog_names.extend(threshold_names)</span>
        <span class="s3">else</span><span class="s1">:</span>
            <span class="s1">self.data.xnames = threshold_names</span>

    <span class="s1">@classmethod</span>
    <span class="s3">def </span><span class="s1">from_formula(cls</span><span class="s3">, </span><span class="s1">formula</span><span class="s3">, </span><span class="s1">data</span><span class="s3">, </span><span class="s1">subset=</span><span class="s3">None, </span><span class="s1">drop_cols=</span><span class="s3">None,</span>
                     <span class="s1">*args</span><span class="s3">, </span><span class="s1">**kwargs):</span>

        <span class="s0"># we want an explicit Intercept in the model that we can remove</span>
        <span class="s0"># Removing constant with &quot;0 +&quot; or &quot;- 1&quot; does not work for categ. exog</span>

        <span class="s1">endog_name = formula.split(</span><span class="s4">&quot;~&quot;</span><span class="s1">)[</span><span class="s5">0</span><span class="s1">].strip()</span>
        <span class="s1">original_endog = data[endog_name]</span>

        <span class="s1">model = super(OrderedModel</span><span class="s3">, </span><span class="s1">cls).from_formula(</span>
            <span class="s1">formula</span><span class="s3">, </span><span class="s1">data=data</span><span class="s3">, </span><span class="s1">drop_cols=[</span><span class="s4">&quot;Intercept&quot;</span><span class="s1">]</span><span class="s3">, </span><span class="s1">*args</span><span class="s3">, </span><span class="s1">**kwargs)</span>

        <span class="s3">if </span><span class="s1">model.endog.ndim == </span><span class="s5">2</span><span class="s1">:</span>
            <span class="s3">if not </span><span class="s1">(isinstance(original_endog.dtype</span><span class="s3">, </span><span class="s1">CategoricalDtype)</span>
                    <span class="s3">and </span><span class="s1">original_endog.dtype.ordered):</span>
                <span class="s1">msg = (</span><span class="s4">&quot;Only ordered pandas Categorical are supported as &quot;</span>
                       <span class="s4">&quot;endog in formulas&quot;</span><span class="s1">)</span>
                <span class="s3">raise </span><span class="s1">ValueError(msg)</span>

            <span class="s1">labels = original_endog.values.categories</span>
            <span class="s1">model._initialize_labels(labels)</span>
            <span class="s1">model.endog = model.endog.argmax(</span><span class="s5">1</span><span class="s1">)</span>
            <span class="s1">model.data.ynames = endog_name</span>

        <span class="s3">return </span><span class="s1">model</span>

    <span class="s1">from_formula.__func__.__doc__ = Model.from_formula.__doc__</span>


    <span class="s3">def </span><span class="s1">cdf(self</span><span class="s3">, </span><span class="s1">x):</span>
        <span class="s2">&quot;&quot;&quot;Cdf evaluated at x. 
 
        Parameters 
        ---------- 
        x : array_like 
            Points at which cdf is evaluated. In the model `x` is the latent 
            variable plus threshold constants. 
 
        Returns 
        ------- 
        Value of the cumulative distribution function of the underlying latent 
        variable evaluated at x. 
        &quot;&quot;&quot;</span>
        <span class="s3">return </span><span class="s1">self.distr.cdf(x)</span>

    <span class="s3">def </span><span class="s1">pdf(self</span><span class="s3">, </span><span class="s1">x):</span>
        <span class="s2">&quot;&quot;&quot;Pdf evaluated at x 
 
        Parameters 
        ---------- 
        x : array_like 
            Points at which cdf is evaluated. In the model `x` is the latent 
            variable plus threshold constants. 
 
        Returns 
        ------- 
        Value of the probability density function of the underlying latent 
        variable evaluated at x. 
        &quot;&quot;&quot;</span>
        <span class="s3">return </span><span class="s1">self.distr.pdf(x)</span>

    <span class="s3">def </span><span class="s1">prob(self</span><span class="s3">, </span><span class="s1">low</span><span class="s3">, </span><span class="s1">upp):</span>
        <span class="s2">&quot;&quot;&quot;Interval probability. 
 
        Probability that value is in interval (low, upp], computed as 
 
            prob = cdf(upp) - cdf(low) 
 
        Parameters 
        ---------- 
        low : array_like 
            lower bound for interval 
        upp : array_like 
            upper bound for interval 
 
        Returns 
        ------- 
        float or ndarray 
            Probability that value falls in interval (low, upp] 
 
        &quot;&quot;&quot;</span>
        <span class="s3">return </span><span class="s1">np.maximum(self.cdf(upp) - self.cdf(low)</span><span class="s3">, </span><span class="s5">0</span><span class="s1">)</span>

    <span class="s3">def </span><span class="s1">transform_threshold_params(self</span><span class="s3">, </span><span class="s1">params):</span>
        <span class="s2">&quot;&quot;&quot;transformation of the parameters in the optimization 
 
        Parameters 
        ---------- 
        params : nd_array 
            Contains (exog_coef, transformed_thresholds) where exog_coef are 
            the coefficient for the explanatory variables in the linear term, 
            transformed threshold or cutoff points. The first, lowest threshold 
            is unchanged, all other thresholds are in terms of exponentiated 
            increments. 
 
        Returns 
        ------- 
        thresh : nd_array 
            Thresh are the thresholds or cutoff constants for the intervals. 
 
        &quot;&quot;&quot;</span>
        <span class="s1">th_params = params[-(self.k_levels - </span><span class="s5">1</span><span class="s1">):]</span>
        <span class="s1">thresh = np.concatenate((th_params[:</span><span class="s5">1</span><span class="s1">]</span><span class="s3">,</span>
                                 <span class="s1">np.exp(th_params[</span><span class="s5">1</span><span class="s1">:]))).cumsum()</span>
        <span class="s1">thresh = np.concatenate(([-np.inf]</span><span class="s3">, </span><span class="s1">thresh</span><span class="s3">, </span><span class="s1">[np.inf]))</span>
        <span class="s3">return </span><span class="s1">thresh</span>

    <span class="s3">def </span><span class="s1">transform_reverse_threshold_params(self</span><span class="s3">, </span><span class="s1">params):</span>
        <span class="s2">&quot;&quot;&quot;obtain transformed thresholds from original thresholds or cutoffs 
 
        Parameters 
        ---------- 
        params : ndarray 
            Threshold values, cutoff constants for choice intervals, which 
            need to be monotonically increasing. 
 
        Returns 
        ------- 
        thresh_params : ndarrray 
            Transformed threshold parameter. 
            The first, lowest threshold is unchanged, all other thresholds are 
            in terms of exponentiated increments. 
            Transformed parameters can be any real number without restrictions. 
 
        &quot;&quot;&quot;</span>
        <span class="s1">thresh_params = np.concatenate((params[:</span><span class="s5">1</span><span class="s1">]</span><span class="s3">,</span>
                                        <span class="s1">np.log(np.diff(params[:-</span><span class="s5">1</span><span class="s1">]))))</span>
        <span class="s3">return </span><span class="s1">thresh_params</span>

    <span class="s3">def </span><span class="s1">predict(self</span><span class="s3">, </span><span class="s1">params</span><span class="s3">, </span><span class="s1">exog=</span><span class="s3">None, </span><span class="s1">offset=</span><span class="s3">None, </span><span class="s1">which=</span><span class="s4">&quot;prob&quot;</span><span class="s1">):</span>
        <span class="s2">&quot;&quot;&quot; 
        Predicted probabilities for each level of the ordinal endog. 
 
        Parameters 
        ---------- 
        params : ndarray 
            Parameters for the Model, (exog_coef, transformed_thresholds). 
        exog : array_like, optional 
            Design / exogenous data. If exog is None, model exog is used. 
        offset : array_like, optional 
            Offset is added to the linear prediction with coefficient 
            equal to 1. If offset is not provided and exog 
            is None, uses the model's offset if present.  If not, uses 
            0 as the default value. 
        which : {&quot;prob&quot;, &quot;linpred&quot;, &quot;cumprob&quot;} 
            Determines which statistic is predicted. 
 
            - prob : predicted probabilities to be in each choice. 2-dim. 
            - linear : 1-dim linear prediction of the latent variable 
              ``x b + offset`` 
            - cumprob : predicted cumulative probability to be in choice k or 
              lower 
 
        Returns 
        ------- 
        predicted values : ndarray 
            If which is &quot;prob&quot;, then 2-dim predicted probabilities with 
            observations in rows and one column for each category or level of 
            the categorical dependent variable. 
            If which is &quot;cumprob&quot;, then &quot;prob&quot; ar cumulatively added to get the 
            cdf at k, i.e. probability of observing choice k or lower. 
            If which is &quot;linpred&quot;, then the conditional prediction of the 
            latent variable is returned. In this case, the return is 
            one-dimensional. 
        &quot;&quot;&quot;</span>
        <span class="s0"># note, exog and offset handling is in linpred</span>

        <span class="s1">thresh = self.transform_threshold_params(params)</span>
        <span class="s1">xb = self._linpred(params</span><span class="s3">, </span><span class="s1">exog=exog</span><span class="s3">, </span><span class="s1">offset=offset)</span>
        <span class="s3">if </span><span class="s1">which == </span><span class="s4">&quot;linpred&quot;</span><span class="s1">:</span>
            <span class="s3">return </span><span class="s1">xb</span>
        <span class="s1">xb = xb[:</span><span class="s3">, None</span><span class="s1">]</span>
        <span class="s1">low = thresh[:-</span><span class="s5">1</span><span class="s1">] - xb</span>
        <span class="s1">upp = thresh[</span><span class="s5">1</span><span class="s1">:] - xb</span>
        <span class="s3">if </span><span class="s1">which == </span><span class="s4">&quot;prob&quot;</span><span class="s1">:</span>
            <span class="s1">prob = self.prob(low</span><span class="s3">, </span><span class="s1">upp)</span>
            <span class="s3">return </span><span class="s1">prob</span>
        <span class="s3">elif </span><span class="s1">which </span><span class="s3">in </span><span class="s1">[</span><span class="s4">&quot;cum&quot;</span><span class="s3">, </span><span class="s4">&quot;cumprob&quot;</span><span class="s1">]:</span>
            <span class="s1">cumprob = self.cdf(upp)</span>
            <span class="s3">return </span><span class="s1">cumprob</span>
        <span class="s3">else</span><span class="s1">:</span>
            <span class="s3">raise </span><span class="s1">ValueError(</span><span class="s4">&quot;`which` is not available&quot;</span><span class="s1">)</span>

    <span class="s3">def </span><span class="s1">_linpred(self</span><span class="s3">, </span><span class="s1">params</span><span class="s3">, </span><span class="s1">exog=</span><span class="s3">None, </span><span class="s1">offset=</span><span class="s3">None</span><span class="s1">):</span>
        <span class="s2">&quot;&quot;&quot;Linear prediction of latent variable `x b + offset`. 
 
        Parameters 
        ---------- 
        params : ndarray 
            Parameters for the model, (exog_coef, transformed_thresholds) 
        exog : array_like, optional 
            Design / exogenous data. Is exog is None, model exog is used. 
        offset : array_like, optional 
            Offset is added to the linear prediction with coefficient 
            equal to 1. If offset is not provided and exog 
            is None, uses the model's offset if present.  If not, uses 
            0 as the default value. 
 
        Returns 
        ------- 
        linear : ndarray 
            1-dim linear prediction given by exog times linear params plus 
            offset. This is the prediction for the underlying latent variable. 
            If exog and offset are None, then the predicted values are zero. 
 
        &quot;&quot;&quot;</span>
        <span class="s3">if </span><span class="s1">exog </span><span class="s3">is None</span><span class="s1">:</span>
            <span class="s1">exog = self.exog</span>
            <span class="s3">if </span><span class="s1">offset </span><span class="s3">is None</span><span class="s1">:</span>
                <span class="s1">offset = self.offset</span>
        <span class="s3">else</span><span class="s1">:</span>
            <span class="s3">if </span><span class="s1">offset </span><span class="s3">is None</span><span class="s1">:</span>
                <span class="s1">offset = </span><span class="s5">0</span>

        <span class="s3">if </span><span class="s1">offset </span><span class="s3">is not None</span><span class="s1">:</span>
            <span class="s1">offset = np.asarray(offset)</span>

        <span class="s3">if </span><span class="s1">exog </span><span class="s3">is not None</span><span class="s1">:</span>
            <span class="s1">_exog = np.asarray(exog)</span>
            <span class="s1">_params = np.asarray(params)</span>
            <span class="s1">linpred = _exog.dot(_params[:-(self.k_levels - </span><span class="s5">1</span><span class="s1">)])</span>
        <span class="s3">else</span><span class="s1">:  </span><span class="s0"># means self.exog is also None</span>
            <span class="s1">linpred = np.zeros(self.nobs)</span>
        <span class="s3">if </span><span class="s1">offset </span><span class="s3">is not None</span><span class="s1">:</span>
            <span class="s1">linpred += offset</span>
        <span class="s3">return </span><span class="s1">linpred</span>

    <span class="s3">def </span><span class="s1">_bounds(self</span><span class="s3">, </span><span class="s1">params):</span>
        <span class="s2">&quot;&quot;&quot;Integration bounds for the observation specific interval. 
 
        This defines the lower and upper bounds for the intervals of the 
        choices of all observations. 
 
        The bounds for observation are given by 
 
            a_{k_i-1} - linpred_i, a_k_i - linpred_i 
 
        where 
        - k_i is the choice in observation i. 
        - a_{k_i-1} and a_k_i are thresholds (cutoffs) for choice k_i 
        - linpred_i is the linear prediction for observation i 
 
        Parameters 
        ---------- 
        params : ndarray 
            Parameters for the model, (exog_coef, transformed_thresholds) 
 
        Return 
        ------ 
        low : ndarray 
            Lower bounds for choice intervals of each observation, 
            1-dim with length nobs 
        upp : ndarray 
            Upper bounds for choice intervals of each observation, 
            1-dim with length nobs. 
 
        &quot;&quot;&quot;</span>
        <span class="s1">thresh = self.transform_threshold_params(params)</span>

        <span class="s1">thresh_i_low = thresh[self.endog]</span>
        <span class="s1">thresh_i_upp = thresh[self.endog + </span><span class="s5">1</span><span class="s1">]</span>
        <span class="s1">xb = self._linpred(params)</span>
        <span class="s1">low = thresh_i_low - xb</span>
        <span class="s1">upp = thresh_i_upp - xb</span>
        <span class="s3">return </span><span class="s1">low</span><span class="s3">, </span><span class="s1">upp</span>

    <span class="s1">@Appender(GenericLikelihoodModel.loglike.__doc__)</span>
    <span class="s3">def </span><span class="s1">loglike(self</span><span class="s3">, </span><span class="s1">params):</span>

        <span class="s3">return </span><span class="s1">self.loglikeobs(params).sum()</span>

    <span class="s3">def </span><span class="s1">loglikeobs(self</span><span class="s3">, </span><span class="s1">params):</span>
        <span class="s2">&quot;&quot;&quot; 
        Log-likelihood of OrderdModel for all observations. 
 
        Parameters 
        ---------- 
        params : array_like 
            The parameters of the model. 
 
        Returns 
        ------- 
        loglike_obs : array_like 
            The log likelihood for each observation of the model evaluated 
            at ``params``. 
        &quot;&quot;&quot;</span>
        <span class="s1">low</span><span class="s3">, </span><span class="s1">upp = self._bounds(params)</span>
        <span class="s1">prob = self.prob(low</span><span class="s3">, </span><span class="s1">upp)</span>
        <span class="s3">return </span><span class="s1">np.log(prob + </span><span class="s5">1e-20</span><span class="s1">)</span>

    <span class="s3">def </span><span class="s1">score_obs_(self</span><span class="s3">, </span><span class="s1">params):</span>
        <span class="s2">&quot;&quot;&quot;score, first derivative of loglike for each observations 
 
        This currently only implements the derivative with respect to the 
        exog parameters, but not with respect to threshold parameters. 
 
        &quot;&quot;&quot;</span>
        <span class="s1">low</span><span class="s3">, </span><span class="s1">upp = self._bounds(params)</span>

        <span class="s1">prob = self.prob(low</span><span class="s3">, </span><span class="s1">upp)</span>
        <span class="s1">pdf_upp = self.pdf(upp)</span>
        <span class="s1">pdf_low = self.pdf(low)</span>

        <span class="s0"># TODO the following doesn't work yet because of the incremental exp</span>
        <span class="s0"># parameterization. The following was written based on Greene for the</span>
        <span class="s0"># simple non-incremental parameterization.</span>
        <span class="s0"># k = self.k_levels - 1</span>
        <span class="s0"># idx = self.endog</span>
        <span class="s0"># score_factor = np.zeros((self.nobs, k + 1 + 2)) #+2 avoids idx bounds</span>
        <span class="s0">#</span>
        <span class="s0"># rows = np.arange(self.nobs)</span>
        <span class="s0"># shift = 1</span>
        <span class="s0"># score_factor[rows, shift + idx-1] = -pdf_low</span>
        <span class="s0"># score_factor[rows, shift + idx] = pdf_upp</span>
        <span class="s0"># score_factor[:, 0] = pdf_upp - pdf_low</span>
        <span class="s1">score_factor = (pdf_upp - pdf_low)[:</span><span class="s3">, None</span><span class="s1">]</span>
        <span class="s1">score_factor /= prob[:</span><span class="s3">, None</span><span class="s1">]</span>

        <span class="s1">so = np.column_stack((-score_factor[:</span><span class="s3">, </span><span class="s1">:</span><span class="s5">1</span><span class="s1">] * self.exog</span><span class="s3">,</span>
                              <span class="s1">score_factor[:</span><span class="s3">, </span><span class="s5">1</span><span class="s1">:]))</span>
        <span class="s3">return </span><span class="s1">so</span>

    <span class="s1">@property</span>
    <span class="s3">def </span><span class="s1">start_params(self):</span>
        <span class="s2">&quot;&quot;&quot;Start parameters for the optimization corresponding to null model. 
 
        The threshold are computed from the observed frequencies and 
        transformed to the exponential increments parameterization. 
        The parameters for explanatory variables are set to zero. 
        &quot;&quot;&quot;</span>
        <span class="s0"># start params based on model without exog</span>
        <span class="s1">freq = np.bincount(self.endog) / len(self.endog)</span>
        <span class="s1">start_ppf = self.distr.ppf(np.clip(freq.cumsum()</span><span class="s3">, </span><span class="s5">0</span><span class="s3">, </span><span class="s5">1</span><span class="s1">))</span>
        <span class="s1">start_threshold = self.transform_reverse_threshold_params(start_ppf)</span>
        <span class="s1">start_params = np.concatenate((np.zeros(self.k_vars)</span><span class="s3">, </span><span class="s1">start_threshold))</span>
        <span class="s3">return </span><span class="s1">start_params</span>

    <span class="s1">@Appender(LikelihoodModel.fit.__doc__)</span>
    <span class="s3">def </span><span class="s1">fit(self</span><span class="s3">, </span><span class="s1">start_params=</span><span class="s3">None, </span><span class="s1">method=</span><span class="s4">'nm'</span><span class="s3">, </span><span class="s1">maxiter=</span><span class="s5">500</span><span class="s3">, </span><span class="s1">full_output=</span><span class="s5">1</span><span class="s3">,</span>
            <span class="s1">disp=</span><span class="s5">1</span><span class="s3">, </span><span class="s1">callback=</span><span class="s3">None, </span><span class="s1">retall=</span><span class="s5">0</span><span class="s3">, </span><span class="s1">**kwargs):</span>

        <span class="s1">fit_method = super(OrderedModel</span><span class="s3">, </span><span class="s1">self).fit</span>
        <span class="s1">mlefit = fit_method(start_params=start_params</span><span class="s3">,</span>
                            <span class="s1">method=method</span><span class="s3">, </span><span class="s1">maxiter=maxiter</span><span class="s3">,</span>
                            <span class="s1">full_output=full_output</span><span class="s3">,</span>
                            <span class="s1">disp=disp</span><span class="s3">, </span><span class="s1">callback=callback</span><span class="s3">, </span><span class="s1">**kwargs)</span>
        <span class="s0"># use the proper result class</span>
        <span class="s1">ordmlefit = OrderedResults(self</span><span class="s3">, </span><span class="s1">mlefit)</span>

        <span class="s0"># TODO: temporary, needs better fix, modelwc adds 1 by default</span>
        <span class="s1">ordmlefit.hasconst = </span><span class="s5">0</span>

        <span class="s1">result = OrderedResultsWrapper(ordmlefit)</span>

        <span class="s3">return </span><span class="s1">result</span>


<span class="s3">class </span><span class="s1">OrderedResults(GenericLikelihoodModelResults):</span>
    <span class="s2">&quot;&quot;&quot;Results class for OrderedModel 
 
    This class inherits from GenericLikelihoodModelResults and not all 
    inherited methods might be appropriate in this case. 
    &quot;&quot;&quot;</span>

    <span class="s3">def </span><span class="s1">pred_table(self):</span>
        <span class="s2">&quot;&quot;&quot;prediction table 
 
        returns pandas DataFrame 
 
        &quot;&quot;&quot;</span>
        <span class="s0"># todo: add category labels</span>
        <span class="s1">categories = np.arange(self.model.k_levels)</span>
        <span class="s1">observed = pd.Categorical(self.model.endog</span><span class="s3">,</span>
                                  <span class="s1">categories=categories</span><span class="s3">, </span><span class="s1">ordered=</span><span class="s3">True</span><span class="s1">)</span>
        <span class="s1">predicted = pd.Categorical(self.predict().argmax(</span><span class="s5">1</span><span class="s1">)</span><span class="s3">,</span>
                                   <span class="s1">categories=categories</span><span class="s3">, </span><span class="s1">ordered=</span><span class="s3">True</span><span class="s1">)</span>
        <span class="s1">table = pd.crosstab(predicted</span><span class="s3">,</span>
                            <span class="s1">observed.astype(int)</span><span class="s3">,</span>
                            <span class="s1">margins=</span><span class="s3">True,</span>
                            <span class="s1">dropna=</span><span class="s3">False</span><span class="s1">).T.fillna(</span><span class="s5">0</span><span class="s1">)</span>
        <span class="s3">return </span><span class="s1">table</span>

    <span class="s1">@cache_readonly</span>
    <span class="s3">def </span><span class="s1">llnull(self):</span>
        <span class="s2">&quot;&quot;&quot; 
        Value of the loglikelihood of model without explanatory variables 
        &quot;&quot;&quot;</span>
        <span class="s1">params_null = self.model.start_params</span>
        <span class="s3">return </span><span class="s1">self.model.loglike(params_null)</span>

    <span class="s0"># next 3 are copied from discrete</span>
    <span class="s1">@cache_readonly</span>
    <span class="s3">def </span><span class="s1">prsquared(self):</span>
        <span class="s2">&quot;&quot;&quot; 
        McFadden's pseudo-R-squared. `1 - (llf / llnull)` 
        &quot;&quot;&quot;</span>
        <span class="s3">return </span><span class="s5">1 </span><span class="s1">- self.llf/self.llnull</span>

    <span class="s1">@cache_readonly</span>
    <span class="s3">def </span><span class="s1">llr(self):</span>
        <span class="s2">&quot;&quot;&quot; 
        Likelihood ratio chi-squared statistic; `-2*(llnull - llf)` 
        &quot;&quot;&quot;</span>
        <span class="s3">return </span><span class="s1">-</span><span class="s5">2</span><span class="s1">*(self.llnull - self.llf)</span>

    <span class="s1">@cache_readonly</span>
    <span class="s3">def </span><span class="s1">llr_pvalue(self):</span>
        <span class="s2">&quot;&quot;&quot; 
        The chi-squared probability of getting a log-likelihood ratio 
        statistic greater than llr.  llr has a chi-squared distribution 
        with degrees of freedom `df_model`. 
        &quot;&quot;&quot;</span>
        <span class="s0"># number of restrictions is number of exog</span>
        <span class="s3">return </span><span class="s1">stats.distributions.chi2.sf(self.llr</span><span class="s3">, </span><span class="s1">self.model.k_vars)</span>

    <span class="s1">@cache_readonly</span>
    <span class="s3">def </span><span class="s1">resid_prob(self):</span>
        <span class="s2">&quot;&quot;&quot;probability residual 
 
        Probability-scale residual is ``P(Y &lt; y) − P(Y &gt; y)`` where `Y` is the 
        observed choice and ``y`` is a random variable corresponding to the 
        predicted distribution. 
 
        References 
        ---------- 
        Shepherd BE, Li C, Liu Q (2016) Probability-scale residuals for 
        continuous, discrete, and censored data. 
        The Canadian Journal of Statistics. 44:463–476. 
 
        Li C and Shepherd BE (2012) A new residual for ordinal outcomes. 
        Biometrika. 99: 473–480 
 
        &quot;&quot;&quot;</span>
        <span class="s3">from </span><span class="s1">statsmodels.stats.diagnostic_gen </span><span class="s3">import </span><span class="s1">prob_larger_ordinal_choice</span>
        <span class="s1">endog = self.model.endog</span>
        <span class="s1">fitted = self.predict()</span>
        <span class="s1">r = prob_larger_ordinal_choice(fitted)[</span><span class="s5">1</span><span class="s1">]</span>
        <span class="s1">resid_prob = r[np.arange(endog.shape[</span><span class="s5">0</span><span class="s1">])</span><span class="s3">, </span><span class="s1">endog]</span>
        <span class="s3">return </span><span class="s1">resid_prob</span>


<span class="s3">class </span><span class="s1">OrderedResultsWrapper(lm.RegressionResultsWrapper):</span>
    <span class="s3">pass</span>


<span class="s1">wrap.populate_wrapper(OrderedResultsWrapper</span><span class="s3">, </span><span class="s1">OrderedResults)</span>
</pre>
</body>
</html>