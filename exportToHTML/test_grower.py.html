<html>
<head>
<title>test_grower.py</title>
<meta http-equiv="Content-Type" content="text/html; charset=utf-8">
<style type="text/css">
.s0 { color: #cc7832;}
.s1 { color: #a9b7c6;}
.s2 { color: #6897bb;}
.s3 { color: #808080;}
.s4 { color: #629755; font-style: italic;}
.s5 { color: #6a8759;}
</style>
</head>
<body bgcolor="#2b2b2b">
<table CELLSPACING=0 CELLPADDING=5 COLS=1 WIDTH="100%" BGCOLOR="#606060" >
<tr><td><center>
<font face="Arial, Helvetica" color="#000000">
test_grower.py</font>
</center></td></tr></table>
<pre><span class="s0">import </span><span class="s1">numpy </span><span class="s0">as </span><span class="s1">np</span>
<span class="s0">import </span><span class="s1">pytest</span>
<span class="s0">from </span><span class="s1">numpy.testing </span><span class="s0">import </span><span class="s1">assert_allclose</span><span class="s0">, </span><span class="s1">assert_array_equal</span>
<span class="s0">from </span><span class="s1">pytest </span><span class="s0">import </span><span class="s1">approx</span>

<span class="s0">from </span><span class="s1">sklearn.ensemble._hist_gradient_boosting.binning </span><span class="s0">import </span><span class="s1">_BinMapper</span>
<span class="s0">from </span><span class="s1">sklearn.ensemble._hist_gradient_boosting.common </span><span class="s0">import </span><span class="s1">(</span>
    <span class="s1">G_H_DTYPE</span><span class="s0">,</span>
    <span class="s1">X_BINNED_DTYPE</span><span class="s0">,</span>
    <span class="s1">X_BITSET_INNER_DTYPE</span><span class="s0">,</span>
    <span class="s1">X_DTYPE</span><span class="s0">,</span>
    <span class="s1">Y_DTYPE</span><span class="s0">,</span>
<span class="s1">)</span>
<span class="s0">from </span><span class="s1">sklearn.ensemble._hist_gradient_boosting.grower </span><span class="s0">import </span><span class="s1">TreeGrower</span>
<span class="s0">from </span><span class="s1">sklearn.preprocessing </span><span class="s0">import </span><span class="s1">OneHotEncoder</span>
<span class="s0">from </span><span class="s1">sklearn.utils._openmp_helpers </span><span class="s0">import </span><span class="s1">_openmp_effective_n_threads</span>

<span class="s1">n_threads = _openmp_effective_n_threads()</span>


<span class="s0">def </span><span class="s1">_make_training_data(n_bins=</span><span class="s2">256</span><span class="s0">, </span><span class="s1">constant_hessian=</span><span class="s0">True</span><span class="s1">):</span>
    <span class="s1">rng = np.random.RandomState(</span><span class="s2">42</span><span class="s1">)</span>
    <span class="s1">n_samples = </span><span class="s2">10000</span>

    <span class="s3"># Generate some test data directly binned so as to test the grower code</span>
    <span class="s3"># independently of the binning logic.</span>
    <span class="s1">X_binned = rng.randint(</span><span class="s2">0</span><span class="s0">, </span><span class="s1">n_bins - </span><span class="s2">1</span><span class="s0">, </span><span class="s1">size=(n_samples</span><span class="s0">, </span><span class="s2">2</span><span class="s1">)</span><span class="s0">, </span><span class="s1">dtype=X_BINNED_DTYPE)</span>
    <span class="s1">X_binned = np.asfortranarray(X_binned)</span>

    <span class="s0">def </span><span class="s1">true_decision_function(input_features):</span>
        <span class="s4">&quot;&quot;&quot;Ground truth decision function 
 
        This is a very simple yet asymmetric decision tree. Therefore the 
        grower code should have no trouble recovering the decision function 
        from 10000 training samples. 
        &quot;&quot;&quot;</span>
        <span class="s0">if </span><span class="s1">input_features[</span><span class="s2">0</span><span class="s1">] &lt;= n_bins // </span><span class="s2">2</span><span class="s1">:</span>
            <span class="s0">return </span><span class="s1">-</span><span class="s2">1</span>
        <span class="s0">else</span><span class="s1">:</span>
            <span class="s0">return </span><span class="s1">-</span><span class="s2">1 </span><span class="s0">if </span><span class="s1">input_features[</span><span class="s2">1</span><span class="s1">] &lt;= n_bins // </span><span class="s2">3 </span><span class="s0">else </span><span class="s2">1</span>

    <span class="s1">target = np.array([true_decision_function(x) </span><span class="s0">for </span><span class="s1">x </span><span class="s0">in </span><span class="s1">X_binned]</span><span class="s0">, </span><span class="s1">dtype=Y_DTYPE)</span>

    <span class="s3"># Assume a square loss applied to an initial model that always predicts 0</span>
    <span class="s3"># (hardcoded for this test):</span>
    <span class="s1">all_gradients = target.astype(G_H_DTYPE)</span>
    <span class="s1">shape_hessians = </span><span class="s2">1 </span><span class="s0">if </span><span class="s1">constant_hessian </span><span class="s0">else </span><span class="s1">all_gradients.shape</span>
    <span class="s1">all_hessians = np.ones(shape=shape_hessians</span><span class="s0">, </span><span class="s1">dtype=G_H_DTYPE)</span>

    <span class="s0">return </span><span class="s1">X_binned</span><span class="s0">, </span><span class="s1">all_gradients</span><span class="s0">, </span><span class="s1">all_hessians</span>


<span class="s0">def </span><span class="s1">_check_children_consistency(parent</span><span class="s0">, </span><span class="s1">left</span><span class="s0">, </span><span class="s1">right):</span>
    <span class="s3"># Make sure the samples are correctly dispatched from a parent to its</span>
    <span class="s3"># children</span>
    <span class="s0">assert </span><span class="s1">parent.left_child </span><span class="s0">is </span><span class="s1">left</span>
    <span class="s0">assert </span><span class="s1">parent.right_child </span><span class="s0">is </span><span class="s1">right</span>

    <span class="s3"># each sample from the parent is propagated to one of the two children</span>
    <span class="s0">assert </span><span class="s1">len(left.sample_indices) + len(right.sample_indices) == len(</span>
        <span class="s1">parent.sample_indices</span>
    <span class="s1">)</span>

    <span class="s0">assert </span><span class="s1">set(left.sample_indices).union(set(right.sample_indices)) == set(</span>
        <span class="s1">parent.sample_indices</span>
    <span class="s1">)</span>

    <span class="s3"># samples are sent either to the left or the right node, never to both</span>
    <span class="s0">assert </span><span class="s1">set(left.sample_indices).intersection(set(right.sample_indices)) == set()</span>


<span class="s1">@pytest.mark.parametrize(</span>
    <span class="s5">&quot;n_bins, constant_hessian, stopping_param, shrinkage&quot;</span><span class="s0">,</span>
    <span class="s1">[</span>
        <span class="s1">(</span><span class="s2">11</span><span class="s0">, True, </span><span class="s5">&quot;min_gain_to_split&quot;</span><span class="s0">, </span><span class="s2">0.5</span><span class="s1">)</span><span class="s0">,</span>
        <span class="s1">(</span><span class="s2">11</span><span class="s0">, False, </span><span class="s5">&quot;min_gain_to_split&quot;</span><span class="s0">, </span><span class="s2">1.0</span><span class="s1">)</span><span class="s0">,</span>
        <span class="s1">(</span><span class="s2">11</span><span class="s0">, True, </span><span class="s5">&quot;max_leaf_nodes&quot;</span><span class="s0">, </span><span class="s2">1.0</span><span class="s1">)</span><span class="s0">,</span>
        <span class="s1">(</span><span class="s2">11</span><span class="s0">, False, </span><span class="s5">&quot;max_leaf_nodes&quot;</span><span class="s0">, </span><span class="s2">0.1</span><span class="s1">)</span><span class="s0">,</span>
        <span class="s1">(</span><span class="s2">42</span><span class="s0">, True, </span><span class="s5">&quot;max_leaf_nodes&quot;</span><span class="s0">, </span><span class="s2">0.01</span><span class="s1">)</span><span class="s0">,</span>
        <span class="s1">(</span><span class="s2">42</span><span class="s0">, False, </span><span class="s5">&quot;max_leaf_nodes&quot;</span><span class="s0">, </span><span class="s2">1.0</span><span class="s1">)</span><span class="s0">,</span>
        <span class="s1">(</span><span class="s2">256</span><span class="s0">, True, </span><span class="s5">&quot;min_gain_to_split&quot;</span><span class="s0">, </span><span class="s2">1.0</span><span class="s1">)</span><span class="s0">,</span>
        <span class="s1">(</span><span class="s2">256</span><span class="s0">, True, </span><span class="s5">&quot;max_leaf_nodes&quot;</span><span class="s0">, </span><span class="s2">0.1</span><span class="s1">)</span><span class="s0">,</span>
    <span class="s1">]</span><span class="s0">,</span>
<span class="s1">)</span>
<span class="s0">def </span><span class="s1">test_grow_tree(n_bins</span><span class="s0">, </span><span class="s1">constant_hessian</span><span class="s0">, </span><span class="s1">stopping_param</span><span class="s0">, </span><span class="s1">shrinkage):</span>
    <span class="s1">X_binned</span><span class="s0">, </span><span class="s1">all_gradients</span><span class="s0">, </span><span class="s1">all_hessians = _make_training_data(</span>
        <span class="s1">n_bins=n_bins</span><span class="s0">, </span><span class="s1">constant_hessian=constant_hessian</span>
    <span class="s1">)</span>
    <span class="s1">n_samples = X_binned.shape[</span><span class="s2">0</span><span class="s1">]</span>

    <span class="s0">if </span><span class="s1">stopping_param == </span><span class="s5">&quot;max_leaf_nodes&quot;</span><span class="s1">:</span>
        <span class="s1">stopping_param = {</span><span class="s5">&quot;max_leaf_nodes&quot;</span><span class="s1">: </span><span class="s2">3</span><span class="s1">}</span>
    <span class="s0">else</span><span class="s1">:</span>
        <span class="s1">stopping_param = {</span><span class="s5">&quot;min_gain_to_split&quot;</span><span class="s1">: </span><span class="s2">0.01</span><span class="s1">}</span>

    <span class="s1">grower = TreeGrower(</span>
        <span class="s1">X_binned</span><span class="s0">,</span>
        <span class="s1">all_gradients</span><span class="s0">,</span>
        <span class="s1">all_hessians</span><span class="s0">,</span>
        <span class="s1">n_bins=n_bins</span><span class="s0">,</span>
        <span class="s1">shrinkage=shrinkage</span><span class="s0">,</span>
        <span class="s1">min_samples_leaf=</span><span class="s2">1</span><span class="s0">,</span>
        <span class="s1">**stopping_param</span><span class="s0">,</span>
    <span class="s1">)</span>

    <span class="s3"># The root node is not yet split, but the best possible split has</span>
    <span class="s3"># already been evaluated:</span>
    <span class="s0">assert </span><span class="s1">grower.root.left_child </span><span class="s0">is None</span>
    <span class="s0">assert </span><span class="s1">grower.root.right_child </span><span class="s0">is None</span>

    <span class="s1">root_split = grower.root.split_info</span>
    <span class="s0">assert </span><span class="s1">root_split.feature_idx == </span><span class="s2">0</span>
    <span class="s0">assert </span><span class="s1">root_split.bin_idx == n_bins // </span><span class="s2">2</span>
    <span class="s0">assert </span><span class="s1">len(grower.splittable_nodes) == </span><span class="s2">1</span>

    <span class="s3"># Calling split next applies the next split and computes the best split</span>
    <span class="s3"># for each of the two newly introduced children nodes.</span>
    <span class="s1">left_node</span><span class="s0">, </span><span class="s1">right_node = grower.split_next()</span>

    <span class="s3"># All training samples have ben split in the two nodes, approximately</span>
    <span class="s3"># 50%/50%</span>
    <span class="s1">_check_children_consistency(grower.root</span><span class="s0">, </span><span class="s1">left_node</span><span class="s0">, </span><span class="s1">right_node)</span>
    <span class="s0">assert </span><span class="s1">len(left_node.sample_indices) &gt; </span><span class="s2">0.4 </span><span class="s1">* n_samples</span>
    <span class="s0">assert </span><span class="s1">len(left_node.sample_indices) &lt; </span><span class="s2">0.6 </span><span class="s1">* n_samples</span>

    <span class="s0">if </span><span class="s1">grower.min_gain_to_split &gt; </span><span class="s2">0</span><span class="s1">:</span>
        <span class="s3"># The left node is too pure: there is no gain to split it further.</span>
        <span class="s0">assert </span><span class="s1">left_node.split_info.gain &lt; grower.min_gain_to_split</span>
        <span class="s0">assert </span><span class="s1">left_node </span><span class="s0">in </span><span class="s1">grower.finalized_leaves</span>

    <span class="s3"># The right node can still be split further, this time on feature #1</span>
    <span class="s1">split_info = right_node.split_info</span>
    <span class="s0">assert </span><span class="s1">split_info.gain &gt; </span><span class="s2">1.0</span>
    <span class="s0">assert </span><span class="s1">split_info.feature_idx == </span><span class="s2">1</span>
    <span class="s0">assert </span><span class="s1">split_info.bin_idx == n_bins // </span><span class="s2">3</span>
    <span class="s0">assert </span><span class="s1">right_node.left_child </span><span class="s0">is None</span>
    <span class="s0">assert </span><span class="s1">right_node.right_child </span><span class="s0">is None</span>

    <span class="s3"># The right split has not been applied yet. Let's do it now:</span>
    <span class="s0">assert </span><span class="s1">len(grower.splittable_nodes) == </span><span class="s2">1</span>
    <span class="s1">right_left_node</span><span class="s0">, </span><span class="s1">right_right_node = grower.split_next()</span>
    <span class="s1">_check_children_consistency(right_node</span><span class="s0">, </span><span class="s1">right_left_node</span><span class="s0">, </span><span class="s1">right_right_node)</span>
    <span class="s0">assert </span><span class="s1">len(right_left_node.sample_indices) &gt; </span><span class="s2">0.1 </span><span class="s1">* n_samples</span>
    <span class="s0">assert </span><span class="s1">len(right_left_node.sample_indices) &lt; </span><span class="s2">0.2 </span><span class="s1">* n_samples</span>

    <span class="s0">assert </span><span class="s1">len(right_right_node.sample_indices) &gt; </span><span class="s2">0.2 </span><span class="s1">* n_samples</span>
    <span class="s0">assert </span><span class="s1">len(right_right_node.sample_indices) &lt; </span><span class="s2">0.4 </span><span class="s1">* n_samples</span>

    <span class="s3"># All the leafs are pure, it is not possible to split any further:</span>
    <span class="s0">assert not </span><span class="s1">grower.splittable_nodes</span>

    <span class="s1">grower._apply_shrinkage()</span>

    <span class="s3"># Check the values of the leaves:</span>
    <span class="s0">assert </span><span class="s1">grower.root.left_child.value == approx(shrinkage)</span>
    <span class="s0">assert </span><span class="s1">grower.root.right_child.left_child.value == approx(shrinkage)</span>
    <span class="s0">assert </span><span class="s1">grower.root.right_child.right_child.value == approx(-shrinkage</span><span class="s0">, </span><span class="s1">rel=</span><span class="s2">1e-3</span><span class="s1">)</span>


<span class="s0">def </span><span class="s1">test_predictor_from_grower():</span>
    <span class="s3"># Build a tree on the toy 3-leaf dataset to extract the predictor.</span>
    <span class="s1">n_bins = </span><span class="s2">256</span>
    <span class="s1">X_binned</span><span class="s0">, </span><span class="s1">all_gradients</span><span class="s0">, </span><span class="s1">all_hessians = _make_training_data(n_bins=n_bins)</span>
    <span class="s1">grower = TreeGrower(</span>
        <span class="s1">X_binned</span><span class="s0">,</span>
        <span class="s1">all_gradients</span><span class="s0">,</span>
        <span class="s1">all_hessians</span><span class="s0">,</span>
        <span class="s1">n_bins=n_bins</span><span class="s0">,</span>
        <span class="s1">shrinkage=</span><span class="s2">1.0</span><span class="s0">,</span>
        <span class="s1">max_leaf_nodes=</span><span class="s2">3</span><span class="s0">,</span>
        <span class="s1">min_samples_leaf=</span><span class="s2">5</span><span class="s0">,</span>
    <span class="s1">)</span>
    <span class="s1">grower.grow()</span>
    <span class="s0">assert </span><span class="s1">grower.n_nodes == </span><span class="s2">5  </span><span class="s3"># (2 decision nodes + 3 leaves)</span>

    <span class="s3"># Check that the node structure can be converted into a predictor</span>
    <span class="s3"># object to perform predictions at scale</span>
    <span class="s3"># We pass undefined binning_thresholds because we won't use predict anyway</span>
    <span class="s1">predictor = grower.make_predictor(</span>
        <span class="s1">binning_thresholds=np.zeros((X_binned.shape[</span><span class="s2">1</span><span class="s1">]</span><span class="s0">, </span><span class="s1">n_bins))</span>
    <span class="s1">)</span>
    <span class="s0">assert </span><span class="s1">predictor.nodes.shape[</span><span class="s2">0</span><span class="s1">] == </span><span class="s2">5</span>
    <span class="s0">assert </span><span class="s1">predictor.nodes[</span><span class="s5">&quot;is_leaf&quot;</span><span class="s1">].sum() == </span><span class="s2">3</span>

    <span class="s3"># Probe some predictions for each leaf of the tree</span>
    <span class="s3"># each group of 3 samples corresponds to a condition in _make_training_data</span>
    <span class="s1">input_data = np.array(</span>
        <span class="s1">[</span>
            <span class="s1">[</span><span class="s2">0</span><span class="s0">, </span><span class="s2">0</span><span class="s1">]</span><span class="s0">,</span>
            <span class="s1">[</span><span class="s2">42</span><span class="s0">, </span><span class="s2">99</span><span class="s1">]</span><span class="s0">,</span>
            <span class="s1">[</span><span class="s2">128</span><span class="s0">, </span><span class="s2">254</span><span class="s1">]</span><span class="s0">,</span>
            <span class="s1">[</span><span class="s2">129</span><span class="s0">, </span><span class="s2">0</span><span class="s1">]</span><span class="s0">,</span>
            <span class="s1">[</span><span class="s2">129</span><span class="s0">, </span><span class="s2">85</span><span class="s1">]</span><span class="s0">,</span>
            <span class="s1">[</span><span class="s2">254</span><span class="s0">, </span><span class="s2">85</span><span class="s1">]</span><span class="s0">,</span>
            <span class="s1">[</span><span class="s2">129</span><span class="s0">, </span><span class="s2">86</span><span class="s1">]</span><span class="s0">,</span>
            <span class="s1">[</span><span class="s2">129</span><span class="s0">, </span><span class="s2">254</span><span class="s1">]</span><span class="s0">,</span>
            <span class="s1">[</span><span class="s2">242</span><span class="s0">, </span><span class="s2">100</span><span class="s1">]</span><span class="s0">,</span>
        <span class="s1">]</span><span class="s0">,</span>
        <span class="s1">dtype=np.uint8</span><span class="s0">,</span>
    <span class="s1">)</span>
    <span class="s1">missing_values_bin_idx = n_bins - </span><span class="s2">1</span>
    <span class="s1">predictions = predictor.predict_binned(</span>
        <span class="s1">input_data</span><span class="s0">, </span><span class="s1">missing_values_bin_idx</span><span class="s0">, </span><span class="s1">n_threads</span>
    <span class="s1">)</span>
    <span class="s1">expected_targets = [</span><span class="s2">1</span><span class="s0">, </span><span class="s2">1</span><span class="s0">, </span><span class="s2">1</span><span class="s0">, </span><span class="s2">1</span><span class="s0">, </span><span class="s2">1</span><span class="s0">, </span><span class="s2">1</span><span class="s0">, </span><span class="s1">-</span><span class="s2">1</span><span class="s0">, </span><span class="s1">-</span><span class="s2">1</span><span class="s0">, </span><span class="s1">-</span><span class="s2">1</span><span class="s1">]</span>
    <span class="s0">assert </span><span class="s1">np.allclose(predictions</span><span class="s0">, </span><span class="s1">expected_targets)</span>

    <span class="s3"># Check that training set can be recovered exactly:</span>
    <span class="s1">predictions = predictor.predict_binned(X_binned</span><span class="s0">, </span><span class="s1">missing_values_bin_idx</span><span class="s0">, </span><span class="s1">n_threads)</span>
    <span class="s0">assert </span><span class="s1">np.allclose(predictions</span><span class="s0">, </span><span class="s1">-all_gradients)</span>


<span class="s1">@pytest.mark.parametrize(</span>
    <span class="s5">&quot;n_samples, min_samples_leaf, n_bins, constant_hessian, noise&quot;</span><span class="s0">,</span>
    <span class="s1">[</span>
        <span class="s1">(</span><span class="s2">11</span><span class="s0">, </span><span class="s2">10</span><span class="s0">, </span><span class="s2">7</span><span class="s0">, True, </span><span class="s2">0</span><span class="s1">)</span><span class="s0">,</span>
        <span class="s1">(</span><span class="s2">13</span><span class="s0">, </span><span class="s2">10</span><span class="s0">, </span><span class="s2">42</span><span class="s0">, False, </span><span class="s2">0</span><span class="s1">)</span><span class="s0">,</span>
        <span class="s1">(</span><span class="s2">56</span><span class="s0">, </span><span class="s2">10</span><span class="s0">, </span><span class="s2">255</span><span class="s0">, True, </span><span class="s2">0.1</span><span class="s1">)</span><span class="s0">,</span>
        <span class="s1">(</span><span class="s2">101</span><span class="s0">, </span><span class="s2">3</span><span class="s0">, </span><span class="s2">7</span><span class="s0">, True, </span><span class="s2">0</span><span class="s1">)</span><span class="s0">,</span>
        <span class="s1">(</span><span class="s2">200</span><span class="s0">, </span><span class="s2">42</span><span class="s0">, </span><span class="s2">42</span><span class="s0">, False, </span><span class="s2">0</span><span class="s1">)</span><span class="s0">,</span>
        <span class="s1">(</span><span class="s2">300</span><span class="s0">, </span><span class="s2">55</span><span class="s0">, </span><span class="s2">255</span><span class="s0">, True, </span><span class="s2">0.1</span><span class="s1">)</span><span class="s0">,</span>
        <span class="s1">(</span><span class="s2">300</span><span class="s0">, </span><span class="s2">301</span><span class="s0">, </span><span class="s2">255</span><span class="s0">, True, </span><span class="s2">0.1</span><span class="s1">)</span><span class="s0">,</span>
    <span class="s1">]</span><span class="s0">,</span>
<span class="s1">)</span>
<span class="s0">def </span><span class="s1">test_min_samples_leaf(n_samples</span><span class="s0">, </span><span class="s1">min_samples_leaf</span><span class="s0">, </span><span class="s1">n_bins</span><span class="s0">, </span><span class="s1">constant_hessian</span><span class="s0">, </span><span class="s1">noise):</span>
    <span class="s1">rng = np.random.RandomState(seed=</span><span class="s2">0</span><span class="s1">)</span>
    <span class="s3"># data = linear target, 3 features, 1 irrelevant.</span>
    <span class="s1">X = rng.normal(size=(n_samples</span><span class="s0">, </span><span class="s2">3</span><span class="s1">))</span>
    <span class="s1">y = X[:</span><span class="s0">, </span><span class="s2">0</span><span class="s1">] - X[:</span><span class="s0">, </span><span class="s2">1</span><span class="s1">]</span>
    <span class="s0">if </span><span class="s1">noise:</span>
        <span class="s1">y_scale = y.std()</span>
        <span class="s1">y += rng.normal(scale=noise</span><span class="s0">, </span><span class="s1">size=n_samples) * y_scale</span>
    <span class="s1">mapper = _BinMapper(n_bins=n_bins)</span>
    <span class="s1">X = mapper.fit_transform(X)</span>

    <span class="s1">all_gradients = y.astype(G_H_DTYPE)</span>
    <span class="s1">shape_hessian = </span><span class="s2">1 </span><span class="s0">if </span><span class="s1">constant_hessian </span><span class="s0">else </span><span class="s1">all_gradients.shape</span>
    <span class="s1">all_hessians = np.ones(shape=shape_hessian</span><span class="s0">, </span><span class="s1">dtype=G_H_DTYPE)</span>
    <span class="s1">grower = TreeGrower(</span>
        <span class="s1">X</span><span class="s0">,</span>
        <span class="s1">all_gradients</span><span class="s0">,</span>
        <span class="s1">all_hessians</span><span class="s0">,</span>
        <span class="s1">n_bins=n_bins</span><span class="s0">,</span>
        <span class="s1">shrinkage=</span><span class="s2">1.0</span><span class="s0">,</span>
        <span class="s1">min_samples_leaf=min_samples_leaf</span><span class="s0">,</span>
        <span class="s1">max_leaf_nodes=n_samples</span><span class="s0">,</span>
    <span class="s1">)</span>
    <span class="s1">grower.grow()</span>
    <span class="s1">predictor = grower.make_predictor(binning_thresholds=mapper.bin_thresholds_)</span>

    <span class="s0">if </span><span class="s1">n_samples &gt;= min_samples_leaf:</span>
        <span class="s0">for </span><span class="s1">node </span><span class="s0">in </span><span class="s1">predictor.nodes:</span>
            <span class="s0">if </span><span class="s1">node[</span><span class="s5">&quot;is_leaf&quot;</span><span class="s1">]:</span>
                <span class="s0">assert </span><span class="s1">node[</span><span class="s5">&quot;count&quot;</span><span class="s1">] &gt;= min_samples_leaf</span>
    <span class="s0">else</span><span class="s1">:</span>
        <span class="s0">assert </span><span class="s1">predictor.nodes.shape[</span><span class="s2">0</span><span class="s1">] == </span><span class="s2">1</span>
        <span class="s0">assert </span><span class="s1">predictor.nodes[</span><span class="s2">0</span><span class="s1">][</span><span class="s5">&quot;is_leaf&quot;</span><span class="s1">]</span>
        <span class="s0">assert </span><span class="s1">predictor.nodes[</span><span class="s2">0</span><span class="s1">][</span><span class="s5">&quot;count&quot;</span><span class="s1">] == n_samples</span>


<span class="s1">@pytest.mark.parametrize(</span><span class="s5">&quot;n_samples, min_samples_leaf&quot;</span><span class="s0">, </span><span class="s1">[(</span><span class="s2">99</span><span class="s0">, </span><span class="s2">50</span><span class="s1">)</span><span class="s0">, </span><span class="s1">(</span><span class="s2">100</span><span class="s0">, </span><span class="s2">50</span><span class="s1">)])</span>
<span class="s0">def </span><span class="s1">test_min_samples_leaf_root(n_samples</span><span class="s0">, </span><span class="s1">min_samples_leaf):</span>
    <span class="s3"># Make sure root node isn't split if n_samples is not at least twice</span>
    <span class="s3"># min_samples_leaf</span>
    <span class="s1">rng = np.random.RandomState(seed=</span><span class="s2">0</span><span class="s1">)</span>

    <span class="s1">n_bins = </span><span class="s2">256</span>

    <span class="s3"># data = linear target, 3 features, 1 irrelevant.</span>
    <span class="s1">X = rng.normal(size=(n_samples</span><span class="s0">, </span><span class="s2">3</span><span class="s1">))</span>
    <span class="s1">y = X[:</span><span class="s0">, </span><span class="s2">0</span><span class="s1">] - X[:</span><span class="s0">, </span><span class="s2">1</span><span class="s1">]</span>
    <span class="s1">mapper = _BinMapper(n_bins=n_bins)</span>
    <span class="s1">X = mapper.fit_transform(X)</span>

    <span class="s1">all_gradients = y.astype(G_H_DTYPE)</span>
    <span class="s1">all_hessians = np.ones(shape=</span><span class="s2">1</span><span class="s0">, </span><span class="s1">dtype=G_H_DTYPE)</span>
    <span class="s1">grower = TreeGrower(</span>
        <span class="s1">X</span><span class="s0">,</span>
        <span class="s1">all_gradients</span><span class="s0">,</span>
        <span class="s1">all_hessians</span><span class="s0">,</span>
        <span class="s1">n_bins=n_bins</span><span class="s0">,</span>
        <span class="s1">shrinkage=</span><span class="s2">1.0</span><span class="s0">,</span>
        <span class="s1">min_samples_leaf=min_samples_leaf</span><span class="s0">,</span>
        <span class="s1">max_leaf_nodes=n_samples</span><span class="s0">,</span>
    <span class="s1">)</span>
    <span class="s1">grower.grow()</span>
    <span class="s0">if </span><span class="s1">n_samples &gt;= min_samples_leaf * </span><span class="s2">2</span><span class="s1">:</span>
        <span class="s0">assert </span><span class="s1">len(grower.finalized_leaves) &gt;= </span><span class="s2">2</span>
    <span class="s0">else</span><span class="s1">:</span>
        <span class="s0">assert </span><span class="s1">len(grower.finalized_leaves) == </span><span class="s2">1</span>


<span class="s0">def </span><span class="s1">assert_is_stump(grower):</span>
    <span class="s3"># To assert that stumps are created when max_depth=1</span>
    <span class="s0">for </span><span class="s1">leaf </span><span class="s0">in </span><span class="s1">(grower.root.left_child</span><span class="s0">, </span><span class="s1">grower.root.right_child):</span>
        <span class="s0">assert </span><span class="s1">leaf.left_child </span><span class="s0">is None</span>
        <span class="s0">assert </span><span class="s1">leaf.right_child </span><span class="s0">is None</span>


<span class="s1">@pytest.mark.parametrize(</span><span class="s5">&quot;max_depth&quot;</span><span class="s0">, </span><span class="s1">[</span><span class="s2">1</span><span class="s0">, </span><span class="s2">2</span><span class="s0">, </span><span class="s2">3</span><span class="s1">])</span>
<span class="s0">def </span><span class="s1">test_max_depth(max_depth):</span>
    <span class="s3"># Make sure max_depth parameter works as expected</span>
    <span class="s1">rng = np.random.RandomState(seed=</span><span class="s2">0</span><span class="s1">)</span>

    <span class="s1">n_bins = </span><span class="s2">256</span>
    <span class="s1">n_samples = </span><span class="s2">1000</span>

    <span class="s3"># data = linear target, 3 features, 1 irrelevant.</span>
    <span class="s1">X = rng.normal(size=(n_samples</span><span class="s0">, </span><span class="s2">3</span><span class="s1">))</span>
    <span class="s1">y = X[:</span><span class="s0">, </span><span class="s2">0</span><span class="s1">] - X[:</span><span class="s0">, </span><span class="s2">1</span><span class="s1">]</span>
    <span class="s1">mapper = _BinMapper(n_bins=n_bins)</span>
    <span class="s1">X = mapper.fit_transform(X)</span>

    <span class="s1">all_gradients = y.astype(G_H_DTYPE)</span>
    <span class="s1">all_hessians = np.ones(shape=</span><span class="s2">1</span><span class="s0">, </span><span class="s1">dtype=G_H_DTYPE)</span>
    <span class="s1">grower = TreeGrower(X</span><span class="s0">, </span><span class="s1">all_gradients</span><span class="s0">, </span><span class="s1">all_hessians</span><span class="s0">, </span><span class="s1">max_depth=max_depth)</span>
    <span class="s1">grower.grow()</span>

    <span class="s1">depth = max(leaf.depth </span><span class="s0">for </span><span class="s1">leaf </span><span class="s0">in </span><span class="s1">grower.finalized_leaves)</span>
    <span class="s0">assert </span><span class="s1">depth == max_depth</span>

    <span class="s0">if </span><span class="s1">max_depth == </span><span class="s2">1</span><span class="s1">:</span>
        <span class="s1">assert_is_stump(grower)</span>


<span class="s0">def </span><span class="s1">test_input_validation():</span>
    <span class="s1">X_binned</span><span class="s0">, </span><span class="s1">all_gradients</span><span class="s0">, </span><span class="s1">all_hessians = _make_training_data()</span>

    <span class="s1">X_binned_float = X_binned.astype(np.float32)</span>
    <span class="s0">with </span><span class="s1">pytest.raises(NotImplementedError</span><span class="s0">, </span><span class="s1">match=</span><span class="s5">&quot;X_binned must be of type uint8&quot;</span><span class="s1">):</span>
        <span class="s1">TreeGrower(X_binned_float</span><span class="s0">, </span><span class="s1">all_gradients</span><span class="s0">, </span><span class="s1">all_hessians)</span>

    <span class="s1">X_binned_C_array = np.ascontiguousarray(X_binned)</span>
    <span class="s0">with </span><span class="s1">pytest.raises(</span>
        <span class="s1">ValueError</span><span class="s0">, </span><span class="s1">match=</span><span class="s5">&quot;X_binned should be passed as Fortran contiguous array&quot;</span>
    <span class="s1">):</span>
        <span class="s1">TreeGrower(X_binned_C_array</span><span class="s0">, </span><span class="s1">all_gradients</span><span class="s0">, </span><span class="s1">all_hessians)</span>


<span class="s0">def </span><span class="s1">test_init_parameters_validation():</span>
    <span class="s1">X_binned</span><span class="s0">, </span><span class="s1">all_gradients</span><span class="s0">, </span><span class="s1">all_hessians = _make_training_data()</span>
    <span class="s0">with </span><span class="s1">pytest.raises(ValueError</span><span class="s0">, </span><span class="s1">match=</span><span class="s5">&quot;min_gain_to_split=-1 must be positive&quot;</span><span class="s1">):</span>
        <span class="s1">TreeGrower(X_binned</span><span class="s0">, </span><span class="s1">all_gradients</span><span class="s0">, </span><span class="s1">all_hessians</span><span class="s0">, </span><span class="s1">min_gain_to_split=-</span><span class="s2">1</span><span class="s1">)</span>

    <span class="s0">with </span><span class="s1">pytest.raises(ValueError</span><span class="s0">, </span><span class="s1">match=</span><span class="s5">&quot;min_hessian_to_split=-1 must be positive&quot;</span><span class="s1">):</span>
        <span class="s1">TreeGrower(X_binned</span><span class="s0">, </span><span class="s1">all_gradients</span><span class="s0">, </span><span class="s1">all_hessians</span><span class="s0">, </span><span class="s1">min_hessian_to_split=-</span><span class="s2">1</span><span class="s1">)</span>


<span class="s0">def </span><span class="s1">test_missing_value_predict_only():</span>
    <span class="s3"># Make sure that missing values are supported at predict time even if they</span>
    <span class="s3"># were not encountered in the training data: the missing values are</span>
    <span class="s3"># assigned to whichever child has the most samples.</span>

    <span class="s1">rng = np.random.RandomState(</span><span class="s2">0</span><span class="s1">)</span>
    <span class="s1">n_samples = </span><span class="s2">100</span>
    <span class="s1">X_binned = rng.randint(</span><span class="s2">0</span><span class="s0">, </span><span class="s2">256</span><span class="s0">, </span><span class="s1">size=(n_samples</span><span class="s0">, </span><span class="s2">1</span><span class="s1">)</span><span class="s0">, </span><span class="s1">dtype=np.uint8)</span>
    <span class="s1">X_binned = np.asfortranarray(X_binned)</span>

    <span class="s1">gradients = rng.normal(size=n_samples).astype(G_H_DTYPE)</span>
    <span class="s1">hessians = np.ones(shape=</span><span class="s2">1</span><span class="s0">, </span><span class="s1">dtype=G_H_DTYPE)</span>

    <span class="s1">grower = TreeGrower(</span>
        <span class="s1">X_binned</span><span class="s0">, </span><span class="s1">gradients</span><span class="s0">, </span><span class="s1">hessians</span><span class="s0">, </span><span class="s1">min_samples_leaf=</span><span class="s2">5</span><span class="s0">, </span><span class="s1">has_missing_values=</span><span class="s0">False</span>
    <span class="s1">)</span>
    <span class="s1">grower.grow()</span>

    <span class="s3"># We pass undefined binning_thresholds because we won't use predict anyway</span>
    <span class="s1">predictor = grower.make_predictor(</span>
        <span class="s1">binning_thresholds=np.zeros((X_binned.shape[</span><span class="s2">1</span><span class="s1">]</span><span class="s0">, </span><span class="s1">X_binned.max() + </span><span class="s2">1</span><span class="s1">))</span>
    <span class="s1">)</span>

    <span class="s3"># go from root to a leaf, always following node with the most samples.</span>
    <span class="s3"># That's the path nans are supposed to take</span>
    <span class="s1">node = predictor.nodes[</span><span class="s2">0</span><span class="s1">]</span>
    <span class="s0">while not </span><span class="s1">node[</span><span class="s5">&quot;is_leaf&quot;</span><span class="s1">]:</span>
        <span class="s1">left = predictor.nodes[node[</span><span class="s5">&quot;left&quot;</span><span class="s1">]]</span>
        <span class="s1">right = predictor.nodes[node[</span><span class="s5">&quot;right&quot;</span><span class="s1">]]</span>
        <span class="s1">node = left </span><span class="s0">if </span><span class="s1">left[</span><span class="s5">&quot;count&quot;</span><span class="s1">] &gt; right[</span><span class="s5">&quot;count&quot;</span><span class="s1">] </span><span class="s0">else </span><span class="s1">right</span>

    <span class="s1">prediction_main_path = node[</span><span class="s5">&quot;value&quot;</span><span class="s1">]</span>

    <span class="s3"># now build X_test with only nans, and make sure all predictions are equal</span>
    <span class="s3"># to prediction_main_path</span>
    <span class="s1">all_nans = np.full(shape=(n_samples</span><span class="s0">, </span><span class="s2">1</span><span class="s1">)</span><span class="s0">, </span><span class="s1">fill_value=np.nan)</span>
    <span class="s1">known_cat_bitsets = np.zeros((</span><span class="s2">0</span><span class="s0">, </span><span class="s2">8</span><span class="s1">)</span><span class="s0">, </span><span class="s1">dtype=X_BITSET_INNER_DTYPE)</span>
    <span class="s1">f_idx_map = np.zeros(</span><span class="s2">0</span><span class="s0">, </span><span class="s1">dtype=np.uint32)</span>

    <span class="s1">y_pred = predictor.predict(all_nans</span><span class="s0">, </span><span class="s1">known_cat_bitsets</span><span class="s0">, </span><span class="s1">f_idx_map</span><span class="s0">, </span><span class="s1">n_threads)</span>
    <span class="s0">assert </span><span class="s1">np.all(y_pred == prediction_main_path)</span>


<span class="s0">def </span><span class="s1">test_split_on_nan_with_infinite_values():</span>
    <span class="s3"># Make sure the split on nan situations are respected even when there are</span>
    <span class="s3"># samples with +inf values (we set the threshold to +inf when we have a</span>
    <span class="s3"># split on nan so this test makes sure this does not introduce edge-case</span>
    <span class="s3"># bugs). We need to use the private API so that we can also test</span>
    <span class="s3"># predict_binned().</span>

    <span class="s1">X = np.array([</span><span class="s2">0</span><span class="s0">, </span><span class="s2">1</span><span class="s0">, </span><span class="s1">np.inf</span><span class="s0">, </span><span class="s1">np.nan</span><span class="s0">, </span><span class="s1">np.nan]).reshape(-</span><span class="s2">1</span><span class="s0">, </span><span class="s2">1</span><span class="s1">)</span>
    <span class="s3"># the gradient values will force a split on nan situation</span>
    <span class="s1">gradients = np.array([</span><span class="s2">0</span><span class="s0">, </span><span class="s2">0</span><span class="s0">, </span><span class="s2">0</span><span class="s0">, </span><span class="s2">100</span><span class="s0">, </span><span class="s2">100</span><span class="s1">]</span><span class="s0">, </span><span class="s1">dtype=G_H_DTYPE)</span>
    <span class="s1">hessians = np.ones(shape=</span><span class="s2">1</span><span class="s0">, </span><span class="s1">dtype=G_H_DTYPE)</span>

    <span class="s1">bin_mapper = _BinMapper()</span>
    <span class="s1">X_binned = bin_mapper.fit_transform(X)</span>

    <span class="s1">n_bins_non_missing = </span><span class="s2">3</span>
    <span class="s1">has_missing_values = </span><span class="s0">True</span>
    <span class="s1">grower = TreeGrower(</span>
        <span class="s1">X_binned</span><span class="s0">,</span>
        <span class="s1">gradients</span><span class="s0">,</span>
        <span class="s1">hessians</span><span class="s0">,</span>
        <span class="s1">n_bins_non_missing=n_bins_non_missing</span><span class="s0">,</span>
        <span class="s1">has_missing_values=has_missing_values</span><span class="s0">,</span>
        <span class="s1">min_samples_leaf=</span><span class="s2">1</span><span class="s0">,</span>
        <span class="s1">n_threads=n_threads</span><span class="s0">,</span>
    <span class="s1">)</span>

    <span class="s1">grower.grow()</span>

    <span class="s1">predictor = grower.make_predictor(binning_thresholds=bin_mapper.bin_thresholds_)</span>

    <span class="s3"># sanity check: this was a split on nan</span>
    <span class="s0">assert </span><span class="s1">predictor.nodes[</span><span class="s2">0</span><span class="s1">][</span><span class="s5">&quot;num_threshold&quot;</span><span class="s1">] == np.inf</span>
    <span class="s0">assert </span><span class="s1">predictor.nodes[</span><span class="s2">0</span><span class="s1">][</span><span class="s5">&quot;bin_threshold&quot;</span><span class="s1">] == n_bins_non_missing - </span><span class="s2">1</span>

    <span class="s1">known_cat_bitsets</span><span class="s0">, </span><span class="s1">f_idx_map = bin_mapper.make_known_categories_bitsets()</span>

    <span class="s3"># Make sure in particular that the +inf sample is mapped to the left child</span>
    <span class="s3"># Note that lightgbm &quot;fails&quot; here and will assign the inf sample to the</span>
    <span class="s3"># right child, even though it's a &quot;split on nan&quot; situation.</span>
    <span class="s1">predictions = predictor.predict(X</span><span class="s0">, </span><span class="s1">known_cat_bitsets</span><span class="s0">, </span><span class="s1">f_idx_map</span><span class="s0">, </span><span class="s1">n_threads)</span>
    <span class="s1">predictions_binned = predictor.predict_binned(</span>
        <span class="s1">X_binned</span><span class="s0">,</span>
        <span class="s1">missing_values_bin_idx=bin_mapper.missing_values_bin_idx_</span><span class="s0">,</span>
        <span class="s1">n_threads=n_threads</span><span class="s0">,</span>
    <span class="s1">)</span>
    <span class="s1">np.testing.assert_allclose(predictions</span><span class="s0">, </span><span class="s1">-gradients)</span>
    <span class="s1">np.testing.assert_allclose(predictions_binned</span><span class="s0">, </span><span class="s1">-gradients)</span>


<span class="s0">def </span><span class="s1">test_grow_tree_categories():</span>
    <span class="s3"># Check that the grower produces the right predictor tree when a split is</span>
    <span class="s3"># categorical</span>
    <span class="s1">X_binned = np.array([[</span><span class="s2">0</span><span class="s0">, </span><span class="s2">1</span><span class="s1">] * </span><span class="s2">11 </span><span class="s1">+ [</span><span class="s2">1</span><span class="s1">]]</span><span class="s0">, </span><span class="s1">dtype=X_BINNED_DTYPE).T</span>
    <span class="s1">X_binned = np.asfortranarray(X_binned)</span>

    <span class="s1">all_gradients = np.array([</span><span class="s2">10</span><span class="s0">, </span><span class="s2">1</span><span class="s1">] * </span><span class="s2">11 </span><span class="s1">+ [</span><span class="s2">1</span><span class="s1">]</span><span class="s0">, </span><span class="s1">dtype=G_H_DTYPE)</span>
    <span class="s1">all_hessians = np.ones(</span><span class="s2">1</span><span class="s0">, </span><span class="s1">dtype=G_H_DTYPE)</span>
    <span class="s1">is_categorical = np.ones(</span><span class="s2">1</span><span class="s0">, </span><span class="s1">dtype=np.uint8)</span>

    <span class="s1">grower = TreeGrower(</span>
        <span class="s1">X_binned</span><span class="s0">,</span>
        <span class="s1">all_gradients</span><span class="s0">,</span>
        <span class="s1">all_hessians</span><span class="s0">,</span>
        <span class="s1">n_bins=</span><span class="s2">4</span><span class="s0">,</span>
        <span class="s1">shrinkage=</span><span class="s2">1.0</span><span class="s0">,</span>
        <span class="s1">min_samples_leaf=</span><span class="s2">1</span><span class="s0">,</span>
        <span class="s1">is_categorical=is_categorical</span><span class="s0">,</span>
        <span class="s1">n_threads=n_threads</span><span class="s0">,</span>
    <span class="s1">)</span>
    <span class="s1">grower.grow()</span>
    <span class="s0">assert </span><span class="s1">grower.n_nodes == </span><span class="s2">3</span>

    <span class="s1">categories = [np.array([</span><span class="s2">4</span><span class="s0">, </span><span class="s2">9</span><span class="s1">]</span><span class="s0">, </span><span class="s1">dtype=X_DTYPE)]</span>
    <span class="s1">predictor = grower.make_predictor(binning_thresholds=categories)</span>
    <span class="s1">root = predictor.nodes[</span><span class="s2">0</span><span class="s1">]</span>
    <span class="s0">assert </span><span class="s1">root[</span><span class="s5">&quot;count&quot;</span><span class="s1">] == </span><span class="s2">23</span>
    <span class="s0">assert </span><span class="s1">root[</span><span class="s5">&quot;depth&quot;</span><span class="s1">] == </span><span class="s2">0</span>
    <span class="s0">assert </span><span class="s1">root[</span><span class="s5">&quot;is_categorical&quot;</span><span class="s1">]</span>

    <span class="s1">left</span><span class="s0">, </span><span class="s1">right = predictor.nodes[root[</span><span class="s5">&quot;left&quot;</span><span class="s1">]]</span><span class="s0">, </span><span class="s1">predictor.nodes[root[</span><span class="s5">&quot;right&quot;</span><span class="s1">]]</span>

    <span class="s3"># arbitrary validation, but this means ones go to the left.</span>
    <span class="s0">assert </span><span class="s1">left[</span><span class="s5">&quot;count&quot;</span><span class="s1">] &gt;= right[</span><span class="s5">&quot;count&quot;</span><span class="s1">]</span>

    <span class="s3"># check binned category value (1)</span>
    <span class="s1">expected_binned_cat_bitset = [</span><span class="s2">2</span><span class="s1">**</span><span class="s2">1</span><span class="s1">] + [</span><span class="s2">0</span><span class="s1">] * </span><span class="s2">7</span>
    <span class="s1">binned_cat_bitset = predictor.binned_left_cat_bitsets</span>
    <span class="s1">assert_array_equal(binned_cat_bitset[</span><span class="s2">0</span><span class="s1">]</span><span class="s0">, </span><span class="s1">expected_binned_cat_bitset)</span>

    <span class="s3"># check raw category value (9)</span>
    <span class="s1">expected_raw_cat_bitsets = [</span><span class="s2">2</span><span class="s1">**</span><span class="s2">9</span><span class="s1">] + [</span><span class="s2">0</span><span class="s1">] * </span><span class="s2">7</span>
    <span class="s1">raw_cat_bitsets = predictor.raw_left_cat_bitsets</span>
    <span class="s1">assert_array_equal(raw_cat_bitsets[</span><span class="s2">0</span><span class="s1">]</span><span class="s0">, </span><span class="s1">expected_raw_cat_bitsets)</span>

    <span class="s3"># Note that since there was no missing values during training, the missing</span>
    <span class="s3"># values aren't part of the bitsets. However, we expect the missing values</span>
    <span class="s3"># to go to the biggest child (i.e. the left one).</span>
    <span class="s3"># The left child has a value of -1 = negative gradient.</span>
    <span class="s0">assert </span><span class="s1">root[</span><span class="s5">&quot;missing_go_to_left&quot;</span><span class="s1">]</span>

    <span class="s3"># make sure binned missing values are mapped to the left child during</span>
    <span class="s3"># prediction</span>
    <span class="s1">prediction_binned = predictor.predict_binned(</span>
        <span class="s1">np.asarray([[</span><span class="s2">6</span><span class="s1">]]).astype(X_BINNED_DTYPE)</span><span class="s0">,</span>
        <span class="s1">missing_values_bin_idx=</span><span class="s2">6</span><span class="s0">,</span>
        <span class="s1">n_threads=n_threads</span><span class="s0">,</span>
    <span class="s1">)</span>
    <span class="s1">assert_allclose(prediction_binned</span><span class="s0">, </span><span class="s1">[-</span><span class="s2">1</span><span class="s1">])  </span><span class="s3"># negative gradient</span>

    <span class="s3"># make sure raw missing values are mapped to the left child during</span>
    <span class="s3"># prediction</span>
    <span class="s1">known_cat_bitsets = np.zeros((</span><span class="s2">1</span><span class="s0">, </span><span class="s2">8</span><span class="s1">)</span><span class="s0">, </span><span class="s1">dtype=np.uint32)  </span><span class="s3"># ignored anyway</span>
    <span class="s1">f_idx_map = np.array([</span><span class="s2">0</span><span class="s1">]</span><span class="s0">, </span><span class="s1">dtype=np.uint32)</span>
    <span class="s1">prediction = predictor.predict(</span>
        <span class="s1">np.array([[np.nan]])</span><span class="s0">, </span><span class="s1">known_cat_bitsets</span><span class="s0">, </span><span class="s1">f_idx_map</span><span class="s0">, </span><span class="s1">n_threads</span>
    <span class="s1">)</span>
    <span class="s1">assert_allclose(prediction</span><span class="s0">, </span><span class="s1">[-</span><span class="s2">1</span><span class="s1">])</span>


<span class="s1">@pytest.mark.parametrize(</span><span class="s5">&quot;min_samples_leaf&quot;</span><span class="s0">, </span><span class="s1">(</span><span class="s2">1</span><span class="s0">, </span><span class="s2">20</span><span class="s1">))</span>
<span class="s1">@pytest.mark.parametrize(</span><span class="s5">&quot;n_unique_categories&quot;</span><span class="s0">, </span><span class="s1">(</span><span class="s2">2</span><span class="s0">, </span><span class="s2">10</span><span class="s0">, </span><span class="s2">100</span><span class="s1">))</span>
<span class="s1">@pytest.mark.parametrize(</span><span class="s5">&quot;target&quot;</span><span class="s0">, </span><span class="s1">(</span><span class="s5">&quot;binary&quot;</span><span class="s0">, </span><span class="s5">&quot;random&quot;</span><span class="s0">, </span><span class="s5">&quot;equal&quot;</span><span class="s1">))</span>
<span class="s0">def </span><span class="s1">test_ohe_equivalence(min_samples_leaf</span><span class="s0">, </span><span class="s1">n_unique_categories</span><span class="s0">, </span><span class="s1">target):</span>
    <span class="s3"># Make sure that native categorical splits are equivalent to using a OHE,</span>
    <span class="s3"># when given enough depth</span>

    <span class="s1">rng = np.random.RandomState(</span><span class="s2">0</span><span class="s1">)</span>
    <span class="s1">n_samples = </span><span class="s2">10_000</span>
    <span class="s1">X_binned = rng.randint(</span><span class="s2">0</span><span class="s0">, </span><span class="s1">n_unique_categories</span><span class="s0">, </span><span class="s1">size=(n_samples</span><span class="s0">, </span><span class="s2">1</span><span class="s1">)</span><span class="s0">, </span><span class="s1">dtype=np.uint8)</span>

    <span class="s1">X_ohe = OneHotEncoder(sparse_output=</span><span class="s0">False</span><span class="s1">).fit_transform(X_binned)</span>
    <span class="s1">X_ohe = np.asfortranarray(X_ohe).astype(np.uint8)</span>

    <span class="s0">if </span><span class="s1">target == </span><span class="s5">&quot;equal&quot;</span><span class="s1">:</span>
        <span class="s1">gradients = X_binned.reshape(-</span><span class="s2">1</span><span class="s1">)</span>
    <span class="s0">elif </span><span class="s1">target == </span><span class="s5">&quot;binary&quot;</span><span class="s1">:</span>
        <span class="s1">gradients = (X_binned % </span><span class="s2">2</span><span class="s1">).reshape(-</span><span class="s2">1</span><span class="s1">)</span>
    <span class="s0">else</span><span class="s1">:</span>
        <span class="s1">gradients = rng.randn(n_samples)</span>
    <span class="s1">gradients = gradients.astype(G_H_DTYPE)</span>

    <span class="s1">hessians = np.ones(shape=</span><span class="s2">1</span><span class="s0">, </span><span class="s1">dtype=G_H_DTYPE)</span>

    <span class="s1">grower_params = {</span>
        <span class="s5">&quot;min_samples_leaf&quot;</span><span class="s1">: min_samples_leaf</span><span class="s0">,</span>
        <span class="s5">&quot;max_depth&quot;</span><span class="s1">: </span><span class="s0">None,</span>
        <span class="s5">&quot;max_leaf_nodes&quot;</span><span class="s1">: </span><span class="s0">None,</span>
    <span class="s1">}</span>

    <span class="s1">grower = TreeGrower(</span>
        <span class="s1">X_binned</span><span class="s0">, </span><span class="s1">gradients</span><span class="s0">, </span><span class="s1">hessians</span><span class="s0">, </span><span class="s1">is_categorical=[</span><span class="s0">True</span><span class="s1">]</span><span class="s0">, </span><span class="s1">**grower_params</span>
    <span class="s1">)</span>
    <span class="s1">grower.grow()</span>
    <span class="s3"># we pass undefined bin_thresholds because we won't use predict()</span>
    <span class="s1">predictor = grower.make_predictor(</span>
        <span class="s1">binning_thresholds=np.zeros((</span><span class="s2">1</span><span class="s0">, </span><span class="s1">n_unique_categories))</span>
    <span class="s1">)</span>
    <span class="s1">preds = predictor.predict_binned(</span>
        <span class="s1">X_binned</span><span class="s0">, </span><span class="s1">missing_values_bin_idx=</span><span class="s2">255</span><span class="s0">, </span><span class="s1">n_threads=n_threads</span>
    <span class="s1">)</span>

    <span class="s1">grower_ohe = TreeGrower(X_ohe</span><span class="s0">, </span><span class="s1">gradients</span><span class="s0">, </span><span class="s1">hessians</span><span class="s0">, </span><span class="s1">**grower_params)</span>
    <span class="s1">grower_ohe.grow()</span>
    <span class="s1">predictor_ohe = grower_ohe.make_predictor(</span>
        <span class="s1">binning_thresholds=np.zeros((X_ohe.shape[</span><span class="s2">1</span><span class="s1">]</span><span class="s0">, </span><span class="s1">n_unique_categories))</span>
    <span class="s1">)</span>
    <span class="s1">preds_ohe = predictor_ohe.predict_binned(</span>
        <span class="s1">X_ohe</span><span class="s0">, </span><span class="s1">missing_values_bin_idx=</span><span class="s2">255</span><span class="s0">, </span><span class="s1">n_threads=n_threads</span>
    <span class="s1">)</span>

    <span class="s0">assert </span><span class="s1">predictor.get_max_depth() &lt;= predictor_ohe.get_max_depth()</span>
    <span class="s0">if </span><span class="s1">target == </span><span class="s5">&quot;binary&quot; </span><span class="s0">and </span><span class="s1">n_unique_categories &gt; </span><span class="s2">2</span><span class="s1">:</span>
        <span class="s3"># OHE needs more splits to achieve the same predictions</span>
        <span class="s0">assert </span><span class="s1">predictor.get_max_depth() &lt; predictor_ohe.get_max_depth()</span>

    <span class="s1">np.testing.assert_allclose(preds</span><span class="s0">, </span><span class="s1">preds_ohe)</span>


<span class="s0">def </span><span class="s1">test_grower_interaction_constraints():</span>
    <span class="s4">&quot;&quot;&quot;Check that grower respects interaction constraints.&quot;&quot;&quot;</span>
    <span class="s1">n_features = </span><span class="s2">6</span>
    <span class="s1">interaction_cst = [{</span><span class="s2">0</span><span class="s0">, </span><span class="s2">1</span><span class="s1">}</span><span class="s0">, </span><span class="s1">{</span><span class="s2">1</span><span class="s0">, </span><span class="s2">2</span><span class="s1">}</span><span class="s0">, </span><span class="s1">{</span><span class="s2">3</span><span class="s0">, </span><span class="s2">4</span><span class="s0">, </span><span class="s2">5</span><span class="s1">}]</span>
    <span class="s1">n_samples = </span><span class="s2">10</span>
    <span class="s1">n_bins = </span><span class="s2">6</span>
    <span class="s1">root_feature_splits = []</span>

    <span class="s0">def </span><span class="s1">get_all_children(node):</span>
        <span class="s1">res = []</span>
        <span class="s0">if </span><span class="s1">node.is_leaf:</span>
            <span class="s0">return </span><span class="s1">res</span>
        <span class="s0">for </span><span class="s1">n </span><span class="s0">in </span><span class="s1">[node.left_child</span><span class="s0">, </span><span class="s1">node.right_child]:</span>
            <span class="s1">res.append(n)</span>
            <span class="s1">res.extend(get_all_children(n))</span>
        <span class="s0">return </span><span class="s1">res</span>

    <span class="s0">for </span><span class="s1">seed </span><span class="s0">in </span><span class="s1">range(</span><span class="s2">20</span><span class="s1">):</span>
        <span class="s1">rng = np.random.RandomState(seed)</span>

        <span class="s1">X_binned = rng.randint(</span>
            <span class="s2">0</span><span class="s0">, </span><span class="s1">n_bins - </span><span class="s2">1</span><span class="s0">, </span><span class="s1">size=(n_samples</span><span class="s0">, </span><span class="s1">n_features)</span><span class="s0">, </span><span class="s1">dtype=X_BINNED_DTYPE</span>
        <span class="s1">)</span>
        <span class="s1">X_binned = np.asfortranarray(X_binned)</span>
        <span class="s1">gradients = rng.normal(size=n_samples).astype(G_H_DTYPE)</span>
        <span class="s1">hessians = np.ones(shape=</span><span class="s2">1</span><span class="s0">, </span><span class="s1">dtype=G_H_DTYPE)</span>

        <span class="s1">grower = TreeGrower(</span>
            <span class="s1">X_binned</span><span class="s0">,</span>
            <span class="s1">gradients</span><span class="s0">,</span>
            <span class="s1">hessians</span><span class="s0">,</span>
            <span class="s1">n_bins=n_bins</span><span class="s0">,</span>
            <span class="s1">min_samples_leaf=</span><span class="s2">1</span><span class="s0">,</span>
            <span class="s1">interaction_cst=interaction_cst</span><span class="s0">,</span>
            <span class="s1">n_threads=n_threads</span><span class="s0">,</span>
        <span class="s1">)</span>
        <span class="s1">grower.grow()</span>

        <span class="s1">root_feature_idx = grower.root.split_info.feature_idx</span>
        <span class="s1">root_feature_splits.append(root_feature_idx)</span>

        <span class="s1">feature_idx_to_constraint_set = {</span>
            <span class="s2">0</span><span class="s1">: {</span><span class="s2">0</span><span class="s0">, </span><span class="s2">1</span><span class="s1">}</span><span class="s0">,</span>
            <span class="s2">1</span><span class="s1">: {</span><span class="s2">0</span><span class="s0">, </span><span class="s2">1</span><span class="s0">, </span><span class="s2">2</span><span class="s1">}</span><span class="s0">,</span>
            <span class="s2">2</span><span class="s1">: {</span><span class="s2">1</span><span class="s0">, </span><span class="s2">2</span><span class="s1">}</span><span class="s0">,</span>
            <span class="s2">3</span><span class="s1">: {</span><span class="s2">3</span><span class="s0">, </span><span class="s2">4</span><span class="s0">, </span><span class="s2">5</span><span class="s1">}</span><span class="s0">,</span>
            <span class="s2">4</span><span class="s1">: {</span><span class="s2">3</span><span class="s0">, </span><span class="s2">4</span><span class="s0">, </span><span class="s2">5</span><span class="s1">}</span><span class="s0">,</span>
            <span class="s2">5</span><span class="s1">: {</span><span class="s2">3</span><span class="s0">, </span><span class="s2">4</span><span class="s0">, </span><span class="s2">5</span><span class="s1">}</span><span class="s0">,</span>
        <span class="s1">}</span>

        <span class="s1">root_constraint_set = feature_idx_to_constraint_set[root_feature_idx]</span>
        <span class="s0">for </span><span class="s1">node </span><span class="s0">in </span><span class="s1">(grower.root.left_child</span><span class="s0">, </span><span class="s1">grower.root.right_child):</span>
            <span class="s3"># Root's children's allowed_features must be the root's constraints set.</span>
            <span class="s1">assert_array_equal(node.allowed_features</span><span class="s0">, </span><span class="s1">list(root_constraint_set))</span>
        <span class="s0">for </span><span class="s1">node </span><span class="s0">in </span><span class="s1">get_all_children(grower.root):</span>
            <span class="s0">if </span><span class="s1">node.is_leaf:</span>
                <span class="s0">continue</span>
            <span class="s3"># Ensure that each node uses a subset of features of its parent node.</span>
            <span class="s1">parent_interaction_cst_indices = set(node.interaction_cst_indices)</span>
            <span class="s1">right_interactions_cst_indices = set(</span>
                <span class="s1">node.right_child.interaction_cst_indices</span>
            <span class="s1">)</span>
            <span class="s1">left_interactions_cst_indices = set(node.left_child.interaction_cst_indices)</span>

            <span class="s0">assert </span><span class="s1">right_interactions_cst_indices.issubset(</span>
                <span class="s1">parent_interaction_cst_indices</span>
            <span class="s1">)</span>
            <span class="s0">assert </span><span class="s1">left_interactions_cst_indices.issubset(</span>
                <span class="s1">parent_interaction_cst_indices</span>
            <span class="s1">)</span>
            <span class="s3"># The features used for split must have been present in the root's</span>
            <span class="s3"># constraint set.</span>
            <span class="s0">assert </span><span class="s1">node.split_info.feature_idx </span><span class="s0">in </span><span class="s1">root_constraint_set</span>

    <span class="s3"># Make sure that every feature is used at least once as split for the root node.</span>
    <span class="s0">assert </span><span class="s1">(</span>
        <span class="s1">len(set(root_feature_splits))</span>
        <span class="s1">== len(set().union(*interaction_cst))</span>
        <span class="s1">== n_features</span>
    <span class="s1">)</span>
</pre>
</body>
</html>