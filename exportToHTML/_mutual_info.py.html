<html>
<head>
<title>_mutual_info.py</title>
<meta http-equiv="Content-Type" content="text/html; charset=utf-8">
<style type="text/css">
.s0 { color: #808080;}
.s1 { color: #a9b7c6;}
.s2 { color: #cc7832;}
.s3 { color: #629755; font-style: italic;}
.s4 { color: #6897bb;}
.s5 { color: #6a8759;}
</style>
</head>
<body bgcolor="#2b2b2b">
<table CELLSPACING=0 CELLPADDING=5 COLS=1 WIDTH="100%" BGCOLOR="#606060" >
<tr><td><center>
<font face="Arial, Helvetica" color="#000000">
_mutual_info.py</font>
</center></td></tr></table>
<pre><span class="s0"># Author: Nikolay Mayorov &lt;n59_ru@hotmail.com&gt;</span>
<span class="s0"># License: 3-clause BSD</span>

<span class="s2">from </span><span class="s1">numbers </span><span class="s2">import </span><span class="s1">Integral</span>

<span class="s2">import </span><span class="s1">numpy </span><span class="s2">as </span><span class="s1">np</span>
<span class="s2">from </span><span class="s1">scipy.sparse </span><span class="s2">import </span><span class="s1">issparse</span>
<span class="s2">from </span><span class="s1">scipy.special </span><span class="s2">import </span><span class="s1">digamma</span>

<span class="s2">from </span><span class="s1">..metrics.cluster </span><span class="s2">import </span><span class="s1">mutual_info_score</span>
<span class="s2">from </span><span class="s1">..neighbors </span><span class="s2">import </span><span class="s1">KDTree</span><span class="s2">, </span><span class="s1">NearestNeighbors</span>
<span class="s2">from </span><span class="s1">..preprocessing </span><span class="s2">import </span><span class="s1">scale</span>
<span class="s2">from </span><span class="s1">..utils </span><span class="s2">import </span><span class="s1">check_random_state</span>
<span class="s2">from </span><span class="s1">..utils._param_validation </span><span class="s2">import </span><span class="s1">Interval</span><span class="s2">, </span><span class="s1">StrOptions</span><span class="s2">, </span><span class="s1">validate_params</span>
<span class="s2">from </span><span class="s1">..utils.multiclass </span><span class="s2">import </span><span class="s1">check_classification_targets</span>
<span class="s2">from </span><span class="s1">..utils.validation </span><span class="s2">import </span><span class="s1">check_array</span><span class="s2">, </span><span class="s1">check_X_y</span>


<span class="s2">def </span><span class="s1">_compute_mi_cc(x</span><span class="s2">, </span><span class="s1">y</span><span class="s2">, </span><span class="s1">n_neighbors):</span>
    <span class="s3">&quot;&quot;&quot;Compute mutual information between two continuous variables. 
 
    Parameters 
    ---------- 
    x, y : ndarray, shape (n_samples,) 
        Samples of two continuous random variables, must have an identical 
        shape. 
 
    n_neighbors : int 
        Number of nearest neighbors to search for each point, see [1]_. 
 
    Returns 
    ------- 
    mi : float 
        Estimated mutual information. If it turned out to be negative it is 
        replace by 0. 
 
    Notes 
    ----- 
    True mutual information can't be negative. If its estimate by a numerical 
    method is negative, it means (providing the method is adequate) that the 
    mutual information is close to 0 and replacing it by 0 is a reasonable 
    strategy. 
 
    References 
    ---------- 
    .. [1] A. Kraskov, H. Stogbauer and P. Grassberger, &quot;Estimating mutual 
           information&quot;. Phys. Rev. E 69, 2004. 
    &quot;&quot;&quot;</span>
    <span class="s1">n_samples = x.size</span>

    <span class="s1">x = x.reshape((-</span><span class="s4">1</span><span class="s2">, </span><span class="s4">1</span><span class="s1">))</span>
    <span class="s1">y = y.reshape((-</span><span class="s4">1</span><span class="s2">, </span><span class="s4">1</span><span class="s1">))</span>
    <span class="s1">xy = np.hstack((x</span><span class="s2">, </span><span class="s1">y))</span>

    <span class="s0"># Here we rely on NearestNeighbors to select the fastest algorithm.</span>
    <span class="s1">nn = NearestNeighbors(metric=</span><span class="s5">&quot;chebyshev&quot;</span><span class="s2">, </span><span class="s1">n_neighbors=n_neighbors)</span>

    <span class="s1">nn.fit(xy)</span>
    <span class="s1">radius = nn.kneighbors()[</span><span class="s4">0</span><span class="s1">]</span>
    <span class="s1">radius = np.nextafter(radius[:</span><span class="s2">, </span><span class="s1">-</span><span class="s4">1</span><span class="s1">]</span><span class="s2">, </span><span class="s4">0</span><span class="s1">)</span>

    <span class="s0"># KDTree is explicitly fit to allow for the querying of number of</span>
    <span class="s0"># neighbors within a specified radius</span>
    <span class="s1">kd = KDTree(x</span><span class="s2">, </span><span class="s1">metric=</span><span class="s5">&quot;chebyshev&quot;</span><span class="s1">)</span>
    <span class="s1">nx = kd.query_radius(x</span><span class="s2">, </span><span class="s1">radius</span><span class="s2">, </span><span class="s1">count_only=</span><span class="s2">True, </span><span class="s1">return_distance=</span><span class="s2">False</span><span class="s1">)</span>
    <span class="s1">nx = np.array(nx) - </span><span class="s4">1.0</span>

    <span class="s1">kd = KDTree(y</span><span class="s2">, </span><span class="s1">metric=</span><span class="s5">&quot;chebyshev&quot;</span><span class="s1">)</span>
    <span class="s1">ny = kd.query_radius(y</span><span class="s2">, </span><span class="s1">radius</span><span class="s2">, </span><span class="s1">count_only=</span><span class="s2">True, </span><span class="s1">return_distance=</span><span class="s2">False</span><span class="s1">)</span>
    <span class="s1">ny = np.array(ny) - </span><span class="s4">1.0</span>

    <span class="s1">mi = (</span>
        <span class="s1">digamma(n_samples)</span>
        <span class="s1">+ digamma(n_neighbors)</span>
        <span class="s1">- np.mean(digamma(nx + </span><span class="s4">1</span><span class="s1">))</span>
        <span class="s1">- np.mean(digamma(ny + </span><span class="s4">1</span><span class="s1">))</span>
    <span class="s1">)</span>

    <span class="s2">return </span><span class="s1">max(</span><span class="s4">0</span><span class="s2">, </span><span class="s1">mi)</span>


<span class="s2">def </span><span class="s1">_compute_mi_cd(c</span><span class="s2">, </span><span class="s1">d</span><span class="s2">, </span><span class="s1">n_neighbors):</span>
    <span class="s3">&quot;&quot;&quot;Compute mutual information between continuous and discrete variables. 
 
    Parameters 
    ---------- 
    c : ndarray, shape (n_samples,) 
        Samples of a continuous random variable. 
 
    d : ndarray, shape (n_samples,) 
        Samples of a discrete random variable. 
 
    n_neighbors : int 
        Number of nearest neighbors to search for each point, see [1]_. 
 
    Returns 
    ------- 
    mi : float 
        Estimated mutual information. If it turned out to be negative it is 
        replace by 0. 
 
    Notes 
    ----- 
    True mutual information can't be negative. If its estimate by a numerical 
    method is negative, it means (providing the method is adequate) that the 
    mutual information is close to 0 and replacing it by 0 is a reasonable 
    strategy. 
 
    References 
    ---------- 
    .. [1] B. C. Ross &quot;Mutual Information between Discrete and Continuous 
       Data Sets&quot;. PLoS ONE 9(2), 2014. 
    &quot;&quot;&quot;</span>
    <span class="s1">n_samples = c.shape[</span><span class="s4">0</span><span class="s1">]</span>
    <span class="s1">c = c.reshape((-</span><span class="s4">1</span><span class="s2">, </span><span class="s4">1</span><span class="s1">))</span>

    <span class="s1">radius = np.empty(n_samples)</span>
    <span class="s1">label_counts = np.empty(n_samples)</span>
    <span class="s1">k_all = np.empty(n_samples)</span>
    <span class="s1">nn = NearestNeighbors()</span>
    <span class="s2">for </span><span class="s1">label </span><span class="s2">in </span><span class="s1">np.unique(d):</span>
        <span class="s1">mask = d == label</span>
        <span class="s1">count = np.sum(mask)</span>
        <span class="s2">if </span><span class="s1">count &gt; </span><span class="s4">1</span><span class="s1">:</span>
            <span class="s1">k = min(n_neighbors</span><span class="s2">, </span><span class="s1">count - </span><span class="s4">1</span><span class="s1">)</span>
            <span class="s1">nn.set_params(n_neighbors=k)</span>
            <span class="s1">nn.fit(c[mask])</span>
            <span class="s1">r = nn.kneighbors()[</span><span class="s4">0</span><span class="s1">]</span>
            <span class="s1">radius[mask] = np.nextafter(r[:</span><span class="s2">, </span><span class="s1">-</span><span class="s4">1</span><span class="s1">]</span><span class="s2">, </span><span class="s4">0</span><span class="s1">)</span>
            <span class="s1">k_all[mask] = k</span>
        <span class="s1">label_counts[mask] = count</span>

    <span class="s0"># Ignore points with unique labels.</span>
    <span class="s1">mask = label_counts &gt; </span><span class="s4">1</span>
    <span class="s1">n_samples = np.sum(mask)</span>
    <span class="s1">label_counts = label_counts[mask]</span>
    <span class="s1">k_all = k_all[mask]</span>
    <span class="s1">c = c[mask]</span>
    <span class="s1">radius = radius[mask]</span>

    <span class="s1">kd = KDTree(c)</span>
    <span class="s1">m_all = kd.query_radius(c</span><span class="s2">, </span><span class="s1">radius</span><span class="s2">, </span><span class="s1">count_only=</span><span class="s2">True, </span><span class="s1">return_distance=</span><span class="s2">False</span><span class="s1">)</span>
    <span class="s1">m_all = np.array(m_all)</span>

    <span class="s1">mi = (</span>
        <span class="s1">digamma(n_samples)</span>
        <span class="s1">+ np.mean(digamma(k_all))</span>
        <span class="s1">- np.mean(digamma(label_counts))</span>
        <span class="s1">- np.mean(digamma(m_all))</span>
    <span class="s1">)</span>

    <span class="s2">return </span><span class="s1">max(</span><span class="s4">0</span><span class="s2">, </span><span class="s1">mi)</span>


<span class="s2">def </span><span class="s1">_compute_mi(x</span><span class="s2">, </span><span class="s1">y</span><span class="s2">, </span><span class="s1">x_discrete</span><span class="s2">, </span><span class="s1">y_discrete</span><span class="s2">, </span><span class="s1">n_neighbors=</span><span class="s4">3</span><span class="s1">):</span>
    <span class="s3">&quot;&quot;&quot;Compute mutual information between two variables. 
 
    This is a simple wrapper which selects a proper function to call based on 
    whether `x` and `y` are discrete or not. 
    &quot;&quot;&quot;</span>
    <span class="s2">if </span><span class="s1">x_discrete </span><span class="s2">and </span><span class="s1">y_discrete:</span>
        <span class="s2">return </span><span class="s1">mutual_info_score(x</span><span class="s2">, </span><span class="s1">y)</span>
    <span class="s2">elif </span><span class="s1">x_discrete </span><span class="s2">and not </span><span class="s1">y_discrete:</span>
        <span class="s2">return </span><span class="s1">_compute_mi_cd(y</span><span class="s2">, </span><span class="s1">x</span><span class="s2">, </span><span class="s1">n_neighbors)</span>
    <span class="s2">elif not </span><span class="s1">x_discrete </span><span class="s2">and </span><span class="s1">y_discrete:</span>
        <span class="s2">return </span><span class="s1">_compute_mi_cd(x</span><span class="s2">, </span><span class="s1">y</span><span class="s2">, </span><span class="s1">n_neighbors)</span>
    <span class="s2">else</span><span class="s1">:</span>
        <span class="s2">return </span><span class="s1">_compute_mi_cc(x</span><span class="s2">, </span><span class="s1">y</span><span class="s2">, </span><span class="s1">n_neighbors)</span>


<span class="s2">def </span><span class="s1">_iterate_columns(X</span><span class="s2">, </span><span class="s1">columns=</span><span class="s2">None</span><span class="s1">):</span>
    <span class="s3">&quot;&quot;&quot;Iterate over columns of a matrix. 
 
    Parameters 
    ---------- 
    X : ndarray or csc_matrix, shape (n_samples, n_features) 
        Matrix over which to iterate. 
 
    columns : iterable or None, default=None 
        Indices of columns to iterate over. If None, iterate over all columns. 
 
    Yields 
    ------ 
    x : ndarray, shape (n_samples,) 
        Columns of `X` in dense format. 
    &quot;&quot;&quot;</span>
    <span class="s2">if </span><span class="s1">columns </span><span class="s2">is None</span><span class="s1">:</span>
        <span class="s1">columns = range(X.shape[</span><span class="s4">1</span><span class="s1">])</span>

    <span class="s2">if </span><span class="s1">issparse(X):</span>
        <span class="s2">for </span><span class="s1">i </span><span class="s2">in </span><span class="s1">columns:</span>
            <span class="s1">x = np.zeros(X.shape[</span><span class="s4">0</span><span class="s1">])</span>
            <span class="s1">start_ptr</span><span class="s2">, </span><span class="s1">end_ptr = X.indptr[i]</span><span class="s2">, </span><span class="s1">X.indptr[i + </span><span class="s4">1</span><span class="s1">]</span>
            <span class="s1">x[X.indices[start_ptr:end_ptr]] = X.data[start_ptr:end_ptr]</span>
            <span class="s2">yield </span><span class="s1">x</span>
    <span class="s2">else</span><span class="s1">:</span>
        <span class="s2">for </span><span class="s1">i </span><span class="s2">in </span><span class="s1">columns:</span>
            <span class="s2">yield </span><span class="s1">X[:</span><span class="s2">, </span><span class="s1">i]</span>


<span class="s2">def </span><span class="s1">_estimate_mi(</span>
    <span class="s1">X</span><span class="s2">,</span>
    <span class="s1">y</span><span class="s2">,</span>
    <span class="s1">discrete_features=</span><span class="s5">&quot;auto&quot;</span><span class="s2">,</span>
    <span class="s1">discrete_target=</span><span class="s2">False,</span>
    <span class="s1">n_neighbors=</span><span class="s4">3</span><span class="s2">,</span>
    <span class="s1">copy=</span><span class="s2">True,</span>
    <span class="s1">random_state=</span><span class="s2">None,</span>
<span class="s1">):</span>
    <span class="s3">&quot;&quot;&quot;Estimate mutual information between the features and the target. 
 
    Parameters 
    ---------- 
    X : array-like or sparse matrix, shape (n_samples, n_features) 
        Feature matrix. 
 
    y : array-like of shape (n_samples,) 
        Target vector. 
 
    discrete_features : {'auto', bool, array-like}, default='auto' 
        If bool, then determines whether to consider all features discrete 
        or continuous. If array, then it should be either a boolean mask 
        with shape (n_features,) or array with indices of discrete features. 
        If 'auto', it is assigned to False for dense `X` and to True for 
        sparse `X`. 
 
    discrete_target : bool, default=False 
        Whether to consider `y` as a discrete variable. 
 
    n_neighbors : int, default=3 
        Number of neighbors to use for MI estimation for continuous variables, 
        see [1]_ and [2]_. Higher values reduce variance of the estimation, but 
        could introduce a bias. 
 
    copy : bool, default=True 
        Whether to make a copy of the given data. If set to False, the initial 
        data will be overwritten. 
 
    random_state : int, RandomState instance or None, default=None 
        Determines random number generation for adding small noise to 
        continuous variables in order to remove repeated values. 
        Pass an int for reproducible results across multiple function calls. 
        See :term:`Glossary &lt;random_state&gt;`. 
 
    Returns 
    ------- 
    mi : ndarray, shape (n_features,) 
        Estimated mutual information between each feature and the target. 
        A negative value will be replaced by 0. 
 
    References 
    ---------- 
    .. [1] A. Kraskov, H. Stogbauer and P. Grassberger, &quot;Estimating mutual 
           information&quot;. Phys. Rev. E 69, 2004. 
    .. [2] B. C. Ross &quot;Mutual Information between Discrete and Continuous 
           Data Sets&quot;. PLoS ONE 9(2), 2014. 
    &quot;&quot;&quot;</span>
    <span class="s1">X</span><span class="s2">, </span><span class="s1">y = check_X_y(X</span><span class="s2">, </span><span class="s1">y</span><span class="s2">, </span><span class="s1">accept_sparse=</span><span class="s5">&quot;csc&quot;</span><span class="s2">, </span><span class="s1">y_numeric=</span><span class="s2">not </span><span class="s1">discrete_target)</span>
    <span class="s1">n_samples</span><span class="s2">, </span><span class="s1">n_features = X.shape</span>

    <span class="s2">if </span><span class="s1">isinstance(discrete_features</span><span class="s2">, </span><span class="s1">(str</span><span class="s2">, </span><span class="s1">bool)):</span>
        <span class="s2">if </span><span class="s1">isinstance(discrete_features</span><span class="s2">, </span><span class="s1">str):</span>
            <span class="s2">if </span><span class="s1">discrete_features == </span><span class="s5">&quot;auto&quot;</span><span class="s1">:</span>
                <span class="s1">discrete_features = issparse(X)</span>
            <span class="s2">else</span><span class="s1">:</span>
                <span class="s2">raise </span><span class="s1">ValueError(</span><span class="s5">&quot;Invalid string value for discrete_features.&quot;</span><span class="s1">)</span>
        <span class="s1">discrete_mask = np.empty(n_features</span><span class="s2">, </span><span class="s1">dtype=bool)</span>
        <span class="s1">discrete_mask.fill(discrete_features)</span>
    <span class="s2">else</span><span class="s1">:</span>
        <span class="s1">discrete_features = check_array(discrete_features</span><span class="s2">, </span><span class="s1">ensure_2d=</span><span class="s2">False</span><span class="s1">)</span>
        <span class="s2">if </span><span class="s1">discrete_features.dtype != </span><span class="s5">&quot;bool&quot;</span><span class="s1">:</span>
            <span class="s1">discrete_mask = np.zeros(n_features</span><span class="s2">, </span><span class="s1">dtype=bool)</span>
            <span class="s1">discrete_mask[discrete_features] = </span><span class="s2">True</span>
        <span class="s2">else</span><span class="s1">:</span>
            <span class="s1">discrete_mask = discrete_features</span>

    <span class="s1">continuous_mask = ~discrete_mask</span>
    <span class="s2">if </span><span class="s1">np.any(continuous_mask) </span><span class="s2">and </span><span class="s1">issparse(X):</span>
        <span class="s2">raise </span><span class="s1">ValueError(</span><span class="s5">&quot;Sparse matrix `X` can't have continuous features.&quot;</span><span class="s1">)</span>

    <span class="s1">rng = check_random_state(random_state)</span>
    <span class="s2">if </span><span class="s1">np.any(continuous_mask):</span>
        <span class="s1">X = X.astype(np.float64</span><span class="s2">, </span><span class="s1">copy=copy)</span>
        <span class="s1">X[:</span><span class="s2">, </span><span class="s1">continuous_mask] = scale(</span>
            <span class="s1">X[:</span><span class="s2">, </span><span class="s1">continuous_mask]</span><span class="s2">, </span><span class="s1">with_mean=</span><span class="s2">False, </span><span class="s1">copy=</span><span class="s2">False</span>
        <span class="s1">)</span>

        <span class="s0"># Add small noise to continuous features as advised in Kraskov et. al.</span>
        <span class="s1">means = np.maximum(</span><span class="s4">1</span><span class="s2">, </span><span class="s1">np.mean(np.abs(X[:</span><span class="s2">, </span><span class="s1">continuous_mask])</span><span class="s2">, </span><span class="s1">axis=</span><span class="s4">0</span><span class="s1">))</span>
        <span class="s1">X[:</span><span class="s2">, </span><span class="s1">continuous_mask] += (</span>
            <span class="s4">1e-10</span>
            <span class="s1">* means</span>
            <span class="s1">* rng.standard_normal(size=(n_samples</span><span class="s2">, </span><span class="s1">np.sum(continuous_mask)))</span>
        <span class="s1">)</span>

    <span class="s2">if not </span><span class="s1">discrete_target:</span>
        <span class="s1">y = scale(y</span><span class="s2">, </span><span class="s1">with_mean=</span><span class="s2">False</span><span class="s1">)</span>
        <span class="s1">y += (</span>
            <span class="s4">1e-10</span>
            <span class="s1">* np.maximum(</span><span class="s4">1</span><span class="s2">, </span><span class="s1">np.mean(np.abs(y)))</span>
            <span class="s1">* rng.standard_normal(size=n_samples)</span>
        <span class="s1">)</span>

    <span class="s1">mi = [</span>
        <span class="s1">_compute_mi(x</span><span class="s2">, </span><span class="s1">y</span><span class="s2">, </span><span class="s1">discrete_feature</span><span class="s2">, </span><span class="s1">discrete_target</span><span class="s2">, </span><span class="s1">n_neighbors)</span>
        <span class="s2">for </span><span class="s1">x</span><span class="s2">, </span><span class="s1">discrete_feature </span><span class="s2">in </span><span class="s1">zip(_iterate_columns(X)</span><span class="s2">, </span><span class="s1">discrete_mask)</span>
    <span class="s1">]</span>

    <span class="s2">return </span><span class="s1">np.array(mi)</span>


<span class="s1">@validate_params(</span>
    <span class="s1">{</span>
        <span class="s5">&quot;X&quot;</span><span class="s1">: [</span><span class="s5">&quot;array-like&quot;</span><span class="s2">, </span><span class="s5">&quot;sparse matrix&quot;</span><span class="s1">]</span><span class="s2">,</span>
        <span class="s5">&quot;y&quot;</span><span class="s1">: [</span><span class="s5">&quot;array-like&quot;</span><span class="s1">]</span><span class="s2">,</span>
        <span class="s5">&quot;discrete_features&quot;</span><span class="s1">: [StrOptions({</span><span class="s5">&quot;auto&quot;</span><span class="s1">})</span><span class="s2">, </span><span class="s5">&quot;boolean&quot;</span><span class="s2">, </span><span class="s5">&quot;array-like&quot;</span><span class="s1">]</span><span class="s2">,</span>
        <span class="s5">&quot;n_neighbors&quot;</span><span class="s1">: [Interval(Integral</span><span class="s2">, </span><span class="s4">1</span><span class="s2">, None, </span><span class="s1">closed=</span><span class="s5">&quot;left&quot;</span><span class="s1">)]</span><span class="s2">,</span>
        <span class="s5">&quot;copy&quot;</span><span class="s1">: [</span><span class="s5">&quot;boolean&quot;</span><span class="s1">]</span><span class="s2">,</span>
        <span class="s5">&quot;random_state&quot;</span><span class="s1">: [</span><span class="s5">&quot;random_state&quot;</span><span class="s1">]</span><span class="s2">,</span>
    <span class="s1">}</span><span class="s2">,</span>
    <span class="s1">prefer_skip_nested_validation=</span><span class="s2">True,</span>
<span class="s1">)</span>
<span class="s2">def </span><span class="s1">mutual_info_regression(</span>
    <span class="s1">X</span><span class="s2">, </span><span class="s1">y</span><span class="s2">, </span><span class="s1">*</span><span class="s2">, </span><span class="s1">discrete_features=</span><span class="s5">&quot;auto&quot;</span><span class="s2">, </span><span class="s1">n_neighbors=</span><span class="s4">3</span><span class="s2">, </span><span class="s1">copy=</span><span class="s2">True, </span><span class="s1">random_state=</span><span class="s2">None</span>
<span class="s1">):</span>
    <span class="s3">&quot;&quot;&quot;Estimate mutual information for a continuous target variable. 
 
    Mutual information (MI) [1]_ between two random variables is a non-negative 
    value, which measures the dependency between the variables. It is equal 
    to zero if and only if two random variables are independent, and higher 
    values mean higher dependency. 
 
    The function relies on nonparametric methods based on entropy estimation 
    from k-nearest neighbors distances as described in [2]_ and [3]_. Both 
    methods are based on the idea originally proposed in [4]_. 
 
    It can be used for univariate features selection, read more in the 
    :ref:`User Guide &lt;univariate_feature_selection&gt;`. 
 
    Parameters 
    ---------- 
    X : array-like or sparse matrix, shape (n_samples, n_features) 
        Feature matrix. 
 
    y : array-like of shape (n_samples,) 
        Target vector. 
 
    discrete_features : {'auto', bool, array-like}, default='auto' 
        If bool, then determines whether to consider all features discrete 
        or continuous. If array, then it should be either a boolean mask 
        with shape (n_features,) or array with indices of discrete features. 
        If 'auto', it is assigned to False for dense `X` and to True for 
        sparse `X`. 
 
    n_neighbors : int, default=3 
        Number of neighbors to use for MI estimation for continuous variables, 
        see [2]_ and [3]_. Higher values reduce variance of the estimation, but 
        could introduce a bias. 
 
    copy : bool, default=True 
        Whether to make a copy of the given data. If set to False, the initial 
        data will be overwritten. 
 
    random_state : int, RandomState instance or None, default=None 
        Determines random number generation for adding small noise to 
        continuous variables in order to remove repeated values. 
        Pass an int for reproducible results across multiple function calls. 
        See :term:`Glossary &lt;random_state&gt;`. 
 
    Returns 
    ------- 
    mi : ndarray, shape (n_features,) 
        Estimated mutual information between each feature and the target. 
 
    Notes 
    ----- 
    1. The term &quot;discrete features&quot; is used instead of naming them 
       &quot;categorical&quot;, because it describes the essence more accurately. 
       For example, pixel intensities of an image are discrete features 
       (but hardly categorical) and you will get better results if mark them 
       as such. Also note, that treating a continuous variable as discrete and 
       vice versa will usually give incorrect results, so be attentive about 
       that. 
    2. True mutual information can't be negative. If its estimate turns out 
       to be negative, it is replaced by zero. 
 
    References 
    ---------- 
    .. [1] `Mutual Information 
           &lt;https://en.wikipedia.org/wiki/Mutual_information&gt;`_ 
           on Wikipedia. 
    .. [2] A. Kraskov, H. Stogbauer and P. Grassberger, &quot;Estimating mutual 
           information&quot;. Phys. Rev. E 69, 2004. 
    .. [3] B. C. Ross &quot;Mutual Information between Discrete and Continuous 
           Data Sets&quot;. PLoS ONE 9(2), 2014. 
    .. [4] L. F. Kozachenko, N. N. Leonenko, &quot;Sample Estimate of the Entropy 
           of a Random Vector&quot;, Probl. Peredachi Inf., 23:2 (1987), 9-16 
    &quot;&quot;&quot;</span>
    <span class="s2">return </span><span class="s1">_estimate_mi(X</span><span class="s2">, </span><span class="s1">y</span><span class="s2">, </span><span class="s1">discrete_features</span><span class="s2">, False, </span><span class="s1">n_neighbors</span><span class="s2">, </span><span class="s1">copy</span><span class="s2">, </span><span class="s1">random_state)</span>


<span class="s1">@validate_params(</span>
    <span class="s1">{</span>
        <span class="s5">&quot;X&quot;</span><span class="s1">: [</span><span class="s5">&quot;array-like&quot;</span><span class="s2">, </span><span class="s5">&quot;sparse matrix&quot;</span><span class="s1">]</span><span class="s2">,</span>
        <span class="s5">&quot;y&quot;</span><span class="s1">: [</span><span class="s5">&quot;array-like&quot;</span><span class="s1">]</span><span class="s2">,</span>
        <span class="s5">&quot;discrete_features&quot;</span><span class="s1">: [StrOptions({</span><span class="s5">&quot;auto&quot;</span><span class="s1">})</span><span class="s2">, </span><span class="s5">&quot;boolean&quot;</span><span class="s2">, </span><span class="s5">&quot;array-like&quot;</span><span class="s1">]</span><span class="s2">,</span>
        <span class="s5">&quot;n_neighbors&quot;</span><span class="s1">: [Interval(Integral</span><span class="s2">, </span><span class="s4">1</span><span class="s2">, None, </span><span class="s1">closed=</span><span class="s5">&quot;left&quot;</span><span class="s1">)]</span><span class="s2">,</span>
        <span class="s5">&quot;copy&quot;</span><span class="s1">: [</span><span class="s5">&quot;boolean&quot;</span><span class="s1">]</span><span class="s2">,</span>
        <span class="s5">&quot;random_state&quot;</span><span class="s1">: [</span><span class="s5">&quot;random_state&quot;</span><span class="s1">]</span><span class="s2">,</span>
    <span class="s1">}</span><span class="s2">,</span>
    <span class="s1">prefer_skip_nested_validation=</span><span class="s2">True,</span>
<span class="s1">)</span>
<span class="s2">def </span><span class="s1">mutual_info_classif(</span>
    <span class="s1">X</span><span class="s2">, </span><span class="s1">y</span><span class="s2">, </span><span class="s1">*</span><span class="s2">, </span><span class="s1">discrete_features=</span><span class="s5">&quot;auto&quot;</span><span class="s2">, </span><span class="s1">n_neighbors=</span><span class="s4">3</span><span class="s2">, </span><span class="s1">copy=</span><span class="s2">True, </span><span class="s1">random_state=</span><span class="s2">None</span>
<span class="s1">):</span>
    <span class="s3">&quot;&quot;&quot;Estimate mutual information for a discrete target variable. 
 
    Mutual information (MI) [1]_ between two random variables is a non-negative 
    value, which measures the dependency between the variables. It is equal 
    to zero if and only if two random variables are independent, and higher 
    values mean higher dependency. 
 
    The function relies on nonparametric methods based on entropy estimation 
    from k-nearest neighbors distances as described in [2]_ and [3]_. Both 
    methods are based on the idea originally proposed in [4]_. 
 
    It can be used for univariate features selection, read more in the 
    :ref:`User Guide &lt;univariate_feature_selection&gt;`. 
 
    Parameters 
    ---------- 
    X : {array-like, sparse matrix} of shape (n_samples, n_features) 
        Feature matrix. 
 
    y : array-like of shape (n_samples,) 
        Target vector. 
 
    discrete_features : 'auto', bool or array-like, default='auto' 
        If bool, then determines whether to consider all features discrete 
        or continuous. If array, then it should be either a boolean mask 
        with shape (n_features,) or array with indices of discrete features. 
        If 'auto', it is assigned to False for dense `X` and to True for 
        sparse `X`. 
 
    n_neighbors : int, default=3 
        Number of neighbors to use for MI estimation for continuous variables, 
        see [2]_ and [3]_. Higher values reduce variance of the estimation, but 
        could introduce a bias. 
 
    copy : bool, default=True 
        Whether to make a copy of the given data. If set to False, the initial 
        data will be overwritten. 
 
    random_state : int, RandomState instance or None, default=None 
        Determines random number generation for adding small noise to 
        continuous variables in order to remove repeated values. 
        Pass an int for reproducible results across multiple function calls. 
        See :term:`Glossary &lt;random_state&gt;`. 
 
    Returns 
    ------- 
    mi : ndarray, shape (n_features,) 
        Estimated mutual information between each feature and the target. 
 
    Notes 
    ----- 
    1. The term &quot;discrete features&quot; is used instead of naming them 
       &quot;categorical&quot;, because it describes the essence more accurately. 
       For example, pixel intensities of an image are discrete features 
       (but hardly categorical) and you will get better results if mark them 
       as such. Also note, that treating a continuous variable as discrete and 
       vice versa will usually give incorrect results, so be attentive about 
       that. 
    2. True mutual information can't be negative. If its estimate turns out 
       to be negative, it is replaced by zero. 
 
    References 
    ---------- 
    .. [1] `Mutual Information 
           &lt;https://en.wikipedia.org/wiki/Mutual_information&gt;`_ 
           on Wikipedia. 
    .. [2] A. Kraskov, H. Stogbauer and P. Grassberger, &quot;Estimating mutual 
           information&quot;. Phys. Rev. E 69, 2004. 
    .. [3] B. C. Ross &quot;Mutual Information between Discrete and Continuous 
           Data Sets&quot;. PLoS ONE 9(2), 2014. 
    .. [4] L. F. Kozachenko, N. N. Leonenko, &quot;Sample Estimate of the Entropy 
           of a Random Vector:, Probl. Peredachi Inf., 23:2 (1987), 9-16 
    &quot;&quot;&quot;</span>
    <span class="s1">check_classification_targets(y)</span>
    <span class="s2">return </span><span class="s1">_estimate_mi(X</span><span class="s2">, </span><span class="s1">y</span><span class="s2">, </span><span class="s1">discrete_features</span><span class="s2">, True, </span><span class="s1">n_neighbors</span><span class="s2">, </span><span class="s1">copy</span><span class="s2">, </span><span class="s1">random_state)</span>
</pre>
</body>
</html>