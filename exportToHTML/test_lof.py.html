<html>
<head>
<title>test_lof.py</title>
<meta http-equiv="Content-Type" content="text/html; charset=utf-8">
<style type="text/css">
.s0 { color: #808080;}
.s1 { color: #a9b7c6;}
.s2 { color: #cc7832;}
.s3 { color: #6897bb;}
.s4 { color: #629755; font-style: italic;}
.s5 { color: #6a8759;}
</style>
</head>
<body bgcolor="#2b2b2b">
<table CELLSPACING=0 CELLPADDING=5 COLS=1 WIDTH="100%" BGCOLOR="#606060" >
<tr><td><center>
<font face="Arial, Helvetica" color="#000000">
test_lof.py</font>
</center></td></tr></table>
<pre><span class="s0"># Authors: Nicolas Goix &lt;nicolas.goix@telecom-paristech.fr&gt;</span>
<span class="s0">#          Alexandre Gramfort &lt;alexandre.gramfort@telecom-paristech.fr&gt;</span>
<span class="s0"># License: BSD 3 clause</span>

<span class="s2">import </span><span class="s1">re</span>
<span class="s2">from </span><span class="s1">math </span><span class="s2">import </span><span class="s1">sqrt</span>

<span class="s2">import </span><span class="s1">numpy </span><span class="s2">as </span><span class="s1">np</span>
<span class="s2">import </span><span class="s1">pytest</span>
<span class="s2">from </span><span class="s1">scipy.sparse </span><span class="s2">import </span><span class="s1">csr_matrix</span>

<span class="s2">from </span><span class="s1">sklearn </span><span class="s2">import </span><span class="s1">metrics</span><span class="s2">, </span><span class="s1">neighbors</span>
<span class="s2">from </span><span class="s1">sklearn.datasets </span><span class="s2">import </span><span class="s1">load_iris</span>
<span class="s2">from </span><span class="s1">sklearn.metrics </span><span class="s2">import </span><span class="s1">roc_auc_score</span>
<span class="s2">from </span><span class="s1">sklearn.utils </span><span class="s2">import </span><span class="s1">check_random_state</span>
<span class="s2">from </span><span class="s1">sklearn.utils._testing </span><span class="s2">import </span><span class="s1">assert_allclose</span><span class="s2">, </span><span class="s1">assert_array_equal</span>
<span class="s2">from </span><span class="s1">sklearn.utils.estimator_checks </span><span class="s2">import </span><span class="s1">(</span>
    <span class="s1">check_outlier_corruption</span><span class="s2">,</span>
    <span class="s1">parametrize_with_checks</span><span class="s2">,</span>
<span class="s1">)</span>

<span class="s0"># load the iris dataset</span>
<span class="s0"># and randomly permute it</span>
<span class="s1">rng = check_random_state(</span><span class="s3">0</span><span class="s1">)</span>
<span class="s1">iris = load_iris()</span>
<span class="s1">perm = rng.permutation(iris.target.size)</span>
<span class="s1">iris.data = iris.data[perm]</span>
<span class="s1">iris.target = iris.target[perm]</span>


<span class="s2">def </span><span class="s1">test_lof(global_dtype):</span>
    <span class="s0"># Toy sample (the last two samples are outliers):</span>
    <span class="s1">X = np.asarray(</span>
        <span class="s1">[[-</span><span class="s3">2</span><span class="s2">, </span><span class="s1">-</span><span class="s3">1</span><span class="s1">]</span><span class="s2">, </span><span class="s1">[-</span><span class="s3">1</span><span class="s2">, </span><span class="s1">-</span><span class="s3">1</span><span class="s1">]</span><span class="s2">, </span><span class="s1">[-</span><span class="s3">1</span><span class="s2">, </span><span class="s1">-</span><span class="s3">2</span><span class="s1">]</span><span class="s2">, </span><span class="s1">[</span><span class="s3">1</span><span class="s2">, </span><span class="s3">1</span><span class="s1">]</span><span class="s2">, </span><span class="s1">[</span><span class="s3">1</span><span class="s2">, </span><span class="s3">2</span><span class="s1">]</span><span class="s2">, </span><span class="s1">[</span><span class="s3">2</span><span class="s2">, </span><span class="s3">1</span><span class="s1">]</span><span class="s2">, </span><span class="s1">[</span><span class="s3">5</span><span class="s2">, </span><span class="s3">3</span><span class="s1">]</span><span class="s2">, </span><span class="s1">[-</span><span class="s3">4</span><span class="s2">, </span><span class="s3">2</span><span class="s1">]]</span><span class="s2">,</span>
        <span class="s1">dtype=global_dtype</span><span class="s2">,</span>
    <span class="s1">)</span>

    <span class="s0"># Test LocalOutlierFactor:</span>
    <span class="s1">clf = neighbors.LocalOutlierFactor(n_neighbors=</span><span class="s3">5</span><span class="s1">)</span>
    <span class="s1">score = clf.fit(X).negative_outlier_factor_</span>
    <span class="s1">assert_array_equal(clf._fit_X</span><span class="s2">, </span><span class="s1">X)</span>

    <span class="s0"># Assert largest outlier score is smaller than smallest inlier score:</span>
    <span class="s2">assert </span><span class="s1">np.min(score[:-</span><span class="s3">2</span><span class="s1">]) &gt; np.max(score[-</span><span class="s3">2</span><span class="s1">:])</span>

    <span class="s0"># Assert predict() works:</span>
    <span class="s1">clf = neighbors.LocalOutlierFactor(contamination=</span><span class="s3">0.25</span><span class="s2">, </span><span class="s1">n_neighbors=</span><span class="s3">5</span><span class="s1">).fit(X)</span>
    <span class="s1">expected_predictions = </span><span class="s3">6 </span><span class="s1">* [</span><span class="s3">1</span><span class="s1">] + </span><span class="s3">2 </span><span class="s1">* [-</span><span class="s3">1</span><span class="s1">]</span>
    <span class="s1">assert_array_equal(clf._predict()</span><span class="s2">, </span><span class="s1">expected_predictions)</span>
    <span class="s1">assert_array_equal(clf.fit_predict(X)</span><span class="s2">, </span><span class="s1">expected_predictions)</span>


<span class="s2">def </span><span class="s1">test_lof_performance(global_dtype):</span>
    <span class="s0"># Generate train/test data</span>
    <span class="s1">rng = check_random_state(</span><span class="s3">2</span><span class="s1">)</span>
    <span class="s1">X = </span><span class="s3">0.3 </span><span class="s1">* rng.randn(</span><span class="s3">120</span><span class="s2">, </span><span class="s3">2</span><span class="s1">).astype(global_dtype</span><span class="s2">, </span><span class="s1">copy=</span><span class="s2">False</span><span class="s1">)</span>
    <span class="s1">X_train = X[:</span><span class="s3">100</span><span class="s1">]</span>

    <span class="s0"># Generate some abnormal novel observations</span>
    <span class="s1">X_outliers = rng.uniform(low=-</span><span class="s3">4</span><span class="s2">, </span><span class="s1">high=</span><span class="s3">4</span><span class="s2">, </span><span class="s1">size=(</span><span class="s3">20</span><span class="s2">, </span><span class="s3">2</span><span class="s1">)).astype(</span>
        <span class="s1">global_dtype</span><span class="s2">, </span><span class="s1">copy=</span><span class="s2">False</span>
    <span class="s1">)</span>
    <span class="s1">X_test = np.r_[X[</span><span class="s3">100</span><span class="s1">:]</span><span class="s2">, </span><span class="s1">X_outliers]</span>
    <span class="s1">y_test = np.array([</span><span class="s3">0</span><span class="s1">] * </span><span class="s3">20 </span><span class="s1">+ [</span><span class="s3">1</span><span class="s1">] * </span><span class="s3">20</span><span class="s1">)</span>

    <span class="s0"># fit the model for novelty detection</span>
    <span class="s1">clf = neighbors.LocalOutlierFactor(novelty=</span><span class="s2">True</span><span class="s1">).fit(X_train)</span>

    <span class="s0"># predict scores (the lower, the more normal)</span>
    <span class="s1">y_pred = -clf.decision_function(X_test)</span>

    <span class="s0"># check that roc_auc is good</span>
    <span class="s2">assert </span><span class="s1">roc_auc_score(y_test</span><span class="s2">, </span><span class="s1">y_pred) &gt; </span><span class="s3">0.99</span>


<span class="s2">def </span><span class="s1">test_lof_values(global_dtype):</span>
    <span class="s0"># toy samples:</span>
    <span class="s1">X_train = np.asarray([[</span><span class="s3">1</span><span class="s2">, </span><span class="s3">1</span><span class="s1">]</span><span class="s2">, </span><span class="s1">[</span><span class="s3">1</span><span class="s2">, </span><span class="s3">2</span><span class="s1">]</span><span class="s2">, </span><span class="s1">[</span><span class="s3">2</span><span class="s2">, </span><span class="s3">1</span><span class="s1">]]</span><span class="s2">, </span><span class="s1">dtype=global_dtype)</span>
    <span class="s1">clf1 = neighbors.LocalOutlierFactor(</span>
        <span class="s1">n_neighbors=</span><span class="s3">2</span><span class="s2">, </span><span class="s1">contamination=</span><span class="s3">0.1</span><span class="s2">, </span><span class="s1">novelty=</span><span class="s2">True</span>
    <span class="s1">).fit(X_train)</span>
    <span class="s1">clf2 = neighbors.LocalOutlierFactor(n_neighbors=</span><span class="s3">2</span><span class="s2">, </span><span class="s1">novelty=</span><span class="s2">True</span><span class="s1">).fit(X_train)</span>
    <span class="s1">s_0 = </span><span class="s3">2.0 </span><span class="s1">* sqrt(</span><span class="s3">2.0</span><span class="s1">) / (</span><span class="s3">1.0 </span><span class="s1">+ sqrt(</span><span class="s3">2.0</span><span class="s1">))</span>
    <span class="s1">s_1 = (</span><span class="s3">1.0 </span><span class="s1">+ sqrt(</span><span class="s3">2</span><span class="s1">)) * (</span><span class="s3">1.0 </span><span class="s1">/ (</span><span class="s3">4.0 </span><span class="s1">* sqrt(</span><span class="s3">2.0</span><span class="s1">)) + </span><span class="s3">1.0 </span><span class="s1">/ (</span><span class="s3">2.0 </span><span class="s1">+ </span><span class="s3">2.0 </span><span class="s1">* sqrt(</span><span class="s3">2</span><span class="s1">)))</span>
    <span class="s0"># check predict()</span>
    <span class="s1">assert_allclose(-clf1.negative_outlier_factor_</span><span class="s2">, </span><span class="s1">[s_0</span><span class="s2">, </span><span class="s1">s_1</span><span class="s2">, </span><span class="s1">s_1])</span>
    <span class="s1">assert_allclose(-clf2.negative_outlier_factor_</span><span class="s2">, </span><span class="s1">[s_0</span><span class="s2">, </span><span class="s1">s_1</span><span class="s2">, </span><span class="s1">s_1])</span>
    <span class="s0"># check predict(one sample not in train)</span>
    <span class="s1">assert_allclose(-clf1.score_samples([[</span><span class="s3">2.0</span><span class="s2">, </span><span class="s3">2.0</span><span class="s1">]])</span><span class="s2">, </span><span class="s1">[s_0])</span>
    <span class="s1">assert_allclose(-clf2.score_samples([[</span><span class="s3">2.0</span><span class="s2">, </span><span class="s3">2.0</span><span class="s1">]])</span><span class="s2">, </span><span class="s1">[s_0])</span>
    <span class="s0"># check predict(one sample already in train)</span>
    <span class="s1">assert_allclose(-clf1.score_samples([[</span><span class="s3">1.0</span><span class="s2">, </span><span class="s3">1.0</span><span class="s1">]])</span><span class="s2">, </span><span class="s1">[s_1])</span>
    <span class="s1">assert_allclose(-clf2.score_samples([[</span><span class="s3">1.0</span><span class="s2">, </span><span class="s3">1.0</span><span class="s1">]])</span><span class="s2">, </span><span class="s1">[s_1])</span>


<span class="s2">def </span><span class="s1">test_lof_precomputed(global_dtype</span><span class="s2">, </span><span class="s1">random_state=</span><span class="s3">42</span><span class="s1">):</span>
    <span class="s4">&quot;&quot;&quot;Tests LOF with a distance matrix.&quot;&quot;&quot;</span>
    <span class="s0"># Note: smaller samples may result in spurious test success</span>
    <span class="s1">rng = np.random.RandomState(random_state)</span>
    <span class="s1">X = rng.random_sample((</span><span class="s3">10</span><span class="s2">, </span><span class="s3">4</span><span class="s1">)).astype(global_dtype</span><span class="s2">, </span><span class="s1">copy=</span><span class="s2">False</span><span class="s1">)</span>
    <span class="s1">Y = rng.random_sample((</span><span class="s3">3</span><span class="s2">, </span><span class="s3">4</span><span class="s1">)).astype(global_dtype</span><span class="s2">, </span><span class="s1">copy=</span><span class="s2">False</span><span class="s1">)</span>
    <span class="s1">DXX = metrics.pairwise_distances(X</span><span class="s2">, </span><span class="s1">metric=</span><span class="s5">&quot;euclidean&quot;</span><span class="s1">)</span>
    <span class="s1">DYX = metrics.pairwise_distances(Y</span><span class="s2">, </span><span class="s1">X</span><span class="s2">, </span><span class="s1">metric=</span><span class="s5">&quot;euclidean&quot;</span><span class="s1">)</span>
    <span class="s0"># As a feature matrix (n_samples by n_features)</span>
    <span class="s1">lof_X = neighbors.LocalOutlierFactor(n_neighbors=</span><span class="s3">3</span><span class="s2">, </span><span class="s1">novelty=</span><span class="s2">True</span><span class="s1">)</span>
    <span class="s1">lof_X.fit(X)</span>
    <span class="s1">pred_X_X = lof_X._predict()</span>
    <span class="s1">pred_X_Y = lof_X.predict(Y)</span>

    <span class="s0"># As a dense distance matrix (n_samples by n_samples)</span>
    <span class="s1">lof_D = neighbors.LocalOutlierFactor(</span>
        <span class="s1">n_neighbors=</span><span class="s3">3</span><span class="s2">, </span><span class="s1">algorithm=</span><span class="s5">&quot;brute&quot;</span><span class="s2">, </span><span class="s1">metric=</span><span class="s5">&quot;precomputed&quot;</span><span class="s2">, </span><span class="s1">novelty=</span><span class="s2">True</span>
    <span class="s1">)</span>
    <span class="s1">lof_D.fit(DXX)</span>
    <span class="s1">pred_D_X = lof_D._predict()</span>
    <span class="s1">pred_D_Y = lof_D.predict(DYX)</span>

    <span class="s1">assert_allclose(pred_X_X</span><span class="s2">, </span><span class="s1">pred_D_X)</span>
    <span class="s1">assert_allclose(pred_X_Y</span><span class="s2">, </span><span class="s1">pred_D_Y)</span>


<span class="s2">def </span><span class="s1">test_n_neighbors_attribute():</span>
    <span class="s1">X = iris.data</span>
    <span class="s1">clf = neighbors.LocalOutlierFactor(n_neighbors=</span><span class="s3">500</span><span class="s1">).fit(X)</span>
    <span class="s2">assert </span><span class="s1">clf.n_neighbors_ == X.shape[</span><span class="s3">0</span><span class="s1">] - </span><span class="s3">1</span>

    <span class="s1">clf = neighbors.LocalOutlierFactor(n_neighbors=</span><span class="s3">500</span><span class="s1">)</span>
    <span class="s1">msg = </span><span class="s5">&quot;n_neighbors will be set to (n_samples - 1)&quot;</span>
    <span class="s2">with </span><span class="s1">pytest.warns(UserWarning</span><span class="s2">, </span><span class="s1">match=re.escape(msg)):</span>
        <span class="s1">clf.fit(X)</span>
    <span class="s2">assert </span><span class="s1">clf.n_neighbors_ == X.shape[</span><span class="s3">0</span><span class="s1">] - </span><span class="s3">1</span>


<span class="s2">def </span><span class="s1">test_score_samples(global_dtype):</span>
    <span class="s1">X_train = np.asarray([[</span><span class="s3">1</span><span class="s2">, </span><span class="s3">1</span><span class="s1">]</span><span class="s2">, </span><span class="s1">[</span><span class="s3">1</span><span class="s2">, </span><span class="s3">2</span><span class="s1">]</span><span class="s2">, </span><span class="s1">[</span><span class="s3">2</span><span class="s2">, </span><span class="s3">1</span><span class="s1">]]</span><span class="s2">, </span><span class="s1">dtype=global_dtype)</span>
    <span class="s1">X_test = np.asarray([[</span><span class="s3">2.0</span><span class="s2">, </span><span class="s3">2.0</span><span class="s1">]]</span><span class="s2">, </span><span class="s1">dtype=global_dtype)</span>
    <span class="s1">clf1 = neighbors.LocalOutlierFactor(</span>
        <span class="s1">n_neighbors=</span><span class="s3">2</span><span class="s2">, </span><span class="s1">contamination=</span><span class="s3">0.1</span><span class="s2">, </span><span class="s1">novelty=</span><span class="s2">True</span>
    <span class="s1">).fit(X_train)</span>
    <span class="s1">clf2 = neighbors.LocalOutlierFactor(n_neighbors=</span><span class="s3">2</span><span class="s2">, </span><span class="s1">novelty=</span><span class="s2">True</span><span class="s1">).fit(X_train)</span>

    <span class="s1">clf1_scores = clf1.score_samples(X_test)</span>
    <span class="s1">clf1_decisions = clf1.decision_function(X_test)</span>

    <span class="s1">clf2_scores = clf2.score_samples(X_test)</span>
    <span class="s1">clf2_decisions = clf2.decision_function(X_test)</span>

    <span class="s1">assert_allclose(</span>
        <span class="s1">clf1_scores</span><span class="s2">,</span>
        <span class="s1">clf1_decisions + clf1.offset_</span><span class="s2">,</span>
    <span class="s1">)</span>
    <span class="s1">assert_allclose(</span>
        <span class="s1">clf2_scores</span><span class="s2">,</span>
        <span class="s1">clf2_decisions + clf2.offset_</span><span class="s2">,</span>
    <span class="s1">)</span>
    <span class="s1">assert_allclose(clf1_scores</span><span class="s2">, </span><span class="s1">clf2_scores)</span>


<span class="s2">def </span><span class="s1">test_novelty_errors():</span>
    <span class="s1">X = iris.data</span>

    <span class="s0"># check errors for novelty=False</span>
    <span class="s1">clf = neighbors.LocalOutlierFactor()</span>
    <span class="s1">clf.fit(X)</span>
    <span class="s0"># predict, decision_function and score_samples raise ValueError</span>
    <span class="s2">for </span><span class="s1">method </span><span class="s2">in </span><span class="s1">[</span><span class="s5">&quot;predict&quot;</span><span class="s2">, </span><span class="s5">&quot;decision_function&quot;</span><span class="s2">, </span><span class="s5">&quot;score_samples&quot;</span><span class="s1">]:</span>
        <span class="s1">msg = </span><span class="s5">&quot;{} is not available when novelty=False&quot;</span><span class="s1">.format(method)</span>
        <span class="s2">with </span><span class="s1">pytest.raises(AttributeError</span><span class="s2">, </span><span class="s1">match=msg):</span>
            <span class="s1">getattr(clf</span><span class="s2">, </span><span class="s1">method)</span>

    <span class="s0"># check errors for novelty=True</span>
    <span class="s1">clf = neighbors.LocalOutlierFactor(novelty=</span><span class="s2">True</span><span class="s1">)</span>
    <span class="s1">msg = </span><span class="s5">&quot;fit_predict is not available when novelty=True&quot;</span>
    <span class="s2">with </span><span class="s1">pytest.raises(AttributeError</span><span class="s2">, </span><span class="s1">match=msg):</span>
        <span class="s1">getattr(clf</span><span class="s2">, </span><span class="s5">&quot;fit_predict&quot;</span><span class="s1">)</span>


<span class="s2">def </span><span class="s1">test_novelty_training_scores(global_dtype):</span>
    <span class="s0"># check that the scores of the training samples are still accessible</span>
    <span class="s0"># when novelty=True through the negative_outlier_factor_ attribute</span>
    <span class="s1">X = iris.data.astype(global_dtype)</span>

    <span class="s0"># fit with novelty=False</span>
    <span class="s1">clf_1 = neighbors.LocalOutlierFactor()</span>
    <span class="s1">clf_1.fit(X)</span>
    <span class="s1">scores_1 = clf_1.negative_outlier_factor_</span>

    <span class="s0"># fit with novelty=True</span>
    <span class="s1">clf_2 = neighbors.LocalOutlierFactor(novelty=</span><span class="s2">True</span><span class="s1">)</span>
    <span class="s1">clf_2.fit(X)</span>
    <span class="s1">scores_2 = clf_2.negative_outlier_factor_</span>

    <span class="s1">assert_allclose(scores_1</span><span class="s2">, </span><span class="s1">scores_2)</span>


<span class="s2">def </span><span class="s1">test_hasattr_prediction():</span>
    <span class="s0"># check availability of prediction methods depending on novelty value.</span>
    <span class="s1">X = [[</span><span class="s3">1</span><span class="s2">, </span><span class="s3">1</span><span class="s1">]</span><span class="s2">, </span><span class="s1">[</span><span class="s3">1</span><span class="s2">, </span><span class="s3">2</span><span class="s1">]</span><span class="s2">, </span><span class="s1">[</span><span class="s3">2</span><span class="s2">, </span><span class="s3">1</span><span class="s1">]]</span>

    <span class="s0"># when novelty=True</span>
    <span class="s1">clf = neighbors.LocalOutlierFactor(novelty=</span><span class="s2">True</span><span class="s1">)</span>
    <span class="s1">clf.fit(X)</span>
    <span class="s2">assert </span><span class="s1">hasattr(clf</span><span class="s2">, </span><span class="s5">&quot;predict&quot;</span><span class="s1">)</span>
    <span class="s2">assert </span><span class="s1">hasattr(clf</span><span class="s2">, </span><span class="s5">&quot;decision_function&quot;</span><span class="s1">)</span>
    <span class="s2">assert </span><span class="s1">hasattr(clf</span><span class="s2">, </span><span class="s5">&quot;score_samples&quot;</span><span class="s1">)</span>
    <span class="s2">assert not </span><span class="s1">hasattr(clf</span><span class="s2">, </span><span class="s5">&quot;fit_predict&quot;</span><span class="s1">)</span>

    <span class="s0"># when novelty=False</span>
    <span class="s1">clf = neighbors.LocalOutlierFactor(novelty=</span><span class="s2">False</span><span class="s1">)</span>
    <span class="s1">clf.fit(X)</span>
    <span class="s2">assert </span><span class="s1">hasattr(clf</span><span class="s2">, </span><span class="s5">&quot;fit_predict&quot;</span><span class="s1">)</span>
    <span class="s2">assert not </span><span class="s1">hasattr(clf</span><span class="s2">, </span><span class="s5">&quot;predict&quot;</span><span class="s1">)</span>
    <span class="s2">assert not </span><span class="s1">hasattr(clf</span><span class="s2">, </span><span class="s5">&quot;decision_function&quot;</span><span class="s1">)</span>
    <span class="s2">assert not </span><span class="s1">hasattr(clf</span><span class="s2">, </span><span class="s5">&quot;score_samples&quot;</span><span class="s1">)</span>


<span class="s1">@parametrize_with_checks([neighbors.LocalOutlierFactor(novelty=</span><span class="s2">True</span><span class="s1">)])</span>
<span class="s2">def </span><span class="s1">test_novelty_true_common_tests(estimator</span><span class="s2">, </span><span class="s1">check):</span>
    <span class="s0"># the common tests are run for the default LOF (novelty=False).</span>
    <span class="s0"># here we run these common tests for LOF when novelty=True</span>
    <span class="s1">check(estimator)</span>


<span class="s1">@pytest.mark.parametrize(</span><span class="s5">&quot;expected_outliers&quot;</span><span class="s2">, </span><span class="s1">[</span><span class="s3">30</span><span class="s2">, </span><span class="s3">53</span><span class="s1">])</span>
<span class="s2">def </span><span class="s1">test_predicted_outlier_number(expected_outliers):</span>
    <span class="s0"># the number of predicted outliers should be equal to the number of</span>
    <span class="s0"># expected outliers unless there are ties in the abnormality scores.</span>
    <span class="s1">X = iris.data</span>
    <span class="s1">n_samples = X.shape[</span><span class="s3">0</span><span class="s1">]</span>
    <span class="s1">contamination = float(expected_outliers) / n_samples</span>

    <span class="s1">clf = neighbors.LocalOutlierFactor(contamination=contamination)</span>
    <span class="s1">y_pred = clf.fit_predict(X)</span>

    <span class="s1">num_outliers = np.sum(y_pred != </span><span class="s3">1</span><span class="s1">)</span>
    <span class="s2">if </span><span class="s1">num_outliers != expected_outliers:</span>
        <span class="s1">y_dec = clf.negative_outlier_factor_</span>
        <span class="s1">check_outlier_corruption(num_outliers</span><span class="s2">, </span><span class="s1">expected_outliers</span><span class="s2">, </span><span class="s1">y_dec)</span>


<span class="s2">def </span><span class="s1">test_sparse():</span>
    <span class="s0"># LocalOutlierFactor must support CSR inputs</span>
    <span class="s0"># TODO: compare results on dense and sparse data as proposed in:</span>
    <span class="s0"># https://github.com/scikit-learn/scikit-learn/pull/23585#discussion_r968388186</span>
    <span class="s1">X = csr_matrix(iris.data)</span>

    <span class="s1">lof = neighbors.LocalOutlierFactor(novelty=</span><span class="s2">True</span><span class="s1">)</span>
    <span class="s1">lof.fit(X)</span>
    <span class="s1">lof.predict(X)</span>
    <span class="s1">lof.score_samples(X)</span>
    <span class="s1">lof.decision_function(X)</span>

    <span class="s1">lof = neighbors.LocalOutlierFactor(novelty=</span><span class="s2">False</span><span class="s1">)</span>
    <span class="s1">lof.fit_predict(X)</span>


<span class="s1">@pytest.mark.parametrize(</span><span class="s5">&quot;algorithm&quot;</span><span class="s2">, </span><span class="s1">[</span><span class="s5">&quot;auto&quot;</span><span class="s2">, </span><span class="s5">&quot;ball_tree&quot;</span><span class="s2">, </span><span class="s5">&quot;kd_tree&quot;</span><span class="s2">, </span><span class="s5">&quot;brute&quot;</span><span class="s1">])</span>
<span class="s1">@pytest.mark.parametrize(</span><span class="s5">&quot;novelty&quot;</span><span class="s2">, </span><span class="s1">[</span><span class="s2">True, False</span><span class="s1">])</span>
<span class="s1">@pytest.mark.parametrize(</span><span class="s5">&quot;contamination&quot;</span><span class="s2">, </span><span class="s1">[</span><span class="s3">0.5</span><span class="s2">, </span><span class="s5">&quot;auto&quot;</span><span class="s1">])</span>
<span class="s2">def </span><span class="s1">test_lof_input_dtype_preservation(global_dtype</span><span class="s2">, </span><span class="s1">algorithm</span><span class="s2">, </span><span class="s1">contamination</span><span class="s2">, </span><span class="s1">novelty):</span>
    <span class="s4">&quot;&quot;&quot;Check that the fitted attributes are stored using the data type of X.&quot;&quot;&quot;</span>
    <span class="s1">X = iris.data.astype(global_dtype</span><span class="s2">, </span><span class="s1">copy=</span><span class="s2">False</span><span class="s1">)</span>

    <span class="s1">iso = neighbors.LocalOutlierFactor(</span>
        <span class="s1">n_neighbors=</span><span class="s3">5</span><span class="s2">, </span><span class="s1">algorithm=algorithm</span><span class="s2">, </span><span class="s1">contamination=contamination</span><span class="s2">, </span><span class="s1">novelty=novelty</span>
    <span class="s1">)</span>
    <span class="s1">iso.fit(X)</span>

    <span class="s2">assert </span><span class="s1">iso.negative_outlier_factor_.dtype == global_dtype</span>

    <span class="s2">for </span><span class="s1">method </span><span class="s2">in </span><span class="s1">(</span><span class="s5">&quot;score_samples&quot;</span><span class="s2">, </span><span class="s5">&quot;decision_function&quot;</span><span class="s1">):</span>
        <span class="s2">if </span><span class="s1">hasattr(iso</span><span class="s2">, </span><span class="s1">method):</span>
            <span class="s1">y_pred = getattr(iso</span><span class="s2">, </span><span class="s1">method)(X)</span>
            <span class="s2">assert </span><span class="s1">y_pred.dtype == global_dtype</span>


<span class="s1">@pytest.mark.parametrize(</span><span class="s5">&quot;algorithm&quot;</span><span class="s2">, </span><span class="s1">[</span><span class="s5">&quot;auto&quot;</span><span class="s2">, </span><span class="s5">&quot;ball_tree&quot;</span><span class="s2">, </span><span class="s5">&quot;kd_tree&quot;</span><span class="s2">, </span><span class="s5">&quot;brute&quot;</span><span class="s1">])</span>
<span class="s1">@pytest.mark.parametrize(</span><span class="s5">&quot;novelty&quot;</span><span class="s2">, </span><span class="s1">[</span><span class="s2">True, False</span><span class="s1">])</span>
<span class="s1">@pytest.mark.parametrize(</span><span class="s5">&quot;contamination&quot;</span><span class="s2">, </span><span class="s1">[</span><span class="s3">0.5</span><span class="s2">, </span><span class="s5">&quot;auto&quot;</span><span class="s1">])</span>
<span class="s2">def </span><span class="s1">test_lof_dtype_equivalence(algorithm</span><span class="s2">, </span><span class="s1">novelty</span><span class="s2">, </span><span class="s1">contamination):</span>
    <span class="s4">&quot;&quot;&quot;Check the equivalence of the results with 32 and 64 bits input.&quot;&quot;&quot;</span>

    <span class="s1">inliers = iris.data[:</span><span class="s3">50</span><span class="s1">]  </span><span class="s0"># setosa iris are really distinct from others</span>
    <span class="s1">outliers = iris.data[-</span><span class="s3">5</span><span class="s1">:]  </span><span class="s0"># virginica will be considered as outliers</span>
    <span class="s0"># lower the precision of the input data to check that we have an equivalence when</span>
    <span class="s0"># making the computation in 32 and 64 bits.</span>
    <span class="s1">X = np.concatenate([inliers</span><span class="s2">, </span><span class="s1">outliers]</span><span class="s2">, </span><span class="s1">axis=</span><span class="s3">0</span><span class="s1">).astype(np.float32)</span>

    <span class="s1">lof_32 = neighbors.LocalOutlierFactor(</span>
        <span class="s1">algorithm=algorithm</span><span class="s2">, </span><span class="s1">novelty=novelty</span><span class="s2">, </span><span class="s1">contamination=contamination</span>
    <span class="s1">)</span>
    <span class="s1">X_32 = X.astype(np.float32</span><span class="s2">, </span><span class="s1">copy=</span><span class="s2">True</span><span class="s1">)</span>
    <span class="s1">lof_32.fit(X_32)</span>

    <span class="s1">lof_64 = neighbors.LocalOutlierFactor(</span>
        <span class="s1">algorithm=algorithm</span><span class="s2">, </span><span class="s1">novelty=novelty</span><span class="s2">, </span><span class="s1">contamination=contamination</span>
    <span class="s1">)</span>
    <span class="s1">X_64 = X.astype(np.float64</span><span class="s2">, </span><span class="s1">copy=</span><span class="s2">True</span><span class="s1">)</span>
    <span class="s1">lof_64.fit(X_64)</span>

    <span class="s1">assert_allclose(lof_32.negative_outlier_factor_</span><span class="s2">, </span><span class="s1">lof_64.negative_outlier_factor_)</span>

    <span class="s2">for </span><span class="s1">method </span><span class="s2">in </span><span class="s1">(</span><span class="s5">&quot;score_samples&quot;</span><span class="s2">, </span><span class="s5">&quot;decision_function&quot;</span><span class="s2">, </span><span class="s5">&quot;predict&quot;</span><span class="s2">, </span><span class="s5">&quot;fit_predict&quot;</span><span class="s1">):</span>
        <span class="s2">if </span><span class="s1">hasattr(lof_32</span><span class="s2">, </span><span class="s1">method):</span>
            <span class="s1">y_pred_32 = getattr(lof_32</span><span class="s2">, </span><span class="s1">method)(X_32)</span>
            <span class="s1">y_pred_64 = getattr(lof_64</span><span class="s2">, </span><span class="s1">method)(X_64)</span>
            <span class="s1">assert_allclose(y_pred_32</span><span class="s2">, </span><span class="s1">y_pred_64</span><span class="s2">, </span><span class="s1">atol=</span><span class="s3">0.0002</span><span class="s1">)</span>
</pre>
</body>
</html>