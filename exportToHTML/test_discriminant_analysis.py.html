<html>
<head>
<title>test_discriminant_analysis.py</title>
<meta http-equiv="Content-Type" content="text/html; charset=utf-8">
<style type="text/css">
.s0 { color: #cc7832;}
.s1 { color: #a9b7c6;}
.s2 { color: #808080;}
.s3 { color: #6897bb;}
.s4 { color: #6a8759;}
.s5 { color: #629755; font-style: italic;}
</style>
</head>
<body bgcolor="#2b2b2b">
<table CELLSPACING=0 CELLPADDING=5 COLS=1 WIDTH="100%" BGCOLOR="#606060" >
<tr><td><center>
<font face="Arial, Helvetica" color="#000000">
test_discriminant_analysis.py</font>
</center></td></tr></table>
<pre><span class="s0">import </span><span class="s1">numpy </span><span class="s0">as </span><span class="s1">np</span>
<span class="s0">import </span><span class="s1">pytest</span>
<span class="s0">from </span><span class="s1">scipy </span><span class="s0">import </span><span class="s1">linalg</span>

<span class="s0">from </span><span class="s1">sklearn.cluster </span><span class="s0">import </span><span class="s1">KMeans</span>
<span class="s0">from </span><span class="s1">sklearn.covariance </span><span class="s0">import </span><span class="s1">LedoitWolf</span><span class="s0">, </span><span class="s1">ShrunkCovariance</span><span class="s0">, </span><span class="s1">ledoit_wolf</span>
<span class="s0">from </span><span class="s1">sklearn.datasets </span><span class="s0">import </span><span class="s1">make_blobs</span>
<span class="s0">from </span><span class="s1">sklearn.discriminant_analysis </span><span class="s0">import </span><span class="s1">(</span>
    <span class="s1">LinearDiscriminantAnalysis</span><span class="s0">,</span>
    <span class="s1">QuadraticDiscriminantAnalysis</span><span class="s0">,</span>
    <span class="s1">_cov</span><span class="s0">,</span>
<span class="s1">)</span>
<span class="s0">from </span><span class="s1">sklearn.preprocessing </span><span class="s0">import </span><span class="s1">StandardScaler</span>
<span class="s0">from </span><span class="s1">sklearn.utils </span><span class="s0">import </span><span class="s1">check_random_state</span>
<span class="s0">from </span><span class="s1">sklearn.utils._testing </span><span class="s0">import </span><span class="s1">(</span>
    <span class="s1">_convert_container</span><span class="s0">,</span>
    <span class="s1">assert_allclose</span><span class="s0">,</span>
    <span class="s1">assert_almost_equal</span><span class="s0">,</span>
    <span class="s1">assert_array_almost_equal</span><span class="s0">,</span>
    <span class="s1">assert_array_equal</span><span class="s0">,</span>
<span class="s1">)</span>

<span class="s2"># Data is just 6 separable points in the plane</span>
<span class="s1">X = np.array([[-</span><span class="s3">2</span><span class="s0">, </span><span class="s1">-</span><span class="s3">1</span><span class="s1">]</span><span class="s0">, </span><span class="s1">[-</span><span class="s3">1</span><span class="s0">, </span><span class="s1">-</span><span class="s3">1</span><span class="s1">]</span><span class="s0">, </span><span class="s1">[-</span><span class="s3">1</span><span class="s0">, </span><span class="s1">-</span><span class="s3">2</span><span class="s1">]</span><span class="s0">, </span><span class="s1">[</span><span class="s3">1</span><span class="s0">, </span><span class="s3">1</span><span class="s1">]</span><span class="s0">, </span><span class="s1">[</span><span class="s3">1</span><span class="s0">, </span><span class="s3">2</span><span class="s1">]</span><span class="s0">, </span><span class="s1">[</span><span class="s3">2</span><span class="s0">, </span><span class="s3">1</span><span class="s1">]]</span><span class="s0">, </span><span class="s1">dtype=</span><span class="s4">&quot;f&quot;</span><span class="s1">)</span>
<span class="s1">y = np.array([</span><span class="s3">1</span><span class="s0">, </span><span class="s3">1</span><span class="s0">, </span><span class="s3">1</span><span class="s0">, </span><span class="s3">2</span><span class="s0">, </span><span class="s3">2</span><span class="s0">, </span><span class="s3">2</span><span class="s1">])</span>
<span class="s1">y3 = np.array([</span><span class="s3">1</span><span class="s0">, </span><span class="s3">1</span><span class="s0">, </span><span class="s3">2</span><span class="s0">, </span><span class="s3">2</span><span class="s0">, </span><span class="s3">3</span><span class="s0">, </span><span class="s3">3</span><span class="s1">])</span>

<span class="s2"># Degenerate data with only one feature (still should be separable)</span>
<span class="s1">X1 = np.array(</span>
    <span class="s1">[[-</span><span class="s3">2</span><span class="s1">]</span><span class="s0">, </span><span class="s1">[-</span><span class="s3">1</span><span class="s1">]</span><span class="s0">, </span><span class="s1">[-</span><span class="s3">1</span><span class="s1">]</span><span class="s0">, </span><span class="s1">[</span><span class="s3">1</span><span class="s1">]</span><span class="s0">, </span><span class="s1">[</span><span class="s3">1</span><span class="s1">]</span><span class="s0">, </span><span class="s1">[</span><span class="s3">2</span><span class="s1">]]</span><span class="s0">,</span>
    <span class="s1">dtype=</span><span class="s4">&quot;f&quot;</span><span class="s0">,</span>
<span class="s1">)</span>

<span class="s2"># Data is just 9 separable points in the plane</span>
<span class="s1">X6 = np.array(</span>
    <span class="s1">[[</span><span class="s3">0</span><span class="s0">, </span><span class="s3">0</span><span class="s1">]</span><span class="s0">, </span><span class="s1">[-</span><span class="s3">2</span><span class="s0">, </span><span class="s1">-</span><span class="s3">2</span><span class="s1">]</span><span class="s0">, </span><span class="s1">[-</span><span class="s3">2</span><span class="s0">, </span><span class="s1">-</span><span class="s3">1</span><span class="s1">]</span><span class="s0">, </span><span class="s1">[-</span><span class="s3">1</span><span class="s0">, </span><span class="s1">-</span><span class="s3">1</span><span class="s1">]</span><span class="s0">, </span><span class="s1">[-</span><span class="s3">1</span><span class="s0">, </span><span class="s1">-</span><span class="s3">2</span><span class="s1">]</span><span class="s0">, </span><span class="s1">[</span><span class="s3">1</span><span class="s0">, </span><span class="s3">3</span><span class="s1">]</span><span class="s0">, </span><span class="s1">[</span><span class="s3">1</span><span class="s0">, </span><span class="s3">2</span><span class="s1">]</span><span class="s0">, </span><span class="s1">[</span><span class="s3">2</span><span class="s0">, </span><span class="s3">1</span><span class="s1">]</span><span class="s0">, </span><span class="s1">[</span><span class="s3">2</span><span class="s0">, </span><span class="s3">2</span><span class="s1">]]</span>
<span class="s1">)</span>
<span class="s1">y6 = np.array([</span><span class="s3">1</span><span class="s0">, </span><span class="s3">1</span><span class="s0">, </span><span class="s3">1</span><span class="s0">, </span><span class="s3">1</span><span class="s0">, </span><span class="s3">1</span><span class="s0">, </span><span class="s3">2</span><span class="s0">, </span><span class="s3">2</span><span class="s0">, </span><span class="s3">2</span><span class="s0">, </span><span class="s3">2</span><span class="s1">])</span>
<span class="s1">y7 = np.array([</span><span class="s3">1</span><span class="s0">, </span><span class="s3">2</span><span class="s0">, </span><span class="s3">3</span><span class="s0">, </span><span class="s3">2</span><span class="s0">, </span><span class="s3">3</span><span class="s0">, </span><span class="s3">1</span><span class="s0">, </span><span class="s3">2</span><span class="s0">, </span><span class="s3">3</span><span class="s0">, </span><span class="s3">1</span><span class="s1">])</span>

<span class="s2"># Degenerate data with 1 feature (still should be separable)</span>
<span class="s1">X7 = np.array([[-</span><span class="s3">3</span><span class="s1">]</span><span class="s0">, </span><span class="s1">[-</span><span class="s3">2</span><span class="s1">]</span><span class="s0">, </span><span class="s1">[-</span><span class="s3">1</span><span class="s1">]</span><span class="s0">, </span><span class="s1">[-</span><span class="s3">1</span><span class="s1">]</span><span class="s0">, </span><span class="s1">[</span><span class="s3">0</span><span class="s1">]</span><span class="s0">, </span><span class="s1">[</span><span class="s3">1</span><span class="s1">]</span><span class="s0">, </span><span class="s1">[</span><span class="s3">1</span><span class="s1">]</span><span class="s0">, </span><span class="s1">[</span><span class="s3">2</span><span class="s1">]</span><span class="s0">, </span><span class="s1">[</span><span class="s3">3</span><span class="s1">]])</span>

<span class="s2"># Data that has zero variance in one dimension and needs regularization</span>
<span class="s1">X2 = np.array(</span>
    <span class="s1">[[-</span><span class="s3">3</span><span class="s0">, </span><span class="s3">0</span><span class="s1">]</span><span class="s0">, </span><span class="s1">[-</span><span class="s3">2</span><span class="s0">, </span><span class="s3">0</span><span class="s1">]</span><span class="s0">, </span><span class="s1">[-</span><span class="s3">1</span><span class="s0">, </span><span class="s3">0</span><span class="s1">]</span><span class="s0">, </span><span class="s1">[-</span><span class="s3">1</span><span class="s0">, </span><span class="s3">0</span><span class="s1">]</span><span class="s0">, </span><span class="s1">[</span><span class="s3">0</span><span class="s0">, </span><span class="s3">0</span><span class="s1">]</span><span class="s0">, </span><span class="s1">[</span><span class="s3">1</span><span class="s0">, </span><span class="s3">0</span><span class="s1">]</span><span class="s0">, </span><span class="s1">[</span><span class="s3">1</span><span class="s0">, </span><span class="s3">0</span><span class="s1">]</span><span class="s0">, </span><span class="s1">[</span><span class="s3">2</span><span class="s0">, </span><span class="s3">0</span><span class="s1">]</span><span class="s0">, </span><span class="s1">[</span><span class="s3">3</span><span class="s0">, </span><span class="s3">0</span><span class="s1">]]</span>
<span class="s1">)</span>

<span class="s2"># One element class</span>
<span class="s1">y4 = np.array([</span><span class="s3">1</span><span class="s0">, </span><span class="s3">1</span><span class="s0">, </span><span class="s3">1</span><span class="s0">, </span><span class="s3">1</span><span class="s0">, </span><span class="s3">1</span><span class="s0">, </span><span class="s3">1</span><span class="s0">, </span><span class="s3">1</span><span class="s0">, </span><span class="s3">1</span><span class="s0">, </span><span class="s3">2</span><span class="s1">])</span>

<span class="s2"># Data with less samples in a class than n_features</span>
<span class="s1">X5 = np.c_[np.arange(</span><span class="s3">8</span><span class="s1">)</span><span class="s0">, </span><span class="s1">np.zeros((</span><span class="s3">8</span><span class="s0">, </span><span class="s3">3</span><span class="s1">))]</span>
<span class="s1">y5 = np.array([</span><span class="s3">0</span><span class="s0">, </span><span class="s3">0</span><span class="s0">, </span><span class="s3">0</span><span class="s0">, </span><span class="s3">0</span><span class="s0">, </span><span class="s3">0</span><span class="s0">, </span><span class="s3">1</span><span class="s0">, </span><span class="s3">1</span><span class="s0">, </span><span class="s3">1</span><span class="s1">])</span>

<span class="s1">solver_shrinkage = [</span>
    <span class="s1">(</span><span class="s4">&quot;svd&quot;</span><span class="s0">, None</span><span class="s1">)</span><span class="s0">,</span>
    <span class="s1">(</span><span class="s4">&quot;lsqr&quot;</span><span class="s0">, None</span><span class="s1">)</span><span class="s0">,</span>
    <span class="s1">(</span><span class="s4">&quot;eigen&quot;</span><span class="s0">, None</span><span class="s1">)</span><span class="s0">,</span>
    <span class="s1">(</span><span class="s4">&quot;lsqr&quot;</span><span class="s0">, </span><span class="s4">&quot;auto&quot;</span><span class="s1">)</span><span class="s0">,</span>
    <span class="s1">(</span><span class="s4">&quot;lsqr&quot;</span><span class="s0">, </span><span class="s3">0</span><span class="s1">)</span><span class="s0">,</span>
    <span class="s1">(</span><span class="s4">&quot;lsqr&quot;</span><span class="s0">, </span><span class="s3">0.43</span><span class="s1">)</span><span class="s0">,</span>
    <span class="s1">(</span><span class="s4">&quot;eigen&quot;</span><span class="s0">, </span><span class="s4">&quot;auto&quot;</span><span class="s1">)</span><span class="s0">,</span>
    <span class="s1">(</span><span class="s4">&quot;eigen&quot;</span><span class="s0">, </span><span class="s3">0</span><span class="s1">)</span><span class="s0">,</span>
    <span class="s1">(</span><span class="s4">&quot;eigen&quot;</span><span class="s0">, </span><span class="s3">0.43</span><span class="s1">)</span><span class="s0">,</span>
<span class="s1">]</span>


<span class="s0">def </span><span class="s1">test_lda_predict():</span>
    <span class="s2"># Test LDA classification.</span>
    <span class="s2"># This checks that LDA implements fit and predict and returns correct</span>
    <span class="s2"># values for simple toy data.</span>
    <span class="s0">for </span><span class="s1">test_case </span><span class="s0">in </span><span class="s1">solver_shrinkage:</span>
        <span class="s1">solver</span><span class="s0">, </span><span class="s1">shrinkage = test_case</span>
        <span class="s1">clf = LinearDiscriminantAnalysis(solver=solver</span><span class="s0">, </span><span class="s1">shrinkage=shrinkage)</span>
        <span class="s1">y_pred = clf.fit(X</span><span class="s0">, </span><span class="s1">y).predict(X)</span>
        <span class="s1">assert_array_equal(y_pred</span><span class="s0">, </span><span class="s1">y</span><span class="s0">, </span><span class="s4">&quot;solver %s&quot; </span><span class="s1">% solver)</span>

        <span class="s2"># Assert that it works with 1D data</span>
        <span class="s1">y_pred1 = clf.fit(X1</span><span class="s0">, </span><span class="s1">y).predict(X1)</span>
        <span class="s1">assert_array_equal(y_pred1</span><span class="s0">, </span><span class="s1">y</span><span class="s0">, </span><span class="s4">&quot;solver %s&quot; </span><span class="s1">% solver)</span>

        <span class="s2"># Test probability estimates</span>
        <span class="s1">y_proba_pred1 = clf.predict_proba(X1)</span>
        <span class="s1">assert_array_equal((y_proba_pred1[:</span><span class="s0">, </span><span class="s3">1</span><span class="s1">] &gt; </span><span class="s3">0.5</span><span class="s1">) + </span><span class="s3">1</span><span class="s0">, </span><span class="s1">y</span><span class="s0">, </span><span class="s4">&quot;solver %s&quot; </span><span class="s1">% solver)</span>
        <span class="s1">y_log_proba_pred1 = clf.predict_log_proba(X1)</span>
        <span class="s1">assert_allclose(</span>
            <span class="s1">np.exp(y_log_proba_pred1)</span><span class="s0">,</span>
            <span class="s1">y_proba_pred1</span><span class="s0">,</span>
            <span class="s1">rtol=</span><span class="s3">1e-6</span><span class="s0">,</span>
            <span class="s1">atol=</span><span class="s3">1e-6</span><span class="s0">,</span>
            <span class="s1">err_msg=</span><span class="s4">&quot;solver %s&quot; </span><span class="s1">% solver</span><span class="s0">,</span>
        <span class="s1">)</span>

        <span class="s2"># Primarily test for commit 2f34950 -- &quot;reuse&quot; of priors</span>
        <span class="s1">y_pred3 = clf.fit(X</span><span class="s0">, </span><span class="s1">y3).predict(X)</span>
        <span class="s2"># LDA shouldn't be able to separate those</span>
        <span class="s0">assert </span><span class="s1">np.any(y_pred3 != y3)</span><span class="s0">, </span><span class="s4">&quot;solver %s&quot; </span><span class="s1">% solver</span>

    <span class="s1">clf = LinearDiscriminantAnalysis(solver=</span><span class="s4">&quot;svd&quot;</span><span class="s0">, </span><span class="s1">shrinkage=</span><span class="s4">&quot;auto&quot;</span><span class="s1">)</span>
    <span class="s0">with </span><span class="s1">pytest.raises(NotImplementedError):</span>
        <span class="s1">clf.fit(X</span><span class="s0">, </span><span class="s1">y)</span>

    <span class="s1">clf = LinearDiscriminantAnalysis(</span>
        <span class="s1">solver=</span><span class="s4">&quot;lsqr&quot;</span><span class="s0">, </span><span class="s1">shrinkage=</span><span class="s3">0.1</span><span class="s0">, </span><span class="s1">covariance_estimator=ShrunkCovariance()</span>
    <span class="s1">)</span>
    <span class="s0">with </span><span class="s1">pytest.raises(</span>
        <span class="s1">ValueError</span><span class="s0">,</span>
        <span class="s1">match=(</span>
            <span class="s4">&quot;covariance_estimator and shrinkage &quot;</span>
            <span class="s4">&quot;parameters are not None. &quot;</span>
            <span class="s4">&quot;Only one of the two can be set.&quot;</span>
        <span class="s1">)</span><span class="s0">,</span>
    <span class="s1">):</span>
        <span class="s1">clf.fit(X</span><span class="s0">, </span><span class="s1">y)</span>

    <span class="s2"># test bad solver with covariance_estimator</span>
    <span class="s1">clf = LinearDiscriminantAnalysis(solver=</span><span class="s4">&quot;svd&quot;</span><span class="s0">, </span><span class="s1">covariance_estimator=LedoitWolf())</span>
    <span class="s0">with </span><span class="s1">pytest.raises(</span>
        <span class="s1">ValueError</span><span class="s0">, </span><span class="s1">match=</span><span class="s4">&quot;covariance estimator is not supported with svd&quot;</span>
    <span class="s1">):</span>
        <span class="s1">clf.fit(X</span><span class="s0">, </span><span class="s1">y)</span>

    <span class="s2"># test bad covariance estimator</span>
    <span class="s1">clf = LinearDiscriminantAnalysis(</span>
        <span class="s1">solver=</span><span class="s4">&quot;lsqr&quot;</span><span class="s0">, </span><span class="s1">covariance_estimator=KMeans(n_clusters=</span><span class="s3">2</span><span class="s0">, </span><span class="s1">n_init=</span><span class="s4">&quot;auto&quot;</span><span class="s1">)</span>
    <span class="s1">)</span>
    <span class="s0">with </span><span class="s1">pytest.raises(ValueError):</span>
        <span class="s1">clf.fit(X</span><span class="s0">, </span><span class="s1">y)</span>


<span class="s1">@pytest.mark.parametrize(</span><span class="s4">&quot;n_classes&quot;</span><span class="s0">, </span><span class="s1">[</span><span class="s3">2</span><span class="s0">, </span><span class="s3">3</span><span class="s1">])</span>
<span class="s1">@pytest.mark.parametrize(</span><span class="s4">&quot;solver&quot;</span><span class="s0">, </span><span class="s1">[</span><span class="s4">&quot;svd&quot;</span><span class="s0">, </span><span class="s4">&quot;lsqr&quot;</span><span class="s0">, </span><span class="s4">&quot;eigen&quot;</span><span class="s1">])</span>
<span class="s0">def </span><span class="s1">test_lda_predict_proba(solver</span><span class="s0">, </span><span class="s1">n_classes):</span>
    <span class="s0">def </span><span class="s1">generate_dataset(n_samples</span><span class="s0">, </span><span class="s1">centers</span><span class="s0">, </span><span class="s1">covariances</span><span class="s0">, </span><span class="s1">random_state=</span><span class="s0">None</span><span class="s1">):</span>
        <span class="s5">&quot;&quot;&quot;Generate a multivariate normal data given some centers and 
        covariances&quot;&quot;&quot;</span>
        <span class="s1">rng = check_random_state(random_state)</span>
        <span class="s1">X = np.vstack(</span>
            <span class="s1">[</span>
                <span class="s1">rng.multivariate_normal(mean</span><span class="s0">, </span><span class="s1">cov</span><span class="s0">, </span><span class="s1">size=n_samples // len(centers))</span>
                <span class="s0">for </span><span class="s1">mean</span><span class="s0">, </span><span class="s1">cov </span><span class="s0">in </span><span class="s1">zip(centers</span><span class="s0">, </span><span class="s1">covariances)</span>
            <span class="s1">]</span>
        <span class="s1">)</span>
        <span class="s1">y = np.hstack(</span>
            <span class="s1">[[clazz] * (n_samples // len(centers)) </span><span class="s0">for </span><span class="s1">clazz </span><span class="s0">in </span><span class="s1">range(len(centers))]</span>
        <span class="s1">)</span>
        <span class="s0">return </span><span class="s1">X</span><span class="s0">, </span><span class="s1">y</span>

    <span class="s1">blob_centers = np.array([[</span><span class="s3">0</span><span class="s0">, </span><span class="s3">0</span><span class="s1">]</span><span class="s0">, </span><span class="s1">[-</span><span class="s3">10</span><span class="s0">, </span><span class="s3">40</span><span class="s1">]</span><span class="s0">, </span><span class="s1">[-</span><span class="s3">30</span><span class="s0">, </span><span class="s3">30</span><span class="s1">]])[:n_classes]</span>
    <span class="s1">blob_stds = np.array([[[</span><span class="s3">10</span><span class="s0">, </span><span class="s3">10</span><span class="s1">]</span><span class="s0">, </span><span class="s1">[</span><span class="s3">10</span><span class="s0">, </span><span class="s3">100</span><span class="s1">]]] * len(blob_centers))</span>
    <span class="s1">X</span><span class="s0">, </span><span class="s1">y = generate_dataset(</span>
        <span class="s1">n_samples=</span><span class="s3">90000</span><span class="s0">, </span><span class="s1">centers=blob_centers</span><span class="s0">, </span><span class="s1">covariances=blob_stds</span><span class="s0">, </span><span class="s1">random_state=</span><span class="s3">42</span>
    <span class="s1">)</span>
    <span class="s1">lda = LinearDiscriminantAnalysis(</span>
        <span class="s1">solver=solver</span><span class="s0">, </span><span class="s1">store_covariance=</span><span class="s0">True, </span><span class="s1">shrinkage=</span><span class="s0">None</span>
    <span class="s1">).fit(X</span><span class="s0">, </span><span class="s1">y)</span>
    <span class="s2"># check that the empirical means and covariances are close enough to the</span>
    <span class="s2"># one used to generate the data</span>
    <span class="s1">assert_allclose(lda.means_</span><span class="s0">, </span><span class="s1">blob_centers</span><span class="s0">, </span><span class="s1">atol=</span><span class="s3">1e-1</span><span class="s1">)</span>
    <span class="s1">assert_allclose(lda.covariance_</span><span class="s0">, </span><span class="s1">blob_stds[</span><span class="s3">0</span><span class="s1">]</span><span class="s0">, </span><span class="s1">atol=</span><span class="s3">1</span><span class="s1">)</span>

    <span class="s2"># implement the method to compute the probability given in The Elements</span>
    <span class="s2"># of Statistical Learning (cf. p.127, Sect. 4.4.5 &quot;Logistic Regression</span>
    <span class="s2"># or LDA?&quot;)</span>
    <span class="s1">precision = linalg.inv(blob_stds[</span><span class="s3">0</span><span class="s1">])</span>
    <span class="s1">alpha_k = []</span>
    <span class="s1">alpha_k_0 = []</span>
    <span class="s0">for </span><span class="s1">clazz </span><span class="s0">in </span><span class="s1">range(len(blob_centers) - </span><span class="s3">1</span><span class="s1">):</span>
        <span class="s1">alpha_k.append(</span>
            <span class="s1">np.dot(precision</span><span class="s0">, </span><span class="s1">(blob_centers[clazz] - blob_centers[-</span><span class="s3">1</span><span class="s1">])[:</span><span class="s0">, </span><span class="s1">np.newaxis])</span>
        <span class="s1">)</span>
        <span class="s1">alpha_k_0.append(</span>
            <span class="s1">np.dot(</span>
                <span class="s1">-</span><span class="s3">0.5 </span><span class="s1">* (blob_centers[clazz] + blob_centers[-</span><span class="s3">1</span><span class="s1">])[np.newaxis</span><span class="s0">, </span><span class="s1">:]</span><span class="s0">,</span>
                <span class="s1">alpha_k[-</span><span class="s3">1</span><span class="s1">]</span><span class="s0">,</span>
            <span class="s1">)</span>
        <span class="s1">)</span>

    <span class="s1">sample = np.array([[-</span><span class="s3">22</span><span class="s0">, </span><span class="s3">22</span><span class="s1">]])</span>

    <span class="s0">def </span><span class="s1">discriminant_func(sample</span><span class="s0">, </span><span class="s1">coef</span><span class="s0">, </span><span class="s1">intercept</span><span class="s0">, </span><span class="s1">clazz):</span>
        <span class="s0">return </span><span class="s1">np.exp(intercept[clazz] + np.dot(sample</span><span class="s0">, </span><span class="s1">coef[clazz])).item()</span>

    <span class="s1">prob = np.array(</span>
        <span class="s1">[</span>
            <span class="s1">float(</span>
                <span class="s1">discriminant_func(sample</span><span class="s0">, </span><span class="s1">alpha_k</span><span class="s0">, </span><span class="s1">alpha_k_0</span><span class="s0">, </span><span class="s1">clazz)</span>
                <span class="s1">/ (</span>
                    <span class="s3">1</span>
                    <span class="s1">+ sum(</span>
                        <span class="s1">[</span>
                            <span class="s1">discriminant_func(sample</span><span class="s0">, </span><span class="s1">alpha_k</span><span class="s0">, </span><span class="s1">alpha_k_0</span><span class="s0">, </span><span class="s1">clazz)</span>
                            <span class="s0">for </span><span class="s1">clazz </span><span class="s0">in </span><span class="s1">range(n_classes - </span><span class="s3">1</span><span class="s1">)</span>
                        <span class="s1">]</span>
                    <span class="s1">)</span>
                <span class="s1">)</span>
            <span class="s1">)</span>
            <span class="s0">for </span><span class="s1">clazz </span><span class="s0">in </span><span class="s1">range(n_classes - </span><span class="s3">1</span><span class="s1">)</span>
        <span class="s1">]</span>
    <span class="s1">)</span>

    <span class="s1">prob_ref = </span><span class="s3">1 </span><span class="s1">- np.sum(prob)</span>

    <span class="s2"># check the consistency of the computed probability</span>
    <span class="s2"># all probabilities should sum to one</span>
    <span class="s1">prob_ref_2 = float(</span>
        <span class="s3">1</span>
        <span class="s1">/ (</span>
            <span class="s3">1</span>
            <span class="s1">+ sum(</span>
                <span class="s1">[</span>
                    <span class="s1">discriminant_func(sample</span><span class="s0">, </span><span class="s1">alpha_k</span><span class="s0">, </span><span class="s1">alpha_k_0</span><span class="s0">, </span><span class="s1">clazz)</span>
                    <span class="s0">for </span><span class="s1">clazz </span><span class="s0">in </span><span class="s1">range(n_classes - </span><span class="s3">1</span><span class="s1">)</span>
                <span class="s1">]</span>
            <span class="s1">)</span>
        <span class="s1">)</span>
    <span class="s1">)</span>

    <span class="s0">assert </span><span class="s1">prob_ref == pytest.approx(prob_ref_2)</span>
    <span class="s2"># check that the probability of LDA are close to the theoretical</span>
    <span class="s2"># probabilities</span>
    <span class="s1">assert_allclose(</span>
        <span class="s1">lda.predict_proba(sample)</span><span class="s0">, </span><span class="s1">np.hstack([prob</span><span class="s0">, </span><span class="s1">prob_ref])[np.newaxis]</span><span class="s0">, </span><span class="s1">atol=</span><span class="s3">1e-2</span>
    <span class="s1">)</span>


<span class="s0">def </span><span class="s1">test_lda_priors():</span>
    <span class="s2"># Test priors (negative priors)</span>
    <span class="s1">priors = np.array([</span><span class="s3">0.5</span><span class="s0">, </span><span class="s1">-</span><span class="s3">0.5</span><span class="s1">])</span>
    <span class="s1">clf = LinearDiscriminantAnalysis(priors=priors)</span>
    <span class="s1">msg = </span><span class="s4">&quot;priors must be non-negative&quot;</span>

    <span class="s0">with </span><span class="s1">pytest.raises(ValueError</span><span class="s0">, </span><span class="s1">match=msg):</span>
        <span class="s1">clf.fit(X</span><span class="s0">, </span><span class="s1">y)</span>

    <span class="s2"># Test that priors passed as a list are correctly handled (run to see if</span>
    <span class="s2"># failure)</span>
    <span class="s1">clf = LinearDiscriminantAnalysis(priors=[</span><span class="s3">0.5</span><span class="s0">, </span><span class="s3">0.5</span><span class="s1">])</span>
    <span class="s1">clf.fit(X</span><span class="s0">, </span><span class="s1">y)</span>

    <span class="s2"># Test that priors always sum to 1</span>
    <span class="s1">priors = np.array([</span><span class="s3">0.5</span><span class="s0">, </span><span class="s3">0.6</span><span class="s1">])</span>
    <span class="s1">prior_norm = np.array([</span><span class="s3">0.45</span><span class="s0">, </span><span class="s3">0.55</span><span class="s1">])</span>
    <span class="s1">clf = LinearDiscriminantAnalysis(priors=priors)</span>

    <span class="s0">with </span><span class="s1">pytest.warns(UserWarning):</span>
        <span class="s1">clf.fit(X</span><span class="s0">, </span><span class="s1">y)</span>

    <span class="s1">assert_array_almost_equal(clf.priors_</span><span class="s0">, </span><span class="s1">prior_norm</span><span class="s0">, </span><span class="s3">2</span><span class="s1">)</span>


<span class="s0">def </span><span class="s1">test_lda_coefs():</span>
    <span class="s2"># Test if the coefficients of the solvers are approximately the same.</span>
    <span class="s1">n_features = </span><span class="s3">2</span>
    <span class="s1">n_classes = </span><span class="s3">2</span>
    <span class="s1">n_samples = </span><span class="s3">1000</span>
    <span class="s1">X</span><span class="s0">, </span><span class="s1">y = make_blobs(</span>
        <span class="s1">n_samples=n_samples</span><span class="s0">, </span><span class="s1">n_features=n_features</span><span class="s0">, </span><span class="s1">centers=n_classes</span><span class="s0">, </span><span class="s1">random_state=</span><span class="s3">11</span>
    <span class="s1">)</span>

    <span class="s1">clf_lda_svd = LinearDiscriminantAnalysis(solver=</span><span class="s4">&quot;svd&quot;</span><span class="s1">)</span>
    <span class="s1">clf_lda_lsqr = LinearDiscriminantAnalysis(solver=</span><span class="s4">&quot;lsqr&quot;</span><span class="s1">)</span>
    <span class="s1">clf_lda_eigen = LinearDiscriminantAnalysis(solver=</span><span class="s4">&quot;eigen&quot;</span><span class="s1">)</span>

    <span class="s1">clf_lda_svd.fit(X</span><span class="s0">, </span><span class="s1">y)</span>
    <span class="s1">clf_lda_lsqr.fit(X</span><span class="s0">, </span><span class="s1">y)</span>
    <span class="s1">clf_lda_eigen.fit(X</span><span class="s0">, </span><span class="s1">y)</span>

    <span class="s1">assert_array_almost_equal(clf_lda_svd.coef_</span><span class="s0">, </span><span class="s1">clf_lda_lsqr.coef_</span><span class="s0">, </span><span class="s3">1</span><span class="s1">)</span>
    <span class="s1">assert_array_almost_equal(clf_lda_svd.coef_</span><span class="s0">, </span><span class="s1">clf_lda_eigen.coef_</span><span class="s0">, </span><span class="s3">1</span><span class="s1">)</span>
    <span class="s1">assert_array_almost_equal(clf_lda_eigen.coef_</span><span class="s0">, </span><span class="s1">clf_lda_lsqr.coef_</span><span class="s0">, </span><span class="s3">1</span><span class="s1">)</span>


<span class="s0">def </span><span class="s1">test_lda_transform():</span>
    <span class="s2"># Test LDA transform.</span>
    <span class="s1">clf = LinearDiscriminantAnalysis(solver=</span><span class="s4">&quot;svd&quot;</span><span class="s0">, </span><span class="s1">n_components=</span><span class="s3">1</span><span class="s1">)</span>
    <span class="s1">X_transformed = clf.fit(X</span><span class="s0">, </span><span class="s1">y).transform(X)</span>
    <span class="s0">assert </span><span class="s1">X_transformed.shape[</span><span class="s3">1</span><span class="s1">] == </span><span class="s3">1</span>
    <span class="s1">clf = LinearDiscriminantAnalysis(solver=</span><span class="s4">&quot;eigen&quot;</span><span class="s0">, </span><span class="s1">n_components=</span><span class="s3">1</span><span class="s1">)</span>
    <span class="s1">X_transformed = clf.fit(X</span><span class="s0">, </span><span class="s1">y).transform(X)</span>
    <span class="s0">assert </span><span class="s1">X_transformed.shape[</span><span class="s3">1</span><span class="s1">] == </span><span class="s3">1</span>

    <span class="s1">clf = LinearDiscriminantAnalysis(solver=</span><span class="s4">&quot;lsqr&quot;</span><span class="s0">, </span><span class="s1">n_components=</span><span class="s3">1</span><span class="s1">)</span>
    <span class="s1">clf.fit(X</span><span class="s0">, </span><span class="s1">y)</span>
    <span class="s1">msg = </span><span class="s4">&quot;transform not implemented for 'lsqr'&quot;</span>

    <span class="s0">with </span><span class="s1">pytest.raises(NotImplementedError</span><span class="s0">, </span><span class="s1">match=msg):</span>
        <span class="s1">clf.transform(X)</span>


<span class="s0">def </span><span class="s1">test_lda_explained_variance_ratio():</span>
    <span class="s2"># Test if the sum of the normalized eigen vectors values equals 1,</span>
    <span class="s2"># Also tests whether the explained_variance_ratio_ formed by the</span>
    <span class="s2"># eigen solver is the same as the explained_variance_ratio_ formed</span>
    <span class="s2"># by the svd solver</span>

    <span class="s1">state = np.random.RandomState(</span><span class="s3">0</span><span class="s1">)</span>
    <span class="s1">X = state.normal(loc=</span><span class="s3">0</span><span class="s0">, </span><span class="s1">scale=</span><span class="s3">100</span><span class="s0">, </span><span class="s1">size=(</span><span class="s3">40</span><span class="s0">, </span><span class="s3">20</span><span class="s1">))</span>
    <span class="s1">y = state.randint(</span><span class="s3">0</span><span class="s0">, </span><span class="s3">3</span><span class="s0">, </span><span class="s1">size=(</span><span class="s3">40</span><span class="s0">,</span><span class="s1">))</span>

    <span class="s1">clf_lda_eigen = LinearDiscriminantAnalysis(solver=</span><span class="s4">&quot;eigen&quot;</span><span class="s1">)</span>
    <span class="s1">clf_lda_eigen.fit(X</span><span class="s0">, </span><span class="s1">y)</span>
    <span class="s1">assert_almost_equal(clf_lda_eigen.explained_variance_ratio_.sum()</span><span class="s0">, </span><span class="s3">1.0</span><span class="s0">, </span><span class="s3">3</span><span class="s1">)</span>
    <span class="s0">assert </span><span class="s1">clf_lda_eigen.explained_variance_ratio_.shape == (</span>
        <span class="s3">2</span><span class="s0">,</span>
    <span class="s1">)</span><span class="s0">, </span><span class="s4">&quot;Unexpected length for explained_variance_ratio_&quot;</span>

    <span class="s1">clf_lda_svd = LinearDiscriminantAnalysis(solver=</span><span class="s4">&quot;svd&quot;</span><span class="s1">)</span>
    <span class="s1">clf_lda_svd.fit(X</span><span class="s0">, </span><span class="s1">y)</span>
    <span class="s1">assert_almost_equal(clf_lda_svd.explained_variance_ratio_.sum()</span><span class="s0">, </span><span class="s3">1.0</span><span class="s0">, </span><span class="s3">3</span><span class="s1">)</span>
    <span class="s0">assert </span><span class="s1">clf_lda_svd.explained_variance_ratio_.shape == (</span>
        <span class="s3">2</span><span class="s0">,</span>
    <span class="s1">)</span><span class="s0">, </span><span class="s4">&quot;Unexpected length for explained_variance_ratio_&quot;</span>

    <span class="s1">assert_array_almost_equal(</span>
        <span class="s1">clf_lda_svd.explained_variance_ratio_</span><span class="s0">, </span><span class="s1">clf_lda_eigen.explained_variance_ratio_</span>
    <span class="s1">)</span>


<span class="s0">def </span><span class="s1">test_lda_orthogonality():</span>
    <span class="s2"># arrange four classes with their means in a kite-shaped pattern</span>
    <span class="s2"># the longer distance should be transformed to the first component, and</span>
    <span class="s2"># the shorter distance to the second component.</span>
    <span class="s1">means = np.array([[</span><span class="s3">0</span><span class="s0">, </span><span class="s3">0</span><span class="s0">, </span><span class="s1">-</span><span class="s3">1</span><span class="s1">]</span><span class="s0">, </span><span class="s1">[</span><span class="s3">0</span><span class="s0">, </span><span class="s3">2</span><span class="s0">, </span><span class="s3">0</span><span class="s1">]</span><span class="s0">, </span><span class="s1">[</span><span class="s3">0</span><span class="s0">, </span><span class="s1">-</span><span class="s3">2</span><span class="s0">, </span><span class="s3">0</span><span class="s1">]</span><span class="s0">, </span><span class="s1">[</span><span class="s3">0</span><span class="s0">, </span><span class="s3">0</span><span class="s0">, </span><span class="s3">5</span><span class="s1">]])</span>

    <span class="s2"># We construct perfectly symmetric distributions, so the LDA can estimate</span>
    <span class="s2"># precise means.</span>
    <span class="s1">scatter = np.array(</span>
        <span class="s1">[</span>
            <span class="s1">[</span><span class="s3">0.1</span><span class="s0">, </span><span class="s3">0</span><span class="s0">, </span><span class="s3">0</span><span class="s1">]</span><span class="s0">,</span>
            <span class="s1">[-</span><span class="s3">0.1</span><span class="s0">, </span><span class="s3">0</span><span class="s0">, </span><span class="s3">0</span><span class="s1">]</span><span class="s0">,</span>
            <span class="s1">[</span><span class="s3">0</span><span class="s0">, </span><span class="s3">0.1</span><span class="s0">, </span><span class="s3">0</span><span class="s1">]</span><span class="s0">,</span>
            <span class="s1">[</span><span class="s3">0</span><span class="s0">, </span><span class="s1">-</span><span class="s3">0.1</span><span class="s0">, </span><span class="s3">0</span><span class="s1">]</span><span class="s0">,</span>
            <span class="s1">[</span><span class="s3">0</span><span class="s0">, </span><span class="s3">0</span><span class="s0">, </span><span class="s3">0.1</span><span class="s1">]</span><span class="s0">,</span>
            <span class="s1">[</span><span class="s3">0</span><span class="s0">, </span><span class="s3">0</span><span class="s0">, </span><span class="s1">-</span><span class="s3">0.1</span><span class="s1">]</span><span class="s0">,</span>
        <span class="s1">]</span>
    <span class="s1">)</span>

    <span class="s1">X = (means[:</span><span class="s0">, </span><span class="s1">np.newaxis</span><span class="s0">, </span><span class="s1">:] + scatter[np.newaxis</span><span class="s0">, </span><span class="s1">:</span><span class="s0">, </span><span class="s1">:]).reshape((-</span><span class="s3">1</span><span class="s0">, </span><span class="s3">3</span><span class="s1">))</span>
    <span class="s1">y = np.repeat(np.arange(means.shape[</span><span class="s3">0</span><span class="s1">])</span><span class="s0">, </span><span class="s1">scatter.shape[</span><span class="s3">0</span><span class="s1">])</span>

    <span class="s2"># Fit LDA and transform the means</span>
    <span class="s1">clf = LinearDiscriminantAnalysis(solver=</span><span class="s4">&quot;svd&quot;</span><span class="s1">).fit(X</span><span class="s0">, </span><span class="s1">y)</span>
    <span class="s1">means_transformed = clf.transform(means)</span>

    <span class="s1">d1 = means_transformed[</span><span class="s3">3</span><span class="s1">] - means_transformed[</span><span class="s3">0</span><span class="s1">]</span>
    <span class="s1">d2 = means_transformed[</span><span class="s3">2</span><span class="s1">] - means_transformed[</span><span class="s3">1</span><span class="s1">]</span>
    <span class="s1">d1 /= np.sqrt(np.sum(d1**</span><span class="s3">2</span><span class="s1">))</span>
    <span class="s1">d2 /= np.sqrt(np.sum(d2**</span><span class="s3">2</span><span class="s1">))</span>

    <span class="s2"># the transformed within-class covariance should be the identity matrix</span>
    <span class="s1">assert_almost_equal(np.cov(clf.transform(scatter).T)</span><span class="s0">, </span><span class="s1">np.eye(</span><span class="s3">2</span><span class="s1">))</span>

    <span class="s2"># the means of classes 0 and 3 should lie on the first component</span>
    <span class="s1">assert_almost_equal(np.abs(np.dot(d1[:</span><span class="s3">2</span><span class="s1">]</span><span class="s0">, </span><span class="s1">[</span><span class="s3">1</span><span class="s0">, </span><span class="s3">0</span><span class="s1">]))</span><span class="s0">, </span><span class="s3">1.0</span><span class="s1">)</span>

    <span class="s2"># the means of classes 1 and 2 should lie on the second component</span>
    <span class="s1">assert_almost_equal(np.abs(np.dot(d2[:</span><span class="s3">2</span><span class="s1">]</span><span class="s0">, </span><span class="s1">[</span><span class="s3">0</span><span class="s0">, </span><span class="s3">1</span><span class="s1">]))</span><span class="s0">, </span><span class="s3">1.0</span><span class="s1">)</span>


<span class="s0">def </span><span class="s1">test_lda_scaling():</span>
    <span class="s2"># Test if classification works correctly with differently scaled features.</span>
    <span class="s1">n = </span><span class="s3">100</span>
    <span class="s1">rng = np.random.RandomState(</span><span class="s3">1234</span><span class="s1">)</span>
    <span class="s2"># use uniform distribution of features to make sure there is absolutely no</span>
    <span class="s2"># overlap between classes.</span>
    <span class="s1">x1 = rng.uniform(-</span><span class="s3">1</span><span class="s0">, </span><span class="s3">1</span><span class="s0">, </span><span class="s1">(n</span><span class="s0">, </span><span class="s3">3</span><span class="s1">)) + [-</span><span class="s3">10</span><span class="s0">, </span><span class="s3">0</span><span class="s0">, </span><span class="s3">0</span><span class="s1">]</span>
    <span class="s1">x2 = rng.uniform(-</span><span class="s3">1</span><span class="s0">, </span><span class="s3">1</span><span class="s0">, </span><span class="s1">(n</span><span class="s0">, </span><span class="s3">3</span><span class="s1">)) + [</span><span class="s3">10</span><span class="s0">, </span><span class="s3">0</span><span class="s0">, </span><span class="s3">0</span><span class="s1">]</span>
    <span class="s1">x = np.vstack((x1</span><span class="s0">, </span><span class="s1">x2)) * [</span><span class="s3">1</span><span class="s0">, </span><span class="s3">100</span><span class="s0">, </span><span class="s3">10000</span><span class="s1">]</span>
    <span class="s1">y = [-</span><span class="s3">1</span><span class="s1">] * n + [</span><span class="s3">1</span><span class="s1">] * n</span>

    <span class="s0">for </span><span class="s1">solver </span><span class="s0">in </span><span class="s1">(</span><span class="s4">&quot;svd&quot;</span><span class="s0">, </span><span class="s4">&quot;lsqr&quot;</span><span class="s0">, </span><span class="s4">&quot;eigen&quot;</span><span class="s1">):</span>
        <span class="s1">clf = LinearDiscriminantAnalysis(solver=solver)</span>
        <span class="s2"># should be able to separate the data perfectly</span>
        <span class="s0">assert </span><span class="s1">clf.fit(x</span><span class="s0">, </span><span class="s1">y).score(x</span><span class="s0">, </span><span class="s1">y) == </span><span class="s3">1.0</span><span class="s0">, </span><span class="s4">&quot;using covariance: %s&quot; </span><span class="s1">% solver</span>


<span class="s0">def </span><span class="s1">test_lda_store_covariance():</span>
    <span class="s2"># Test for solver 'lsqr' and 'eigen'</span>
    <span class="s2"># 'store_covariance' has no effect on 'lsqr' and 'eigen' solvers</span>
    <span class="s0">for </span><span class="s1">solver </span><span class="s0">in </span><span class="s1">(</span><span class="s4">&quot;lsqr&quot;</span><span class="s0">, </span><span class="s4">&quot;eigen&quot;</span><span class="s1">):</span>
        <span class="s1">clf = LinearDiscriminantAnalysis(solver=solver).fit(X6</span><span class="s0">, </span><span class="s1">y6)</span>
        <span class="s0">assert </span><span class="s1">hasattr(clf</span><span class="s0">, </span><span class="s4">&quot;covariance_&quot;</span><span class="s1">)</span>

        <span class="s2"># Test the actual attribute:</span>
        <span class="s1">clf = LinearDiscriminantAnalysis(solver=solver</span><span class="s0">, </span><span class="s1">store_covariance=</span><span class="s0">True</span><span class="s1">).fit(</span>
            <span class="s1">X6</span><span class="s0">, </span><span class="s1">y6</span>
        <span class="s1">)</span>
        <span class="s0">assert </span><span class="s1">hasattr(clf</span><span class="s0">, </span><span class="s4">&quot;covariance_&quot;</span><span class="s1">)</span>

        <span class="s1">assert_array_almost_equal(</span>
            <span class="s1">clf.covariance_</span><span class="s0">, </span><span class="s1">np.array([[</span><span class="s3">0.422222</span><span class="s0">, </span><span class="s3">0.088889</span><span class="s1">]</span><span class="s0">, </span><span class="s1">[</span><span class="s3">0.088889</span><span class="s0">, </span><span class="s3">0.533333</span><span class="s1">]])</span>
        <span class="s1">)</span>

    <span class="s2"># Test for SVD solver, the default is to not set the covariances_ attribute</span>
    <span class="s1">clf = LinearDiscriminantAnalysis(solver=</span><span class="s4">&quot;svd&quot;</span><span class="s1">).fit(X6</span><span class="s0">, </span><span class="s1">y6)</span>
    <span class="s0">assert not </span><span class="s1">hasattr(clf</span><span class="s0">, </span><span class="s4">&quot;covariance_&quot;</span><span class="s1">)</span>

    <span class="s2"># Test the actual attribute:</span>
    <span class="s1">clf = LinearDiscriminantAnalysis(solver=solver</span><span class="s0">, </span><span class="s1">store_covariance=</span><span class="s0">True</span><span class="s1">).fit(X6</span><span class="s0">, </span><span class="s1">y6)</span>
    <span class="s0">assert </span><span class="s1">hasattr(clf</span><span class="s0">, </span><span class="s4">&quot;covariance_&quot;</span><span class="s1">)</span>

    <span class="s1">assert_array_almost_equal(</span>
        <span class="s1">clf.covariance_</span><span class="s0">, </span><span class="s1">np.array([[</span><span class="s3">0.422222</span><span class="s0">, </span><span class="s3">0.088889</span><span class="s1">]</span><span class="s0">, </span><span class="s1">[</span><span class="s3">0.088889</span><span class="s0">, </span><span class="s3">0.533333</span><span class="s1">]])</span>
    <span class="s1">)</span>


<span class="s1">@pytest.mark.parametrize(</span><span class="s4">&quot;seed&quot;</span><span class="s0">, </span><span class="s1">range(</span><span class="s3">10</span><span class="s1">))</span>
<span class="s0">def </span><span class="s1">test_lda_shrinkage(seed):</span>
    <span class="s2"># Test that shrunk covariance estimator and shrinkage parameter behave the</span>
    <span class="s2"># same</span>
    <span class="s1">rng = np.random.RandomState(seed)</span>
    <span class="s1">X = rng.rand(</span><span class="s3">100</span><span class="s0">, </span><span class="s3">10</span><span class="s1">)</span>
    <span class="s1">y = rng.randint(</span><span class="s3">3</span><span class="s0">, </span><span class="s1">size=(</span><span class="s3">100</span><span class="s1">))</span>
    <span class="s1">c1 = LinearDiscriminantAnalysis(store_covariance=</span><span class="s0">True, </span><span class="s1">shrinkage=</span><span class="s3">0.5</span><span class="s0">, </span><span class="s1">solver=</span><span class="s4">&quot;lsqr&quot;</span><span class="s1">)</span>
    <span class="s1">c2 = LinearDiscriminantAnalysis(</span>
        <span class="s1">store_covariance=</span><span class="s0">True,</span>
        <span class="s1">covariance_estimator=ShrunkCovariance(shrinkage=</span><span class="s3">0.5</span><span class="s1">)</span><span class="s0">,</span>
        <span class="s1">solver=</span><span class="s4">&quot;lsqr&quot;</span><span class="s0">,</span>
    <span class="s1">)</span>
    <span class="s1">c1.fit(X</span><span class="s0">, </span><span class="s1">y)</span>
    <span class="s1">c2.fit(X</span><span class="s0">, </span><span class="s1">y)</span>
    <span class="s1">assert_allclose(c1.means_</span><span class="s0">, </span><span class="s1">c2.means_)</span>
    <span class="s1">assert_allclose(c1.covariance_</span><span class="s0">, </span><span class="s1">c2.covariance_)</span>


<span class="s0">def </span><span class="s1">test_lda_ledoitwolf():</span>
    <span class="s2"># When shrinkage=&quot;auto&quot; current implementation uses ledoitwolf estimation</span>
    <span class="s2"># of covariance after standardizing the data. This checks that it is indeed</span>
    <span class="s2"># the case</span>
    <span class="s0">class </span><span class="s1">StandardizedLedoitWolf:</span>
        <span class="s0">def </span><span class="s1">fit(self</span><span class="s0">, </span><span class="s1">X):</span>
            <span class="s1">sc = StandardScaler()  </span><span class="s2"># standardize features</span>
            <span class="s1">X_sc = sc.fit_transform(X)</span>
            <span class="s1">s = ledoit_wolf(X_sc)[</span><span class="s3">0</span><span class="s1">]</span>
            <span class="s2"># rescale</span>
            <span class="s1">s = sc.scale_[:</span><span class="s0">, </span><span class="s1">np.newaxis] * s * sc.scale_[np.newaxis</span><span class="s0">, </span><span class="s1">:]</span>
            <span class="s1">self.covariance_ = s</span>

    <span class="s1">rng = np.random.RandomState(</span><span class="s3">0</span><span class="s1">)</span>
    <span class="s1">X = rng.rand(</span><span class="s3">100</span><span class="s0">, </span><span class="s3">10</span><span class="s1">)</span>
    <span class="s1">y = rng.randint(</span><span class="s3">3</span><span class="s0">, </span><span class="s1">size=(</span><span class="s3">100</span><span class="s0">,</span><span class="s1">))</span>
    <span class="s1">c1 = LinearDiscriminantAnalysis(</span>
        <span class="s1">store_covariance=</span><span class="s0">True, </span><span class="s1">shrinkage=</span><span class="s4">&quot;auto&quot;</span><span class="s0">, </span><span class="s1">solver=</span><span class="s4">&quot;lsqr&quot;</span>
    <span class="s1">)</span>
    <span class="s1">c2 = LinearDiscriminantAnalysis(</span>
        <span class="s1">store_covariance=</span><span class="s0">True,</span>
        <span class="s1">covariance_estimator=StandardizedLedoitWolf()</span><span class="s0">,</span>
        <span class="s1">solver=</span><span class="s4">&quot;lsqr&quot;</span><span class="s0">,</span>
    <span class="s1">)</span>
    <span class="s1">c1.fit(X</span><span class="s0">, </span><span class="s1">y)</span>
    <span class="s1">c2.fit(X</span><span class="s0">, </span><span class="s1">y)</span>
    <span class="s1">assert_allclose(c1.means_</span><span class="s0">, </span><span class="s1">c2.means_)</span>
    <span class="s1">assert_allclose(c1.covariance_</span><span class="s0">, </span><span class="s1">c2.covariance_)</span>


<span class="s1">@pytest.mark.parametrize(</span><span class="s4">&quot;n_features&quot;</span><span class="s0">, </span><span class="s1">[</span><span class="s3">3</span><span class="s0">, </span><span class="s3">5</span><span class="s1">])</span>
<span class="s1">@pytest.mark.parametrize(</span><span class="s4">&quot;n_classes&quot;</span><span class="s0">, </span><span class="s1">[</span><span class="s3">5</span><span class="s0">, </span><span class="s3">3</span><span class="s1">])</span>
<span class="s0">def </span><span class="s1">test_lda_dimension_warning(n_classes</span><span class="s0">, </span><span class="s1">n_features):</span>
    <span class="s1">rng = check_random_state(</span><span class="s3">0</span><span class="s1">)</span>
    <span class="s1">n_samples = </span><span class="s3">10</span>
    <span class="s1">X = rng.randn(n_samples</span><span class="s0">, </span><span class="s1">n_features)</span>
    <span class="s2"># we create n_classes labels by repeating and truncating a</span>
    <span class="s2"># range(n_classes) until n_samples</span>
    <span class="s1">y = np.tile(range(n_classes)</span><span class="s0">, </span><span class="s1">n_samples // n_classes + </span><span class="s3">1</span><span class="s1">)[:n_samples]</span>
    <span class="s1">max_components = min(n_features</span><span class="s0">, </span><span class="s1">n_classes - </span><span class="s3">1</span><span class="s1">)</span>

    <span class="s0">for </span><span class="s1">n_components </span><span class="s0">in </span><span class="s1">[max_components - </span><span class="s3">1</span><span class="s0">, None, </span><span class="s1">max_components]:</span>
        <span class="s2"># if n_components &lt;= min(n_classes - 1, n_features), no warning</span>
        <span class="s1">lda = LinearDiscriminantAnalysis(n_components=n_components)</span>
        <span class="s1">lda.fit(X</span><span class="s0">, </span><span class="s1">y)</span>

    <span class="s0">for </span><span class="s1">n_components </span><span class="s0">in </span><span class="s1">[max_components + </span><span class="s3">1</span><span class="s0">, </span><span class="s1">max(n_features</span><span class="s0">, </span><span class="s1">n_classes - </span><span class="s3">1</span><span class="s1">) + </span><span class="s3">1</span><span class="s1">]:</span>
        <span class="s2"># if n_components &gt; min(n_classes - 1, n_features), raise error.</span>
        <span class="s2"># We test one unit higher than max_components, and then something</span>
        <span class="s2"># larger than both n_features and n_classes - 1 to ensure the test</span>
        <span class="s2"># works for any value of n_component</span>
        <span class="s1">lda = LinearDiscriminantAnalysis(n_components=n_components)</span>
        <span class="s1">msg = </span><span class="s4">&quot;n_components cannot be larger than &quot;</span>
        <span class="s0">with </span><span class="s1">pytest.raises(ValueError</span><span class="s0">, </span><span class="s1">match=msg):</span>
            <span class="s1">lda.fit(X</span><span class="s0">, </span><span class="s1">y)</span>


<span class="s1">@pytest.mark.parametrize(</span>
    <span class="s4">&quot;data_type, expected_type&quot;</span><span class="s0">,</span>
    <span class="s1">[</span>
        <span class="s1">(np.float32</span><span class="s0">, </span><span class="s1">np.float32)</span><span class="s0">,</span>
        <span class="s1">(np.float64</span><span class="s0">, </span><span class="s1">np.float64)</span><span class="s0">,</span>
        <span class="s1">(np.int32</span><span class="s0">, </span><span class="s1">np.float64)</span><span class="s0">,</span>
        <span class="s1">(np.int64</span><span class="s0">, </span><span class="s1">np.float64)</span><span class="s0">,</span>
    <span class="s1">]</span><span class="s0">,</span>
<span class="s1">)</span>
<span class="s0">def </span><span class="s1">test_lda_dtype_match(data_type</span><span class="s0">, </span><span class="s1">expected_type):</span>
    <span class="s0">for </span><span class="s1">solver</span><span class="s0">, </span><span class="s1">shrinkage </span><span class="s0">in </span><span class="s1">solver_shrinkage:</span>
        <span class="s1">clf = LinearDiscriminantAnalysis(solver=solver</span><span class="s0">, </span><span class="s1">shrinkage=shrinkage)</span>
        <span class="s1">clf.fit(X.astype(data_type)</span><span class="s0">, </span><span class="s1">y.astype(data_type))</span>
        <span class="s0">assert </span><span class="s1">clf.coef_.dtype == expected_type</span>


<span class="s0">def </span><span class="s1">test_lda_numeric_consistency_float32_float64():</span>
    <span class="s0">for </span><span class="s1">solver</span><span class="s0">, </span><span class="s1">shrinkage </span><span class="s0">in </span><span class="s1">solver_shrinkage:</span>
        <span class="s1">clf_32 = LinearDiscriminantAnalysis(solver=solver</span><span class="s0">, </span><span class="s1">shrinkage=shrinkage)</span>
        <span class="s1">clf_32.fit(X.astype(np.float32)</span><span class="s0">, </span><span class="s1">y.astype(np.float32))</span>
        <span class="s1">clf_64 = LinearDiscriminantAnalysis(solver=solver</span><span class="s0">, </span><span class="s1">shrinkage=shrinkage)</span>
        <span class="s1">clf_64.fit(X.astype(np.float64)</span><span class="s0">, </span><span class="s1">y.astype(np.float64))</span>

        <span class="s2"># Check value consistency between types</span>
        <span class="s1">rtol = </span><span class="s3">1e-6</span>
        <span class="s1">assert_allclose(clf_32.coef_</span><span class="s0">, </span><span class="s1">clf_64.coef_</span><span class="s0">, </span><span class="s1">rtol=rtol)</span>


<span class="s0">def </span><span class="s1">test_qda():</span>
    <span class="s2"># QDA classification.</span>
    <span class="s2"># This checks that QDA implements fit and predict and returns</span>
    <span class="s2"># correct values for a simple toy dataset.</span>
    <span class="s1">clf = QuadraticDiscriminantAnalysis()</span>
    <span class="s1">y_pred = clf.fit(X6</span><span class="s0">, </span><span class="s1">y6).predict(X6)</span>
    <span class="s1">assert_array_equal(y_pred</span><span class="s0">, </span><span class="s1">y6)</span>

    <span class="s2"># Assure that it works with 1D data</span>
    <span class="s1">y_pred1 = clf.fit(X7</span><span class="s0">, </span><span class="s1">y6).predict(X7)</span>
    <span class="s1">assert_array_equal(y_pred1</span><span class="s0">, </span><span class="s1">y6)</span>

    <span class="s2"># Test probas estimates</span>
    <span class="s1">y_proba_pred1 = clf.predict_proba(X7)</span>
    <span class="s1">assert_array_equal((y_proba_pred1[:</span><span class="s0">, </span><span class="s3">1</span><span class="s1">] &gt; </span><span class="s3">0.5</span><span class="s1">) + </span><span class="s3">1</span><span class="s0">, </span><span class="s1">y6)</span>
    <span class="s1">y_log_proba_pred1 = clf.predict_log_proba(X7)</span>
    <span class="s1">assert_array_almost_equal(np.exp(y_log_proba_pred1)</span><span class="s0">, </span><span class="s1">y_proba_pred1</span><span class="s0">, </span><span class="s3">8</span><span class="s1">)</span>

    <span class="s1">y_pred3 = clf.fit(X6</span><span class="s0">, </span><span class="s1">y7).predict(X6)</span>
    <span class="s2"># QDA shouldn't be able to separate those</span>
    <span class="s0">assert </span><span class="s1">np.any(y_pred3 != y7)</span>

    <span class="s2"># Classes should have at least 2 elements</span>
    <span class="s0">with </span><span class="s1">pytest.raises(ValueError):</span>
        <span class="s1">clf.fit(X6</span><span class="s0">, </span><span class="s1">y4)</span>


<span class="s0">def </span><span class="s1">test_qda_priors():</span>
    <span class="s1">clf = QuadraticDiscriminantAnalysis()</span>
    <span class="s1">y_pred = clf.fit(X6</span><span class="s0">, </span><span class="s1">y6).predict(X6)</span>
    <span class="s1">n_pos = np.sum(y_pred == </span><span class="s3">2</span><span class="s1">)</span>

    <span class="s1">neg = </span><span class="s3">1e-10</span>
    <span class="s1">clf = QuadraticDiscriminantAnalysis(priors=np.array([neg</span><span class="s0">, </span><span class="s3">1 </span><span class="s1">- neg]))</span>
    <span class="s1">y_pred = clf.fit(X6</span><span class="s0">, </span><span class="s1">y6).predict(X6)</span>
    <span class="s1">n_pos2 = np.sum(y_pred == </span><span class="s3">2</span><span class="s1">)</span>

    <span class="s0">assert </span><span class="s1">n_pos2 &gt; n_pos</span>


<span class="s1">@pytest.mark.parametrize(</span><span class="s4">&quot;priors_type&quot;</span><span class="s0">, </span><span class="s1">[</span><span class="s4">&quot;list&quot;</span><span class="s0">, </span><span class="s4">&quot;tuple&quot;</span><span class="s0">, </span><span class="s4">&quot;array&quot;</span><span class="s1">])</span>
<span class="s0">def </span><span class="s1">test_qda_prior_type(priors_type):</span>
    <span class="s5">&quot;&quot;&quot;Check that priors accept array-like.&quot;&quot;&quot;</span>
    <span class="s1">priors = [</span><span class="s3">0.5</span><span class="s0">, </span><span class="s3">0.5</span><span class="s1">]</span>
    <span class="s1">clf = QuadraticDiscriminantAnalysis(</span>
        <span class="s1">priors=_convert_container([</span><span class="s3">0.5</span><span class="s0">, </span><span class="s3">0.5</span><span class="s1">]</span><span class="s0">, </span><span class="s1">priors_type)</span>
    <span class="s1">).fit(X6</span><span class="s0">, </span><span class="s1">y6)</span>
    <span class="s0">assert </span><span class="s1">isinstance(clf.priors_</span><span class="s0">, </span><span class="s1">np.ndarray)</span>
    <span class="s1">assert_array_equal(clf.priors_</span><span class="s0">, </span><span class="s1">priors)</span>


<span class="s0">def </span><span class="s1">test_qda_prior_copy():</span>
    <span class="s5">&quot;&quot;&quot;Check that altering `priors` without `fit` doesn't change `priors_`&quot;&quot;&quot;</span>
    <span class="s1">priors = np.array([</span><span class="s3">0.5</span><span class="s0">, </span><span class="s3">0.5</span><span class="s1">])</span>
    <span class="s1">qda = QuadraticDiscriminantAnalysis(priors=priors).fit(X</span><span class="s0">, </span><span class="s1">y)</span>

    <span class="s2"># we expect the following</span>
    <span class="s1">assert_array_equal(qda.priors_</span><span class="s0">, </span><span class="s1">qda.priors)</span>

    <span class="s2"># altering `priors` without `fit` should not change `priors_`</span>
    <span class="s1">priors[</span><span class="s3">0</span><span class="s1">] = </span><span class="s3">0.2</span>
    <span class="s0">assert </span><span class="s1">qda.priors_[</span><span class="s3">0</span><span class="s1">] != qda.priors[</span><span class="s3">0</span><span class="s1">]</span>


<span class="s0">def </span><span class="s1">test_qda_store_covariance():</span>
    <span class="s2"># The default is to not set the covariances_ attribute</span>
    <span class="s1">clf = QuadraticDiscriminantAnalysis().fit(X6</span><span class="s0">, </span><span class="s1">y6)</span>
    <span class="s0">assert not </span><span class="s1">hasattr(clf</span><span class="s0">, </span><span class="s4">&quot;covariance_&quot;</span><span class="s1">)</span>

    <span class="s2"># Test the actual attribute:</span>
    <span class="s1">clf = QuadraticDiscriminantAnalysis(store_covariance=</span><span class="s0">True</span><span class="s1">).fit(X6</span><span class="s0">, </span><span class="s1">y6)</span>
    <span class="s0">assert </span><span class="s1">hasattr(clf</span><span class="s0">, </span><span class="s4">&quot;covariance_&quot;</span><span class="s1">)</span>

    <span class="s1">assert_array_almost_equal(clf.covariance_[</span><span class="s3">0</span><span class="s1">]</span><span class="s0">, </span><span class="s1">np.array([[</span><span class="s3">0.7</span><span class="s0">, </span><span class="s3">0.45</span><span class="s1">]</span><span class="s0">, </span><span class="s1">[</span><span class="s3">0.45</span><span class="s0">, </span><span class="s3">0.7</span><span class="s1">]]))</span>

    <span class="s1">assert_array_almost_equal(</span>
        <span class="s1">clf.covariance_[</span><span class="s3">1</span><span class="s1">]</span><span class="s0">,</span>
        <span class="s1">np.array([[</span><span class="s3">0.33333333</span><span class="s0">, </span><span class="s1">-</span><span class="s3">0.33333333</span><span class="s1">]</span><span class="s0">, </span><span class="s1">[-</span><span class="s3">0.33333333</span><span class="s0">, </span><span class="s3">0.66666667</span><span class="s1">]])</span><span class="s0">,</span>
    <span class="s1">)</span>


<span class="s0">def </span><span class="s1">test_qda_regularization():</span>
    <span class="s2"># The default is reg_param=0. and will cause issues when there is a</span>
    <span class="s2"># constant variable.</span>

    <span class="s2"># Fitting on data with constant variable triggers an UserWarning.</span>
    <span class="s1">collinear_msg = </span><span class="s4">&quot;Variables are collinear&quot;</span>
    <span class="s1">clf = QuadraticDiscriminantAnalysis()</span>
    <span class="s0">with </span><span class="s1">pytest.warns(UserWarning</span><span class="s0">, </span><span class="s1">match=collinear_msg):</span>
        <span class="s1">y_pred = clf.fit(X2</span><span class="s0">, </span><span class="s1">y6)</span>

    <span class="s2"># XXX: RuntimeWarning is also raised at predict time because of divisions</span>
    <span class="s2"># by zero when the model is fit with a constant feature and without</span>
    <span class="s2"># regularization: should this be considered a bug? Either by the fit-time</span>
    <span class="s2"># message more informative, raising and exception instead of a warning in</span>
    <span class="s2"># this case or somehow changing predict to avoid division by zero.</span>
    <span class="s0">with </span><span class="s1">pytest.warns(RuntimeWarning</span><span class="s0">, </span><span class="s1">match=</span><span class="s4">&quot;divide by zero&quot;</span><span class="s1">):</span>
        <span class="s1">y_pred = clf.predict(X2)</span>
    <span class="s0">assert </span><span class="s1">np.any(y_pred != y6)</span>

    <span class="s2"># Adding a little regularization fixes the division by zero at predict</span>
    <span class="s2"># time. But UserWarning will persist at fit time.</span>
    <span class="s1">clf = QuadraticDiscriminantAnalysis(reg_param=</span><span class="s3">0.01</span><span class="s1">)</span>
    <span class="s0">with </span><span class="s1">pytest.warns(UserWarning</span><span class="s0">, </span><span class="s1">match=collinear_msg):</span>
        <span class="s1">clf.fit(X2</span><span class="s0">, </span><span class="s1">y6)</span>
    <span class="s1">y_pred = clf.predict(X2)</span>
    <span class="s1">assert_array_equal(y_pred</span><span class="s0">, </span><span class="s1">y6)</span>

    <span class="s2"># UserWarning should also be there for the n_samples_in_a_class &lt;</span>
    <span class="s2"># n_features case.</span>
    <span class="s1">clf = QuadraticDiscriminantAnalysis(reg_param=</span><span class="s3">0.1</span><span class="s1">)</span>
    <span class="s0">with </span><span class="s1">pytest.warns(UserWarning</span><span class="s0">, </span><span class="s1">match=collinear_msg):</span>
        <span class="s1">clf.fit(X5</span><span class="s0">, </span><span class="s1">y5)</span>
    <span class="s1">y_pred5 = clf.predict(X5)</span>
    <span class="s1">assert_array_equal(y_pred5</span><span class="s0">, </span><span class="s1">y5)</span>


<span class="s0">def </span><span class="s1">test_covariance():</span>
    <span class="s1">x</span><span class="s0">, </span><span class="s1">y = make_blobs(n_samples=</span><span class="s3">100</span><span class="s0">, </span><span class="s1">n_features=</span><span class="s3">5</span><span class="s0">, </span><span class="s1">centers=</span><span class="s3">1</span><span class="s0">, </span><span class="s1">random_state=</span><span class="s3">42</span><span class="s1">)</span>

    <span class="s2"># make features correlated</span>
    <span class="s1">x = np.dot(x</span><span class="s0">, </span><span class="s1">np.arange(x.shape[</span><span class="s3">1</span><span class="s1">] ** </span><span class="s3">2</span><span class="s1">).reshape(x.shape[</span><span class="s3">1</span><span class="s1">]</span><span class="s0">, </span><span class="s1">x.shape[</span><span class="s3">1</span><span class="s1">]))</span>

    <span class="s1">c_e = _cov(x</span><span class="s0">, </span><span class="s4">&quot;empirical&quot;</span><span class="s1">)</span>
    <span class="s1">assert_almost_equal(c_e</span><span class="s0">, </span><span class="s1">c_e.T)</span>

    <span class="s1">c_s = _cov(x</span><span class="s0">, </span><span class="s4">&quot;auto&quot;</span><span class="s1">)</span>
    <span class="s1">assert_almost_equal(c_s</span><span class="s0">, </span><span class="s1">c_s.T)</span>


<span class="s1">@pytest.mark.parametrize(</span><span class="s4">&quot;solver&quot;</span><span class="s0">, </span><span class="s1">[</span><span class="s4">&quot;svd&quot;</span><span class="s0">, </span><span class="s4">&quot;lsqr&quot;</span><span class="s0">, </span><span class="s4">&quot;eigen&quot;</span><span class="s1">])</span>
<span class="s0">def </span><span class="s1">test_raises_value_error_on_same_number_of_classes_and_samples(solver):</span>
    <span class="s5">&quot;&quot;&quot; 
    Tests that if the number of samples equals the number 
    of classes, a ValueError is raised. 
    &quot;&quot;&quot;</span>
    <span class="s1">X = np.array([[</span><span class="s3">0.5</span><span class="s0">, </span><span class="s3">0.6</span><span class="s1">]</span><span class="s0">, </span><span class="s1">[</span><span class="s3">0.6</span><span class="s0">, </span><span class="s3">0.5</span><span class="s1">]])</span>
    <span class="s1">y = np.array([</span><span class="s4">&quot;a&quot;</span><span class="s0">, </span><span class="s4">&quot;b&quot;</span><span class="s1">])</span>
    <span class="s1">clf = LinearDiscriminantAnalysis(solver=solver)</span>
    <span class="s0">with </span><span class="s1">pytest.raises(ValueError</span><span class="s0">, </span><span class="s1">match=</span><span class="s4">&quot;The number of samples must be more&quot;</span><span class="s1">):</span>
        <span class="s1">clf.fit(X</span><span class="s0">, </span><span class="s1">y)</span>


<span class="s0">def </span><span class="s1">test_get_feature_names_out():</span>
    <span class="s5">&quot;&quot;&quot;Check get_feature_names_out uses class name as prefix.&quot;&quot;&quot;</span>

    <span class="s1">est = LinearDiscriminantAnalysis().fit(X</span><span class="s0">, </span><span class="s1">y)</span>
    <span class="s1">names_out = est.get_feature_names_out()</span>

    <span class="s1">class_name_lower = </span><span class="s4">&quot;LinearDiscriminantAnalysis&quot;</span><span class="s1">.lower()</span>
    <span class="s1">expected_names_out = np.array(</span>
        <span class="s1">[</span>
            <span class="s4">f&quot;</span><span class="s0">{</span><span class="s1">class_name_lower</span><span class="s0">}{</span><span class="s1">i</span><span class="s0">}</span><span class="s4">&quot;</span>
            <span class="s0">for </span><span class="s1">i </span><span class="s0">in </span><span class="s1">range(est.explained_variance_ratio_.shape[</span><span class="s3">0</span><span class="s1">])</span>
        <span class="s1">]</span><span class="s0">,</span>
        <span class="s1">dtype=object</span><span class="s0">,</span>
    <span class="s1">)</span>
    <span class="s1">assert_array_equal(names_out</span><span class="s0">, </span><span class="s1">expected_names_out)</span>
</pre>
</body>
</html>