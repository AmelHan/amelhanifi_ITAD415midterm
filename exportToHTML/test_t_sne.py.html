<html>
<head>
<title>test_t_sne.py</title>
<meta http-equiv="Content-Type" content="text/html; charset=utf-8">
<style type="text/css">
.s0 { color: #cc7832;}
.s1 { color: #a9b7c6;}
.s2 { color: #808080;}
.s3 { color: #6897bb;}
.s4 { color: #6a8759;}
.s5 { color: #629755; font-style: italic;}
</style>
</head>
<body bgcolor="#2b2b2b">
<table CELLSPACING=0 CELLPADDING=5 COLS=1 WIDTH="100%" BGCOLOR="#606060" >
<tr><td><center>
<font face="Arial, Helvetica" color="#000000">
test_t_sne.py</font>
</center></td></tr></table>
<pre><span class="s0">import </span><span class="s1">sys</span>
<span class="s0">from </span><span class="s1">io </span><span class="s0">import </span><span class="s1">StringIO</span>

<span class="s0">import </span><span class="s1">numpy </span><span class="s0">as </span><span class="s1">np</span>
<span class="s0">import </span><span class="s1">pytest</span>
<span class="s0">import </span><span class="s1">scipy.sparse </span><span class="s0">as </span><span class="s1">sp</span>
<span class="s0">from </span><span class="s1">numpy.testing </span><span class="s0">import </span><span class="s1">assert_allclose</span>
<span class="s0">from </span><span class="s1">scipy.optimize </span><span class="s0">import </span><span class="s1">check_grad</span>
<span class="s0">from </span><span class="s1">scipy.spatial.distance </span><span class="s0">import </span><span class="s1">pdist</span><span class="s0">, </span><span class="s1">squareform</span>

<span class="s0">from </span><span class="s1">sklearn </span><span class="s0">import </span><span class="s1">config_context</span>
<span class="s0">from </span><span class="s1">sklearn.datasets </span><span class="s0">import </span><span class="s1">make_blobs</span>
<span class="s0">from </span><span class="s1">sklearn.exceptions </span><span class="s0">import </span><span class="s1">EfficiencyWarning</span>

<span class="s2"># mypy error: Module 'sklearn.manifold' has no attribute '_barnes_hut_tsne'</span>
<span class="s0">from </span><span class="s1">sklearn.manifold </span><span class="s0">import </span><span class="s1">(  </span><span class="s2"># type: ignore</span>
    <span class="s1">TSNE</span><span class="s0">,</span>
    <span class="s1">_barnes_hut_tsne</span><span class="s0">,</span>
<span class="s1">)</span>
<span class="s0">from </span><span class="s1">sklearn.manifold._t_sne </span><span class="s0">import </span><span class="s1">(</span>
    <span class="s1">_gradient_descent</span><span class="s0">,</span>
    <span class="s1">_joint_probabilities</span><span class="s0">,</span>
    <span class="s1">_joint_probabilities_nn</span><span class="s0">,</span>
    <span class="s1">_kl_divergence</span><span class="s0">,</span>
    <span class="s1">_kl_divergence_bh</span><span class="s0">,</span>
    <span class="s1">trustworthiness</span><span class="s0">,</span>
<span class="s1">)</span>
<span class="s0">from </span><span class="s1">sklearn.manifold._utils </span><span class="s0">import </span><span class="s1">_binary_search_perplexity</span>
<span class="s0">from </span><span class="s1">sklearn.metrics.pairwise </span><span class="s0">import </span><span class="s1">(</span>
    <span class="s1">cosine_distances</span><span class="s0">,</span>
    <span class="s1">manhattan_distances</span><span class="s0">,</span>
    <span class="s1">pairwise_distances</span><span class="s0">,</span>
<span class="s1">)</span>
<span class="s0">from </span><span class="s1">sklearn.neighbors </span><span class="s0">import </span><span class="s1">NearestNeighbors</span><span class="s0">, </span><span class="s1">kneighbors_graph</span>
<span class="s0">from </span><span class="s1">sklearn.utils </span><span class="s0">import </span><span class="s1">check_random_state</span>
<span class="s0">from </span><span class="s1">sklearn.utils._testing </span><span class="s0">import </span><span class="s1">(</span>
    <span class="s1">assert_almost_equal</span><span class="s0">,</span>
    <span class="s1">assert_array_almost_equal</span><span class="s0">,</span>
    <span class="s1">assert_array_equal</span><span class="s0">,</span>
    <span class="s1">ignore_warnings</span><span class="s0">,</span>
    <span class="s1">skip_if_32bit</span><span class="s0">,</span>
<span class="s1">)</span>

<span class="s1">x = np.linspace(</span><span class="s3">0</span><span class="s0">, </span><span class="s3">1</span><span class="s0">, </span><span class="s3">10</span><span class="s1">)</span>
<span class="s1">xx</span><span class="s0">, </span><span class="s1">yy = np.meshgrid(x</span><span class="s0">, </span><span class="s1">x)</span>
<span class="s1">X_2d_grid = np.hstack(</span>
    <span class="s1">[</span>
        <span class="s1">xx.ravel().reshape(-</span><span class="s3">1</span><span class="s0">, </span><span class="s3">1</span><span class="s1">)</span><span class="s0">,</span>
        <span class="s1">yy.ravel().reshape(-</span><span class="s3">1</span><span class="s0">, </span><span class="s3">1</span><span class="s1">)</span><span class="s0">,</span>
    <span class="s1">]</span>
<span class="s1">)</span>


<span class="s0">def </span><span class="s1">test_gradient_descent_stops():</span>
    <span class="s2"># Test stopping conditions of gradient descent.</span>
    <span class="s0">class </span><span class="s1">ObjectiveSmallGradient:</span>
        <span class="s0">def </span><span class="s1">__init__(self):</span>
            <span class="s1">self.it = -</span><span class="s3">1</span>

        <span class="s0">def </span><span class="s1">__call__(self</span><span class="s0">, </span><span class="s1">_</span><span class="s0">, </span><span class="s1">compute_error=</span><span class="s0">True</span><span class="s1">):</span>
            <span class="s1">self.it += </span><span class="s3">1</span>
            <span class="s0">return </span><span class="s1">(</span><span class="s3">10 </span><span class="s1">- self.it) / </span><span class="s3">10.0</span><span class="s0">, </span><span class="s1">np.array([</span><span class="s3">1e-5</span><span class="s1">])</span>

    <span class="s0">def </span><span class="s1">flat_function(_</span><span class="s0">, </span><span class="s1">compute_error=</span><span class="s0">True</span><span class="s1">):</span>
        <span class="s0">return </span><span class="s3">0.0</span><span class="s0">, </span><span class="s1">np.ones(</span><span class="s3">1</span><span class="s1">)</span>

    <span class="s2"># Gradient norm</span>
    <span class="s1">old_stdout = sys.stdout</span>
    <span class="s1">sys.stdout = StringIO()</span>
    <span class="s0">try</span><span class="s1">:</span>
        <span class="s1">_</span><span class="s0">, </span><span class="s1">error</span><span class="s0">, </span><span class="s1">it = _gradient_descent(</span>
            <span class="s1">ObjectiveSmallGradient()</span><span class="s0">,</span>
            <span class="s1">np.zeros(</span><span class="s3">1</span><span class="s1">)</span><span class="s0">,</span>
            <span class="s3">0</span><span class="s0">,</span>
            <span class="s1">n_iter=</span><span class="s3">100</span><span class="s0">,</span>
            <span class="s1">n_iter_without_progress=</span><span class="s3">100</span><span class="s0">,</span>
            <span class="s1">momentum=</span><span class="s3">0.0</span><span class="s0">,</span>
            <span class="s1">learning_rate=</span><span class="s3">0.0</span><span class="s0">,</span>
            <span class="s1">min_gain=</span><span class="s3">0.0</span><span class="s0">,</span>
            <span class="s1">min_grad_norm=</span><span class="s3">1e-5</span><span class="s0">,</span>
            <span class="s1">verbose=</span><span class="s3">2</span><span class="s0">,</span>
        <span class="s1">)</span>
    <span class="s0">finally</span><span class="s1">:</span>
        <span class="s1">out = sys.stdout.getvalue()</span>
        <span class="s1">sys.stdout.close()</span>
        <span class="s1">sys.stdout = old_stdout</span>
    <span class="s0">assert </span><span class="s1">error == </span><span class="s3">1.0</span>
    <span class="s0">assert </span><span class="s1">it == </span><span class="s3">0</span>
    <span class="s0">assert </span><span class="s4">&quot;gradient norm&quot; </span><span class="s0">in </span><span class="s1">out</span>

    <span class="s2"># Maximum number of iterations without improvement</span>
    <span class="s1">old_stdout = sys.stdout</span>
    <span class="s1">sys.stdout = StringIO()</span>
    <span class="s0">try</span><span class="s1">:</span>
        <span class="s1">_</span><span class="s0">, </span><span class="s1">error</span><span class="s0">, </span><span class="s1">it = _gradient_descent(</span>
            <span class="s1">flat_function</span><span class="s0">,</span>
            <span class="s1">np.zeros(</span><span class="s3">1</span><span class="s1">)</span><span class="s0">,</span>
            <span class="s3">0</span><span class="s0">,</span>
            <span class="s1">n_iter=</span><span class="s3">100</span><span class="s0">,</span>
            <span class="s1">n_iter_without_progress=</span><span class="s3">10</span><span class="s0">,</span>
            <span class="s1">momentum=</span><span class="s3">0.0</span><span class="s0">,</span>
            <span class="s1">learning_rate=</span><span class="s3">0.0</span><span class="s0">,</span>
            <span class="s1">min_gain=</span><span class="s3">0.0</span><span class="s0">,</span>
            <span class="s1">min_grad_norm=</span><span class="s3">0.0</span><span class="s0">,</span>
            <span class="s1">verbose=</span><span class="s3">2</span><span class="s0">,</span>
        <span class="s1">)</span>
    <span class="s0">finally</span><span class="s1">:</span>
        <span class="s1">out = sys.stdout.getvalue()</span>
        <span class="s1">sys.stdout.close()</span>
        <span class="s1">sys.stdout = old_stdout</span>
    <span class="s0">assert </span><span class="s1">error == </span><span class="s3">0.0</span>
    <span class="s0">assert </span><span class="s1">it == </span><span class="s3">11</span>
    <span class="s0">assert </span><span class="s4">&quot;did not make any progress&quot; </span><span class="s0">in </span><span class="s1">out</span>

    <span class="s2"># Maximum number of iterations</span>
    <span class="s1">old_stdout = sys.stdout</span>
    <span class="s1">sys.stdout = StringIO()</span>
    <span class="s0">try</span><span class="s1">:</span>
        <span class="s1">_</span><span class="s0">, </span><span class="s1">error</span><span class="s0">, </span><span class="s1">it = _gradient_descent(</span>
            <span class="s1">ObjectiveSmallGradient()</span><span class="s0">,</span>
            <span class="s1">np.zeros(</span><span class="s3">1</span><span class="s1">)</span><span class="s0">,</span>
            <span class="s3">0</span><span class="s0">,</span>
            <span class="s1">n_iter=</span><span class="s3">11</span><span class="s0">,</span>
            <span class="s1">n_iter_without_progress=</span><span class="s3">100</span><span class="s0">,</span>
            <span class="s1">momentum=</span><span class="s3">0.0</span><span class="s0">,</span>
            <span class="s1">learning_rate=</span><span class="s3">0.0</span><span class="s0">,</span>
            <span class="s1">min_gain=</span><span class="s3">0.0</span><span class="s0">,</span>
            <span class="s1">min_grad_norm=</span><span class="s3">0.0</span><span class="s0">,</span>
            <span class="s1">verbose=</span><span class="s3">2</span><span class="s0">,</span>
        <span class="s1">)</span>
    <span class="s0">finally</span><span class="s1">:</span>
        <span class="s1">out = sys.stdout.getvalue()</span>
        <span class="s1">sys.stdout.close()</span>
        <span class="s1">sys.stdout = old_stdout</span>
    <span class="s0">assert </span><span class="s1">error == </span><span class="s3">0.0</span>
    <span class="s0">assert </span><span class="s1">it == </span><span class="s3">10</span>
    <span class="s0">assert </span><span class="s4">&quot;Iteration 10&quot; </span><span class="s0">in </span><span class="s1">out</span>


<span class="s0">def </span><span class="s1">test_binary_search():</span>
    <span class="s2"># Test if the binary search finds Gaussians with desired perplexity.</span>
    <span class="s1">random_state = check_random_state(</span><span class="s3">0</span><span class="s1">)</span>
    <span class="s1">data = random_state.randn(</span><span class="s3">50</span><span class="s0">, </span><span class="s3">5</span><span class="s1">)</span>
    <span class="s1">distances = pairwise_distances(data).astype(np.float32)</span>
    <span class="s1">desired_perplexity = </span><span class="s3">25.0</span>
    <span class="s1">P = _binary_search_perplexity(distances</span><span class="s0">, </span><span class="s1">desired_perplexity</span><span class="s0">, </span><span class="s1">verbose=</span><span class="s3">0</span><span class="s1">)</span>
    <span class="s1">P = np.maximum(P</span><span class="s0">, </span><span class="s1">np.finfo(np.double).eps)</span>
    <span class="s1">mean_perplexity = np.mean(</span>
        <span class="s1">[np.exp(-np.sum(P[i] * np.log(P[i]))) </span><span class="s0">for </span><span class="s1">i </span><span class="s0">in </span><span class="s1">range(P.shape[</span><span class="s3">0</span><span class="s1">])]</span>
    <span class="s1">)</span>
    <span class="s1">assert_almost_equal(mean_perplexity</span><span class="s0">, </span><span class="s1">desired_perplexity</span><span class="s0">, </span><span class="s1">decimal=</span><span class="s3">3</span><span class="s1">)</span>


<span class="s0">def </span><span class="s1">test_binary_search_underflow():</span>
    <span class="s2"># Test if the binary search finds Gaussians with desired perplexity.</span>
    <span class="s2"># A more challenging case than the one above, producing numeric</span>
    <span class="s2"># underflow in float precision (see issue #19471 and PR #19472).</span>
    <span class="s1">random_state = check_random_state(</span><span class="s3">42</span><span class="s1">)</span>
    <span class="s1">data = random_state.randn(</span><span class="s3">1</span><span class="s0">, </span><span class="s3">90</span><span class="s1">).astype(np.float32) + </span><span class="s3">100</span>
    <span class="s1">desired_perplexity = </span><span class="s3">30.0</span>
    <span class="s1">P = _binary_search_perplexity(data</span><span class="s0">, </span><span class="s1">desired_perplexity</span><span class="s0">, </span><span class="s1">verbose=</span><span class="s3">0</span><span class="s1">)</span>
    <span class="s1">perplexity = </span><span class="s3">2 </span><span class="s1">** -np.nansum(P[</span><span class="s3">0</span><span class="s0">, </span><span class="s3">1</span><span class="s1">:] * np.log2(P[</span><span class="s3">0</span><span class="s0">, </span><span class="s3">1</span><span class="s1">:]))</span>
    <span class="s1">assert_almost_equal(perplexity</span><span class="s0">, </span><span class="s1">desired_perplexity</span><span class="s0">, </span><span class="s1">decimal=</span><span class="s3">3</span><span class="s1">)</span>


<span class="s0">def </span><span class="s1">test_binary_search_neighbors():</span>
    <span class="s2"># Binary perplexity search approximation.</span>
    <span class="s2"># Should be approximately equal to the slow method when we use</span>
    <span class="s2"># all points as neighbors.</span>
    <span class="s1">n_samples = </span><span class="s3">200</span>
    <span class="s1">desired_perplexity = </span><span class="s3">25.0</span>
    <span class="s1">random_state = check_random_state(</span><span class="s3">0</span><span class="s1">)</span>
    <span class="s1">data = random_state.randn(n_samples</span><span class="s0">, </span><span class="s3">2</span><span class="s1">).astype(np.float32</span><span class="s0">, </span><span class="s1">copy=</span><span class="s0">False</span><span class="s1">)</span>
    <span class="s1">distances = pairwise_distances(data)</span>
    <span class="s1">P1 = _binary_search_perplexity(distances</span><span class="s0">, </span><span class="s1">desired_perplexity</span><span class="s0">, </span><span class="s1">verbose=</span><span class="s3">0</span><span class="s1">)</span>

    <span class="s2"># Test that when we use all the neighbors the results are identical</span>
    <span class="s1">n_neighbors = n_samples - </span><span class="s3">1</span>
    <span class="s1">nn = NearestNeighbors().fit(data)</span>
    <span class="s1">distance_graph = nn.kneighbors_graph(n_neighbors=n_neighbors</span><span class="s0">, </span><span class="s1">mode=</span><span class="s4">&quot;distance&quot;</span><span class="s1">)</span>
    <span class="s1">distances_nn = distance_graph.data.astype(np.float32</span><span class="s0">, </span><span class="s1">copy=</span><span class="s0">False</span><span class="s1">)</span>
    <span class="s1">distances_nn = distances_nn.reshape(n_samples</span><span class="s0">, </span><span class="s1">n_neighbors)</span>
    <span class="s1">P2 = _binary_search_perplexity(distances_nn</span><span class="s0">, </span><span class="s1">desired_perplexity</span><span class="s0">, </span><span class="s1">verbose=</span><span class="s3">0</span><span class="s1">)</span>

    <span class="s1">indptr = distance_graph.indptr</span>
    <span class="s1">P1_nn = np.array(</span>
        <span class="s1">[</span>
            <span class="s1">P1[k</span><span class="s0">, </span><span class="s1">distance_graph.indices[indptr[k] : indptr[k + </span><span class="s3">1</span><span class="s1">]]]</span>
            <span class="s0">for </span><span class="s1">k </span><span class="s0">in </span><span class="s1">range(n_samples)</span>
        <span class="s1">]</span>
    <span class="s1">)</span>
    <span class="s1">assert_array_almost_equal(P1_nn</span><span class="s0">, </span><span class="s1">P2</span><span class="s0">, </span><span class="s1">decimal=</span><span class="s3">4</span><span class="s1">)</span>

    <span class="s2"># Test that the highest P_ij are the same when fewer neighbors are used</span>
    <span class="s0">for </span><span class="s1">k </span><span class="s0">in </span><span class="s1">np.linspace(</span><span class="s3">150</span><span class="s0">, </span><span class="s1">n_samples - </span><span class="s3">1</span><span class="s0">, </span><span class="s3">5</span><span class="s1">):</span>
        <span class="s1">k = int(k)</span>
        <span class="s1">topn = k * </span><span class="s3">10  </span><span class="s2"># check the top 10 * k entries out of k * k entries</span>
        <span class="s1">distance_graph = nn.kneighbors_graph(n_neighbors=k</span><span class="s0">, </span><span class="s1">mode=</span><span class="s4">&quot;distance&quot;</span><span class="s1">)</span>
        <span class="s1">distances_nn = distance_graph.data.astype(np.float32</span><span class="s0">, </span><span class="s1">copy=</span><span class="s0">False</span><span class="s1">)</span>
        <span class="s1">distances_nn = distances_nn.reshape(n_samples</span><span class="s0">, </span><span class="s1">k)</span>
        <span class="s1">P2k = _binary_search_perplexity(distances_nn</span><span class="s0">, </span><span class="s1">desired_perplexity</span><span class="s0">, </span><span class="s1">verbose=</span><span class="s3">0</span><span class="s1">)</span>
        <span class="s1">assert_array_almost_equal(P1_nn</span><span class="s0">, </span><span class="s1">P2</span><span class="s0">, </span><span class="s1">decimal=</span><span class="s3">2</span><span class="s1">)</span>
        <span class="s1">idx = np.argsort(P1.ravel())[::-</span><span class="s3">1</span><span class="s1">]</span>
        <span class="s1">P1top = P1.ravel()[idx][:topn]</span>
        <span class="s1">idx = np.argsort(P2k.ravel())[::-</span><span class="s3">1</span><span class="s1">]</span>
        <span class="s1">P2top = P2k.ravel()[idx][:topn]</span>
        <span class="s1">assert_array_almost_equal(P1top</span><span class="s0">, </span><span class="s1">P2top</span><span class="s0">, </span><span class="s1">decimal=</span><span class="s3">2</span><span class="s1">)</span>


<span class="s0">def </span><span class="s1">test_binary_perplexity_stability():</span>
    <span class="s2"># Binary perplexity search should be stable.</span>
    <span class="s2"># The binary_search_perplexity had a bug wherein the P array</span>
    <span class="s2"># was uninitialized, leading to sporadically failing tests.</span>
    <span class="s1">n_neighbors = </span><span class="s3">10</span>
    <span class="s1">n_samples = </span><span class="s3">100</span>
    <span class="s1">random_state = check_random_state(</span><span class="s3">0</span><span class="s1">)</span>
    <span class="s1">data = random_state.randn(n_samples</span><span class="s0">, </span><span class="s3">5</span><span class="s1">)</span>
    <span class="s1">nn = NearestNeighbors().fit(data)</span>
    <span class="s1">distance_graph = nn.kneighbors_graph(n_neighbors=n_neighbors</span><span class="s0">, </span><span class="s1">mode=</span><span class="s4">&quot;distance&quot;</span><span class="s1">)</span>
    <span class="s1">distances = distance_graph.data.astype(np.float32</span><span class="s0">, </span><span class="s1">copy=</span><span class="s0">False</span><span class="s1">)</span>
    <span class="s1">distances = distances.reshape(n_samples</span><span class="s0">, </span><span class="s1">n_neighbors)</span>
    <span class="s1">last_P = </span><span class="s0">None</span>
    <span class="s1">desired_perplexity = </span><span class="s3">3</span>
    <span class="s0">for </span><span class="s1">_ </span><span class="s0">in </span><span class="s1">range(</span><span class="s3">100</span><span class="s1">):</span>
        <span class="s1">P = _binary_search_perplexity(distances.copy()</span><span class="s0">, </span><span class="s1">desired_perplexity</span><span class="s0">, </span><span class="s1">verbose=</span><span class="s3">0</span><span class="s1">)</span>
        <span class="s1">P1 = _joint_probabilities_nn(distance_graph</span><span class="s0">, </span><span class="s1">desired_perplexity</span><span class="s0">, </span><span class="s1">verbose=</span><span class="s3">0</span><span class="s1">)</span>
        <span class="s2"># Convert the sparse matrix to a dense one for testing</span>
        <span class="s1">P1 = P1.toarray()</span>
        <span class="s0">if </span><span class="s1">last_P </span><span class="s0">is None</span><span class="s1">:</span>
            <span class="s1">last_P = P</span>
            <span class="s1">last_P1 = P1</span>
        <span class="s0">else</span><span class="s1">:</span>
            <span class="s1">assert_array_almost_equal(P</span><span class="s0">, </span><span class="s1">last_P</span><span class="s0">, </span><span class="s1">decimal=</span><span class="s3">4</span><span class="s1">)</span>
            <span class="s1">assert_array_almost_equal(P1</span><span class="s0">, </span><span class="s1">last_P1</span><span class="s0">, </span><span class="s1">decimal=</span><span class="s3">4</span><span class="s1">)</span>


<span class="s0">def </span><span class="s1">test_gradient():</span>
    <span class="s2"># Test gradient of Kullback-Leibler divergence.</span>
    <span class="s1">random_state = check_random_state(</span><span class="s3">0</span><span class="s1">)</span>

    <span class="s1">n_samples = </span><span class="s3">50</span>
    <span class="s1">n_features = </span><span class="s3">2</span>
    <span class="s1">n_components = </span><span class="s3">2</span>
    <span class="s1">alpha = </span><span class="s3">1.0</span>

    <span class="s1">distances = random_state.randn(n_samples</span><span class="s0">, </span><span class="s1">n_features).astype(np.float32)</span>
    <span class="s1">distances = np.abs(distances.dot(distances.T))</span>
    <span class="s1">np.fill_diagonal(distances</span><span class="s0">, </span><span class="s3">0.0</span><span class="s1">)</span>
    <span class="s1">X_embedded = random_state.randn(n_samples</span><span class="s0">, </span><span class="s1">n_components).astype(np.float32)</span>

    <span class="s1">P = _joint_probabilities(distances</span><span class="s0">, </span><span class="s1">desired_perplexity=</span><span class="s3">25.0</span><span class="s0">, </span><span class="s1">verbose=</span><span class="s3">0</span><span class="s1">)</span>

    <span class="s0">def </span><span class="s1">fun(params):</span>
        <span class="s0">return </span><span class="s1">_kl_divergence(params</span><span class="s0">, </span><span class="s1">P</span><span class="s0">, </span><span class="s1">alpha</span><span class="s0">, </span><span class="s1">n_samples</span><span class="s0">, </span><span class="s1">n_components)[</span><span class="s3">0</span><span class="s1">]</span>

    <span class="s0">def </span><span class="s1">grad(params):</span>
        <span class="s0">return </span><span class="s1">_kl_divergence(params</span><span class="s0">, </span><span class="s1">P</span><span class="s0">, </span><span class="s1">alpha</span><span class="s0">, </span><span class="s1">n_samples</span><span class="s0">, </span><span class="s1">n_components)[</span><span class="s3">1</span><span class="s1">]</span>

    <span class="s1">assert_almost_equal(check_grad(fun</span><span class="s0">, </span><span class="s1">grad</span><span class="s0">, </span><span class="s1">X_embedded.ravel())</span><span class="s0">, </span><span class="s3">0.0</span><span class="s0">, </span><span class="s1">decimal=</span><span class="s3">5</span><span class="s1">)</span>


<span class="s0">def </span><span class="s1">test_trustworthiness():</span>
    <span class="s2"># Test trustworthiness score.</span>
    <span class="s1">random_state = check_random_state(</span><span class="s3">0</span><span class="s1">)</span>

    <span class="s2"># Affine transformation</span>
    <span class="s1">X = random_state.randn(</span><span class="s3">100</span><span class="s0">, </span><span class="s3">2</span><span class="s1">)</span>
    <span class="s0">assert </span><span class="s1">trustworthiness(X</span><span class="s0">, </span><span class="s3">5.0 </span><span class="s1">+ X / </span><span class="s3">10.0</span><span class="s1">) == </span><span class="s3">1.0</span>

    <span class="s2"># Randomly shuffled</span>
    <span class="s1">X = np.arange(</span><span class="s3">100</span><span class="s1">).reshape(-</span><span class="s3">1</span><span class="s0">, </span><span class="s3">1</span><span class="s1">)</span>
    <span class="s1">X_embedded = X.copy()</span>
    <span class="s1">random_state.shuffle(X_embedded)</span>
    <span class="s0">assert </span><span class="s1">trustworthiness(X</span><span class="s0">, </span><span class="s1">X_embedded) &lt; </span><span class="s3">0.6</span>

    <span class="s2"># Completely different</span>
    <span class="s1">X = np.arange(</span><span class="s3">5</span><span class="s1">).reshape(-</span><span class="s3">1</span><span class="s0">, </span><span class="s3">1</span><span class="s1">)</span>
    <span class="s1">X_embedded = np.array([[</span><span class="s3">0</span><span class="s1">]</span><span class="s0">, </span><span class="s1">[</span><span class="s3">2</span><span class="s1">]</span><span class="s0">, </span><span class="s1">[</span><span class="s3">4</span><span class="s1">]</span><span class="s0">, </span><span class="s1">[</span><span class="s3">1</span><span class="s1">]</span><span class="s0">, </span><span class="s1">[</span><span class="s3">3</span><span class="s1">]])</span>
    <span class="s1">assert_almost_equal(trustworthiness(X</span><span class="s0">, </span><span class="s1">X_embedded</span><span class="s0">, </span><span class="s1">n_neighbors=</span><span class="s3">1</span><span class="s1">)</span><span class="s0">, </span><span class="s3">0.2</span><span class="s1">)</span>


<span class="s0">def </span><span class="s1">test_trustworthiness_n_neighbors_error():</span>
    <span class="s5">&quot;&quot;&quot;Raise an error when n_neighbors &gt;= n_samples / 2. 
 
    Non-regression test for #18567. 
    &quot;&quot;&quot;</span>
    <span class="s1">regex = </span><span class="s4">&quot;n_neighbors .+ should be less than .+&quot;</span>
    <span class="s1">rng = np.random.RandomState(</span><span class="s3">42</span><span class="s1">)</span>
    <span class="s1">X = rng.rand(</span><span class="s3">7</span><span class="s0">, </span><span class="s3">4</span><span class="s1">)</span>
    <span class="s1">X_embedded = rng.rand(</span><span class="s3">7</span><span class="s0">, </span><span class="s3">2</span><span class="s1">)</span>
    <span class="s0">with </span><span class="s1">pytest.raises(ValueError</span><span class="s0">, </span><span class="s1">match=regex):</span>
        <span class="s1">trustworthiness(X</span><span class="s0">, </span><span class="s1">X_embedded</span><span class="s0">, </span><span class="s1">n_neighbors=</span><span class="s3">5</span><span class="s1">)</span>

    <span class="s1">trust = trustworthiness(X</span><span class="s0">, </span><span class="s1">X_embedded</span><span class="s0">, </span><span class="s1">n_neighbors=</span><span class="s3">3</span><span class="s1">)</span>
    <span class="s0">assert </span><span class="s3">0 </span><span class="s1">&lt;= trust &lt;= </span><span class="s3">1</span>


<span class="s1">@pytest.mark.parametrize(</span><span class="s4">&quot;method&quot;</span><span class="s0">, </span><span class="s1">[</span><span class="s4">&quot;exact&quot;</span><span class="s0">, </span><span class="s4">&quot;barnes_hut&quot;</span><span class="s1">])</span>
<span class="s1">@pytest.mark.parametrize(</span><span class="s4">&quot;init&quot;</span><span class="s0">, </span><span class="s1">(</span><span class="s4">&quot;random&quot;</span><span class="s0">, </span><span class="s4">&quot;pca&quot;</span><span class="s1">))</span>
<span class="s0">def </span><span class="s1">test_preserve_trustworthiness_approximately(method</span><span class="s0">, </span><span class="s1">init):</span>
    <span class="s2"># Nearest neighbors should be preserved approximately.</span>
    <span class="s1">random_state = check_random_state(</span><span class="s3">0</span><span class="s1">)</span>
    <span class="s1">n_components = </span><span class="s3">2</span>
    <span class="s1">X = random_state.randn(</span><span class="s3">50</span><span class="s0">, </span><span class="s1">n_components).astype(np.float32)</span>
    <span class="s1">tsne = TSNE(</span>
        <span class="s1">n_components=n_components</span><span class="s0">,</span>
        <span class="s1">init=init</span><span class="s0">,</span>
        <span class="s1">random_state=</span><span class="s3">0</span><span class="s0">,</span>
        <span class="s1">method=method</span><span class="s0">,</span>
        <span class="s1">n_iter=</span><span class="s3">700</span><span class="s0">,</span>
        <span class="s1">learning_rate=</span><span class="s4">&quot;auto&quot;</span><span class="s0">,</span>
    <span class="s1">)</span>
    <span class="s1">X_embedded = tsne.fit_transform(X)</span>
    <span class="s1">t = trustworthiness(X</span><span class="s0">, </span><span class="s1">X_embedded</span><span class="s0">, </span><span class="s1">n_neighbors=</span><span class="s3">1</span><span class="s1">)</span>
    <span class="s0">assert </span><span class="s1">t &gt; </span><span class="s3">0.85</span>


<span class="s0">def </span><span class="s1">test_optimization_minimizes_kl_divergence():</span>
    <span class="s5">&quot;&quot;&quot;t-SNE should give a lower KL divergence with more iterations.&quot;&quot;&quot;</span>
    <span class="s1">random_state = check_random_state(</span><span class="s3">0</span><span class="s1">)</span>
    <span class="s1">X</span><span class="s0">, </span><span class="s1">_ = make_blobs(n_features=</span><span class="s3">3</span><span class="s0">, </span><span class="s1">random_state=random_state)</span>
    <span class="s1">kl_divergences = []</span>
    <span class="s0">for </span><span class="s1">n_iter </span><span class="s0">in </span><span class="s1">[</span><span class="s3">250</span><span class="s0">, </span><span class="s3">300</span><span class="s0">, </span><span class="s3">350</span><span class="s1">]:</span>
        <span class="s1">tsne = TSNE(</span>
            <span class="s1">n_components=</span><span class="s3">2</span><span class="s0">,</span>
            <span class="s1">init=</span><span class="s4">&quot;random&quot;</span><span class="s0">,</span>
            <span class="s1">perplexity=</span><span class="s3">10</span><span class="s0">,</span>
            <span class="s1">learning_rate=</span><span class="s3">100.0</span><span class="s0">,</span>
            <span class="s1">n_iter=n_iter</span><span class="s0">,</span>
            <span class="s1">random_state=</span><span class="s3">0</span><span class="s0">,</span>
        <span class="s1">)</span>
        <span class="s1">tsne.fit_transform(X)</span>
        <span class="s1">kl_divergences.append(tsne.kl_divergence_)</span>
    <span class="s0">assert </span><span class="s1">kl_divergences[</span><span class="s3">1</span><span class="s1">] &lt;= kl_divergences[</span><span class="s3">0</span><span class="s1">]</span>
    <span class="s0">assert </span><span class="s1">kl_divergences[</span><span class="s3">2</span><span class="s1">] &lt;= kl_divergences[</span><span class="s3">1</span><span class="s1">]</span>


<span class="s1">@pytest.mark.parametrize(</span><span class="s4">&quot;method&quot;</span><span class="s0">, </span><span class="s1">[</span><span class="s4">&quot;exact&quot;</span><span class="s0">, </span><span class="s4">&quot;barnes_hut&quot;</span><span class="s1">])</span>
<span class="s0">def </span><span class="s1">test_fit_transform_csr_matrix(method):</span>
    <span class="s2"># TODO: compare results on dense and sparse data as proposed in:</span>
    <span class="s2"># https://github.com/scikit-learn/scikit-learn/pull/23585#discussion_r968388186</span>
    <span class="s2"># X can be a sparse matrix.</span>
    <span class="s1">rng = check_random_state(</span><span class="s3">0</span><span class="s1">)</span>
    <span class="s1">X = rng.randn(</span><span class="s3">50</span><span class="s0">, </span><span class="s3">2</span><span class="s1">)</span>
    <span class="s1">X[(rng.randint(</span><span class="s3">0</span><span class="s0">, </span><span class="s3">50</span><span class="s0">, </span><span class="s3">25</span><span class="s1">)</span><span class="s0">, </span><span class="s1">rng.randint(</span><span class="s3">0</span><span class="s0">, </span><span class="s3">2</span><span class="s0">, </span><span class="s3">25</span><span class="s1">))] = </span><span class="s3">0.0</span>
    <span class="s1">X_csr = sp.csr_matrix(X)</span>
    <span class="s1">tsne = TSNE(</span>
        <span class="s1">n_components=</span><span class="s3">2</span><span class="s0">,</span>
        <span class="s1">init=</span><span class="s4">&quot;random&quot;</span><span class="s0">,</span>
        <span class="s1">perplexity=</span><span class="s3">10</span><span class="s0">,</span>
        <span class="s1">learning_rate=</span><span class="s3">100.0</span><span class="s0">,</span>
        <span class="s1">random_state=</span><span class="s3">0</span><span class="s0">,</span>
        <span class="s1">method=method</span><span class="s0">,</span>
        <span class="s1">n_iter=</span><span class="s3">750</span><span class="s0">,</span>
    <span class="s1">)</span>
    <span class="s1">X_embedded = tsne.fit_transform(X_csr)</span>
    <span class="s1">assert_allclose(trustworthiness(X_csr</span><span class="s0">, </span><span class="s1">X_embedded</span><span class="s0">, </span><span class="s1">n_neighbors=</span><span class="s3">1</span><span class="s1">)</span><span class="s0">, </span><span class="s3">1.0</span><span class="s0">, </span><span class="s1">rtol=</span><span class="s3">1.1e-1</span><span class="s1">)</span>


<span class="s0">def </span><span class="s1">test_preserve_trustworthiness_approximately_with_precomputed_distances():</span>
    <span class="s2"># Nearest neighbors should be preserved approximately.</span>
    <span class="s1">random_state = check_random_state(</span><span class="s3">0</span><span class="s1">)</span>
    <span class="s0">for </span><span class="s1">i </span><span class="s0">in </span><span class="s1">range(</span><span class="s3">3</span><span class="s1">):</span>
        <span class="s1">X = random_state.randn(</span><span class="s3">80</span><span class="s0">, </span><span class="s3">2</span><span class="s1">)</span>
        <span class="s1">D = squareform(pdist(X)</span><span class="s0">, </span><span class="s4">&quot;sqeuclidean&quot;</span><span class="s1">)</span>
        <span class="s1">tsne = TSNE(</span>
            <span class="s1">n_components=</span><span class="s3">2</span><span class="s0">,</span>
            <span class="s1">perplexity=</span><span class="s3">2</span><span class="s0">,</span>
            <span class="s1">learning_rate=</span><span class="s3">100.0</span><span class="s0">,</span>
            <span class="s1">early_exaggeration=</span><span class="s3">2.0</span><span class="s0">,</span>
            <span class="s1">metric=</span><span class="s4">&quot;precomputed&quot;</span><span class="s0">,</span>
            <span class="s1">random_state=i</span><span class="s0">,</span>
            <span class="s1">verbose=</span><span class="s3">0</span><span class="s0">,</span>
            <span class="s1">n_iter=</span><span class="s3">500</span><span class="s0">,</span>
            <span class="s1">init=</span><span class="s4">&quot;random&quot;</span><span class="s0">,</span>
        <span class="s1">)</span>
        <span class="s1">X_embedded = tsne.fit_transform(D)</span>
        <span class="s1">t = trustworthiness(D</span><span class="s0">, </span><span class="s1">X_embedded</span><span class="s0">, </span><span class="s1">n_neighbors=</span><span class="s3">1</span><span class="s0">, </span><span class="s1">metric=</span><span class="s4">&quot;precomputed&quot;</span><span class="s1">)</span>
        <span class="s0">assert </span><span class="s1">t &gt; </span><span class="s3">0.95</span>


<span class="s0">def </span><span class="s1">test_trustworthiness_not_euclidean_metric():</span>
    <span class="s2"># Test trustworthiness with a metric different from 'euclidean' and</span>
    <span class="s2"># 'precomputed'</span>
    <span class="s1">random_state = check_random_state(</span><span class="s3">0</span><span class="s1">)</span>
    <span class="s1">X = random_state.randn(</span><span class="s3">100</span><span class="s0">, </span><span class="s3">2</span><span class="s1">)</span>
    <span class="s0">assert </span><span class="s1">trustworthiness(X</span><span class="s0">, </span><span class="s1">X</span><span class="s0">, </span><span class="s1">metric=</span><span class="s4">&quot;cosine&quot;</span><span class="s1">) == trustworthiness(</span>
        <span class="s1">pairwise_distances(X</span><span class="s0">, </span><span class="s1">metric=</span><span class="s4">&quot;cosine&quot;</span><span class="s1">)</span><span class="s0">, </span><span class="s1">X</span><span class="s0">, </span><span class="s1">metric=</span><span class="s4">&quot;precomputed&quot;</span>
    <span class="s1">)</span>


<span class="s1">@pytest.mark.parametrize(</span>
    <span class="s4">&quot;method, retype&quot;</span><span class="s0">,</span>
    <span class="s1">[</span>
        <span class="s1">(</span><span class="s4">&quot;exact&quot;</span><span class="s0">, </span><span class="s1">np.asarray)</span><span class="s0">,</span>
        <span class="s1">(</span><span class="s4">&quot;barnes_hut&quot;</span><span class="s0">, </span><span class="s1">np.asarray)</span><span class="s0">,</span>
        <span class="s1">(</span><span class="s4">&quot;barnes_hut&quot;</span><span class="s0">, </span><span class="s1">sp.csr_matrix)</span><span class="s0">,</span>
    <span class="s1">]</span><span class="s0">,</span>
<span class="s1">)</span>
<span class="s1">@pytest.mark.parametrize(</span>
    <span class="s4">&quot;D, message_regex&quot;</span><span class="s0">,</span>
    <span class="s1">[</span>
        <span class="s1">([[</span><span class="s3">0.0</span><span class="s1">]</span><span class="s0">, </span><span class="s1">[</span><span class="s3">1.0</span><span class="s1">]]</span><span class="s0">, </span><span class="s4">&quot;.* square distance matrix&quot;</span><span class="s1">)</span><span class="s0">,</span>
        <span class="s1">([[</span><span class="s3">0.0</span><span class="s0">, </span><span class="s1">-</span><span class="s3">1.0</span><span class="s1">]</span><span class="s0">, </span><span class="s1">[</span><span class="s3">1.0</span><span class="s0">, </span><span class="s3">0.0</span><span class="s1">]]</span><span class="s0">, </span><span class="s4">&quot;.* positive.*&quot;</span><span class="s1">)</span><span class="s0">,</span>
    <span class="s1">]</span><span class="s0">,</span>
<span class="s1">)</span>
<span class="s0">def </span><span class="s1">test_bad_precomputed_distances(method</span><span class="s0">, </span><span class="s1">D</span><span class="s0">, </span><span class="s1">retype</span><span class="s0">, </span><span class="s1">message_regex):</span>
    <span class="s1">tsne = TSNE(</span>
        <span class="s1">metric=</span><span class="s4">&quot;precomputed&quot;</span><span class="s0">,</span>
        <span class="s1">method=method</span><span class="s0">,</span>
        <span class="s1">init=</span><span class="s4">&quot;random&quot;</span><span class="s0">,</span>
        <span class="s1">random_state=</span><span class="s3">42</span><span class="s0">,</span>
        <span class="s1">perplexity=</span><span class="s3">1</span><span class="s0">,</span>
    <span class="s1">)</span>
    <span class="s0">with </span><span class="s1">pytest.raises(ValueError</span><span class="s0">, </span><span class="s1">match=message_regex):</span>
        <span class="s1">tsne.fit_transform(retype(D))</span>


<span class="s0">def </span><span class="s1">test_exact_no_precomputed_sparse():</span>
    <span class="s1">tsne = TSNE(</span>
        <span class="s1">metric=</span><span class="s4">&quot;precomputed&quot;</span><span class="s0">,</span>
        <span class="s1">method=</span><span class="s4">&quot;exact&quot;</span><span class="s0">,</span>
        <span class="s1">init=</span><span class="s4">&quot;random&quot;</span><span class="s0">,</span>
        <span class="s1">random_state=</span><span class="s3">42</span><span class="s0">,</span>
        <span class="s1">perplexity=</span><span class="s3">1</span><span class="s0">,</span>
    <span class="s1">)</span>
    <span class="s0">with </span><span class="s1">pytest.raises(TypeError</span><span class="s0">, </span><span class="s1">match=</span><span class="s4">&quot;sparse&quot;</span><span class="s1">):</span>
        <span class="s1">tsne.fit_transform(sp.csr_matrix([[</span><span class="s3">0</span><span class="s0">, </span><span class="s3">5</span><span class="s1">]</span><span class="s0">, </span><span class="s1">[</span><span class="s3">5</span><span class="s0">, </span><span class="s3">0</span><span class="s1">]]))</span>


<span class="s0">def </span><span class="s1">test_high_perplexity_precomputed_sparse_distances():</span>
    <span class="s2"># Perplexity should be less than 50</span>
    <span class="s1">dist = np.array([[</span><span class="s3">1.0</span><span class="s0">, </span><span class="s3">0.0</span><span class="s0">, </span><span class="s3">0.0</span><span class="s1">]</span><span class="s0">, </span><span class="s1">[</span><span class="s3">0.0</span><span class="s0">, </span><span class="s3">1.0</span><span class="s0">, </span><span class="s3">0.0</span><span class="s1">]</span><span class="s0">, </span><span class="s1">[</span><span class="s3">1.0</span><span class="s0">, </span><span class="s3">0.0</span><span class="s0">, </span><span class="s3">0.0</span><span class="s1">]])</span>
    <span class="s1">bad_dist = sp.csr_matrix(dist)</span>
    <span class="s1">tsne = TSNE(metric=</span><span class="s4">&quot;precomputed&quot;</span><span class="s0">, </span><span class="s1">init=</span><span class="s4">&quot;random&quot;</span><span class="s0">, </span><span class="s1">random_state=</span><span class="s3">42</span><span class="s0">, </span><span class="s1">perplexity=</span><span class="s3">1</span><span class="s1">)</span>
    <span class="s1">msg = </span><span class="s4">&quot;3 neighbors per samples are required, but some samples have only 1&quot;</span>
    <span class="s0">with </span><span class="s1">pytest.raises(ValueError</span><span class="s0">, </span><span class="s1">match=msg):</span>
        <span class="s1">tsne.fit_transform(bad_dist)</span>


<span class="s1">@ignore_warnings(category=EfficiencyWarning)</span>
<span class="s0">def </span><span class="s1">test_sparse_precomputed_distance():</span>
    <span class="s5">&quot;&quot;&quot;Make sure that TSNE works identically for sparse and dense matrix&quot;&quot;&quot;</span>
    <span class="s1">random_state = check_random_state(</span><span class="s3">0</span><span class="s1">)</span>
    <span class="s1">X = random_state.randn(</span><span class="s3">100</span><span class="s0">, </span><span class="s3">2</span><span class="s1">)</span>

    <span class="s1">D_sparse = kneighbors_graph(X</span><span class="s0">, </span><span class="s1">n_neighbors=</span><span class="s3">100</span><span class="s0">, </span><span class="s1">mode=</span><span class="s4">&quot;distance&quot;</span><span class="s0">, </span><span class="s1">include_self=</span><span class="s0">True</span><span class="s1">)</span>
    <span class="s1">D = pairwise_distances(X)</span>
    <span class="s0">assert </span><span class="s1">sp.issparse(D_sparse)</span>
    <span class="s1">assert_almost_equal(D_sparse.toarray()</span><span class="s0">, </span><span class="s1">D)</span>

    <span class="s1">tsne = TSNE(</span>
        <span class="s1">metric=</span><span class="s4">&quot;precomputed&quot;</span><span class="s0">, </span><span class="s1">random_state=</span><span class="s3">0</span><span class="s0">, </span><span class="s1">init=</span><span class="s4">&quot;random&quot;</span><span class="s0">, </span><span class="s1">learning_rate=</span><span class="s4">&quot;auto&quot;</span>
    <span class="s1">)</span>
    <span class="s1">Xt_dense = tsne.fit_transform(D)</span>

    <span class="s0">for </span><span class="s1">fmt </span><span class="s0">in </span><span class="s1">[</span><span class="s4">&quot;csr&quot;</span><span class="s0">, </span><span class="s4">&quot;lil&quot;</span><span class="s1">]:</span>
        <span class="s1">Xt_sparse = tsne.fit_transform(D_sparse.asformat(fmt))</span>
        <span class="s1">assert_almost_equal(Xt_dense</span><span class="s0">, </span><span class="s1">Xt_sparse)</span>


<span class="s0">def </span><span class="s1">test_non_positive_computed_distances():</span>
    <span class="s2"># Computed distance matrices must be positive.</span>
    <span class="s0">def </span><span class="s1">metric(x</span><span class="s0">, </span><span class="s1">y):</span>
        <span class="s0">return </span><span class="s1">-</span><span class="s3">1</span>

    <span class="s2"># Negative computed distances should be caught even if result is squared</span>
    <span class="s1">tsne = TSNE(metric=metric</span><span class="s0">, </span><span class="s1">method=</span><span class="s4">&quot;exact&quot;</span><span class="s0">, </span><span class="s1">perplexity=</span><span class="s3">1</span><span class="s1">)</span>
    <span class="s1">X = np.array([[</span><span class="s3">0.0</span><span class="s0">, </span><span class="s3">0.0</span><span class="s1">]</span><span class="s0">, </span><span class="s1">[</span><span class="s3">1.0</span><span class="s0">, </span><span class="s3">1.0</span><span class="s1">]])</span>
    <span class="s0">with </span><span class="s1">pytest.raises(ValueError</span><span class="s0">, </span><span class="s1">match=</span><span class="s4">&quot;All distances .*metric given.*&quot;</span><span class="s1">):</span>
        <span class="s1">tsne.fit_transform(X)</span>


<span class="s0">def </span><span class="s1">test_init_ndarray():</span>
    <span class="s2"># Initialize TSNE with ndarray and test fit</span>
    <span class="s1">tsne = TSNE(init=np.zeros((</span><span class="s3">100</span><span class="s0">, </span><span class="s3">2</span><span class="s1">))</span><span class="s0">, </span><span class="s1">learning_rate=</span><span class="s4">&quot;auto&quot;</span><span class="s1">)</span>
    <span class="s1">X_embedded = tsne.fit_transform(np.ones((</span><span class="s3">100</span><span class="s0">, </span><span class="s3">5</span><span class="s1">)))</span>
    <span class="s1">assert_array_equal(np.zeros((</span><span class="s3">100</span><span class="s0">, </span><span class="s3">2</span><span class="s1">))</span><span class="s0">, </span><span class="s1">X_embedded)</span>


<span class="s0">def </span><span class="s1">test_init_ndarray_precomputed():</span>
    <span class="s2"># Initialize TSNE with ndarray and metric 'precomputed'</span>
    <span class="s2"># Make sure no FutureWarning is thrown from _fit</span>
    <span class="s1">tsne = TSNE(</span>
        <span class="s1">init=np.zeros((</span><span class="s3">100</span><span class="s0">, </span><span class="s3">2</span><span class="s1">))</span><span class="s0">,</span>
        <span class="s1">metric=</span><span class="s4">&quot;precomputed&quot;</span><span class="s0">,</span>
        <span class="s1">learning_rate=</span><span class="s3">50.0</span><span class="s0">,</span>
    <span class="s1">)</span>
    <span class="s1">tsne.fit(np.zeros((</span><span class="s3">100</span><span class="s0">, </span><span class="s3">100</span><span class="s1">)))</span>


<span class="s0">def </span><span class="s1">test_pca_initialization_not_compatible_with_precomputed_kernel():</span>
    <span class="s2"># Precomputed distance matrices cannot use PCA initialization.</span>
    <span class="s1">tsne = TSNE(metric=</span><span class="s4">&quot;precomputed&quot;</span><span class="s0">, </span><span class="s1">init=</span><span class="s4">&quot;pca&quot;</span><span class="s0">, </span><span class="s1">perplexity=</span><span class="s3">1</span><span class="s1">)</span>
    <span class="s0">with </span><span class="s1">pytest.raises(</span>
        <span class="s1">ValueError</span><span class="s0">,</span>
        <span class="s1">match=</span><span class="s4">'The parameter init=&quot;pca&quot; cannot be used with metric=&quot;precomputed&quot;.'</span><span class="s0">,</span>
    <span class="s1">):</span>
        <span class="s1">tsne.fit_transform(np.array([[</span><span class="s3">0.0</span><span class="s1">]</span><span class="s0">, </span><span class="s1">[</span><span class="s3">1.0</span><span class="s1">]]))</span>


<span class="s0">def </span><span class="s1">test_pca_initialization_not_compatible_with_sparse_input():</span>
    <span class="s2"># Sparse input matrices cannot use PCA initialization.</span>
    <span class="s1">tsne = TSNE(init=</span><span class="s4">&quot;pca&quot;</span><span class="s0">, </span><span class="s1">learning_rate=</span><span class="s3">100.0</span><span class="s0">, </span><span class="s1">perplexity=</span><span class="s3">1</span><span class="s1">)</span>
    <span class="s0">with </span><span class="s1">pytest.raises(TypeError</span><span class="s0">, </span><span class="s1">match=</span><span class="s4">&quot;PCA initialization.*&quot;</span><span class="s1">):</span>
        <span class="s1">tsne.fit_transform(sp.csr_matrix([[</span><span class="s3">0</span><span class="s0">, </span><span class="s3">5</span><span class="s1">]</span><span class="s0">, </span><span class="s1">[</span><span class="s3">5</span><span class="s0">, </span><span class="s3">0</span><span class="s1">]]))</span>


<span class="s0">def </span><span class="s1">test_n_components_range():</span>
    <span class="s2"># barnes_hut method should only be used with n_components &lt;= 3</span>
    <span class="s1">tsne = TSNE(n_components=</span><span class="s3">4</span><span class="s0">, </span><span class="s1">method=</span><span class="s4">&quot;barnes_hut&quot;</span><span class="s0">, </span><span class="s1">perplexity=</span><span class="s3">1</span><span class="s1">)</span>
    <span class="s0">with </span><span class="s1">pytest.raises(ValueError</span><span class="s0">, </span><span class="s1">match=</span><span class="s4">&quot;'n_components' should be .*&quot;</span><span class="s1">):</span>
        <span class="s1">tsne.fit_transform(np.array([[</span><span class="s3">0.0</span><span class="s1">]</span><span class="s0">, </span><span class="s1">[</span><span class="s3">1.0</span><span class="s1">]]))</span>


<span class="s0">def </span><span class="s1">test_early_exaggeration_used():</span>
    <span class="s2"># check that the ``early_exaggeration`` parameter has an effect</span>
    <span class="s1">random_state = check_random_state(</span><span class="s3">0</span><span class="s1">)</span>
    <span class="s1">n_components = </span><span class="s3">2</span>
    <span class="s1">methods = [</span><span class="s4">&quot;exact&quot;</span><span class="s0">, </span><span class="s4">&quot;barnes_hut&quot;</span><span class="s1">]</span>
    <span class="s1">X = random_state.randn(</span><span class="s3">25</span><span class="s0">, </span><span class="s1">n_components).astype(np.float32)</span>
    <span class="s0">for </span><span class="s1">method </span><span class="s0">in </span><span class="s1">methods:</span>
        <span class="s1">tsne = TSNE(</span>
            <span class="s1">n_components=n_components</span><span class="s0">,</span>
            <span class="s1">perplexity=</span><span class="s3">1</span><span class="s0">,</span>
            <span class="s1">learning_rate=</span><span class="s3">100.0</span><span class="s0">,</span>
            <span class="s1">init=</span><span class="s4">&quot;pca&quot;</span><span class="s0">,</span>
            <span class="s1">random_state=</span><span class="s3">0</span><span class="s0">,</span>
            <span class="s1">method=method</span><span class="s0">,</span>
            <span class="s1">early_exaggeration=</span><span class="s3">1.0</span><span class="s0">,</span>
            <span class="s1">n_iter=</span><span class="s3">250</span><span class="s0">,</span>
        <span class="s1">)</span>
        <span class="s1">X_embedded1 = tsne.fit_transform(X)</span>
        <span class="s1">tsne = TSNE(</span>
            <span class="s1">n_components=n_components</span><span class="s0">,</span>
            <span class="s1">perplexity=</span><span class="s3">1</span><span class="s0">,</span>
            <span class="s1">learning_rate=</span><span class="s3">100.0</span><span class="s0">,</span>
            <span class="s1">init=</span><span class="s4">&quot;pca&quot;</span><span class="s0">,</span>
            <span class="s1">random_state=</span><span class="s3">0</span><span class="s0">,</span>
            <span class="s1">method=method</span><span class="s0">,</span>
            <span class="s1">early_exaggeration=</span><span class="s3">10.0</span><span class="s0">,</span>
            <span class="s1">n_iter=</span><span class="s3">250</span><span class="s0">,</span>
        <span class="s1">)</span>
        <span class="s1">X_embedded2 = tsne.fit_transform(X)</span>

        <span class="s0">assert not </span><span class="s1">np.allclose(X_embedded1</span><span class="s0">, </span><span class="s1">X_embedded2)</span>


<span class="s0">def </span><span class="s1">test_n_iter_used():</span>
    <span class="s2"># check that the ``n_iter`` parameter has an effect</span>
    <span class="s1">random_state = check_random_state(</span><span class="s3">0</span><span class="s1">)</span>
    <span class="s1">n_components = </span><span class="s3">2</span>
    <span class="s1">methods = [</span><span class="s4">&quot;exact&quot;</span><span class="s0">, </span><span class="s4">&quot;barnes_hut&quot;</span><span class="s1">]</span>
    <span class="s1">X = random_state.randn(</span><span class="s3">25</span><span class="s0">, </span><span class="s1">n_components).astype(np.float32)</span>
    <span class="s0">for </span><span class="s1">method </span><span class="s0">in </span><span class="s1">methods:</span>
        <span class="s0">for </span><span class="s1">n_iter </span><span class="s0">in </span><span class="s1">[</span><span class="s3">251</span><span class="s0">, </span><span class="s3">500</span><span class="s1">]:</span>
            <span class="s1">tsne = TSNE(</span>
                <span class="s1">n_components=n_components</span><span class="s0">,</span>
                <span class="s1">perplexity=</span><span class="s3">1</span><span class="s0">,</span>
                <span class="s1">learning_rate=</span><span class="s3">0.5</span><span class="s0">,</span>
                <span class="s1">init=</span><span class="s4">&quot;random&quot;</span><span class="s0">,</span>
                <span class="s1">random_state=</span><span class="s3">0</span><span class="s0">,</span>
                <span class="s1">method=method</span><span class="s0">,</span>
                <span class="s1">early_exaggeration=</span><span class="s3">1.0</span><span class="s0">,</span>
                <span class="s1">n_iter=n_iter</span><span class="s0">,</span>
            <span class="s1">)</span>
            <span class="s1">tsne.fit_transform(X)</span>

            <span class="s0">assert </span><span class="s1">tsne.n_iter_ == n_iter - </span><span class="s3">1</span>


<span class="s0">def </span><span class="s1">test_answer_gradient_two_points():</span>
    <span class="s2"># Test the tree with only a single set of children.</span>
    <span class="s2">#</span>
    <span class="s2"># These tests &amp; answers have been checked against the reference</span>
    <span class="s2"># implementation by LvdM.</span>
    <span class="s1">pos_input = np.array([[</span><span class="s3">1.0</span><span class="s0">, </span><span class="s3">0.0</span><span class="s1">]</span><span class="s0">, </span><span class="s1">[</span><span class="s3">0.0</span><span class="s0">, </span><span class="s3">1.0</span><span class="s1">]])</span>
    <span class="s1">pos_output = np.array(</span>
        <span class="s1">[[-</span><span class="s3">4.961291e-05</span><span class="s0">, </span><span class="s1">-</span><span class="s3">1.072243e-04</span><span class="s1">]</span><span class="s0">, </span><span class="s1">[</span><span class="s3">9.259460e-05</span><span class="s0">, </span><span class="s3">2.702024e-04</span><span class="s1">]]</span>
    <span class="s1">)</span>
    <span class="s1">neighbors = np.array([[</span><span class="s3">1</span><span class="s1">]</span><span class="s0">, </span><span class="s1">[</span><span class="s3">0</span><span class="s1">]])</span>
    <span class="s1">grad_output = np.array(</span>
        <span class="s1">[[-</span><span class="s3">2.37012478e-05</span><span class="s0">, </span><span class="s1">-</span><span class="s3">6.29044398e-05</span><span class="s1">]</span><span class="s0">, </span><span class="s1">[</span><span class="s3">2.37012478e-05</span><span class="s0">, </span><span class="s3">6.29044398e-05</span><span class="s1">]]</span>
    <span class="s1">)</span>
    <span class="s1">_run_answer_test(pos_input</span><span class="s0">, </span><span class="s1">pos_output</span><span class="s0">, </span><span class="s1">neighbors</span><span class="s0">, </span><span class="s1">grad_output)</span>


<span class="s0">def </span><span class="s1">test_answer_gradient_four_points():</span>
    <span class="s2"># Four points tests the tree with multiple levels of children.</span>
    <span class="s2">#</span>
    <span class="s2"># These tests &amp; answers have been checked against the reference</span>
    <span class="s2"># implementation by LvdM.</span>
    <span class="s1">pos_input = np.array([[</span><span class="s3">1.0</span><span class="s0">, </span><span class="s3">0.0</span><span class="s1">]</span><span class="s0">, </span><span class="s1">[</span><span class="s3">0.0</span><span class="s0">, </span><span class="s3">1.0</span><span class="s1">]</span><span class="s0">, </span><span class="s1">[</span><span class="s3">5.0</span><span class="s0">, </span><span class="s3">2.0</span><span class="s1">]</span><span class="s0">, </span><span class="s1">[</span><span class="s3">7.3</span><span class="s0">, </span><span class="s3">2.2</span><span class="s1">]])</span>
    <span class="s1">pos_output = np.array(</span>
        <span class="s1">[</span>
            <span class="s1">[</span><span class="s3">6.080564e-05</span><span class="s0">, </span><span class="s1">-</span><span class="s3">7.120823e-05</span><span class="s1">]</span><span class="s0">,</span>
            <span class="s1">[-</span><span class="s3">1.718945e-04</span><span class="s0">, </span><span class="s1">-</span><span class="s3">4.000536e-05</span><span class="s1">]</span><span class="s0">,</span>
            <span class="s1">[-</span><span class="s3">2.271720e-04</span><span class="s0">, </span><span class="s3">8.663310e-05</span><span class="s1">]</span><span class="s0">,</span>
            <span class="s1">[-</span><span class="s3">1.032577e-04</span><span class="s0">, </span><span class="s1">-</span><span class="s3">3.582033e-05</span><span class="s1">]</span><span class="s0">,</span>
        <span class="s1">]</span>
    <span class="s1">)</span>
    <span class="s1">neighbors = np.array([[</span><span class="s3">1</span><span class="s0">, </span><span class="s3">2</span><span class="s0">, </span><span class="s3">3</span><span class="s1">]</span><span class="s0">, </span><span class="s1">[</span><span class="s3">0</span><span class="s0">, </span><span class="s3">2</span><span class="s0">, </span><span class="s3">3</span><span class="s1">]</span><span class="s0">, </span><span class="s1">[</span><span class="s3">1</span><span class="s0">, </span><span class="s3">0</span><span class="s0">, </span><span class="s3">3</span><span class="s1">]</span><span class="s0">, </span><span class="s1">[</span><span class="s3">1</span><span class="s0">, </span><span class="s3">2</span><span class="s0">, </span><span class="s3">0</span><span class="s1">]])</span>
    <span class="s1">grad_output = np.array(</span>
        <span class="s1">[</span>
            <span class="s1">[</span><span class="s3">5.81128448e-05</span><span class="s0">, </span><span class="s1">-</span><span class="s3">7.78033454e-06</span><span class="s1">]</span><span class="s0">,</span>
            <span class="s1">[-</span><span class="s3">5.81526851e-05</span><span class="s0">, </span><span class="s3">7.80976444e-06</span><span class="s1">]</span><span class="s0">,</span>
            <span class="s1">[</span><span class="s3">4.24275173e-08</span><span class="s0">, </span><span class="s1">-</span><span class="s3">3.69569698e-08</span><span class="s1">]</span><span class="s0">,</span>
            <span class="s1">[-</span><span class="s3">2.58720939e-09</span><span class="s0">, </span><span class="s3">7.52706374e-09</span><span class="s1">]</span><span class="s0">,</span>
        <span class="s1">]</span>
    <span class="s1">)</span>
    <span class="s1">_run_answer_test(pos_input</span><span class="s0">, </span><span class="s1">pos_output</span><span class="s0">, </span><span class="s1">neighbors</span><span class="s0">, </span><span class="s1">grad_output)</span>


<span class="s0">def </span><span class="s1">test_skip_num_points_gradient():</span>
    <span class="s2"># Test the kwargs option skip_num_points.</span>
    <span class="s2">#</span>
    <span class="s2"># Skip num points should make it such that the Barnes_hut gradient</span>
    <span class="s2"># is not calculated for indices below skip_num_point.</span>
    <span class="s2"># Aside from skip_num_points=2 and the first two gradient rows</span>
    <span class="s2"># being set to zero, these data points are the same as in</span>
    <span class="s2"># test_answer_gradient_four_points()</span>
    <span class="s1">pos_input = np.array([[</span><span class="s3">1.0</span><span class="s0">, </span><span class="s3">0.0</span><span class="s1">]</span><span class="s0">, </span><span class="s1">[</span><span class="s3">0.0</span><span class="s0">, </span><span class="s3">1.0</span><span class="s1">]</span><span class="s0">, </span><span class="s1">[</span><span class="s3">5.0</span><span class="s0">, </span><span class="s3">2.0</span><span class="s1">]</span><span class="s0">, </span><span class="s1">[</span><span class="s3">7.3</span><span class="s0">, </span><span class="s3">2.2</span><span class="s1">]])</span>
    <span class="s1">pos_output = np.array(</span>
        <span class="s1">[</span>
            <span class="s1">[</span><span class="s3">6.080564e-05</span><span class="s0">, </span><span class="s1">-</span><span class="s3">7.120823e-05</span><span class="s1">]</span><span class="s0">,</span>
            <span class="s1">[-</span><span class="s3">1.718945e-04</span><span class="s0">, </span><span class="s1">-</span><span class="s3">4.000536e-05</span><span class="s1">]</span><span class="s0">,</span>
            <span class="s1">[-</span><span class="s3">2.271720e-04</span><span class="s0">, </span><span class="s3">8.663310e-05</span><span class="s1">]</span><span class="s0">,</span>
            <span class="s1">[-</span><span class="s3">1.032577e-04</span><span class="s0">, </span><span class="s1">-</span><span class="s3">3.582033e-05</span><span class="s1">]</span><span class="s0">,</span>
        <span class="s1">]</span>
    <span class="s1">)</span>
    <span class="s1">neighbors = np.array([[</span><span class="s3">1</span><span class="s0">, </span><span class="s3">2</span><span class="s0">, </span><span class="s3">3</span><span class="s1">]</span><span class="s0">, </span><span class="s1">[</span><span class="s3">0</span><span class="s0">, </span><span class="s3">2</span><span class="s0">, </span><span class="s3">3</span><span class="s1">]</span><span class="s0">, </span><span class="s1">[</span><span class="s3">1</span><span class="s0">, </span><span class="s3">0</span><span class="s0">, </span><span class="s3">3</span><span class="s1">]</span><span class="s0">, </span><span class="s1">[</span><span class="s3">1</span><span class="s0">, </span><span class="s3">2</span><span class="s0">, </span><span class="s3">0</span><span class="s1">]])</span>
    <span class="s1">grad_output = np.array(</span>
        <span class="s1">[</span>
            <span class="s1">[</span><span class="s3">0.0</span><span class="s0">, </span><span class="s3">0.0</span><span class="s1">]</span><span class="s0">,</span>
            <span class="s1">[</span><span class="s3">0.0</span><span class="s0">, </span><span class="s3">0.0</span><span class="s1">]</span><span class="s0">,</span>
            <span class="s1">[</span><span class="s3">4.24275173e-08</span><span class="s0">, </span><span class="s1">-</span><span class="s3">3.69569698e-08</span><span class="s1">]</span><span class="s0">,</span>
            <span class="s1">[-</span><span class="s3">2.58720939e-09</span><span class="s0">, </span><span class="s3">7.52706374e-09</span><span class="s1">]</span><span class="s0">,</span>
        <span class="s1">]</span>
    <span class="s1">)</span>
    <span class="s1">_run_answer_test(pos_input</span><span class="s0">, </span><span class="s1">pos_output</span><span class="s0">, </span><span class="s1">neighbors</span><span class="s0">, </span><span class="s1">grad_output</span><span class="s0">, False, </span><span class="s3">0.1</span><span class="s0">, </span><span class="s3">2</span><span class="s1">)</span>


<span class="s0">def </span><span class="s1">_run_answer_test(</span>
    <span class="s1">pos_input</span><span class="s0">,</span>
    <span class="s1">pos_output</span><span class="s0">,</span>
    <span class="s1">neighbors</span><span class="s0">,</span>
    <span class="s1">grad_output</span><span class="s0">,</span>
    <span class="s1">verbose=</span><span class="s0">False,</span>
    <span class="s1">perplexity=</span><span class="s3">0.1</span><span class="s0">,</span>
    <span class="s1">skip_num_points=</span><span class="s3">0</span><span class="s0">,</span>
<span class="s1">):</span>
    <span class="s1">distances = pairwise_distances(pos_input).astype(np.float32)</span>
    <span class="s1">args = distances</span><span class="s0">, </span><span class="s1">perplexity</span><span class="s0">, </span><span class="s1">verbose</span>
    <span class="s1">pos_output = pos_output.astype(np.float32)</span>
    <span class="s1">neighbors = neighbors.astype(np.int64</span><span class="s0">, </span><span class="s1">copy=</span><span class="s0">False</span><span class="s1">)</span>
    <span class="s1">pij_input = _joint_probabilities(*args)</span>
    <span class="s1">pij_input = squareform(pij_input).astype(np.float32)</span>
    <span class="s1">grad_bh = np.zeros(pos_output.shape</span><span class="s0">, </span><span class="s1">dtype=np.float32)</span>

    <span class="s0">from </span><span class="s1">scipy.sparse </span><span class="s0">import </span><span class="s1">csr_matrix</span>

    <span class="s1">P = csr_matrix(pij_input)</span>

    <span class="s1">neighbors = P.indices.astype(np.int64)</span>
    <span class="s1">indptr = P.indptr.astype(np.int64)</span>

    <span class="s1">_barnes_hut_tsne.gradient(</span>
        <span class="s1">P.data</span><span class="s0">, </span><span class="s1">pos_output</span><span class="s0">, </span><span class="s1">neighbors</span><span class="s0">, </span><span class="s1">indptr</span><span class="s0">, </span><span class="s1">grad_bh</span><span class="s0">, </span><span class="s3">0.5</span><span class="s0">, </span><span class="s3">2</span><span class="s0">, </span><span class="s3">1</span><span class="s0">, </span><span class="s1">skip_num_points=</span><span class="s3">0</span>
    <span class="s1">)</span>
    <span class="s1">assert_array_almost_equal(grad_bh</span><span class="s0">, </span><span class="s1">grad_output</span><span class="s0">, </span><span class="s1">decimal=</span><span class="s3">4</span><span class="s1">)</span>


<span class="s0">def </span><span class="s1">test_verbose():</span>
    <span class="s2"># Verbose options write to stdout.</span>
    <span class="s1">random_state = check_random_state(</span><span class="s3">0</span><span class="s1">)</span>
    <span class="s1">tsne = TSNE(verbose=</span><span class="s3">2</span><span class="s0">, </span><span class="s1">perplexity=</span><span class="s3">4</span><span class="s1">)</span>
    <span class="s1">X = random_state.randn(</span><span class="s3">5</span><span class="s0">, </span><span class="s3">2</span><span class="s1">)</span>

    <span class="s1">old_stdout = sys.stdout</span>
    <span class="s1">sys.stdout = StringIO()</span>
    <span class="s0">try</span><span class="s1">:</span>
        <span class="s1">tsne.fit_transform(X)</span>
    <span class="s0">finally</span><span class="s1">:</span>
        <span class="s1">out = sys.stdout.getvalue()</span>
        <span class="s1">sys.stdout.close()</span>
        <span class="s1">sys.stdout = old_stdout</span>

    <span class="s0">assert </span><span class="s4">&quot;[t-SNE]&quot; </span><span class="s0">in </span><span class="s1">out</span>
    <span class="s0">assert </span><span class="s4">&quot;nearest neighbors...&quot; </span><span class="s0">in </span><span class="s1">out</span>
    <span class="s0">assert </span><span class="s4">&quot;Computed conditional probabilities&quot; </span><span class="s0">in </span><span class="s1">out</span>
    <span class="s0">assert </span><span class="s4">&quot;Mean sigma&quot; </span><span class="s0">in </span><span class="s1">out</span>
    <span class="s0">assert </span><span class="s4">&quot;early exaggeration&quot; </span><span class="s0">in </span><span class="s1">out</span>


<span class="s0">def </span><span class="s1">test_chebyshev_metric():</span>
    <span class="s2"># t-SNE should allow metrics that cannot be squared (issue #3526).</span>
    <span class="s1">random_state = check_random_state(</span><span class="s3">0</span><span class="s1">)</span>
    <span class="s1">tsne = TSNE(metric=</span><span class="s4">&quot;chebyshev&quot;</span><span class="s0">, </span><span class="s1">perplexity=</span><span class="s3">4</span><span class="s1">)</span>
    <span class="s1">X = random_state.randn(</span><span class="s3">5</span><span class="s0">, </span><span class="s3">2</span><span class="s1">)</span>
    <span class="s1">tsne.fit_transform(X)</span>


<span class="s0">def </span><span class="s1">test_reduction_to_one_component():</span>
    <span class="s2"># t-SNE should allow reduction to one component (issue #4154).</span>
    <span class="s1">random_state = check_random_state(</span><span class="s3">0</span><span class="s1">)</span>
    <span class="s1">tsne = TSNE(n_components=</span><span class="s3">1</span><span class="s0">, </span><span class="s1">perplexity=</span><span class="s3">4</span><span class="s1">)</span>
    <span class="s1">X = random_state.randn(</span><span class="s3">5</span><span class="s0">, </span><span class="s3">2</span><span class="s1">)</span>
    <span class="s1">X_embedded = tsne.fit(X).embedding_</span>
    <span class="s0">assert </span><span class="s1">np.all(np.isfinite(X_embedded))</span>


<span class="s1">@pytest.mark.parametrize(</span><span class="s4">&quot;method&quot;</span><span class="s0">, </span><span class="s1">[</span><span class="s4">&quot;barnes_hut&quot;</span><span class="s0">, </span><span class="s4">&quot;exact&quot;</span><span class="s1">])</span>
<span class="s1">@pytest.mark.parametrize(</span><span class="s4">&quot;dt&quot;</span><span class="s0">, </span><span class="s1">[np.float32</span><span class="s0">, </span><span class="s1">np.float64])</span>
<span class="s0">def </span><span class="s1">test_64bit(method</span><span class="s0">, </span><span class="s1">dt):</span>
    <span class="s2"># Ensure 64bit arrays are handled correctly.</span>
    <span class="s1">random_state = check_random_state(</span><span class="s3">0</span><span class="s1">)</span>

    <span class="s1">X = random_state.randn(</span><span class="s3">10</span><span class="s0">, </span><span class="s3">2</span><span class="s1">).astype(dt</span><span class="s0">, </span><span class="s1">copy=</span><span class="s0">False</span><span class="s1">)</span>
    <span class="s1">tsne = TSNE(</span>
        <span class="s1">n_components=</span><span class="s3">2</span><span class="s0">,</span>
        <span class="s1">perplexity=</span><span class="s3">2</span><span class="s0">,</span>
        <span class="s1">learning_rate=</span><span class="s3">100.0</span><span class="s0">,</span>
        <span class="s1">random_state=</span><span class="s3">0</span><span class="s0">,</span>
        <span class="s1">method=method</span><span class="s0">,</span>
        <span class="s1">verbose=</span><span class="s3">0</span><span class="s0">,</span>
        <span class="s1">n_iter=</span><span class="s3">300</span><span class="s0">,</span>
        <span class="s1">init=</span><span class="s4">&quot;random&quot;</span><span class="s0">,</span>
    <span class="s1">)</span>
    <span class="s1">X_embedded = tsne.fit_transform(X)</span>
    <span class="s1">effective_type = X_embedded.dtype</span>

    <span class="s2"># tsne cython code is only single precision, so the output will</span>
    <span class="s2"># always be single precision, irrespectively of the input dtype</span>
    <span class="s0">assert </span><span class="s1">effective_type == np.float32</span>


<span class="s1">@pytest.mark.parametrize(</span><span class="s4">&quot;method&quot;</span><span class="s0">, </span><span class="s1">[</span><span class="s4">&quot;barnes_hut&quot;</span><span class="s0">, </span><span class="s4">&quot;exact&quot;</span><span class="s1">])</span>
<span class="s0">def </span><span class="s1">test_kl_divergence_not_nan(method):</span>
    <span class="s2"># Ensure kl_divergence_ is computed at last iteration</span>
    <span class="s2"># even though n_iter % n_iter_check != 0, i.e. 1003 % 50 != 0</span>
    <span class="s1">random_state = check_random_state(</span><span class="s3">0</span><span class="s1">)</span>

    <span class="s1">X = random_state.randn(</span><span class="s3">50</span><span class="s0">, </span><span class="s3">2</span><span class="s1">)</span>
    <span class="s1">tsne = TSNE(</span>
        <span class="s1">n_components=</span><span class="s3">2</span><span class="s0">,</span>
        <span class="s1">perplexity=</span><span class="s3">2</span><span class="s0">,</span>
        <span class="s1">learning_rate=</span><span class="s3">100.0</span><span class="s0">,</span>
        <span class="s1">random_state=</span><span class="s3">0</span><span class="s0">,</span>
        <span class="s1">method=method</span><span class="s0">,</span>
        <span class="s1">verbose=</span><span class="s3">0</span><span class="s0">,</span>
        <span class="s1">n_iter=</span><span class="s3">503</span><span class="s0">,</span>
        <span class="s1">init=</span><span class="s4">&quot;random&quot;</span><span class="s0">,</span>
    <span class="s1">)</span>
    <span class="s1">tsne.fit_transform(X)</span>

    <span class="s0">assert not </span><span class="s1">np.isnan(tsne.kl_divergence_)</span>


<span class="s0">def </span><span class="s1">test_barnes_hut_angle():</span>
    <span class="s2"># When Barnes-Hut's angle=0 this corresponds to the exact method.</span>
    <span class="s1">angle = </span><span class="s3">0.0</span>
    <span class="s1">perplexity = </span><span class="s3">10</span>
    <span class="s1">n_samples = </span><span class="s3">100</span>
    <span class="s0">for </span><span class="s1">n_components </span><span class="s0">in </span><span class="s1">[</span><span class="s3">2</span><span class="s0">, </span><span class="s3">3</span><span class="s1">]:</span>
        <span class="s1">n_features = </span><span class="s3">5</span>
        <span class="s1">degrees_of_freedom = float(n_components - </span><span class="s3">1.0</span><span class="s1">)</span>

        <span class="s1">random_state = check_random_state(</span><span class="s3">0</span><span class="s1">)</span>
        <span class="s1">data = random_state.randn(n_samples</span><span class="s0">, </span><span class="s1">n_features)</span>
        <span class="s1">distances = pairwise_distances(data)</span>
        <span class="s1">params = random_state.randn(n_samples</span><span class="s0">, </span><span class="s1">n_components)</span>
        <span class="s1">P = _joint_probabilities(distances</span><span class="s0">, </span><span class="s1">perplexity</span><span class="s0">, </span><span class="s1">verbose=</span><span class="s3">0</span><span class="s1">)</span>
        <span class="s1">kl_exact</span><span class="s0">, </span><span class="s1">grad_exact = _kl_divergence(</span>
            <span class="s1">params</span><span class="s0">, </span><span class="s1">P</span><span class="s0">, </span><span class="s1">degrees_of_freedom</span><span class="s0">, </span><span class="s1">n_samples</span><span class="s0">, </span><span class="s1">n_components</span>
        <span class="s1">)</span>

        <span class="s1">n_neighbors = n_samples - </span><span class="s3">1</span>
        <span class="s1">distances_csr = (</span>
            <span class="s1">NearestNeighbors()</span>
            <span class="s1">.fit(data)</span>
            <span class="s1">.kneighbors_graph(n_neighbors=n_neighbors</span><span class="s0">, </span><span class="s1">mode=</span><span class="s4">&quot;distance&quot;</span><span class="s1">)</span>
        <span class="s1">)</span>
        <span class="s1">P_bh = _joint_probabilities_nn(distances_csr</span><span class="s0">, </span><span class="s1">perplexity</span><span class="s0">, </span><span class="s1">verbose=</span><span class="s3">0</span><span class="s1">)</span>
        <span class="s1">kl_bh</span><span class="s0">, </span><span class="s1">grad_bh = _kl_divergence_bh(</span>
            <span class="s1">params</span><span class="s0">,</span>
            <span class="s1">P_bh</span><span class="s0">,</span>
            <span class="s1">degrees_of_freedom</span><span class="s0">,</span>
            <span class="s1">n_samples</span><span class="s0">,</span>
            <span class="s1">n_components</span><span class="s0">,</span>
            <span class="s1">angle=angle</span><span class="s0">,</span>
            <span class="s1">skip_num_points=</span><span class="s3">0</span><span class="s0">,</span>
            <span class="s1">verbose=</span><span class="s3">0</span><span class="s0">,</span>
        <span class="s1">)</span>

        <span class="s1">P = squareform(P)</span>
        <span class="s1">P_bh = P_bh.toarray()</span>
        <span class="s1">assert_array_almost_equal(P_bh</span><span class="s0">, </span><span class="s1">P</span><span class="s0">, </span><span class="s1">decimal=</span><span class="s3">5</span><span class="s1">)</span>
        <span class="s1">assert_almost_equal(kl_exact</span><span class="s0">, </span><span class="s1">kl_bh</span><span class="s0">, </span><span class="s1">decimal=</span><span class="s3">3</span><span class="s1">)</span>


<span class="s1">@skip_if_32bit</span>
<span class="s0">def </span><span class="s1">test_n_iter_without_progress():</span>
    <span class="s2"># Use a dummy negative n_iter_without_progress and check output on stdout</span>
    <span class="s1">random_state = check_random_state(</span><span class="s3">0</span><span class="s1">)</span>
    <span class="s1">X = random_state.randn(</span><span class="s3">100</span><span class="s0">, </span><span class="s3">10</span><span class="s1">)</span>
    <span class="s0">for </span><span class="s1">method </span><span class="s0">in </span><span class="s1">[</span><span class="s4">&quot;barnes_hut&quot;</span><span class="s0">, </span><span class="s4">&quot;exact&quot;</span><span class="s1">]:</span>
        <span class="s1">tsne = TSNE(</span>
            <span class="s1">n_iter_without_progress=-</span><span class="s3">1</span><span class="s0">,</span>
            <span class="s1">verbose=</span><span class="s3">2</span><span class="s0">,</span>
            <span class="s1">learning_rate=</span><span class="s3">1e8</span><span class="s0">,</span>
            <span class="s1">random_state=</span><span class="s3">0</span><span class="s0">,</span>
            <span class="s1">method=method</span><span class="s0">,</span>
            <span class="s1">n_iter=</span><span class="s3">351</span><span class="s0">,</span>
            <span class="s1">init=</span><span class="s4">&quot;random&quot;</span><span class="s0">,</span>
        <span class="s1">)</span>
        <span class="s1">tsne._N_ITER_CHECK = </span><span class="s3">1</span>
        <span class="s1">tsne._EXPLORATION_N_ITER = </span><span class="s3">0</span>

        <span class="s1">old_stdout = sys.stdout</span>
        <span class="s1">sys.stdout = StringIO()</span>
        <span class="s0">try</span><span class="s1">:</span>
            <span class="s1">tsne.fit_transform(X)</span>
        <span class="s0">finally</span><span class="s1">:</span>
            <span class="s1">out = sys.stdout.getvalue()</span>
            <span class="s1">sys.stdout.close()</span>
            <span class="s1">sys.stdout = old_stdout</span>

        <span class="s2"># The output needs to contain the value of n_iter_without_progress</span>
        <span class="s0">assert </span><span class="s4">&quot;did not make any progress during the last -1 episodes. Finished.&quot; </span><span class="s0">in </span><span class="s1">out</span>


<span class="s0">def </span><span class="s1">test_min_grad_norm():</span>
    <span class="s2"># Make sure that the parameter min_grad_norm is used correctly</span>
    <span class="s1">random_state = check_random_state(</span><span class="s3">0</span><span class="s1">)</span>
    <span class="s1">X = random_state.randn(</span><span class="s3">100</span><span class="s0">, </span><span class="s3">2</span><span class="s1">)</span>
    <span class="s1">min_grad_norm = </span><span class="s3">0.002</span>
    <span class="s1">tsne = TSNE(min_grad_norm=min_grad_norm</span><span class="s0">, </span><span class="s1">verbose=</span><span class="s3">2</span><span class="s0">, </span><span class="s1">random_state=</span><span class="s3">0</span><span class="s0">, </span><span class="s1">method=</span><span class="s4">&quot;exact&quot;</span><span class="s1">)</span>

    <span class="s1">old_stdout = sys.stdout</span>
    <span class="s1">sys.stdout = StringIO()</span>
    <span class="s0">try</span><span class="s1">:</span>
        <span class="s1">tsne.fit_transform(X)</span>
    <span class="s0">finally</span><span class="s1">:</span>
        <span class="s1">out = sys.stdout.getvalue()</span>
        <span class="s1">sys.stdout.close()</span>
        <span class="s1">sys.stdout = old_stdout</span>

    <span class="s1">lines_out = out.split(</span><span class="s4">&quot;</span><span class="s0">\n</span><span class="s4">&quot;</span><span class="s1">)</span>

    <span class="s2"># extract the gradient norm from the verbose output</span>
    <span class="s1">gradient_norm_values = []</span>
    <span class="s0">for </span><span class="s1">line </span><span class="s0">in </span><span class="s1">lines_out:</span>
        <span class="s2"># When the computation is Finished just an old gradient norm value</span>
        <span class="s2"># is repeated that we do not need to store</span>
        <span class="s0">if </span><span class="s4">&quot;Finished&quot; </span><span class="s0">in </span><span class="s1">line:</span>
            <span class="s0">break</span>

        <span class="s1">start_grad_norm = line.find(</span><span class="s4">&quot;gradient norm&quot;</span><span class="s1">)</span>
        <span class="s0">if </span><span class="s1">start_grad_norm &gt;= </span><span class="s3">0</span><span class="s1">:</span>
            <span class="s1">line = line[start_grad_norm:]</span>
            <span class="s1">line = line.replace(</span><span class="s4">&quot;gradient norm = &quot;</span><span class="s0">, </span><span class="s4">&quot;&quot;</span><span class="s1">).split(</span><span class="s4">&quot; &quot;</span><span class="s1">)[</span><span class="s3">0</span><span class="s1">]</span>
            <span class="s1">gradient_norm_values.append(float(line))</span>

    <span class="s2"># Compute how often the gradient norm is smaller than min_grad_norm</span>
    <span class="s1">gradient_norm_values = np.array(gradient_norm_values)</span>
    <span class="s1">n_smaller_gradient_norms = len(</span>
        <span class="s1">gradient_norm_values[gradient_norm_values &lt;= min_grad_norm]</span>
    <span class="s1">)</span>

    <span class="s2"># The gradient norm can be smaller than min_grad_norm at most once,</span>
    <span class="s2"># because in the moment it becomes smaller the optimization stops</span>
    <span class="s0">assert </span><span class="s1">n_smaller_gradient_norms &lt;= </span><span class="s3">1</span>


<span class="s0">def </span><span class="s1">test_accessible_kl_divergence():</span>
    <span class="s2"># Ensures that the accessible kl_divergence matches the computed value</span>
    <span class="s1">random_state = check_random_state(</span><span class="s3">0</span><span class="s1">)</span>
    <span class="s1">X = random_state.randn(</span><span class="s3">50</span><span class="s0">, </span><span class="s3">2</span><span class="s1">)</span>
    <span class="s1">tsne = TSNE(</span>
        <span class="s1">n_iter_without_progress=</span><span class="s3">2</span><span class="s0">, </span><span class="s1">verbose=</span><span class="s3">2</span><span class="s0">, </span><span class="s1">random_state=</span><span class="s3">0</span><span class="s0">, </span><span class="s1">method=</span><span class="s4">&quot;exact&quot;</span><span class="s0">, </span><span class="s1">n_iter=</span><span class="s3">500</span>
    <span class="s1">)</span>

    <span class="s1">old_stdout = sys.stdout</span>
    <span class="s1">sys.stdout = StringIO()</span>
    <span class="s0">try</span><span class="s1">:</span>
        <span class="s1">tsne.fit_transform(X)</span>
    <span class="s0">finally</span><span class="s1">:</span>
        <span class="s1">out = sys.stdout.getvalue()</span>
        <span class="s1">sys.stdout.close()</span>
        <span class="s1">sys.stdout = old_stdout</span>

    <span class="s2"># The output needs to contain the accessible kl_divergence as the error at</span>
    <span class="s2"># the last iteration</span>
    <span class="s0">for </span><span class="s1">line </span><span class="s0">in </span><span class="s1">out.split(</span><span class="s4">&quot;</span><span class="s0">\n</span><span class="s4">&quot;</span><span class="s1">)[::-</span><span class="s3">1</span><span class="s1">]:</span>
        <span class="s0">if </span><span class="s4">&quot;Iteration&quot; </span><span class="s0">in </span><span class="s1">line:</span>
            <span class="s1">_</span><span class="s0">, </span><span class="s1">_</span><span class="s0">, </span><span class="s1">error = line.partition(</span><span class="s4">&quot;error = &quot;</span><span class="s1">)</span>
            <span class="s0">if </span><span class="s1">error:</span>
                <span class="s1">error</span><span class="s0">, </span><span class="s1">_</span><span class="s0">, </span><span class="s1">_ = error.partition(</span><span class="s4">&quot;,&quot;</span><span class="s1">)</span>
                <span class="s0">break</span>
    <span class="s1">assert_almost_equal(tsne.kl_divergence_</span><span class="s0">, </span><span class="s1">float(error)</span><span class="s0">, </span><span class="s1">decimal=</span><span class="s3">5</span><span class="s1">)</span>


<span class="s1">@pytest.mark.parametrize(</span><span class="s4">&quot;method&quot;</span><span class="s0">, </span><span class="s1">[</span><span class="s4">&quot;barnes_hut&quot;</span><span class="s0">, </span><span class="s4">&quot;exact&quot;</span><span class="s1">])</span>
<span class="s0">def </span><span class="s1">test_uniform_grid(method):</span>
    <span class="s5">&quot;&quot;&quot;Make sure that TSNE can approximately recover a uniform 2D grid 
 
    Due to ties in distances between point in X_2d_grid, this test is platform 
    dependent for ``method='barnes_hut'`` due to numerical imprecision. 
 
    Also, t-SNE is not assured to converge to the right solution because bad 
    initialization can lead to convergence to bad local minimum (the 
    optimization problem is non-convex). To avoid breaking the test too often, 
    we re-run t-SNE from the final point when the convergence is not good 
    enough. 
    &quot;&quot;&quot;</span>
    <span class="s1">seeds = range(</span><span class="s3">3</span><span class="s1">)</span>
    <span class="s1">n_iter = </span><span class="s3">500</span>
    <span class="s0">for </span><span class="s1">seed </span><span class="s0">in </span><span class="s1">seeds:</span>
        <span class="s1">tsne = TSNE(</span>
            <span class="s1">n_components=</span><span class="s3">2</span><span class="s0">,</span>
            <span class="s1">init=</span><span class="s4">&quot;random&quot;</span><span class="s0">,</span>
            <span class="s1">random_state=seed</span><span class="s0">,</span>
            <span class="s1">perplexity=</span><span class="s3">50</span><span class="s0">,</span>
            <span class="s1">n_iter=n_iter</span><span class="s0">,</span>
            <span class="s1">method=method</span><span class="s0">,</span>
            <span class="s1">learning_rate=</span><span class="s4">&quot;auto&quot;</span><span class="s0">,</span>
        <span class="s1">)</span>
        <span class="s1">Y = tsne.fit_transform(X_2d_grid)</span>

        <span class="s1">try_name = </span><span class="s4">&quot;{}_{}&quot;</span><span class="s1">.format(method</span><span class="s0">, </span><span class="s1">seed)</span>
        <span class="s0">try</span><span class="s1">:</span>
            <span class="s1">assert_uniform_grid(Y</span><span class="s0">, </span><span class="s1">try_name)</span>
        <span class="s0">except </span><span class="s1">AssertionError:</span>
            <span class="s2"># If the test fails a first time, re-run with init=Y to see if</span>
            <span class="s2"># this was caused by a bad initialization. Note that this will</span>
            <span class="s2"># also run an early_exaggeration step.</span>
            <span class="s1">try_name += </span><span class="s4">&quot;:rerun&quot;</span>
            <span class="s1">tsne.init = Y</span>
            <span class="s1">Y = tsne.fit_transform(X_2d_grid)</span>
            <span class="s1">assert_uniform_grid(Y</span><span class="s0">, </span><span class="s1">try_name)</span>


<span class="s0">def </span><span class="s1">assert_uniform_grid(Y</span><span class="s0">, </span><span class="s1">try_name=</span><span class="s0">None</span><span class="s1">):</span>
    <span class="s2"># Ensure that the resulting embedding leads to approximately</span>
    <span class="s2"># uniformly spaced points: the distance to the closest neighbors</span>
    <span class="s2"># should be non-zero and approximately constant.</span>
    <span class="s1">nn = NearestNeighbors(n_neighbors=</span><span class="s3">1</span><span class="s1">).fit(Y)</span>
    <span class="s1">dist_to_nn = nn.kneighbors(return_distance=</span><span class="s0">True</span><span class="s1">)[</span><span class="s3">0</span><span class="s1">].ravel()</span>
    <span class="s0">assert </span><span class="s1">dist_to_nn.min() &gt; </span><span class="s3">0.1</span>

    <span class="s1">smallest_to_mean = dist_to_nn.min() / np.mean(dist_to_nn)</span>
    <span class="s1">largest_to_mean = dist_to_nn.max() / np.mean(dist_to_nn)</span>

    <span class="s0">assert </span><span class="s1">smallest_to_mean &gt; </span><span class="s3">0.5</span><span class="s0">, </span><span class="s1">try_name</span>
    <span class="s0">assert </span><span class="s1">largest_to_mean &lt; </span><span class="s3">2</span><span class="s0">, </span><span class="s1">try_name</span>


<span class="s0">def </span><span class="s1">test_bh_match_exact():</span>
    <span class="s2"># check that the ``barnes_hut`` method match the exact one when</span>
    <span class="s2"># ``angle = 0`` and ``perplexity &gt; n_samples / 3``</span>
    <span class="s1">random_state = check_random_state(</span><span class="s3">0</span><span class="s1">)</span>
    <span class="s1">n_features = </span><span class="s3">10</span>
    <span class="s1">X = random_state.randn(</span><span class="s3">30</span><span class="s0">, </span><span class="s1">n_features).astype(np.float32)</span>
    <span class="s1">X_embeddeds = {}</span>
    <span class="s1">n_iter = {}</span>
    <span class="s0">for </span><span class="s1">method </span><span class="s0">in </span><span class="s1">[</span><span class="s4">&quot;exact&quot;</span><span class="s0">, </span><span class="s4">&quot;barnes_hut&quot;</span><span class="s1">]:</span>
        <span class="s1">tsne = TSNE(</span>
            <span class="s1">n_components=</span><span class="s3">2</span><span class="s0">,</span>
            <span class="s1">method=method</span><span class="s0">,</span>
            <span class="s1">learning_rate=</span><span class="s3">1.0</span><span class="s0">,</span>
            <span class="s1">init=</span><span class="s4">&quot;random&quot;</span><span class="s0">,</span>
            <span class="s1">random_state=</span><span class="s3">0</span><span class="s0">,</span>
            <span class="s1">n_iter=</span><span class="s3">251</span><span class="s0">,</span>
            <span class="s1">perplexity=</span><span class="s3">29.5</span><span class="s0">,</span>
            <span class="s1">angle=</span><span class="s3">0</span><span class="s0">,</span>
        <span class="s1">)</span>
        <span class="s2"># Kill the early_exaggeration</span>
        <span class="s1">tsne._EXPLORATION_N_ITER = </span><span class="s3">0</span>
        <span class="s1">X_embeddeds[method] = tsne.fit_transform(X)</span>
        <span class="s1">n_iter[method] = tsne.n_iter_</span>

    <span class="s0">assert </span><span class="s1">n_iter[</span><span class="s4">&quot;exact&quot;</span><span class="s1">] == n_iter[</span><span class="s4">&quot;barnes_hut&quot;</span><span class="s1">]</span>
    <span class="s1">assert_allclose(X_embeddeds[</span><span class="s4">&quot;exact&quot;</span><span class="s1">]</span><span class="s0">, </span><span class="s1">X_embeddeds[</span><span class="s4">&quot;barnes_hut&quot;</span><span class="s1">]</span><span class="s0">, </span><span class="s1">rtol=</span><span class="s3">1e-4</span><span class="s1">)</span>


<span class="s0">def </span><span class="s1">test_gradient_bh_multithread_match_sequential():</span>
    <span class="s2"># check that the bh gradient with different num_threads gives the same</span>
    <span class="s2"># results</span>

    <span class="s1">n_features = </span><span class="s3">10</span>
    <span class="s1">n_samples = </span><span class="s3">30</span>
    <span class="s1">n_components = </span><span class="s3">2</span>
    <span class="s1">degrees_of_freedom = </span><span class="s3">1</span>

    <span class="s1">angle = </span><span class="s3">3</span>
    <span class="s1">perplexity = </span><span class="s3">5</span>

    <span class="s1">random_state = check_random_state(</span><span class="s3">0</span><span class="s1">)</span>
    <span class="s1">data = random_state.randn(n_samples</span><span class="s0">, </span><span class="s1">n_features).astype(np.float32)</span>
    <span class="s1">params = random_state.randn(n_samples</span><span class="s0">, </span><span class="s1">n_components)</span>

    <span class="s1">n_neighbors = n_samples - </span><span class="s3">1</span>
    <span class="s1">distances_csr = (</span>
        <span class="s1">NearestNeighbors()</span>
        <span class="s1">.fit(data)</span>
        <span class="s1">.kneighbors_graph(n_neighbors=n_neighbors</span><span class="s0">, </span><span class="s1">mode=</span><span class="s4">&quot;distance&quot;</span><span class="s1">)</span>
    <span class="s1">)</span>
    <span class="s1">P_bh = _joint_probabilities_nn(distances_csr</span><span class="s0">, </span><span class="s1">perplexity</span><span class="s0">, </span><span class="s1">verbose=</span><span class="s3">0</span><span class="s1">)</span>
    <span class="s1">kl_sequential</span><span class="s0">, </span><span class="s1">grad_sequential = _kl_divergence_bh(</span>
        <span class="s1">params</span><span class="s0">,</span>
        <span class="s1">P_bh</span><span class="s0">,</span>
        <span class="s1">degrees_of_freedom</span><span class="s0">,</span>
        <span class="s1">n_samples</span><span class="s0">,</span>
        <span class="s1">n_components</span><span class="s0">,</span>
        <span class="s1">angle=angle</span><span class="s0">,</span>
        <span class="s1">skip_num_points=</span><span class="s3">0</span><span class="s0">,</span>
        <span class="s1">verbose=</span><span class="s3">0</span><span class="s0">,</span>
        <span class="s1">num_threads=</span><span class="s3">1</span><span class="s0">,</span>
    <span class="s1">)</span>
    <span class="s0">for </span><span class="s1">num_threads </span><span class="s0">in </span><span class="s1">[</span><span class="s3">2</span><span class="s0">, </span><span class="s3">4</span><span class="s1">]:</span>
        <span class="s1">kl_multithread</span><span class="s0">, </span><span class="s1">grad_multithread = _kl_divergence_bh(</span>
            <span class="s1">params</span><span class="s0">,</span>
            <span class="s1">P_bh</span><span class="s0">,</span>
            <span class="s1">degrees_of_freedom</span><span class="s0">,</span>
            <span class="s1">n_samples</span><span class="s0">,</span>
            <span class="s1">n_components</span><span class="s0">,</span>
            <span class="s1">angle=angle</span><span class="s0">,</span>
            <span class="s1">skip_num_points=</span><span class="s3">0</span><span class="s0">,</span>
            <span class="s1">verbose=</span><span class="s3">0</span><span class="s0">,</span>
            <span class="s1">num_threads=num_threads</span><span class="s0">,</span>
        <span class="s1">)</span>

        <span class="s1">assert_allclose(kl_multithread</span><span class="s0">, </span><span class="s1">kl_sequential</span><span class="s0">, </span><span class="s1">rtol=</span><span class="s3">1e-6</span><span class="s1">)</span>
        <span class="s1">assert_allclose(grad_multithread</span><span class="s0">, </span><span class="s1">grad_multithread)</span>


<span class="s1">@pytest.mark.parametrize(</span>
    <span class="s4">&quot;metric, dist_func&quot;</span><span class="s0">,</span>
    <span class="s1">[(</span><span class="s4">&quot;manhattan&quot;</span><span class="s0">, </span><span class="s1">manhattan_distances)</span><span class="s0">, </span><span class="s1">(</span><span class="s4">&quot;cosine&quot;</span><span class="s0">, </span><span class="s1">cosine_distances)]</span><span class="s0">,</span>
<span class="s1">)</span>
<span class="s1">@pytest.mark.parametrize(</span><span class="s4">&quot;method&quot;</span><span class="s0">, </span><span class="s1">[</span><span class="s4">&quot;barnes_hut&quot;</span><span class="s0">, </span><span class="s4">&quot;exact&quot;</span><span class="s1">])</span>
<span class="s0">def </span><span class="s1">test_tsne_with_different_distance_metrics(metric</span><span class="s0">, </span><span class="s1">dist_func</span><span class="s0">, </span><span class="s1">method):</span>
    <span class="s5">&quot;&quot;&quot;Make sure that TSNE works for different distance metrics&quot;&quot;&quot;</span>

    <span class="s0">if </span><span class="s1">method == </span><span class="s4">&quot;barnes_hut&quot; </span><span class="s0">and </span><span class="s1">metric == </span><span class="s4">&quot;manhattan&quot;</span><span class="s1">:</span>
        <span class="s2"># The distances computed by `manhattan_distances` differ slightly from those</span>
        <span class="s2"># computed internally by NearestNeighbors via the PairwiseDistancesReduction</span>
        <span class="s2"># Cython code-based. This in turns causes T-SNE to converge to a different</span>
        <span class="s2"># solution but this should not impact the qualitative results as both</span>
        <span class="s2"># methods.</span>
        <span class="s2"># NOTE: it's probably not valid from a mathematical point of view to use the</span>
        <span class="s2"># Manhattan distance for T-SNE...</span>
        <span class="s2"># TODO: re-enable this test if/when `manhattan_distances` is refactored to</span>
        <span class="s2"># reuse the same underlying Cython code NearestNeighbors.</span>
        <span class="s2"># For reference, see:</span>
        <span class="s2"># https://github.com/scikit-learn/scikit-learn/pull/23865/files#r925721573</span>
        <span class="s1">pytest.xfail(</span>
            <span class="s4">&quot;Distance computations are different for method == 'barnes_hut' and metric&quot;</span>
            <span class="s4">&quot; == 'manhattan', but this is expected.&quot;</span>
        <span class="s1">)</span>

    <span class="s1">random_state = check_random_state(</span><span class="s3">0</span><span class="s1">)</span>
    <span class="s1">n_components_original = </span><span class="s3">3</span>
    <span class="s1">n_components_embedding = </span><span class="s3">2</span>
    <span class="s1">X = random_state.randn(</span><span class="s3">50</span><span class="s0">, </span><span class="s1">n_components_original).astype(np.float32)</span>
    <span class="s1">X_transformed_tsne = TSNE(</span>
        <span class="s1">metric=metric</span><span class="s0">,</span>
        <span class="s1">method=method</span><span class="s0">,</span>
        <span class="s1">n_components=n_components_embedding</span><span class="s0">,</span>
        <span class="s1">random_state=</span><span class="s3">0</span><span class="s0">,</span>
        <span class="s1">n_iter=</span><span class="s3">300</span><span class="s0">,</span>
        <span class="s1">init=</span><span class="s4">&quot;random&quot;</span><span class="s0">,</span>
        <span class="s1">learning_rate=</span><span class="s4">&quot;auto&quot;</span><span class="s0">,</span>
    <span class="s1">).fit_transform(X)</span>
    <span class="s1">X_transformed_tsne_precomputed = TSNE(</span>
        <span class="s1">metric=</span><span class="s4">&quot;precomputed&quot;</span><span class="s0">,</span>
        <span class="s1">method=method</span><span class="s0">,</span>
        <span class="s1">n_components=n_components_embedding</span><span class="s0">,</span>
        <span class="s1">random_state=</span><span class="s3">0</span><span class="s0">,</span>
        <span class="s1">n_iter=</span><span class="s3">300</span><span class="s0">,</span>
        <span class="s1">init=</span><span class="s4">&quot;random&quot;</span><span class="s0">,</span>
        <span class="s1">learning_rate=</span><span class="s4">&quot;auto&quot;</span><span class="s0">,</span>
    <span class="s1">).fit_transform(dist_func(X))</span>
    <span class="s1">assert_array_equal(X_transformed_tsne</span><span class="s0">, </span><span class="s1">X_transformed_tsne_precomputed)</span>


<span class="s1">@pytest.mark.parametrize(</span><span class="s4">&quot;method&quot;</span><span class="s0">, </span><span class="s1">[</span><span class="s4">&quot;exact&quot;</span><span class="s0">, </span><span class="s4">&quot;barnes_hut&quot;</span><span class="s1">])</span>
<span class="s0">def </span><span class="s1">test_tsne_n_jobs(method):</span>
    <span class="s5">&quot;&quot;&quot;Make sure that the n_jobs parameter doesn't impact the output&quot;&quot;&quot;</span>
    <span class="s1">random_state = check_random_state(</span><span class="s3">0</span><span class="s1">)</span>
    <span class="s1">n_features = </span><span class="s3">10</span>
    <span class="s1">X = random_state.randn(</span><span class="s3">30</span><span class="s0">, </span><span class="s1">n_features)</span>
    <span class="s1">X_tr_ref = TSNE(</span>
        <span class="s1">n_components=</span><span class="s3">2</span><span class="s0">,</span>
        <span class="s1">method=method</span><span class="s0">,</span>
        <span class="s1">perplexity=</span><span class="s3">25.0</span><span class="s0">,</span>
        <span class="s1">angle=</span><span class="s3">0</span><span class="s0">,</span>
        <span class="s1">n_jobs=</span><span class="s3">1</span><span class="s0">,</span>
        <span class="s1">random_state=</span><span class="s3">0</span><span class="s0">,</span>
        <span class="s1">init=</span><span class="s4">&quot;random&quot;</span><span class="s0">,</span>
        <span class="s1">learning_rate=</span><span class="s4">&quot;auto&quot;</span><span class="s0">,</span>
    <span class="s1">).fit_transform(X)</span>
    <span class="s1">X_tr = TSNE(</span>
        <span class="s1">n_components=</span><span class="s3">2</span><span class="s0">,</span>
        <span class="s1">method=method</span><span class="s0">,</span>
        <span class="s1">perplexity=</span><span class="s3">25.0</span><span class="s0">,</span>
        <span class="s1">angle=</span><span class="s3">0</span><span class="s0">,</span>
        <span class="s1">n_jobs=</span><span class="s3">2</span><span class="s0">,</span>
        <span class="s1">random_state=</span><span class="s3">0</span><span class="s0">,</span>
        <span class="s1">init=</span><span class="s4">&quot;random&quot;</span><span class="s0">,</span>
        <span class="s1">learning_rate=</span><span class="s4">&quot;auto&quot;</span><span class="s0">,</span>
    <span class="s1">).fit_transform(X)</span>

    <span class="s1">assert_allclose(X_tr_ref</span><span class="s0">, </span><span class="s1">X_tr)</span>


<span class="s0">def </span><span class="s1">test_tsne_with_mahalanobis_distance():</span>
    <span class="s5">&quot;&quot;&quot;Make sure that method_parameters works with mahalanobis distance.&quot;&quot;&quot;</span>
    <span class="s1">random_state = check_random_state(</span><span class="s3">0</span><span class="s1">)</span>
    <span class="s1">n_samples</span><span class="s0">, </span><span class="s1">n_features = </span><span class="s3">300</span><span class="s0">, </span><span class="s3">10</span>
    <span class="s1">X = random_state.randn(n_samples</span><span class="s0">, </span><span class="s1">n_features)</span>
    <span class="s1">default_params = {</span>
        <span class="s4">&quot;perplexity&quot;</span><span class="s1">: </span><span class="s3">40</span><span class="s0">,</span>
        <span class="s4">&quot;n_iter&quot;</span><span class="s1">: </span><span class="s3">250</span><span class="s0">,</span>
        <span class="s4">&quot;learning_rate&quot;</span><span class="s1">: </span><span class="s4">&quot;auto&quot;</span><span class="s0">,</span>
        <span class="s4">&quot;init&quot;</span><span class="s1">: </span><span class="s4">&quot;random&quot;</span><span class="s0">,</span>
        <span class="s4">&quot;n_components&quot;</span><span class="s1">: </span><span class="s3">3</span><span class="s0">,</span>
        <span class="s4">&quot;random_state&quot;</span><span class="s1">: </span><span class="s3">0</span><span class="s0">,</span>
    <span class="s1">}</span>

    <span class="s1">tsne = TSNE(metric=</span><span class="s4">&quot;mahalanobis&quot;</span><span class="s0">, </span><span class="s1">**default_params)</span>
    <span class="s1">msg = </span><span class="s4">&quot;Must provide either V or VI for Mahalanobis distance&quot;</span>
    <span class="s0">with </span><span class="s1">pytest.raises(ValueError</span><span class="s0">, </span><span class="s1">match=msg):</span>
        <span class="s1">tsne.fit_transform(X)</span>

    <span class="s1">precomputed_X = squareform(pdist(X</span><span class="s0">, </span><span class="s1">metric=</span><span class="s4">&quot;mahalanobis&quot;</span><span class="s1">)</span><span class="s0">, </span><span class="s1">checks=</span><span class="s0">True</span><span class="s1">)</span>
    <span class="s1">X_trans_expected = TSNE(metric=</span><span class="s4">&quot;precomputed&quot;</span><span class="s0">, </span><span class="s1">**default_params).fit_transform(</span>
        <span class="s1">precomputed_X</span>
    <span class="s1">)</span>

    <span class="s1">X_trans = TSNE(</span>
        <span class="s1">metric=</span><span class="s4">&quot;mahalanobis&quot;</span><span class="s0">, </span><span class="s1">metric_params={</span><span class="s4">&quot;V&quot;</span><span class="s1">: np.cov(X.T)}</span><span class="s0">, </span><span class="s1">**default_params</span>
    <span class="s1">).fit_transform(X)</span>
    <span class="s1">assert_allclose(X_trans</span><span class="s0">, </span><span class="s1">X_trans_expected)</span>


<span class="s1">@pytest.mark.parametrize(</span><span class="s4">&quot;perplexity&quot;</span><span class="s0">, </span><span class="s1">(</span><span class="s3">20</span><span class="s0">, </span><span class="s3">30</span><span class="s1">))</span>
<span class="s0">def </span><span class="s1">test_tsne_perplexity_validation(perplexity):</span>
    <span class="s5">&quot;&quot;&quot;Make sure that perplexity &gt; n_samples results in a ValueError&quot;&quot;&quot;</span>

    <span class="s1">random_state = check_random_state(</span><span class="s3">0</span><span class="s1">)</span>
    <span class="s1">X = random_state.randn(</span><span class="s3">20</span><span class="s0">, </span><span class="s3">2</span><span class="s1">)</span>
    <span class="s1">est = TSNE(</span>
        <span class="s1">learning_rate=</span><span class="s4">&quot;auto&quot;</span><span class="s0">,</span>
        <span class="s1">init=</span><span class="s4">&quot;pca&quot;</span><span class="s0">,</span>
        <span class="s1">perplexity=perplexity</span><span class="s0">,</span>
        <span class="s1">random_state=random_state</span><span class="s0">,</span>
    <span class="s1">)</span>
    <span class="s1">msg = </span><span class="s4">&quot;perplexity must be less than n_samples&quot;</span>
    <span class="s0">with </span><span class="s1">pytest.raises(ValueError</span><span class="s0">, </span><span class="s1">match=msg):</span>
        <span class="s1">est.fit_transform(X)</span>


<span class="s0">def </span><span class="s1">test_tsne_works_with_pandas_output():</span>
    <span class="s5">&quot;&quot;&quot;Make sure that TSNE works when the output is set to &quot;pandas&quot;. 
 
    Non-regression test for gh-25365. 
    &quot;&quot;&quot;</span>
    <span class="s1">pytest.importorskip(</span><span class="s4">&quot;pandas&quot;</span><span class="s1">)</span>
    <span class="s0">with </span><span class="s1">config_context(transform_output=</span><span class="s4">&quot;pandas&quot;</span><span class="s1">):</span>
        <span class="s1">arr = np.arange(</span><span class="s3">35 </span><span class="s1">* </span><span class="s3">4</span><span class="s1">).reshape(</span><span class="s3">35</span><span class="s0">, </span><span class="s3">4</span><span class="s1">)</span>
        <span class="s1">TSNE(n_components=</span><span class="s3">2</span><span class="s1">).fit_transform(arr)</span>
</pre>
</body>
</html>