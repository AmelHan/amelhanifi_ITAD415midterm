<html>
<head>
<title>test_sgd.py</title>
<meta http-equiv="Content-Type" content="text/html; charset=utf-8">
<style type="text/css">
.s0 { color: #cc7832;}
.s1 { color: #a9b7c6;}
.s2 { color: #6a8759;}
.s3 { color: #6897bb;}
.s4 { color: #808080;}
.s5 { color: #629755; font-style: italic;}
</style>
</head>
<body bgcolor="#2b2b2b">
<table CELLSPACING=0 CELLPADDING=5 COLS=1 WIDTH="100%" BGCOLOR="#606060" >
<tr><td><center>
<font face="Arial, Helvetica" color="#000000">
test_sgd.py</font>
</center></td></tr></table>
<pre><span class="s0">import </span><span class="s1">pickle</span>
<span class="s0">from </span><span class="s1">unittest.mock </span><span class="s0">import </span><span class="s1">Mock</span>

<span class="s0">import </span><span class="s1">joblib</span>
<span class="s0">import </span><span class="s1">numpy </span><span class="s0">as </span><span class="s1">np</span>
<span class="s0">import </span><span class="s1">pytest</span>
<span class="s0">import </span><span class="s1">scipy.sparse </span><span class="s0">as </span><span class="s1">sp</span>

<span class="s0">from </span><span class="s1">sklearn </span><span class="s0">import </span><span class="s1">datasets</span><span class="s0">, </span><span class="s1">linear_model</span><span class="s0">, </span><span class="s1">metrics</span>
<span class="s0">from </span><span class="s1">sklearn.base </span><span class="s0">import </span><span class="s1">clone</span><span class="s0">, </span><span class="s1">is_classifier</span>
<span class="s0">from </span><span class="s1">sklearn.exceptions </span><span class="s0">import </span><span class="s1">ConvergenceWarning</span>
<span class="s0">from </span><span class="s1">sklearn.kernel_approximation </span><span class="s0">import </span><span class="s1">Nystroem</span>
<span class="s0">from </span><span class="s1">sklearn.linear_model </span><span class="s0">import </span><span class="s1">_sgd_fast </span><span class="s0">as </span><span class="s1">sgd_fast</span>
<span class="s0">from </span><span class="s1">sklearn.linear_model </span><span class="s0">import </span><span class="s1">_stochastic_gradient</span>
<span class="s0">from </span><span class="s1">sklearn.model_selection </span><span class="s0">import </span><span class="s1">(</span>
    <span class="s1">RandomizedSearchCV</span><span class="s0">,</span>
    <span class="s1">ShuffleSplit</span><span class="s0">,</span>
    <span class="s1">StratifiedShuffleSplit</span><span class="s0">,</span>
<span class="s1">)</span>
<span class="s0">from </span><span class="s1">sklearn.pipeline </span><span class="s0">import </span><span class="s1">make_pipeline</span>
<span class="s0">from </span><span class="s1">sklearn.preprocessing </span><span class="s0">import </span><span class="s1">LabelEncoder</span><span class="s0">, </span><span class="s1">MinMaxScaler</span><span class="s0">, </span><span class="s1">StandardScaler</span><span class="s0">, </span><span class="s1">scale</span>
<span class="s0">from </span><span class="s1">sklearn.svm </span><span class="s0">import </span><span class="s1">OneClassSVM</span>
<span class="s0">from </span><span class="s1">sklearn.utils._testing </span><span class="s0">import </span><span class="s1">(</span>
    <span class="s1">assert_allclose</span><span class="s0">,</span>
    <span class="s1">assert_almost_equal</span><span class="s0">,</span>
    <span class="s1">assert_array_almost_equal</span><span class="s0">,</span>
    <span class="s1">assert_array_equal</span><span class="s0">,</span>
    <span class="s1">ignore_warnings</span><span class="s0">,</span>
<span class="s1">)</span>


<span class="s0">def </span><span class="s1">_update_kwargs(kwargs):</span>
    <span class="s0">if </span><span class="s2">&quot;random_state&quot; </span><span class="s0">not in </span><span class="s1">kwargs:</span>
        <span class="s1">kwargs[</span><span class="s2">&quot;random_state&quot;</span><span class="s1">] = </span><span class="s3">42</span>

    <span class="s0">if </span><span class="s2">&quot;tol&quot; </span><span class="s0">not in </span><span class="s1">kwargs:</span>
        <span class="s1">kwargs[</span><span class="s2">&quot;tol&quot;</span><span class="s1">] = </span><span class="s0">None</span>
    <span class="s0">if </span><span class="s2">&quot;max_iter&quot; </span><span class="s0">not in </span><span class="s1">kwargs:</span>
        <span class="s1">kwargs[</span><span class="s2">&quot;max_iter&quot;</span><span class="s1">] = </span><span class="s3">5</span>


<span class="s0">class </span><span class="s1">_SparseSGDClassifier(linear_model.SGDClassifier):</span>
    <span class="s0">def </span><span class="s1">fit(self</span><span class="s0">, </span><span class="s1">X</span><span class="s0">, </span><span class="s1">y</span><span class="s0">, </span><span class="s1">*args</span><span class="s0">, </span><span class="s1">**kw):</span>
        <span class="s1">X = sp.csr_matrix(X)</span>
        <span class="s0">return </span><span class="s1">super().fit(X</span><span class="s0">, </span><span class="s1">y</span><span class="s0">, </span><span class="s1">*args</span><span class="s0">, </span><span class="s1">**kw)</span>

    <span class="s0">def </span><span class="s1">partial_fit(self</span><span class="s0">, </span><span class="s1">X</span><span class="s0">, </span><span class="s1">y</span><span class="s0">, </span><span class="s1">*args</span><span class="s0">, </span><span class="s1">**kw):</span>
        <span class="s1">X = sp.csr_matrix(X)</span>
        <span class="s0">return </span><span class="s1">super().partial_fit(X</span><span class="s0">, </span><span class="s1">y</span><span class="s0">, </span><span class="s1">*args</span><span class="s0">, </span><span class="s1">**kw)</span>

    <span class="s0">def </span><span class="s1">decision_function(self</span><span class="s0">, </span><span class="s1">X):</span>
        <span class="s1">X = sp.csr_matrix(X)</span>
        <span class="s0">return </span><span class="s1">super().decision_function(X)</span>

    <span class="s0">def </span><span class="s1">predict_proba(self</span><span class="s0">, </span><span class="s1">X):</span>
        <span class="s1">X = sp.csr_matrix(X)</span>
        <span class="s0">return </span><span class="s1">super().predict_proba(X)</span>


<span class="s0">class </span><span class="s1">_SparseSGDRegressor(linear_model.SGDRegressor):</span>
    <span class="s0">def </span><span class="s1">fit(self</span><span class="s0">, </span><span class="s1">X</span><span class="s0">, </span><span class="s1">y</span><span class="s0">, </span><span class="s1">*args</span><span class="s0">, </span><span class="s1">**kw):</span>
        <span class="s1">X = sp.csr_matrix(X)</span>
        <span class="s0">return </span><span class="s1">linear_model.SGDRegressor.fit(self</span><span class="s0">, </span><span class="s1">X</span><span class="s0">, </span><span class="s1">y</span><span class="s0">, </span><span class="s1">*args</span><span class="s0">, </span><span class="s1">**kw)</span>

    <span class="s0">def </span><span class="s1">partial_fit(self</span><span class="s0">, </span><span class="s1">X</span><span class="s0">, </span><span class="s1">y</span><span class="s0">, </span><span class="s1">*args</span><span class="s0">, </span><span class="s1">**kw):</span>
        <span class="s1">X = sp.csr_matrix(X)</span>
        <span class="s0">return </span><span class="s1">linear_model.SGDRegressor.partial_fit(self</span><span class="s0">, </span><span class="s1">X</span><span class="s0">, </span><span class="s1">y</span><span class="s0">, </span><span class="s1">*args</span><span class="s0">, </span><span class="s1">**kw)</span>

    <span class="s0">def </span><span class="s1">decision_function(self</span><span class="s0">, </span><span class="s1">X</span><span class="s0">, </span><span class="s1">*args</span><span class="s0">, </span><span class="s1">**kw):</span>
        <span class="s4"># XXX untested as of v0.22</span>
        <span class="s1">X = sp.csr_matrix(X)</span>
        <span class="s0">return </span><span class="s1">linear_model.SGDRegressor.decision_function(self</span><span class="s0">, </span><span class="s1">X</span><span class="s0">, </span><span class="s1">*args</span><span class="s0">, </span><span class="s1">**kw)</span>


<span class="s0">class </span><span class="s1">_SparseSGDOneClassSVM(linear_model.SGDOneClassSVM):</span>
    <span class="s0">def </span><span class="s1">fit(self</span><span class="s0">, </span><span class="s1">X</span><span class="s0">, </span><span class="s1">*args</span><span class="s0">, </span><span class="s1">**kw):</span>
        <span class="s1">X = sp.csr_matrix(X)</span>
        <span class="s0">return </span><span class="s1">linear_model.SGDOneClassSVM.fit(self</span><span class="s0">, </span><span class="s1">X</span><span class="s0">, </span><span class="s1">*args</span><span class="s0">, </span><span class="s1">**kw)</span>

    <span class="s0">def </span><span class="s1">partial_fit(self</span><span class="s0">, </span><span class="s1">X</span><span class="s0">, </span><span class="s1">*args</span><span class="s0">, </span><span class="s1">**kw):</span>
        <span class="s1">X = sp.csr_matrix(X)</span>
        <span class="s0">return </span><span class="s1">linear_model.SGDOneClassSVM.partial_fit(self</span><span class="s0">, </span><span class="s1">X</span><span class="s0">, </span><span class="s1">*args</span><span class="s0">, </span><span class="s1">**kw)</span>

    <span class="s0">def </span><span class="s1">decision_function(self</span><span class="s0">, </span><span class="s1">X</span><span class="s0">, </span><span class="s1">*args</span><span class="s0">, </span><span class="s1">**kw):</span>
        <span class="s1">X = sp.csr_matrix(X)</span>
        <span class="s0">return </span><span class="s1">linear_model.SGDOneClassSVM.decision_function(self</span><span class="s0">, </span><span class="s1">X</span><span class="s0">, </span><span class="s1">*args</span><span class="s0">, </span><span class="s1">**kw)</span>


<span class="s0">def </span><span class="s1">SGDClassifier(**kwargs):</span>
    <span class="s1">_update_kwargs(kwargs)</span>
    <span class="s0">return </span><span class="s1">linear_model.SGDClassifier(**kwargs)</span>


<span class="s0">def </span><span class="s1">SGDRegressor(**kwargs):</span>
    <span class="s1">_update_kwargs(kwargs)</span>
    <span class="s0">return </span><span class="s1">linear_model.SGDRegressor(**kwargs)</span>


<span class="s0">def </span><span class="s1">SGDOneClassSVM(**kwargs):</span>
    <span class="s1">_update_kwargs(kwargs)</span>
    <span class="s0">return </span><span class="s1">linear_model.SGDOneClassSVM(**kwargs)</span>


<span class="s0">def </span><span class="s1">SparseSGDClassifier(**kwargs):</span>
    <span class="s1">_update_kwargs(kwargs)</span>
    <span class="s0">return </span><span class="s1">_SparseSGDClassifier(**kwargs)</span>


<span class="s0">def </span><span class="s1">SparseSGDRegressor(**kwargs):</span>
    <span class="s1">_update_kwargs(kwargs)</span>
    <span class="s0">return </span><span class="s1">_SparseSGDRegressor(**kwargs)</span>


<span class="s0">def </span><span class="s1">SparseSGDOneClassSVM(**kwargs):</span>
    <span class="s1">_update_kwargs(kwargs)</span>
    <span class="s0">return </span><span class="s1">_SparseSGDOneClassSVM(**kwargs)</span>


<span class="s4"># Test Data</span>

<span class="s4"># test sample 1</span>
<span class="s1">X = np.array([[-</span><span class="s3">2</span><span class="s0">, </span><span class="s1">-</span><span class="s3">1</span><span class="s1">]</span><span class="s0">, </span><span class="s1">[-</span><span class="s3">1</span><span class="s0">, </span><span class="s1">-</span><span class="s3">1</span><span class="s1">]</span><span class="s0">, </span><span class="s1">[-</span><span class="s3">1</span><span class="s0">, </span><span class="s1">-</span><span class="s3">2</span><span class="s1">]</span><span class="s0">, </span><span class="s1">[</span><span class="s3">1</span><span class="s0">, </span><span class="s3">1</span><span class="s1">]</span><span class="s0">, </span><span class="s1">[</span><span class="s3">1</span><span class="s0">, </span><span class="s3">2</span><span class="s1">]</span><span class="s0">, </span><span class="s1">[</span><span class="s3">2</span><span class="s0">, </span><span class="s3">1</span><span class="s1">]])</span>
<span class="s1">Y = [</span><span class="s3">1</span><span class="s0">, </span><span class="s3">1</span><span class="s0">, </span><span class="s3">1</span><span class="s0">, </span><span class="s3">2</span><span class="s0">, </span><span class="s3">2</span><span class="s0">, </span><span class="s3">2</span><span class="s1">]</span>
<span class="s1">T = np.array([[-</span><span class="s3">1</span><span class="s0">, </span><span class="s1">-</span><span class="s3">1</span><span class="s1">]</span><span class="s0">, </span><span class="s1">[</span><span class="s3">2</span><span class="s0">, </span><span class="s3">2</span><span class="s1">]</span><span class="s0">, </span><span class="s1">[</span><span class="s3">3</span><span class="s0">, </span><span class="s3">2</span><span class="s1">]])</span>
<span class="s1">true_result = [</span><span class="s3">1</span><span class="s0">, </span><span class="s3">2</span><span class="s0">, </span><span class="s3">2</span><span class="s1">]</span>

<span class="s4"># test sample 2; string class labels</span>
<span class="s1">X2 = np.array(</span>
    <span class="s1">[</span>
        <span class="s1">[-</span><span class="s3">1</span><span class="s0">, </span><span class="s3">1</span><span class="s1">]</span><span class="s0">,</span>
        <span class="s1">[-</span><span class="s3">0.75</span><span class="s0">, </span><span class="s3">0.5</span><span class="s1">]</span><span class="s0">,</span>
        <span class="s1">[-</span><span class="s3">1.5</span><span class="s0">, </span><span class="s3">1.5</span><span class="s1">]</span><span class="s0">,</span>
        <span class="s1">[</span><span class="s3">1</span><span class="s0">, </span><span class="s3">1</span><span class="s1">]</span><span class="s0">,</span>
        <span class="s1">[</span><span class="s3">0.75</span><span class="s0">, </span><span class="s3">0.5</span><span class="s1">]</span><span class="s0">,</span>
        <span class="s1">[</span><span class="s3">1.5</span><span class="s0">, </span><span class="s3">1.5</span><span class="s1">]</span><span class="s0">,</span>
        <span class="s1">[-</span><span class="s3">1</span><span class="s0">, </span><span class="s1">-</span><span class="s3">1</span><span class="s1">]</span><span class="s0">,</span>
        <span class="s1">[</span><span class="s3">0</span><span class="s0">, </span><span class="s1">-</span><span class="s3">0.5</span><span class="s1">]</span><span class="s0">,</span>
        <span class="s1">[</span><span class="s3">1</span><span class="s0">, </span><span class="s1">-</span><span class="s3">1</span><span class="s1">]</span><span class="s0">,</span>
    <span class="s1">]</span>
<span class="s1">)</span>
<span class="s1">Y2 = [</span><span class="s2">&quot;one&quot;</span><span class="s1">] * </span><span class="s3">3 </span><span class="s1">+ [</span><span class="s2">&quot;two&quot;</span><span class="s1">] * </span><span class="s3">3 </span><span class="s1">+ [</span><span class="s2">&quot;three&quot;</span><span class="s1">] * </span><span class="s3">3</span>
<span class="s1">T2 = np.array([[-</span><span class="s3">1.5</span><span class="s0">, </span><span class="s3">0.5</span><span class="s1">]</span><span class="s0">, </span><span class="s1">[</span><span class="s3">1</span><span class="s0">, </span><span class="s3">2</span><span class="s1">]</span><span class="s0">, </span><span class="s1">[</span><span class="s3">0</span><span class="s0">, </span><span class="s1">-</span><span class="s3">2</span><span class="s1">]])</span>
<span class="s1">true_result2 = [</span><span class="s2">&quot;one&quot;</span><span class="s0">, </span><span class="s2">&quot;two&quot;</span><span class="s0">, </span><span class="s2">&quot;three&quot;</span><span class="s1">]</span>

<span class="s4"># test sample 3</span>
<span class="s1">X3 = np.array(</span>
    <span class="s1">[</span>
        <span class="s1">[</span><span class="s3">1</span><span class="s0">, </span><span class="s3">1</span><span class="s0">, </span><span class="s3">0</span><span class="s0">, </span><span class="s3">0</span><span class="s0">, </span><span class="s3">0</span><span class="s0">, </span><span class="s3">0</span><span class="s1">]</span><span class="s0">,</span>
        <span class="s1">[</span><span class="s3">1</span><span class="s0">, </span><span class="s3">1</span><span class="s0">, </span><span class="s3">0</span><span class="s0">, </span><span class="s3">0</span><span class="s0">, </span><span class="s3">0</span><span class="s0">, </span><span class="s3">0</span><span class="s1">]</span><span class="s0">,</span>
        <span class="s1">[</span><span class="s3">0</span><span class="s0">, </span><span class="s3">0</span><span class="s0">, </span><span class="s3">1</span><span class="s0">, </span><span class="s3">0</span><span class="s0">, </span><span class="s3">0</span><span class="s0">, </span><span class="s3">0</span><span class="s1">]</span><span class="s0">,</span>
        <span class="s1">[</span><span class="s3">0</span><span class="s0">, </span><span class="s3">0</span><span class="s0">, </span><span class="s3">1</span><span class="s0">, </span><span class="s3">0</span><span class="s0">, </span><span class="s3">0</span><span class="s0">, </span><span class="s3">0</span><span class="s1">]</span><span class="s0">,</span>
        <span class="s1">[</span><span class="s3">0</span><span class="s0">, </span><span class="s3">0</span><span class="s0">, </span><span class="s3">0</span><span class="s0">, </span><span class="s3">0</span><span class="s0">, </span><span class="s3">1</span><span class="s0">, </span><span class="s3">1</span><span class="s1">]</span><span class="s0">,</span>
        <span class="s1">[</span><span class="s3">0</span><span class="s0">, </span><span class="s3">0</span><span class="s0">, </span><span class="s3">0</span><span class="s0">, </span><span class="s3">0</span><span class="s0">, </span><span class="s3">1</span><span class="s0">, </span><span class="s3">1</span><span class="s1">]</span><span class="s0">,</span>
        <span class="s1">[</span><span class="s3">0</span><span class="s0">, </span><span class="s3">0</span><span class="s0">, </span><span class="s3">0</span><span class="s0">, </span><span class="s3">1</span><span class="s0">, </span><span class="s3">0</span><span class="s0">, </span><span class="s3">0</span><span class="s1">]</span><span class="s0">,</span>
        <span class="s1">[</span><span class="s3">0</span><span class="s0">, </span><span class="s3">0</span><span class="s0">, </span><span class="s3">0</span><span class="s0">, </span><span class="s3">1</span><span class="s0">, </span><span class="s3">0</span><span class="s0">, </span><span class="s3">0</span><span class="s1">]</span><span class="s0">,</span>
    <span class="s1">]</span>
<span class="s1">)</span>
<span class="s1">Y3 = np.array([</span><span class="s3">1</span><span class="s0">, </span><span class="s3">1</span><span class="s0">, </span><span class="s3">1</span><span class="s0">, </span><span class="s3">1</span><span class="s0">, </span><span class="s3">2</span><span class="s0">, </span><span class="s3">2</span><span class="s0">, </span><span class="s3">2</span><span class="s0">, </span><span class="s3">2</span><span class="s1">])</span>

<span class="s4"># test sample 4 - two more or less redundant feature groups</span>
<span class="s1">X4 = np.array(</span>
    <span class="s1">[</span>
        <span class="s1">[</span><span class="s3">1</span><span class="s0">, </span><span class="s3">0.9</span><span class="s0">, </span><span class="s3">0.8</span><span class="s0">, </span><span class="s3">0</span><span class="s0">, </span><span class="s3">0</span><span class="s0">, </span><span class="s3">0</span><span class="s1">]</span><span class="s0">,</span>
        <span class="s1">[</span><span class="s3">1</span><span class="s0">, </span><span class="s3">0.84</span><span class="s0">, </span><span class="s3">0.98</span><span class="s0">, </span><span class="s3">0</span><span class="s0">, </span><span class="s3">0</span><span class="s0">, </span><span class="s3">0</span><span class="s1">]</span><span class="s0">,</span>
        <span class="s1">[</span><span class="s3">1</span><span class="s0">, </span><span class="s3">0.96</span><span class="s0">, </span><span class="s3">0.88</span><span class="s0">, </span><span class="s3">0</span><span class="s0">, </span><span class="s3">0</span><span class="s0">, </span><span class="s3">0</span><span class="s1">]</span><span class="s0">,</span>
        <span class="s1">[</span><span class="s3">1</span><span class="s0">, </span><span class="s3">0.91</span><span class="s0">, </span><span class="s3">0.99</span><span class="s0">, </span><span class="s3">0</span><span class="s0">, </span><span class="s3">0</span><span class="s0">, </span><span class="s3">0</span><span class="s1">]</span><span class="s0">,</span>
        <span class="s1">[</span><span class="s3">0</span><span class="s0">, </span><span class="s3">0</span><span class="s0">, </span><span class="s3">0</span><span class="s0">, </span><span class="s3">0.89</span><span class="s0">, </span><span class="s3">0.91</span><span class="s0">, </span><span class="s3">1</span><span class="s1">]</span><span class="s0">,</span>
        <span class="s1">[</span><span class="s3">0</span><span class="s0">, </span><span class="s3">0</span><span class="s0">, </span><span class="s3">0</span><span class="s0">, </span><span class="s3">0.79</span><span class="s0">, </span><span class="s3">0.84</span><span class="s0">, </span><span class="s3">1</span><span class="s1">]</span><span class="s0">,</span>
        <span class="s1">[</span><span class="s3">0</span><span class="s0">, </span><span class="s3">0</span><span class="s0">, </span><span class="s3">0</span><span class="s0">, </span><span class="s3">0.91</span><span class="s0">, </span><span class="s3">0.95</span><span class="s0">, </span><span class="s3">1</span><span class="s1">]</span><span class="s0">,</span>
        <span class="s1">[</span><span class="s3">0</span><span class="s0">, </span><span class="s3">0</span><span class="s0">, </span><span class="s3">0</span><span class="s0">, </span><span class="s3">0.93</span><span class="s0">, </span><span class="s3">1</span><span class="s0">, </span><span class="s3">1</span><span class="s1">]</span><span class="s0">,</span>
    <span class="s1">]</span>
<span class="s1">)</span>
<span class="s1">Y4 = np.array([</span><span class="s3">1</span><span class="s0">, </span><span class="s3">1</span><span class="s0">, </span><span class="s3">1</span><span class="s0">, </span><span class="s3">1</span><span class="s0">, </span><span class="s3">2</span><span class="s0">, </span><span class="s3">2</span><span class="s0">, </span><span class="s3">2</span><span class="s0">, </span><span class="s3">2</span><span class="s1">])</span>

<span class="s1">iris = datasets.load_iris()</span>

<span class="s4"># test sample 5 - test sample 1 as binary classification problem</span>
<span class="s1">X5 = np.array([[-</span><span class="s3">2</span><span class="s0">, </span><span class="s1">-</span><span class="s3">1</span><span class="s1">]</span><span class="s0">, </span><span class="s1">[-</span><span class="s3">1</span><span class="s0">, </span><span class="s1">-</span><span class="s3">1</span><span class="s1">]</span><span class="s0">, </span><span class="s1">[-</span><span class="s3">1</span><span class="s0">, </span><span class="s1">-</span><span class="s3">2</span><span class="s1">]</span><span class="s0">, </span><span class="s1">[</span><span class="s3">1</span><span class="s0">, </span><span class="s3">1</span><span class="s1">]</span><span class="s0">, </span><span class="s1">[</span><span class="s3">1</span><span class="s0">, </span><span class="s3">2</span><span class="s1">]</span><span class="s0">, </span><span class="s1">[</span><span class="s3">2</span><span class="s0">, </span><span class="s3">1</span><span class="s1">]])</span>
<span class="s1">Y5 = [</span><span class="s3">1</span><span class="s0">, </span><span class="s3">1</span><span class="s0">, </span><span class="s3">1</span><span class="s0">, </span><span class="s3">2</span><span class="s0">, </span><span class="s3">2</span><span class="s0">, </span><span class="s3">2</span><span class="s1">]</span>
<span class="s1">true_result5 = [</span><span class="s3">0</span><span class="s0">, </span><span class="s3">1</span><span class="s0">, </span><span class="s3">1</span><span class="s1">]</span>


<span class="s4">###############################################################################</span>
<span class="s4"># Common Test Case to classification and regression</span>


<span class="s4"># a simple implementation of ASGD to use for testing</span>
<span class="s4"># uses squared loss to find the gradient</span>
<span class="s0">def </span><span class="s1">asgd(klass</span><span class="s0">, </span><span class="s1">X</span><span class="s0">, </span><span class="s1">y</span><span class="s0">, </span><span class="s1">eta</span><span class="s0">, </span><span class="s1">alpha</span><span class="s0">, </span><span class="s1">weight_init=</span><span class="s0">None, </span><span class="s1">intercept_init=</span><span class="s3">0.0</span><span class="s1">):</span>
    <span class="s0">if </span><span class="s1">weight_init </span><span class="s0">is None</span><span class="s1">:</span>
        <span class="s1">weights = np.zeros(X.shape[</span><span class="s3">1</span><span class="s1">])</span>
    <span class="s0">else</span><span class="s1">:</span>
        <span class="s1">weights = weight_init</span>

    <span class="s1">average_weights = np.zeros(X.shape[</span><span class="s3">1</span><span class="s1">])</span>
    <span class="s1">intercept = intercept_init</span>
    <span class="s1">average_intercept = </span><span class="s3">0.0</span>
    <span class="s1">decay = </span><span class="s3">1.0</span>

    <span class="s4"># sparse data has a fixed decay of .01</span>
    <span class="s0">if </span><span class="s1">klass </span><span class="s0">in </span><span class="s1">(SparseSGDClassifier</span><span class="s0">, </span><span class="s1">SparseSGDRegressor):</span>
        <span class="s1">decay = </span><span class="s3">0.01</span>

    <span class="s0">for </span><span class="s1">i</span><span class="s0">, </span><span class="s1">entry </span><span class="s0">in </span><span class="s1">enumerate(X):</span>
        <span class="s1">p = np.dot(entry</span><span class="s0">, </span><span class="s1">weights)</span>
        <span class="s1">p += intercept</span>
        <span class="s1">gradient = p - y[i]</span>
        <span class="s1">weights *= </span><span class="s3">1.0 </span><span class="s1">- (eta * alpha)</span>
        <span class="s1">weights += -(eta * gradient * entry)</span>
        <span class="s1">intercept += -(eta * gradient) * decay</span>

        <span class="s1">average_weights *= i</span>
        <span class="s1">average_weights += weights</span>
        <span class="s1">average_weights /= i + </span><span class="s3">1.0</span>

        <span class="s1">average_intercept *= i</span>
        <span class="s1">average_intercept += intercept</span>
        <span class="s1">average_intercept /= i + </span><span class="s3">1.0</span>

    <span class="s0">return </span><span class="s1">average_weights</span><span class="s0">, </span><span class="s1">average_intercept</span>


<span class="s0">def </span><span class="s1">_test_warm_start(klass</span><span class="s0">, </span><span class="s1">X</span><span class="s0">, </span><span class="s1">Y</span><span class="s0">, </span><span class="s1">lr):</span>
    <span class="s4"># Test that explicit warm restart...</span>
    <span class="s1">clf = klass(alpha=</span><span class="s3">0.01</span><span class="s0">, </span><span class="s1">eta0=</span><span class="s3">0.01</span><span class="s0">, </span><span class="s1">shuffle=</span><span class="s0">False, </span><span class="s1">learning_rate=lr)</span>
    <span class="s1">clf.fit(X</span><span class="s0">, </span><span class="s1">Y)</span>

    <span class="s1">clf2 = klass(alpha=</span><span class="s3">0.001</span><span class="s0">, </span><span class="s1">eta0=</span><span class="s3">0.01</span><span class="s0">, </span><span class="s1">shuffle=</span><span class="s0">False, </span><span class="s1">learning_rate=lr)</span>
    <span class="s1">clf2.fit(X</span><span class="s0">, </span><span class="s1">Y</span><span class="s0">, </span><span class="s1">coef_init=clf.coef_.copy()</span><span class="s0">, </span><span class="s1">intercept_init=clf.intercept_.copy())</span>

    <span class="s4"># ... and implicit warm restart are equivalent.</span>
    <span class="s1">clf3 = klass(</span>
        <span class="s1">alpha=</span><span class="s3">0.01</span><span class="s0">, </span><span class="s1">eta0=</span><span class="s3">0.01</span><span class="s0">, </span><span class="s1">shuffle=</span><span class="s0">False, </span><span class="s1">warm_start=</span><span class="s0">True, </span><span class="s1">learning_rate=lr</span>
    <span class="s1">)</span>
    <span class="s1">clf3.fit(X</span><span class="s0">, </span><span class="s1">Y)</span>

    <span class="s0">assert </span><span class="s1">clf3.t_ == clf.t_</span>
    <span class="s1">assert_array_almost_equal(clf3.coef_</span><span class="s0">, </span><span class="s1">clf.coef_)</span>

    <span class="s1">clf3.set_params(alpha=</span><span class="s3">0.001</span><span class="s1">)</span>
    <span class="s1">clf3.fit(X</span><span class="s0">, </span><span class="s1">Y)</span>

    <span class="s0">assert </span><span class="s1">clf3.t_ == clf2.t_</span>
    <span class="s1">assert_array_almost_equal(clf3.coef_</span><span class="s0">, </span><span class="s1">clf2.coef_)</span>


<span class="s1">@pytest.mark.parametrize(</span>
    <span class="s2">&quot;klass&quot;</span><span class="s0">, </span><span class="s1">[SGDClassifier</span><span class="s0">, </span><span class="s1">SparseSGDClassifier</span><span class="s0">, </span><span class="s1">SGDRegressor</span><span class="s0">, </span><span class="s1">SparseSGDRegressor]</span>
<span class="s1">)</span>
<span class="s1">@pytest.mark.parametrize(</span><span class="s2">&quot;lr&quot;</span><span class="s0">, </span><span class="s1">[</span><span class="s2">&quot;constant&quot;</span><span class="s0">, </span><span class="s2">&quot;optimal&quot;</span><span class="s0">, </span><span class="s2">&quot;invscaling&quot;</span><span class="s0">, </span><span class="s2">&quot;adaptive&quot;</span><span class="s1">])</span>
<span class="s0">def </span><span class="s1">test_warm_start(klass</span><span class="s0">, </span><span class="s1">lr):</span>
    <span class="s1">_test_warm_start(klass</span><span class="s0">, </span><span class="s1">X</span><span class="s0">, </span><span class="s1">Y</span><span class="s0">, </span><span class="s1">lr)</span>


<span class="s1">@pytest.mark.parametrize(</span>
    <span class="s2">&quot;klass&quot;</span><span class="s0">, </span><span class="s1">[SGDClassifier</span><span class="s0">, </span><span class="s1">SparseSGDClassifier</span><span class="s0">, </span><span class="s1">SGDRegressor</span><span class="s0">, </span><span class="s1">SparseSGDRegressor]</span>
<span class="s1">)</span>
<span class="s0">def </span><span class="s1">test_input_format(klass):</span>
    <span class="s4"># Input format tests.</span>
    <span class="s1">clf = klass(alpha=</span><span class="s3">0.01</span><span class="s0">, </span><span class="s1">shuffle=</span><span class="s0">False</span><span class="s1">)</span>
    <span class="s1">clf.fit(X</span><span class="s0">, </span><span class="s1">Y)</span>
    <span class="s1">Y_ = np.array(Y)[:</span><span class="s0">, </span><span class="s1">np.newaxis]</span>

    <span class="s1">Y_ = np.c_[Y_</span><span class="s0">, </span><span class="s1">Y_]</span>
    <span class="s0">with </span><span class="s1">pytest.raises(ValueError):</span>
        <span class="s1">clf.fit(X</span><span class="s0">, </span><span class="s1">Y_)</span>


<span class="s1">@pytest.mark.parametrize(</span>
    <span class="s2">&quot;klass&quot;</span><span class="s0">, </span><span class="s1">[SGDClassifier</span><span class="s0">, </span><span class="s1">SparseSGDClassifier</span><span class="s0">, </span><span class="s1">SGDRegressor</span><span class="s0">, </span><span class="s1">SparseSGDRegressor]</span>
<span class="s1">)</span>
<span class="s0">def </span><span class="s1">test_clone(klass):</span>
    <span class="s4"># Test whether clone works ok.</span>
    <span class="s1">clf = klass(alpha=</span><span class="s3">0.01</span><span class="s0">, </span><span class="s1">penalty=</span><span class="s2">&quot;l1&quot;</span><span class="s1">)</span>
    <span class="s1">clf = clone(clf)</span>
    <span class="s1">clf.set_params(penalty=</span><span class="s2">&quot;l2&quot;</span><span class="s1">)</span>
    <span class="s1">clf.fit(X</span><span class="s0">, </span><span class="s1">Y)</span>

    <span class="s1">clf2 = klass(alpha=</span><span class="s3">0.01</span><span class="s0">, </span><span class="s1">penalty=</span><span class="s2">&quot;l2&quot;</span><span class="s1">)</span>
    <span class="s1">clf2.fit(X</span><span class="s0">, </span><span class="s1">Y)</span>

    <span class="s1">assert_array_equal(clf.coef_</span><span class="s0">, </span><span class="s1">clf2.coef_)</span>


<span class="s1">@pytest.mark.parametrize(</span>
    <span class="s2">&quot;klass&quot;</span><span class="s0">,</span>
    <span class="s1">[</span>
        <span class="s1">SGDClassifier</span><span class="s0">,</span>
        <span class="s1">SparseSGDClassifier</span><span class="s0">,</span>
        <span class="s1">SGDRegressor</span><span class="s0">,</span>
        <span class="s1">SparseSGDRegressor</span><span class="s0">,</span>
        <span class="s1">SGDOneClassSVM</span><span class="s0">,</span>
        <span class="s1">SparseSGDOneClassSVM</span><span class="s0">,</span>
    <span class="s1">]</span><span class="s0">,</span>
<span class="s1">)</span>
<span class="s0">def </span><span class="s1">test_plain_has_no_average_attr(klass):</span>
    <span class="s1">clf = klass(average=</span><span class="s0">True, </span><span class="s1">eta0=</span><span class="s3">0.01</span><span class="s1">)</span>
    <span class="s1">clf.fit(X</span><span class="s0">, </span><span class="s1">Y)</span>

    <span class="s0">assert </span><span class="s1">hasattr(clf</span><span class="s0">, </span><span class="s2">&quot;_average_coef&quot;</span><span class="s1">)</span>
    <span class="s0">assert </span><span class="s1">hasattr(clf</span><span class="s0">, </span><span class="s2">&quot;_average_intercept&quot;</span><span class="s1">)</span>
    <span class="s0">assert </span><span class="s1">hasattr(clf</span><span class="s0">, </span><span class="s2">&quot;_standard_intercept&quot;</span><span class="s1">)</span>
    <span class="s0">assert </span><span class="s1">hasattr(clf</span><span class="s0">, </span><span class="s2">&quot;_standard_coef&quot;</span><span class="s1">)</span>

    <span class="s1">clf = klass()</span>
    <span class="s1">clf.fit(X</span><span class="s0">, </span><span class="s1">Y)</span>

    <span class="s0">assert not </span><span class="s1">hasattr(clf</span><span class="s0">, </span><span class="s2">&quot;_average_coef&quot;</span><span class="s1">)</span>
    <span class="s0">assert not </span><span class="s1">hasattr(clf</span><span class="s0">, </span><span class="s2">&quot;_average_intercept&quot;</span><span class="s1">)</span>
    <span class="s0">assert not </span><span class="s1">hasattr(clf</span><span class="s0">, </span><span class="s2">&quot;_standard_intercept&quot;</span><span class="s1">)</span>
    <span class="s0">assert not </span><span class="s1">hasattr(clf</span><span class="s0">, </span><span class="s2">&quot;_standard_coef&quot;</span><span class="s1">)</span>


<span class="s1">@pytest.mark.parametrize(</span>
    <span class="s2">&quot;klass&quot;</span><span class="s0">,</span>
    <span class="s1">[</span>
        <span class="s1">SGDClassifier</span><span class="s0">,</span>
        <span class="s1">SparseSGDClassifier</span><span class="s0">,</span>
        <span class="s1">SGDRegressor</span><span class="s0">,</span>
        <span class="s1">SparseSGDRegressor</span><span class="s0">,</span>
        <span class="s1">SGDOneClassSVM</span><span class="s0">,</span>
        <span class="s1">SparseSGDOneClassSVM</span><span class="s0">,</span>
    <span class="s1">]</span><span class="s0">,</span>
<span class="s1">)</span>
<span class="s0">def </span><span class="s1">test_late_onset_averaging_not_reached(klass):</span>
    <span class="s1">clf1 = klass(average=</span><span class="s3">600</span><span class="s1">)</span>
    <span class="s1">clf2 = klass()</span>
    <span class="s0">for </span><span class="s1">_ </span><span class="s0">in </span><span class="s1">range(</span><span class="s3">100</span><span class="s1">):</span>
        <span class="s0">if </span><span class="s1">is_classifier(clf1):</span>
            <span class="s1">clf1.partial_fit(X</span><span class="s0">, </span><span class="s1">Y</span><span class="s0">, </span><span class="s1">classes=np.unique(Y))</span>
            <span class="s1">clf2.partial_fit(X</span><span class="s0">, </span><span class="s1">Y</span><span class="s0">, </span><span class="s1">classes=np.unique(Y))</span>
        <span class="s0">else</span><span class="s1">:</span>
            <span class="s1">clf1.partial_fit(X</span><span class="s0">, </span><span class="s1">Y)</span>
            <span class="s1">clf2.partial_fit(X</span><span class="s0">, </span><span class="s1">Y)</span>

    <span class="s1">assert_array_almost_equal(clf1.coef_</span><span class="s0">, </span><span class="s1">clf2.coef_</span><span class="s0">, </span><span class="s1">decimal=</span><span class="s3">16</span><span class="s1">)</span>
    <span class="s0">if </span><span class="s1">klass </span><span class="s0">in </span><span class="s1">[SGDClassifier</span><span class="s0">, </span><span class="s1">SparseSGDClassifier</span><span class="s0">, </span><span class="s1">SGDRegressor</span><span class="s0">, </span><span class="s1">SparseSGDRegressor]:</span>
        <span class="s1">assert_almost_equal(clf1.intercept_</span><span class="s0">, </span><span class="s1">clf2.intercept_</span><span class="s0">, </span><span class="s1">decimal=</span><span class="s3">16</span><span class="s1">)</span>
    <span class="s0">elif </span><span class="s1">klass </span><span class="s0">in </span><span class="s1">[SGDOneClassSVM</span><span class="s0">, </span><span class="s1">SparseSGDOneClassSVM]:</span>
        <span class="s1">assert_allclose(clf1.offset_</span><span class="s0">, </span><span class="s1">clf2.offset_)</span>


<span class="s1">@pytest.mark.parametrize(</span>
    <span class="s2">&quot;klass&quot;</span><span class="s0">, </span><span class="s1">[SGDClassifier</span><span class="s0">, </span><span class="s1">SparseSGDClassifier</span><span class="s0">, </span><span class="s1">SGDRegressor</span><span class="s0">, </span><span class="s1">SparseSGDRegressor]</span>
<span class="s1">)</span>
<span class="s0">def </span><span class="s1">test_late_onset_averaging_reached(klass):</span>
    <span class="s1">eta0 = </span><span class="s3">0.001</span>
    <span class="s1">alpha = </span><span class="s3">0.0001</span>
    <span class="s1">Y_encode = np.array(Y)</span>
    <span class="s1">Y_encode[Y_encode == </span><span class="s3">1</span><span class="s1">] = -</span><span class="s3">1.0</span>
    <span class="s1">Y_encode[Y_encode == </span><span class="s3">2</span><span class="s1">] = </span><span class="s3">1.0</span>

    <span class="s1">clf1 = klass(</span>
        <span class="s1">average=</span><span class="s3">7</span><span class="s0">,</span>
        <span class="s1">learning_rate=</span><span class="s2">&quot;constant&quot;</span><span class="s0">,</span>
        <span class="s1">loss=</span><span class="s2">&quot;squared_error&quot;</span><span class="s0">,</span>
        <span class="s1">eta0=eta0</span><span class="s0">,</span>
        <span class="s1">alpha=alpha</span><span class="s0">,</span>
        <span class="s1">max_iter=</span><span class="s3">2</span><span class="s0">,</span>
        <span class="s1">shuffle=</span><span class="s0">False,</span>
    <span class="s1">)</span>
    <span class="s1">clf2 = klass(</span>
        <span class="s1">average=</span><span class="s3">0</span><span class="s0">,</span>
        <span class="s1">learning_rate=</span><span class="s2">&quot;constant&quot;</span><span class="s0">,</span>
        <span class="s1">loss=</span><span class="s2">&quot;squared_error&quot;</span><span class="s0">,</span>
        <span class="s1">eta0=eta0</span><span class="s0">,</span>
        <span class="s1">alpha=alpha</span><span class="s0">,</span>
        <span class="s1">max_iter=</span><span class="s3">1</span><span class="s0">,</span>
        <span class="s1">shuffle=</span><span class="s0">False,</span>
    <span class="s1">)</span>

    <span class="s1">clf1.fit(X</span><span class="s0">, </span><span class="s1">Y_encode)</span>
    <span class="s1">clf2.fit(X</span><span class="s0">, </span><span class="s1">Y_encode)</span>

    <span class="s1">average_weights</span><span class="s0">, </span><span class="s1">average_intercept = asgd(</span>
        <span class="s1">klass</span><span class="s0">,</span>
        <span class="s1">X</span><span class="s0">,</span>
        <span class="s1">Y_encode</span><span class="s0">,</span>
        <span class="s1">eta0</span><span class="s0">,</span>
        <span class="s1">alpha</span><span class="s0">,</span>
        <span class="s1">weight_init=clf2.coef_.ravel()</span><span class="s0">,</span>
        <span class="s1">intercept_init=clf2.intercept_</span><span class="s0">,</span>
    <span class="s1">)</span>

    <span class="s1">assert_array_almost_equal(clf1.coef_.ravel()</span><span class="s0">, </span><span class="s1">average_weights.ravel()</span><span class="s0">, </span><span class="s1">decimal=</span><span class="s3">16</span><span class="s1">)</span>
    <span class="s1">assert_almost_equal(clf1.intercept_</span><span class="s0">, </span><span class="s1">average_intercept</span><span class="s0">, </span><span class="s1">decimal=</span><span class="s3">16</span><span class="s1">)</span>


<span class="s1">@pytest.mark.parametrize(</span>
    <span class="s2">&quot;klass&quot;</span><span class="s0">, </span><span class="s1">[SGDClassifier</span><span class="s0">, </span><span class="s1">SparseSGDClassifier</span><span class="s0">, </span><span class="s1">SGDRegressor</span><span class="s0">, </span><span class="s1">SparseSGDRegressor]</span>
<span class="s1">)</span>
<span class="s0">def </span><span class="s1">test_early_stopping(klass):</span>
    <span class="s1">X = iris.data[iris.target &gt; </span><span class="s3">0</span><span class="s1">]</span>
    <span class="s1">Y = iris.target[iris.target &gt; </span><span class="s3">0</span><span class="s1">]</span>
    <span class="s0">for </span><span class="s1">early_stopping </span><span class="s0">in </span><span class="s1">[</span><span class="s0">True, False</span><span class="s1">]:</span>
        <span class="s1">max_iter = </span><span class="s3">1000</span>
        <span class="s1">clf = klass(early_stopping=early_stopping</span><span class="s0">, </span><span class="s1">tol=</span><span class="s3">1e-3</span><span class="s0">, </span><span class="s1">max_iter=max_iter).fit(</span>
            <span class="s1">X</span><span class="s0">, </span><span class="s1">Y</span>
        <span class="s1">)</span>
        <span class="s0">assert </span><span class="s1">clf.n_iter_ &lt; max_iter</span>


<span class="s1">@pytest.mark.parametrize(</span>
    <span class="s2">&quot;klass&quot;</span><span class="s0">, </span><span class="s1">[SGDClassifier</span><span class="s0">, </span><span class="s1">SparseSGDClassifier</span><span class="s0">, </span><span class="s1">SGDRegressor</span><span class="s0">, </span><span class="s1">SparseSGDRegressor]</span>
<span class="s1">)</span>
<span class="s0">def </span><span class="s1">test_adaptive_longer_than_constant(klass):</span>
    <span class="s1">clf1 = klass(learning_rate=</span><span class="s2">&quot;adaptive&quot;</span><span class="s0">, </span><span class="s1">eta0=</span><span class="s3">0.01</span><span class="s0">, </span><span class="s1">tol=</span><span class="s3">1e-3</span><span class="s0">, </span><span class="s1">max_iter=</span><span class="s3">100</span><span class="s1">)</span>
    <span class="s1">clf1.fit(iris.data</span><span class="s0">, </span><span class="s1">iris.target)</span>
    <span class="s1">clf2 = klass(learning_rate=</span><span class="s2">&quot;constant&quot;</span><span class="s0">, </span><span class="s1">eta0=</span><span class="s3">0.01</span><span class="s0">, </span><span class="s1">tol=</span><span class="s3">1e-3</span><span class="s0">, </span><span class="s1">max_iter=</span><span class="s3">100</span><span class="s1">)</span>
    <span class="s1">clf2.fit(iris.data</span><span class="s0">, </span><span class="s1">iris.target)</span>
    <span class="s0">assert </span><span class="s1">clf1.n_iter_ &gt; clf2.n_iter_</span>


<span class="s1">@pytest.mark.parametrize(</span>
    <span class="s2">&quot;klass&quot;</span><span class="s0">, </span><span class="s1">[SGDClassifier</span><span class="s0">, </span><span class="s1">SparseSGDClassifier</span><span class="s0">, </span><span class="s1">SGDRegressor</span><span class="s0">, </span><span class="s1">SparseSGDRegressor]</span>
<span class="s1">)</span>
<span class="s0">def </span><span class="s1">test_validation_set_not_used_for_training(klass):</span>
    <span class="s1">X</span><span class="s0">, </span><span class="s1">Y = iris.data</span><span class="s0">, </span><span class="s1">iris.target</span>
    <span class="s1">validation_fraction = </span><span class="s3">0.4</span>
    <span class="s1">seed = </span><span class="s3">42</span>
    <span class="s1">shuffle = </span><span class="s0">False</span>
    <span class="s1">max_iter = </span><span class="s3">10</span>
    <span class="s1">clf1 = klass(</span>
        <span class="s1">early_stopping=</span><span class="s0">True,</span>
        <span class="s1">random_state=np.random.RandomState(seed)</span><span class="s0">,</span>
        <span class="s1">validation_fraction=validation_fraction</span><span class="s0">,</span>
        <span class="s1">learning_rate=</span><span class="s2">&quot;constant&quot;</span><span class="s0">,</span>
        <span class="s1">eta0=</span><span class="s3">0.01</span><span class="s0">,</span>
        <span class="s1">tol=</span><span class="s0">None,</span>
        <span class="s1">max_iter=max_iter</span><span class="s0">,</span>
        <span class="s1">shuffle=shuffle</span><span class="s0">,</span>
    <span class="s1">)</span>
    <span class="s1">clf1.fit(X</span><span class="s0">, </span><span class="s1">Y)</span>
    <span class="s0">assert </span><span class="s1">clf1.n_iter_ == max_iter</span>

    <span class="s1">clf2 = klass(</span>
        <span class="s1">early_stopping=</span><span class="s0">False,</span>
        <span class="s1">random_state=np.random.RandomState(seed)</span><span class="s0">,</span>
        <span class="s1">learning_rate=</span><span class="s2">&quot;constant&quot;</span><span class="s0">,</span>
        <span class="s1">eta0=</span><span class="s3">0.01</span><span class="s0">,</span>
        <span class="s1">tol=</span><span class="s0">None,</span>
        <span class="s1">max_iter=max_iter</span><span class="s0">,</span>
        <span class="s1">shuffle=shuffle</span><span class="s0">,</span>
    <span class="s1">)</span>

    <span class="s0">if </span><span class="s1">is_classifier(clf2):</span>
        <span class="s1">cv = StratifiedShuffleSplit(test_size=validation_fraction</span><span class="s0">, </span><span class="s1">random_state=seed)</span>
    <span class="s0">else</span><span class="s1">:</span>
        <span class="s1">cv = ShuffleSplit(test_size=validation_fraction</span><span class="s0">, </span><span class="s1">random_state=seed)</span>
    <span class="s1">idx_train</span><span class="s0">, </span><span class="s1">idx_val = next(cv.split(X</span><span class="s0">, </span><span class="s1">Y))</span>
    <span class="s1">idx_train = np.sort(idx_train)  </span><span class="s4"># remove shuffling</span>
    <span class="s1">clf2.fit(X[idx_train]</span><span class="s0">, </span><span class="s1">Y[idx_train])</span>
    <span class="s0">assert </span><span class="s1">clf2.n_iter_ == max_iter</span>

    <span class="s1">assert_array_equal(clf1.coef_</span><span class="s0">, </span><span class="s1">clf2.coef_)</span>


<span class="s1">@pytest.mark.parametrize(</span>
    <span class="s2">&quot;klass&quot;</span><span class="s0">, </span><span class="s1">[SGDClassifier</span><span class="s0">, </span><span class="s1">SparseSGDClassifier</span><span class="s0">, </span><span class="s1">SGDRegressor</span><span class="s0">, </span><span class="s1">SparseSGDRegressor]</span>
<span class="s1">)</span>
<span class="s0">def </span><span class="s1">test_n_iter_no_change(klass):</span>
    <span class="s1">X</span><span class="s0">, </span><span class="s1">Y = iris.data</span><span class="s0">, </span><span class="s1">iris.target</span>
    <span class="s4"># test that n_iter_ increases monotonically with n_iter_no_change</span>
    <span class="s0">for </span><span class="s1">early_stopping </span><span class="s0">in </span><span class="s1">[</span><span class="s0">True, False</span><span class="s1">]:</span>
        <span class="s1">n_iter_list = [</span>
            <span class="s1">klass(</span>
                <span class="s1">early_stopping=early_stopping</span><span class="s0">,</span>
                <span class="s1">n_iter_no_change=n_iter_no_change</span><span class="s0">,</span>
                <span class="s1">tol=</span><span class="s3">1e-4</span><span class="s0">,</span>
                <span class="s1">max_iter=</span><span class="s3">1000</span><span class="s0">,</span>
            <span class="s1">)</span>
            <span class="s1">.fit(X</span><span class="s0">, </span><span class="s1">Y)</span>
            <span class="s1">.n_iter_</span>
            <span class="s0">for </span><span class="s1">n_iter_no_change </span><span class="s0">in </span><span class="s1">[</span><span class="s3">2</span><span class="s0">, </span><span class="s3">3</span><span class="s0">, </span><span class="s3">10</span><span class="s1">]</span>
        <span class="s1">]</span>
        <span class="s1">assert_array_equal(n_iter_list</span><span class="s0">, </span><span class="s1">sorted(n_iter_list))</span>


<span class="s1">@pytest.mark.parametrize(</span>
    <span class="s2">&quot;klass&quot;</span><span class="s0">, </span><span class="s1">[SGDClassifier</span><span class="s0">, </span><span class="s1">SparseSGDClassifier</span><span class="s0">, </span><span class="s1">SGDRegressor</span><span class="s0">, </span><span class="s1">SparseSGDRegressor]</span>
<span class="s1">)</span>
<span class="s0">def </span><span class="s1">test_not_enough_sample_for_early_stopping(klass):</span>
    <span class="s4"># test an error is raised if the training or validation set is empty</span>
    <span class="s1">clf = klass(early_stopping=</span><span class="s0">True, </span><span class="s1">validation_fraction=</span><span class="s3">0.99</span><span class="s1">)</span>
    <span class="s0">with </span><span class="s1">pytest.raises(ValueError):</span>
        <span class="s1">clf.fit(X3</span><span class="s0">, </span><span class="s1">Y3)</span>


<span class="s4">###############################################################################</span>
<span class="s4"># Classification Test Case</span>


<span class="s1">@pytest.mark.parametrize(</span><span class="s2">&quot;klass&quot;</span><span class="s0">, </span><span class="s1">[SGDClassifier</span><span class="s0">, </span><span class="s1">SparseSGDClassifier])</span>
<span class="s0">def </span><span class="s1">test_sgd_clf(klass):</span>
    <span class="s4"># Check that SGD gives any results :-)</span>

    <span class="s0">for </span><span class="s1">loss </span><span class="s0">in </span><span class="s1">(</span><span class="s2">&quot;hinge&quot;</span><span class="s0">, </span><span class="s2">&quot;squared_hinge&quot;</span><span class="s0">, </span><span class="s2">&quot;log_loss&quot;</span><span class="s0">, </span><span class="s2">&quot;modified_huber&quot;</span><span class="s1">):</span>
        <span class="s1">clf = klass(</span>
            <span class="s1">penalty=</span><span class="s2">&quot;l2&quot;</span><span class="s0">,</span>
            <span class="s1">alpha=</span><span class="s3">0.01</span><span class="s0">,</span>
            <span class="s1">fit_intercept=</span><span class="s0">True,</span>
            <span class="s1">loss=loss</span><span class="s0">,</span>
            <span class="s1">max_iter=</span><span class="s3">10</span><span class="s0">,</span>
            <span class="s1">shuffle=</span><span class="s0">True,</span>
        <span class="s1">)</span>
        <span class="s1">clf.fit(X</span><span class="s0">, </span><span class="s1">Y)</span>
        <span class="s4"># assert_almost_equal(clf.coef_[0], clf.coef_[1], decimal=7)</span>
        <span class="s1">assert_array_equal(clf.predict(T)</span><span class="s0">, </span><span class="s1">true_result)</span>


<span class="s1">@pytest.mark.parametrize(</span>
    <span class="s2">&quot;klass&quot;</span><span class="s0">, </span><span class="s1">[SGDClassifier</span><span class="s0">, </span><span class="s1">SparseSGDClassifier</span><span class="s0">, </span><span class="s1">SGDOneClassSVM</span><span class="s0">, </span><span class="s1">SparseSGDOneClassSVM]</span>
<span class="s1">)</span>
<span class="s0">def </span><span class="s1">test_provide_coef(klass):</span>
    <span class="s5">&quot;&quot;&quot;Check that the shape of `coef_init` is validated.&quot;&quot;&quot;</span>
    <span class="s0">with </span><span class="s1">pytest.raises(ValueError</span><span class="s0">, </span><span class="s1">match=</span><span class="s2">&quot;Provided coef_init does not match dataset&quot;</span><span class="s1">):</span>
        <span class="s1">klass().fit(X</span><span class="s0">, </span><span class="s1">Y</span><span class="s0">, </span><span class="s1">coef_init=np.zeros((</span><span class="s3">3</span><span class="s0">,</span><span class="s1">)))</span>


<span class="s1">@pytest.mark.parametrize(</span>
    <span class="s2">&quot;klass, fit_params&quot;</span><span class="s0">,</span>
    <span class="s1">[</span>
        <span class="s1">(SGDClassifier</span><span class="s0">, </span><span class="s1">{</span><span class="s2">&quot;intercept_init&quot;</span><span class="s1">: np.zeros((</span><span class="s3">3</span><span class="s0">,</span><span class="s1">))})</span><span class="s0">,</span>
        <span class="s1">(SparseSGDClassifier</span><span class="s0">, </span><span class="s1">{</span><span class="s2">&quot;intercept_init&quot;</span><span class="s1">: np.zeros((</span><span class="s3">3</span><span class="s0">,</span><span class="s1">))})</span><span class="s0">,</span>
        <span class="s1">(SGDOneClassSVM</span><span class="s0">, </span><span class="s1">{</span><span class="s2">&quot;offset_init&quot;</span><span class="s1">: np.zeros((</span><span class="s3">3</span><span class="s0">,</span><span class="s1">))})</span><span class="s0">,</span>
        <span class="s1">(SparseSGDOneClassSVM</span><span class="s0">, </span><span class="s1">{</span><span class="s2">&quot;offset_init&quot;</span><span class="s1">: np.zeros((</span><span class="s3">3</span><span class="s0">,</span><span class="s1">))})</span><span class="s0">,</span>
    <span class="s1">]</span><span class="s0">,</span>
<span class="s1">)</span>
<span class="s0">def </span><span class="s1">test_set_intercept_offset(klass</span><span class="s0">, </span><span class="s1">fit_params):</span>
    <span class="s5">&quot;&quot;&quot;Check that `intercept_init` or `offset_init` is validated.&quot;&quot;&quot;</span>
    <span class="s1">sgd_estimator = klass()</span>
    <span class="s0">with </span><span class="s1">pytest.raises(ValueError</span><span class="s0">, </span><span class="s1">match=</span><span class="s2">&quot;does not match dataset&quot;</span><span class="s1">):</span>
        <span class="s1">sgd_estimator.fit(X</span><span class="s0">, </span><span class="s1">Y</span><span class="s0">, </span><span class="s1">**fit_params)</span>


<span class="s1">@pytest.mark.parametrize(</span>
    <span class="s2">&quot;klass&quot;</span><span class="s0">, </span><span class="s1">[SGDClassifier</span><span class="s0">, </span><span class="s1">SparseSGDClassifier</span><span class="s0">, </span><span class="s1">SGDRegressor</span><span class="s0">, </span><span class="s1">SparseSGDRegressor]</span>
<span class="s1">)</span>
<span class="s0">def </span><span class="s1">test_sgd_early_stopping_with_partial_fit(klass):</span>
    <span class="s5">&quot;&quot;&quot;Check that we raise an error for `early_stopping` used with 
    `partial_fit`. 
    &quot;&quot;&quot;</span>
    <span class="s1">err_msg = </span><span class="s2">&quot;early_stopping should be False with partial_fit&quot;</span>
    <span class="s0">with </span><span class="s1">pytest.raises(ValueError</span><span class="s0">, </span><span class="s1">match=err_msg):</span>
        <span class="s1">klass(early_stopping=</span><span class="s0">True</span><span class="s1">).partial_fit(X</span><span class="s0">, </span><span class="s1">Y)</span>


<span class="s1">@pytest.mark.parametrize(</span>
    <span class="s2">&quot;klass, fit_params&quot;</span><span class="s0">,</span>
    <span class="s1">[</span>
        <span class="s1">(SGDClassifier</span><span class="s0">, </span><span class="s1">{</span><span class="s2">&quot;intercept_init&quot;</span><span class="s1">: </span><span class="s3">0</span><span class="s1">})</span><span class="s0">,</span>
        <span class="s1">(SparseSGDClassifier</span><span class="s0">, </span><span class="s1">{</span><span class="s2">&quot;intercept_init&quot;</span><span class="s1">: </span><span class="s3">0</span><span class="s1">})</span><span class="s0">,</span>
        <span class="s1">(SGDOneClassSVM</span><span class="s0">, </span><span class="s1">{</span><span class="s2">&quot;offset_init&quot;</span><span class="s1">: </span><span class="s3">0</span><span class="s1">})</span><span class="s0">,</span>
        <span class="s1">(SparseSGDOneClassSVM</span><span class="s0">, </span><span class="s1">{</span><span class="s2">&quot;offset_init&quot;</span><span class="s1">: </span><span class="s3">0</span><span class="s1">})</span><span class="s0">,</span>
    <span class="s1">]</span><span class="s0">,</span>
<span class="s1">)</span>
<span class="s0">def </span><span class="s1">test_set_intercept_offset_binary(klass</span><span class="s0">, </span><span class="s1">fit_params):</span>
    <span class="s5">&quot;&quot;&quot;Check that we can pass a scaler with binary classification to 
    `intercept_init` or `offset_init`.&quot;&quot;&quot;</span>
    <span class="s1">klass().fit(X5</span><span class="s0">, </span><span class="s1">Y5</span><span class="s0">, </span><span class="s1">**fit_params)</span>


<span class="s1">@pytest.mark.parametrize(</span><span class="s2">&quot;klass&quot;</span><span class="s0">, </span><span class="s1">[SGDClassifier</span><span class="s0">, </span><span class="s1">SparseSGDClassifier])</span>
<span class="s0">def </span><span class="s1">test_average_binary_computed_correctly(klass):</span>
    <span class="s4"># Checks the SGDClassifier correctly computes the average weights</span>
    <span class="s1">eta = </span><span class="s3">0.1</span>
    <span class="s1">alpha = </span><span class="s3">2.0</span>
    <span class="s1">n_samples = </span><span class="s3">20</span>
    <span class="s1">n_features = </span><span class="s3">10</span>
    <span class="s1">rng = np.random.RandomState(</span><span class="s3">0</span><span class="s1">)</span>
    <span class="s1">X = rng.normal(size=(n_samples</span><span class="s0">, </span><span class="s1">n_features))</span>
    <span class="s1">w = rng.normal(size=n_features)</span>

    <span class="s1">clf = klass(</span>
        <span class="s1">loss=</span><span class="s2">&quot;squared_error&quot;</span><span class="s0">,</span>
        <span class="s1">learning_rate=</span><span class="s2">&quot;constant&quot;</span><span class="s0">,</span>
        <span class="s1">eta0=eta</span><span class="s0">,</span>
        <span class="s1">alpha=alpha</span><span class="s0">,</span>
        <span class="s1">fit_intercept=</span><span class="s0">True,</span>
        <span class="s1">max_iter=</span><span class="s3">1</span><span class="s0">,</span>
        <span class="s1">average=</span><span class="s0">True,</span>
        <span class="s1">shuffle=</span><span class="s0">False,</span>
    <span class="s1">)</span>

    <span class="s4"># simple linear function without noise</span>
    <span class="s1">y = np.dot(X</span><span class="s0">, </span><span class="s1">w)</span>
    <span class="s1">y = np.sign(y)</span>

    <span class="s1">clf.fit(X</span><span class="s0">, </span><span class="s1">y)</span>

    <span class="s1">average_weights</span><span class="s0">, </span><span class="s1">average_intercept = asgd(klass</span><span class="s0">, </span><span class="s1">X</span><span class="s0">, </span><span class="s1">y</span><span class="s0">, </span><span class="s1">eta</span><span class="s0">, </span><span class="s1">alpha)</span>
    <span class="s1">average_weights = average_weights.reshape(</span><span class="s3">1</span><span class="s0">, </span><span class="s1">-</span><span class="s3">1</span><span class="s1">)</span>
    <span class="s1">assert_array_almost_equal(clf.coef_</span><span class="s0">, </span><span class="s1">average_weights</span><span class="s0">, </span><span class="s1">decimal=</span><span class="s3">14</span><span class="s1">)</span>
    <span class="s1">assert_almost_equal(clf.intercept_</span><span class="s0">, </span><span class="s1">average_intercept</span><span class="s0">, </span><span class="s1">decimal=</span><span class="s3">14</span><span class="s1">)</span>


<span class="s1">@pytest.mark.parametrize(</span><span class="s2">&quot;klass&quot;</span><span class="s0">, </span><span class="s1">[SGDClassifier</span><span class="s0">, </span><span class="s1">SparseSGDClassifier])</span>
<span class="s0">def </span><span class="s1">test_set_intercept_to_intercept(klass):</span>
    <span class="s4"># Checks intercept_ shape consistency for the warm starts</span>
    <span class="s4"># Inconsistent intercept_ shape.</span>
    <span class="s1">clf = klass().fit(X5</span><span class="s0">, </span><span class="s1">Y5)</span>
    <span class="s1">klass().fit(X5</span><span class="s0">, </span><span class="s1">Y5</span><span class="s0">, </span><span class="s1">intercept_init=clf.intercept_)</span>
    <span class="s1">clf = klass().fit(X</span><span class="s0">, </span><span class="s1">Y)</span>
    <span class="s1">klass().fit(X</span><span class="s0">, </span><span class="s1">Y</span><span class="s0">, </span><span class="s1">intercept_init=clf.intercept_)</span>


<span class="s1">@pytest.mark.parametrize(</span><span class="s2">&quot;klass&quot;</span><span class="s0">, </span><span class="s1">[SGDClassifier</span><span class="s0">, </span><span class="s1">SparseSGDClassifier])</span>
<span class="s0">def </span><span class="s1">test_sgd_at_least_two_labels(klass):</span>
    <span class="s4"># Target must have at least two labels</span>
    <span class="s1">clf = klass(alpha=</span><span class="s3">0.01</span><span class="s0">, </span><span class="s1">max_iter=</span><span class="s3">20</span><span class="s1">)</span>
    <span class="s0">with </span><span class="s1">pytest.raises(ValueError):</span>
        <span class="s1">clf.fit(X2</span><span class="s0">, </span><span class="s1">np.ones(</span><span class="s3">9</span><span class="s1">))</span>


<span class="s1">@pytest.mark.parametrize(</span><span class="s2">&quot;klass&quot;</span><span class="s0">, </span><span class="s1">[SGDClassifier</span><span class="s0">, </span><span class="s1">SparseSGDClassifier])</span>
<span class="s0">def </span><span class="s1">test_partial_fit_weight_class_balanced(klass):</span>
    <span class="s4"># partial_fit with class_weight='balanced' not supported&quot;&quot;&quot;</span>
    <span class="s1">regex = (</span>
        <span class="s2">r&quot;class_weight 'balanced' is not supported for &quot;</span>
        <span class="s2">r&quot;partial_fit\. In order to use 'balanced' weights, &quot;</span>
        <span class="s2">r&quot;use compute_class_weight\('balanced', classes=classes, y=y\). &quot;</span>
        <span class="s2">r&quot;In place of y you can use a large enough sample &quot;</span>
        <span class="s2">r&quot;of the full training set target to properly &quot;</span>
        <span class="s2">r&quot;estimate the class frequency distributions\. &quot;</span>
        <span class="s2">r&quot;Pass the resulting weights as the class_weight &quot;</span>
        <span class="s2">r&quot;parameter\.&quot;</span>
    <span class="s1">)</span>
    <span class="s0">with </span><span class="s1">pytest.raises(ValueError</span><span class="s0">, </span><span class="s1">match=regex):</span>
        <span class="s1">klass(class_weight=</span><span class="s2">&quot;balanced&quot;</span><span class="s1">).partial_fit(X</span><span class="s0">, </span><span class="s1">Y</span><span class="s0">, </span><span class="s1">classes=np.unique(Y))</span>


<span class="s1">@pytest.mark.parametrize(</span><span class="s2">&quot;klass&quot;</span><span class="s0">, </span><span class="s1">[SGDClassifier</span><span class="s0">, </span><span class="s1">SparseSGDClassifier])</span>
<span class="s0">def </span><span class="s1">test_sgd_multiclass(klass):</span>
    <span class="s4"># Multi-class test case</span>
    <span class="s1">clf = klass(alpha=</span><span class="s3">0.01</span><span class="s0">, </span><span class="s1">max_iter=</span><span class="s3">20</span><span class="s1">).fit(X2</span><span class="s0">, </span><span class="s1">Y2)</span>
    <span class="s0">assert </span><span class="s1">clf.coef_.shape == (</span><span class="s3">3</span><span class="s0">, </span><span class="s3">2</span><span class="s1">)</span>
    <span class="s0">assert </span><span class="s1">clf.intercept_.shape == (</span><span class="s3">3</span><span class="s0">,</span><span class="s1">)</span>
    <span class="s0">assert </span><span class="s1">clf.decision_function([[</span><span class="s3">0</span><span class="s0">, </span><span class="s3">0</span><span class="s1">]]).shape == (</span><span class="s3">1</span><span class="s0">, </span><span class="s3">3</span><span class="s1">)</span>
    <span class="s1">pred = clf.predict(T2)</span>
    <span class="s1">assert_array_equal(pred</span><span class="s0">, </span><span class="s1">true_result2)</span>


<span class="s1">@pytest.mark.parametrize(</span><span class="s2">&quot;klass&quot;</span><span class="s0">, </span><span class="s1">[SGDClassifier</span><span class="s0">, </span><span class="s1">SparseSGDClassifier])</span>
<span class="s0">def </span><span class="s1">test_sgd_multiclass_average(klass):</span>
    <span class="s1">eta = </span><span class="s3">0.001</span>
    <span class="s1">alpha = </span><span class="s3">0.01</span>
    <span class="s4"># Multi-class average test case</span>
    <span class="s1">clf = klass(</span>
        <span class="s1">loss=</span><span class="s2">&quot;squared_error&quot;</span><span class="s0">,</span>
        <span class="s1">learning_rate=</span><span class="s2">&quot;constant&quot;</span><span class="s0">,</span>
        <span class="s1">eta0=eta</span><span class="s0">,</span>
        <span class="s1">alpha=alpha</span><span class="s0">,</span>
        <span class="s1">fit_intercept=</span><span class="s0">True,</span>
        <span class="s1">max_iter=</span><span class="s3">1</span><span class="s0">,</span>
        <span class="s1">average=</span><span class="s0">True,</span>
        <span class="s1">shuffle=</span><span class="s0">False,</span>
    <span class="s1">)</span>

    <span class="s1">np_Y2 = np.array(Y2)</span>
    <span class="s1">clf.fit(X2</span><span class="s0">, </span><span class="s1">np_Y2)</span>
    <span class="s1">classes = np.unique(np_Y2)</span>

    <span class="s0">for </span><span class="s1">i</span><span class="s0">, </span><span class="s1">cl </span><span class="s0">in </span><span class="s1">enumerate(classes):</span>
        <span class="s1">y_i = np.ones(np_Y2.shape[</span><span class="s3">0</span><span class="s1">])</span>
        <span class="s1">y_i[np_Y2 != cl] = -</span><span class="s3">1</span>
        <span class="s1">average_coef</span><span class="s0">, </span><span class="s1">average_intercept = asgd(klass</span><span class="s0">, </span><span class="s1">X2</span><span class="s0">, </span><span class="s1">y_i</span><span class="s0">, </span><span class="s1">eta</span><span class="s0">, </span><span class="s1">alpha)</span>
        <span class="s1">assert_array_almost_equal(average_coef</span><span class="s0">, </span><span class="s1">clf.coef_[i]</span><span class="s0">, </span><span class="s1">decimal=</span><span class="s3">16</span><span class="s1">)</span>
        <span class="s1">assert_almost_equal(average_intercept</span><span class="s0">, </span><span class="s1">clf.intercept_[i]</span><span class="s0">, </span><span class="s1">decimal=</span><span class="s3">16</span><span class="s1">)</span>


<span class="s1">@pytest.mark.parametrize(</span><span class="s2">&quot;klass&quot;</span><span class="s0">, </span><span class="s1">[SGDClassifier</span><span class="s0">, </span><span class="s1">SparseSGDClassifier])</span>
<span class="s0">def </span><span class="s1">test_sgd_multiclass_with_init_coef(klass):</span>
    <span class="s4"># Multi-class test case</span>
    <span class="s1">clf = klass(alpha=</span><span class="s3">0.01</span><span class="s0">, </span><span class="s1">max_iter=</span><span class="s3">20</span><span class="s1">)</span>
    <span class="s1">clf.fit(X2</span><span class="s0">, </span><span class="s1">Y2</span><span class="s0">, </span><span class="s1">coef_init=np.zeros((</span><span class="s3">3</span><span class="s0">, </span><span class="s3">2</span><span class="s1">))</span><span class="s0">, </span><span class="s1">intercept_init=np.zeros(</span><span class="s3">3</span><span class="s1">))</span>
    <span class="s0">assert </span><span class="s1">clf.coef_.shape == (</span><span class="s3">3</span><span class="s0">, </span><span class="s3">2</span><span class="s1">)</span>
    <span class="s0">assert </span><span class="s1">clf.intercept_.shape</span><span class="s0">, </span><span class="s1">(</span><span class="s3">3</span><span class="s0">,</span><span class="s1">)</span>
    <span class="s1">pred = clf.predict(T2)</span>
    <span class="s1">assert_array_equal(pred</span><span class="s0">, </span><span class="s1">true_result2)</span>


<span class="s1">@pytest.mark.parametrize(</span><span class="s2">&quot;klass&quot;</span><span class="s0">, </span><span class="s1">[SGDClassifier</span><span class="s0">, </span><span class="s1">SparseSGDClassifier])</span>
<span class="s0">def </span><span class="s1">test_sgd_multiclass_njobs(klass):</span>
    <span class="s4"># Multi-class test case with multi-core support</span>
    <span class="s1">clf = klass(alpha=</span><span class="s3">0.01</span><span class="s0">, </span><span class="s1">max_iter=</span><span class="s3">20</span><span class="s0">, </span><span class="s1">n_jobs=</span><span class="s3">2</span><span class="s1">).fit(X2</span><span class="s0">, </span><span class="s1">Y2)</span>
    <span class="s0">assert </span><span class="s1">clf.coef_.shape == (</span><span class="s3">3</span><span class="s0">, </span><span class="s3">2</span><span class="s1">)</span>
    <span class="s0">assert </span><span class="s1">clf.intercept_.shape == (</span><span class="s3">3</span><span class="s0">,</span><span class="s1">)</span>
    <span class="s0">assert </span><span class="s1">clf.decision_function([[</span><span class="s3">0</span><span class="s0">, </span><span class="s3">0</span><span class="s1">]]).shape == (</span><span class="s3">1</span><span class="s0">, </span><span class="s3">3</span><span class="s1">)</span>
    <span class="s1">pred = clf.predict(T2)</span>
    <span class="s1">assert_array_equal(pred</span><span class="s0">, </span><span class="s1">true_result2)</span>


<span class="s1">@pytest.mark.parametrize(</span><span class="s2">&quot;klass&quot;</span><span class="s0">, </span><span class="s1">[SGDClassifier</span><span class="s0">, </span><span class="s1">SparseSGDClassifier])</span>
<span class="s0">def </span><span class="s1">test_set_coef_multiclass(klass):</span>
    <span class="s4"># Checks coef_init and intercept_init shape for multi-class</span>
    <span class="s4"># problems</span>
    <span class="s4"># Provided coef_ does not match dataset</span>
    <span class="s1">clf = klass()</span>
    <span class="s0">with </span><span class="s1">pytest.raises(ValueError):</span>
        <span class="s1">clf.fit(X2</span><span class="s0">, </span><span class="s1">Y2</span><span class="s0">, </span><span class="s1">coef_init=np.zeros((</span><span class="s3">2</span><span class="s0">, </span><span class="s3">2</span><span class="s1">)))</span>

    <span class="s4"># Provided coef_ does match dataset</span>
    <span class="s1">clf = klass().fit(X2</span><span class="s0">, </span><span class="s1">Y2</span><span class="s0">, </span><span class="s1">coef_init=np.zeros((</span><span class="s3">3</span><span class="s0">, </span><span class="s3">2</span><span class="s1">)))</span>

    <span class="s4"># Provided intercept_ does not match dataset</span>
    <span class="s1">clf = klass()</span>
    <span class="s0">with </span><span class="s1">pytest.raises(ValueError):</span>
        <span class="s1">clf.fit(X2</span><span class="s0">, </span><span class="s1">Y2</span><span class="s0">, </span><span class="s1">intercept_init=np.zeros((</span><span class="s3">1</span><span class="s0">,</span><span class="s1">)))</span>

    <span class="s4"># Provided intercept_ does match dataset.</span>
    <span class="s1">clf = klass().fit(X2</span><span class="s0">, </span><span class="s1">Y2</span><span class="s0">, </span><span class="s1">intercept_init=np.zeros((</span><span class="s3">3</span><span class="s0">,</span><span class="s1">)))</span>


<span class="s1">@pytest.mark.parametrize(</span><span class="s2">&quot;klass&quot;</span><span class="s0">, </span><span class="s1">[SGDClassifier</span><span class="s0">, </span><span class="s1">SparseSGDClassifier])</span>
<span class="s0">def </span><span class="s1">test_sgd_predict_proba_method_access(klass):</span>
    <span class="s4"># Checks that SGDClassifier predict_proba and predict_log_proba methods</span>
    <span class="s4"># can either be accessed or raise an appropriate error message</span>
    <span class="s4"># otherwise. See</span>
    <span class="s4"># https://github.com/scikit-learn/scikit-learn/issues/10938 for more</span>
    <span class="s4"># details.</span>
    <span class="s0">for </span><span class="s1">loss </span><span class="s0">in </span><span class="s1">linear_model.SGDClassifier.loss_functions:</span>
        <span class="s1">clf = SGDClassifier(loss=loss)</span>
        <span class="s0">if </span><span class="s1">loss </span><span class="s0">in </span><span class="s1">(</span><span class="s2">&quot;log_loss&quot;</span><span class="s0">, </span><span class="s2">&quot;modified_huber&quot;</span><span class="s1">):</span>
            <span class="s0">assert </span><span class="s1">hasattr(clf</span><span class="s0">, </span><span class="s2">&quot;predict_proba&quot;</span><span class="s1">)</span>
            <span class="s0">assert </span><span class="s1">hasattr(clf</span><span class="s0">, </span><span class="s2">&quot;predict_log_proba&quot;</span><span class="s1">)</span>
        <span class="s0">else</span><span class="s1">:</span>
            <span class="s1">message = </span><span class="s2">&quot;probability estimates are not available for loss={!r}&quot;</span><span class="s1">.format(</span>
                <span class="s1">loss</span>
            <span class="s1">)</span>
            <span class="s0">assert not </span><span class="s1">hasattr(clf</span><span class="s0">, </span><span class="s2">&quot;predict_proba&quot;</span><span class="s1">)</span>
            <span class="s0">assert not </span><span class="s1">hasattr(clf</span><span class="s0">, </span><span class="s2">&quot;predict_log_proba&quot;</span><span class="s1">)</span>
            <span class="s0">with </span><span class="s1">pytest.raises(AttributeError</span><span class="s0">, </span><span class="s1">match=message):</span>
                <span class="s1">clf.predict_proba</span>
            <span class="s0">with </span><span class="s1">pytest.raises(AttributeError</span><span class="s0">, </span><span class="s1">match=message):</span>
                <span class="s1">clf.predict_log_proba</span>


<span class="s1">@pytest.mark.parametrize(</span><span class="s2">&quot;klass&quot;</span><span class="s0">, </span><span class="s1">[SGDClassifier</span><span class="s0">, </span><span class="s1">SparseSGDClassifier])</span>
<span class="s0">def </span><span class="s1">test_sgd_proba(klass):</span>
    <span class="s4"># Check SGD.predict_proba</span>

    <span class="s4"># Hinge loss does not allow for conditional prob estimate.</span>
    <span class="s4"># We cannot use the factory here, because it defines predict_proba</span>
    <span class="s4"># anyway.</span>
    <span class="s1">clf = SGDClassifier(loss=</span><span class="s2">&quot;hinge&quot;</span><span class="s0">, </span><span class="s1">alpha=</span><span class="s3">0.01</span><span class="s0">, </span><span class="s1">max_iter=</span><span class="s3">10</span><span class="s0">, </span><span class="s1">tol=</span><span class="s0">None</span><span class="s1">).fit(X</span><span class="s0">, </span><span class="s1">Y)</span>
    <span class="s0">assert not </span><span class="s1">hasattr(clf</span><span class="s0">, </span><span class="s2">&quot;predict_proba&quot;</span><span class="s1">)</span>
    <span class="s0">assert not </span><span class="s1">hasattr(clf</span><span class="s0">, </span><span class="s2">&quot;predict_log_proba&quot;</span><span class="s1">)</span>

    <span class="s4"># log and modified_huber losses can output probability estimates</span>
    <span class="s4"># binary case</span>
    <span class="s0">for </span><span class="s1">loss </span><span class="s0">in </span><span class="s1">[</span><span class="s2">&quot;log_loss&quot;</span><span class="s0">, </span><span class="s2">&quot;modified_huber&quot;</span><span class="s1">]:</span>
        <span class="s1">clf = klass(loss=loss</span><span class="s0">, </span><span class="s1">alpha=</span><span class="s3">0.01</span><span class="s0">, </span><span class="s1">max_iter=</span><span class="s3">10</span><span class="s1">)</span>
        <span class="s1">clf.fit(X</span><span class="s0">, </span><span class="s1">Y)</span>
        <span class="s1">p = clf.predict_proba([[</span><span class="s3">3</span><span class="s0">, </span><span class="s3">2</span><span class="s1">]])</span>
        <span class="s0">assert </span><span class="s1">p[</span><span class="s3">0</span><span class="s0">, </span><span class="s3">1</span><span class="s1">] &gt; </span><span class="s3">0.5</span>
        <span class="s1">p = clf.predict_proba([[-</span><span class="s3">1</span><span class="s0">, </span><span class="s1">-</span><span class="s3">1</span><span class="s1">]])</span>
        <span class="s0">assert </span><span class="s1">p[</span><span class="s3">0</span><span class="s0">, </span><span class="s3">1</span><span class="s1">] &lt; </span><span class="s3">0.5</span>

        <span class="s1">p = clf.predict_log_proba([[</span><span class="s3">3</span><span class="s0">, </span><span class="s3">2</span><span class="s1">]])</span>
        <span class="s0">assert </span><span class="s1">p[</span><span class="s3">0</span><span class="s0">, </span><span class="s3">1</span><span class="s1">] &gt; p[</span><span class="s3">0</span><span class="s0">, </span><span class="s3">0</span><span class="s1">]</span>
        <span class="s1">p = clf.predict_log_proba([[-</span><span class="s3">1</span><span class="s0">, </span><span class="s1">-</span><span class="s3">1</span><span class="s1">]])</span>
        <span class="s0">assert </span><span class="s1">p[</span><span class="s3">0</span><span class="s0">, </span><span class="s3">1</span><span class="s1">] &lt; p[</span><span class="s3">0</span><span class="s0">, </span><span class="s3">0</span><span class="s1">]</span>

    <span class="s4"># log loss multiclass probability estimates</span>
    <span class="s1">clf = klass(loss=</span><span class="s2">&quot;log_loss&quot;</span><span class="s0">, </span><span class="s1">alpha=</span><span class="s3">0.01</span><span class="s0">, </span><span class="s1">max_iter=</span><span class="s3">10</span><span class="s1">).fit(X2</span><span class="s0">, </span><span class="s1">Y2)</span>

    <span class="s1">d = clf.decision_function([[</span><span class="s3">0.1</span><span class="s0">, </span><span class="s1">-</span><span class="s3">0.1</span><span class="s1">]</span><span class="s0">, </span><span class="s1">[</span><span class="s3">0.3</span><span class="s0">, </span><span class="s3">0.2</span><span class="s1">]])</span>
    <span class="s1">p = clf.predict_proba([[</span><span class="s3">0.1</span><span class="s0">, </span><span class="s1">-</span><span class="s3">0.1</span><span class="s1">]</span><span class="s0">, </span><span class="s1">[</span><span class="s3">0.3</span><span class="s0">, </span><span class="s3">0.2</span><span class="s1">]])</span>
    <span class="s1">assert_array_equal(np.argmax(p</span><span class="s0">, </span><span class="s1">axis=</span><span class="s3">1</span><span class="s1">)</span><span class="s0">, </span><span class="s1">np.argmax(d</span><span class="s0">, </span><span class="s1">axis=</span><span class="s3">1</span><span class="s1">))</span>
    <span class="s1">assert_almost_equal(p[</span><span class="s3">0</span><span class="s1">].sum()</span><span class="s0">, </span><span class="s3">1</span><span class="s1">)</span>
    <span class="s0">assert </span><span class="s1">np.all(p[</span><span class="s3">0</span><span class="s1">] &gt;= </span><span class="s3">0</span><span class="s1">)</span>

    <span class="s1">p = clf.predict_proba([[-</span><span class="s3">1</span><span class="s0">, </span><span class="s1">-</span><span class="s3">1</span><span class="s1">]])</span>
    <span class="s1">d = clf.decision_function([[-</span><span class="s3">1</span><span class="s0">, </span><span class="s1">-</span><span class="s3">1</span><span class="s1">]])</span>
    <span class="s1">assert_array_equal(np.argsort(p[</span><span class="s3">0</span><span class="s1">])</span><span class="s0">, </span><span class="s1">np.argsort(d[</span><span class="s3">0</span><span class="s1">]))</span>

    <span class="s1">lp = clf.predict_log_proba([[</span><span class="s3">3</span><span class="s0">, </span><span class="s3">2</span><span class="s1">]])</span>
    <span class="s1">p = clf.predict_proba([[</span><span class="s3">3</span><span class="s0">, </span><span class="s3">2</span><span class="s1">]])</span>
    <span class="s1">assert_array_almost_equal(np.log(p)</span><span class="s0">, </span><span class="s1">lp)</span>

    <span class="s1">lp = clf.predict_log_proba([[-</span><span class="s3">1</span><span class="s0">, </span><span class="s1">-</span><span class="s3">1</span><span class="s1">]])</span>
    <span class="s1">p = clf.predict_proba([[-</span><span class="s3">1</span><span class="s0">, </span><span class="s1">-</span><span class="s3">1</span><span class="s1">]])</span>
    <span class="s1">assert_array_almost_equal(np.log(p)</span><span class="s0">, </span><span class="s1">lp)</span>

    <span class="s4"># Modified Huber multiclass probability estimates; requires a separate</span>
    <span class="s4"># test because the hard zero/one probabilities may destroy the</span>
    <span class="s4"># ordering present in decision_function output.</span>
    <span class="s1">clf = klass(loss=</span><span class="s2">&quot;modified_huber&quot;</span><span class="s0">, </span><span class="s1">alpha=</span><span class="s3">0.01</span><span class="s0">, </span><span class="s1">max_iter=</span><span class="s3">10</span><span class="s1">)</span>
    <span class="s1">clf.fit(X2</span><span class="s0">, </span><span class="s1">Y2)</span>
    <span class="s1">d = clf.decision_function([[</span><span class="s3">3</span><span class="s0">, </span><span class="s3">2</span><span class="s1">]])</span>
    <span class="s1">p = clf.predict_proba([[</span><span class="s3">3</span><span class="s0">, </span><span class="s3">2</span><span class="s1">]])</span>
    <span class="s0">if </span><span class="s1">klass != SparseSGDClassifier:</span>
        <span class="s0">assert </span><span class="s1">np.argmax(d</span><span class="s0">, </span><span class="s1">axis=</span><span class="s3">1</span><span class="s1">) == np.argmax(p</span><span class="s0">, </span><span class="s1">axis=</span><span class="s3">1</span><span class="s1">)</span>
    <span class="s0">else</span><span class="s1">:  </span><span class="s4"># XXX the sparse test gets a different X2 (?)</span>
        <span class="s0">assert </span><span class="s1">np.argmin(d</span><span class="s0">, </span><span class="s1">axis=</span><span class="s3">1</span><span class="s1">) == np.argmin(p</span><span class="s0">, </span><span class="s1">axis=</span><span class="s3">1</span><span class="s1">)</span>

    <span class="s4"># the following sample produces decision_function values &lt; -1,</span>
    <span class="s4"># which would cause naive normalization to fail (see comment</span>
    <span class="s4"># in SGDClassifier.predict_proba)</span>
    <span class="s1">x = X.mean(axis=</span><span class="s3">0</span><span class="s1">)</span>
    <span class="s1">d = clf.decision_function([x])</span>
    <span class="s0">if </span><span class="s1">np.all(d &lt; -</span><span class="s3">1</span><span class="s1">):  </span><span class="s4"># XXX not true in sparse test case (why?)</span>
        <span class="s1">p = clf.predict_proba([x])</span>
        <span class="s1">assert_array_almost_equal(p[</span><span class="s3">0</span><span class="s1">]</span><span class="s0">, </span><span class="s1">[</span><span class="s3">1 </span><span class="s1">/ </span><span class="s3">3.0</span><span class="s1">] * </span><span class="s3">3</span><span class="s1">)</span>


<span class="s1">@pytest.mark.parametrize(</span><span class="s2">&quot;klass&quot;</span><span class="s0">, </span><span class="s1">[SGDClassifier</span><span class="s0">, </span><span class="s1">SparseSGDClassifier])</span>
<span class="s0">def </span><span class="s1">test_sgd_l1(klass):</span>
    <span class="s4"># Test L1 regularization</span>
    <span class="s1">n = len(X4)</span>
    <span class="s1">rng = np.random.RandomState(</span><span class="s3">13</span><span class="s1">)</span>
    <span class="s1">idx = np.arange(n)</span>
    <span class="s1">rng.shuffle(idx)</span>

    <span class="s1">X = X4[idx</span><span class="s0">, </span><span class="s1">:]</span>
    <span class="s1">Y = Y4[idx]</span>

    <span class="s1">clf = klass(</span>
        <span class="s1">penalty=</span><span class="s2">&quot;l1&quot;</span><span class="s0">,</span>
        <span class="s1">alpha=</span><span class="s3">0.2</span><span class="s0">,</span>
        <span class="s1">fit_intercept=</span><span class="s0">False,</span>
        <span class="s1">max_iter=</span><span class="s3">2000</span><span class="s0">,</span>
        <span class="s1">tol=</span><span class="s0">None,</span>
        <span class="s1">shuffle=</span><span class="s0">False,</span>
    <span class="s1">)</span>
    <span class="s1">clf.fit(X</span><span class="s0">, </span><span class="s1">Y)</span>
    <span class="s1">assert_array_equal(clf.coef_[</span><span class="s3">0</span><span class="s0">, </span><span class="s3">1</span><span class="s1">:-</span><span class="s3">1</span><span class="s1">]</span><span class="s0">, </span><span class="s1">np.zeros((</span><span class="s3">4</span><span class="s0">,</span><span class="s1">)))</span>
    <span class="s1">pred = clf.predict(X)</span>
    <span class="s1">assert_array_equal(pred</span><span class="s0">, </span><span class="s1">Y)</span>

    <span class="s4"># test sparsify with dense inputs</span>
    <span class="s1">clf.sparsify()</span>
    <span class="s0">assert </span><span class="s1">sp.issparse(clf.coef_)</span>
    <span class="s1">pred = clf.predict(X)</span>
    <span class="s1">assert_array_equal(pred</span><span class="s0">, </span><span class="s1">Y)</span>

    <span class="s4"># pickle and unpickle with sparse coef_</span>
    <span class="s1">clf = pickle.loads(pickle.dumps(clf))</span>
    <span class="s0">assert </span><span class="s1">sp.issparse(clf.coef_)</span>
    <span class="s1">pred = clf.predict(X)</span>
    <span class="s1">assert_array_equal(pred</span><span class="s0">, </span><span class="s1">Y)</span>


<span class="s1">@pytest.mark.parametrize(</span><span class="s2">&quot;klass&quot;</span><span class="s0">, </span><span class="s1">[SGDClassifier</span><span class="s0">, </span><span class="s1">SparseSGDClassifier])</span>
<span class="s0">def </span><span class="s1">test_class_weights(klass):</span>
    <span class="s4"># Test class weights.</span>
    <span class="s1">X = np.array([[-</span><span class="s3">1.0</span><span class="s0">, </span><span class="s1">-</span><span class="s3">1.0</span><span class="s1">]</span><span class="s0">, </span><span class="s1">[-</span><span class="s3">1.0</span><span class="s0">, </span><span class="s3">0</span><span class="s1">]</span><span class="s0">, </span><span class="s1">[-</span><span class="s3">0.8</span><span class="s0">, </span><span class="s1">-</span><span class="s3">1.0</span><span class="s1">]</span><span class="s0">, </span><span class="s1">[</span><span class="s3">1.0</span><span class="s0">, </span><span class="s3">1.0</span><span class="s1">]</span><span class="s0">, </span><span class="s1">[</span><span class="s3">1.0</span><span class="s0">, </span><span class="s3">0.0</span><span class="s1">]])</span>
    <span class="s1">y = [</span><span class="s3">1</span><span class="s0">, </span><span class="s3">1</span><span class="s0">, </span><span class="s3">1</span><span class="s0">, </span><span class="s1">-</span><span class="s3">1</span><span class="s0">, </span><span class="s1">-</span><span class="s3">1</span><span class="s1">]</span>

    <span class="s1">clf = klass(alpha=</span><span class="s3">0.1</span><span class="s0">, </span><span class="s1">max_iter=</span><span class="s3">1000</span><span class="s0">, </span><span class="s1">fit_intercept=</span><span class="s0">False, </span><span class="s1">class_weight=</span><span class="s0">None</span><span class="s1">)</span>
    <span class="s1">clf.fit(X</span><span class="s0">, </span><span class="s1">y)</span>
    <span class="s1">assert_array_equal(clf.predict([[</span><span class="s3">0.2</span><span class="s0">, </span><span class="s1">-</span><span class="s3">1.0</span><span class="s1">]])</span><span class="s0">, </span><span class="s1">np.array([</span><span class="s3">1</span><span class="s1">]))</span>

    <span class="s4"># we give a small weights to class 1</span>
    <span class="s1">clf = klass(alpha=</span><span class="s3">0.1</span><span class="s0">, </span><span class="s1">max_iter=</span><span class="s3">1000</span><span class="s0">, </span><span class="s1">fit_intercept=</span><span class="s0">False, </span><span class="s1">class_weight={</span><span class="s3">1</span><span class="s1">: </span><span class="s3">0.001</span><span class="s1">})</span>
    <span class="s1">clf.fit(X</span><span class="s0">, </span><span class="s1">y)</span>

    <span class="s4"># now the hyperplane should rotate clock-wise and</span>
    <span class="s4"># the prediction on this point should shift</span>
    <span class="s1">assert_array_equal(clf.predict([[</span><span class="s3">0.2</span><span class="s0">, </span><span class="s1">-</span><span class="s3">1.0</span><span class="s1">]])</span><span class="s0">, </span><span class="s1">np.array([-</span><span class="s3">1</span><span class="s1">]))</span>


<span class="s1">@pytest.mark.parametrize(</span><span class="s2">&quot;klass&quot;</span><span class="s0">, </span><span class="s1">[SGDClassifier</span><span class="s0">, </span><span class="s1">SparseSGDClassifier])</span>
<span class="s0">def </span><span class="s1">test_equal_class_weight(klass):</span>
    <span class="s4"># Test if equal class weights approx. equals no class weights.</span>
    <span class="s1">X = [[</span><span class="s3">1</span><span class="s0">, </span><span class="s3">0</span><span class="s1">]</span><span class="s0">, </span><span class="s1">[</span><span class="s3">1</span><span class="s0">, </span><span class="s3">0</span><span class="s1">]</span><span class="s0">, </span><span class="s1">[</span><span class="s3">0</span><span class="s0">, </span><span class="s3">1</span><span class="s1">]</span><span class="s0">, </span><span class="s1">[</span><span class="s3">0</span><span class="s0">, </span><span class="s3">1</span><span class="s1">]]</span>
    <span class="s1">y = [</span><span class="s3">0</span><span class="s0">, </span><span class="s3">0</span><span class="s0">, </span><span class="s3">1</span><span class="s0">, </span><span class="s3">1</span><span class="s1">]</span>
    <span class="s1">clf = klass(alpha=</span><span class="s3">0.1</span><span class="s0">, </span><span class="s1">max_iter=</span><span class="s3">1000</span><span class="s0">, </span><span class="s1">class_weight=</span><span class="s0">None</span><span class="s1">)</span>
    <span class="s1">clf.fit(X</span><span class="s0">, </span><span class="s1">y)</span>

    <span class="s1">X = [[</span><span class="s3">1</span><span class="s0">, </span><span class="s3">0</span><span class="s1">]</span><span class="s0">, </span><span class="s1">[</span><span class="s3">0</span><span class="s0">, </span><span class="s3">1</span><span class="s1">]]</span>
    <span class="s1">y = [</span><span class="s3">0</span><span class="s0">, </span><span class="s3">1</span><span class="s1">]</span>
    <span class="s1">clf_weighted = klass(alpha=</span><span class="s3">0.1</span><span class="s0">, </span><span class="s1">max_iter=</span><span class="s3">1000</span><span class="s0">, </span><span class="s1">class_weight={</span><span class="s3">0</span><span class="s1">: </span><span class="s3">0.5</span><span class="s0">, </span><span class="s3">1</span><span class="s1">: </span><span class="s3">0.5</span><span class="s1">})</span>
    <span class="s1">clf_weighted.fit(X</span><span class="s0">, </span><span class="s1">y)</span>

    <span class="s4"># should be similar up to some epsilon due to learning rate schedule</span>
    <span class="s1">assert_almost_equal(clf.coef_</span><span class="s0">, </span><span class="s1">clf_weighted.coef_</span><span class="s0">, </span><span class="s1">decimal=</span><span class="s3">2</span><span class="s1">)</span>


<span class="s1">@pytest.mark.parametrize(</span><span class="s2">&quot;klass&quot;</span><span class="s0">, </span><span class="s1">[SGDClassifier</span><span class="s0">, </span><span class="s1">SparseSGDClassifier])</span>
<span class="s0">def </span><span class="s1">test_wrong_class_weight_label(klass):</span>
    <span class="s4"># ValueError due to not existing class label.</span>
    <span class="s1">clf = klass(alpha=</span><span class="s3">0.1</span><span class="s0">, </span><span class="s1">max_iter=</span><span class="s3">1000</span><span class="s0">, </span><span class="s1">class_weight={</span><span class="s3">0</span><span class="s1">: </span><span class="s3">0.5</span><span class="s1">})</span>
    <span class="s0">with </span><span class="s1">pytest.raises(ValueError):</span>
        <span class="s1">clf.fit(X</span><span class="s0">, </span><span class="s1">Y)</span>


<span class="s1">@pytest.mark.parametrize(</span><span class="s2">&quot;klass&quot;</span><span class="s0">, </span><span class="s1">[SGDClassifier</span><span class="s0">, </span><span class="s1">SparseSGDClassifier])</span>
<span class="s0">def </span><span class="s1">test_weights_multiplied(klass):</span>
    <span class="s4"># Tests that class_weight and sample_weight are multiplicative</span>
    <span class="s1">class_weights = {</span><span class="s3">1</span><span class="s1">: </span><span class="s3">0.6</span><span class="s0">, </span><span class="s3">2</span><span class="s1">: </span><span class="s3">0.3</span><span class="s1">}</span>
    <span class="s1">rng = np.random.RandomState(</span><span class="s3">0</span><span class="s1">)</span>
    <span class="s1">sample_weights = rng.random_sample(Y4.shape[</span><span class="s3">0</span><span class="s1">])</span>
    <span class="s1">multiplied_together = np.copy(sample_weights)</span>
    <span class="s1">multiplied_together[Y4 == </span><span class="s3">1</span><span class="s1">] *= class_weights[</span><span class="s3">1</span><span class="s1">]</span>
    <span class="s1">multiplied_together[Y4 == </span><span class="s3">2</span><span class="s1">] *= class_weights[</span><span class="s3">2</span><span class="s1">]</span>

    <span class="s1">clf1 = klass(alpha=</span><span class="s3">0.1</span><span class="s0">, </span><span class="s1">max_iter=</span><span class="s3">20</span><span class="s0">, </span><span class="s1">class_weight=class_weights)</span>
    <span class="s1">clf2 = klass(alpha=</span><span class="s3">0.1</span><span class="s0">, </span><span class="s1">max_iter=</span><span class="s3">20</span><span class="s1">)</span>

    <span class="s1">clf1.fit(X4</span><span class="s0">, </span><span class="s1">Y4</span><span class="s0">, </span><span class="s1">sample_weight=sample_weights)</span>
    <span class="s1">clf2.fit(X4</span><span class="s0">, </span><span class="s1">Y4</span><span class="s0">, </span><span class="s1">sample_weight=multiplied_together)</span>

    <span class="s1">assert_almost_equal(clf1.coef_</span><span class="s0">, </span><span class="s1">clf2.coef_)</span>


<span class="s1">@pytest.mark.parametrize(</span><span class="s2">&quot;klass&quot;</span><span class="s0">, </span><span class="s1">[SGDClassifier</span><span class="s0">, </span><span class="s1">SparseSGDClassifier])</span>
<span class="s0">def </span><span class="s1">test_balanced_weight(klass):</span>
    <span class="s4"># Test class weights for imbalanced data&quot;&quot;&quot;</span>
    <span class="s4"># compute reference metrics on iris dataset that is quite balanced by</span>
    <span class="s4"># default</span>
    <span class="s1">X</span><span class="s0">, </span><span class="s1">y = iris.data</span><span class="s0">, </span><span class="s1">iris.target</span>
    <span class="s1">X = scale(X)</span>
    <span class="s1">idx = np.arange(X.shape[</span><span class="s3">0</span><span class="s1">])</span>
    <span class="s1">rng = np.random.RandomState(</span><span class="s3">6</span><span class="s1">)</span>
    <span class="s1">rng.shuffle(idx)</span>
    <span class="s1">X = X[idx]</span>
    <span class="s1">y = y[idx]</span>
    <span class="s1">clf = klass(alpha=</span><span class="s3">0.0001</span><span class="s0">, </span><span class="s1">max_iter=</span><span class="s3">1000</span><span class="s0">, </span><span class="s1">class_weight=</span><span class="s0">None, </span><span class="s1">shuffle=</span><span class="s0">False</span><span class="s1">).fit(X</span><span class="s0">, </span><span class="s1">y)</span>
    <span class="s1">f1 = metrics.f1_score(y</span><span class="s0">, </span><span class="s1">clf.predict(X)</span><span class="s0">, </span><span class="s1">average=</span><span class="s2">&quot;weighted&quot;</span><span class="s1">)</span>
    <span class="s1">assert_almost_equal(f1</span><span class="s0">, </span><span class="s3">0.96</span><span class="s0">, </span><span class="s1">decimal=</span><span class="s3">1</span><span class="s1">)</span>

    <span class="s4"># make the same prediction using balanced class_weight</span>
    <span class="s1">clf_balanced = klass(</span>
        <span class="s1">alpha=</span><span class="s3">0.0001</span><span class="s0">, </span><span class="s1">max_iter=</span><span class="s3">1000</span><span class="s0">, </span><span class="s1">class_weight=</span><span class="s2">&quot;balanced&quot;</span><span class="s0">, </span><span class="s1">shuffle=</span><span class="s0">False</span>
    <span class="s1">).fit(X</span><span class="s0">, </span><span class="s1">y)</span>
    <span class="s1">f1 = metrics.f1_score(y</span><span class="s0">, </span><span class="s1">clf_balanced.predict(X)</span><span class="s0">, </span><span class="s1">average=</span><span class="s2">&quot;weighted&quot;</span><span class="s1">)</span>
    <span class="s1">assert_almost_equal(f1</span><span class="s0">, </span><span class="s3">0.96</span><span class="s0">, </span><span class="s1">decimal=</span><span class="s3">1</span><span class="s1">)</span>

    <span class="s4"># Make sure that in the balanced case it does not change anything</span>
    <span class="s4"># to use &quot;balanced&quot;</span>
    <span class="s1">assert_array_almost_equal(clf.coef_</span><span class="s0">, </span><span class="s1">clf_balanced.coef_</span><span class="s0">, </span><span class="s3">6</span><span class="s1">)</span>

    <span class="s4"># build an very very imbalanced dataset out of iris data</span>
    <span class="s1">X_0 = X[y == </span><span class="s3">0</span><span class="s0">, </span><span class="s1">:]</span>
    <span class="s1">y_0 = y[y == </span><span class="s3">0</span><span class="s1">]</span>

    <span class="s1">X_imbalanced = np.vstack([X] + [X_0] * </span><span class="s3">10</span><span class="s1">)</span>
    <span class="s1">y_imbalanced = np.concatenate([y] + [y_0] * </span><span class="s3">10</span><span class="s1">)</span>

    <span class="s4"># fit a model on the imbalanced data without class weight info</span>
    <span class="s1">clf = klass(max_iter=</span><span class="s3">1000</span><span class="s0">, </span><span class="s1">class_weight=</span><span class="s0">None, </span><span class="s1">shuffle=</span><span class="s0">False</span><span class="s1">)</span>
    <span class="s1">clf.fit(X_imbalanced</span><span class="s0">, </span><span class="s1">y_imbalanced)</span>
    <span class="s1">y_pred = clf.predict(X)</span>
    <span class="s0">assert </span><span class="s1">metrics.f1_score(y</span><span class="s0">, </span><span class="s1">y_pred</span><span class="s0">, </span><span class="s1">average=</span><span class="s2">&quot;weighted&quot;</span><span class="s1">) &lt; </span><span class="s3">0.96</span>

    <span class="s4"># fit a model with balanced class_weight enabled</span>
    <span class="s1">clf = klass(max_iter=</span><span class="s3">1000</span><span class="s0">, </span><span class="s1">class_weight=</span><span class="s2">&quot;balanced&quot;</span><span class="s0">, </span><span class="s1">shuffle=</span><span class="s0">False</span><span class="s1">)</span>
    <span class="s1">clf.fit(X_imbalanced</span><span class="s0">, </span><span class="s1">y_imbalanced)</span>
    <span class="s1">y_pred = clf.predict(X)</span>
    <span class="s0">assert </span><span class="s1">metrics.f1_score(y</span><span class="s0">, </span><span class="s1">y_pred</span><span class="s0">, </span><span class="s1">average=</span><span class="s2">&quot;weighted&quot;</span><span class="s1">) &gt; </span><span class="s3">0.96</span>


<span class="s1">@pytest.mark.parametrize(</span><span class="s2">&quot;klass&quot;</span><span class="s0">, </span><span class="s1">[SGDClassifier</span><span class="s0">, </span><span class="s1">SparseSGDClassifier])</span>
<span class="s0">def </span><span class="s1">test_sample_weights(klass):</span>
    <span class="s4"># Test weights on individual samples</span>
    <span class="s1">X = np.array([[-</span><span class="s3">1.0</span><span class="s0">, </span><span class="s1">-</span><span class="s3">1.0</span><span class="s1">]</span><span class="s0">, </span><span class="s1">[-</span><span class="s3">1.0</span><span class="s0">, </span><span class="s3">0</span><span class="s1">]</span><span class="s0">, </span><span class="s1">[-</span><span class="s3">0.8</span><span class="s0">, </span><span class="s1">-</span><span class="s3">1.0</span><span class="s1">]</span><span class="s0">, </span><span class="s1">[</span><span class="s3">1.0</span><span class="s0">, </span><span class="s3">1.0</span><span class="s1">]</span><span class="s0">, </span><span class="s1">[</span><span class="s3">1.0</span><span class="s0">, </span><span class="s3">0.0</span><span class="s1">]])</span>
    <span class="s1">y = [</span><span class="s3">1</span><span class="s0">, </span><span class="s3">1</span><span class="s0">, </span><span class="s3">1</span><span class="s0">, </span><span class="s1">-</span><span class="s3">1</span><span class="s0">, </span><span class="s1">-</span><span class="s3">1</span><span class="s1">]</span>

    <span class="s1">clf = klass(alpha=</span><span class="s3">0.1</span><span class="s0">, </span><span class="s1">max_iter=</span><span class="s3">1000</span><span class="s0">, </span><span class="s1">fit_intercept=</span><span class="s0">False</span><span class="s1">)</span>
    <span class="s1">clf.fit(X</span><span class="s0">, </span><span class="s1">y)</span>
    <span class="s1">assert_array_equal(clf.predict([[</span><span class="s3">0.2</span><span class="s0">, </span><span class="s1">-</span><span class="s3">1.0</span><span class="s1">]])</span><span class="s0">, </span><span class="s1">np.array([</span><span class="s3">1</span><span class="s1">]))</span>

    <span class="s4"># we give a small weights to class 1</span>
    <span class="s1">clf.fit(X</span><span class="s0">, </span><span class="s1">y</span><span class="s0">, </span><span class="s1">sample_weight=[</span><span class="s3">0.001</span><span class="s1">] * </span><span class="s3">3 </span><span class="s1">+ [</span><span class="s3">1</span><span class="s1">] * </span><span class="s3">2</span><span class="s1">)</span>

    <span class="s4"># now the hyperplane should rotate clock-wise and</span>
    <span class="s4"># the prediction on this point should shift</span>
    <span class="s1">assert_array_equal(clf.predict([[</span><span class="s3">0.2</span><span class="s0">, </span><span class="s1">-</span><span class="s3">1.0</span><span class="s1">]])</span><span class="s0">, </span><span class="s1">np.array([-</span><span class="s3">1</span><span class="s1">]))</span>


<span class="s1">@pytest.mark.parametrize(</span>
    <span class="s2">&quot;klass&quot;</span><span class="s0">, </span><span class="s1">[SGDClassifier</span><span class="s0">, </span><span class="s1">SparseSGDClassifier</span><span class="s0">, </span><span class="s1">SGDOneClassSVM</span><span class="s0">, </span><span class="s1">SparseSGDOneClassSVM]</span>
<span class="s1">)</span>
<span class="s0">def </span><span class="s1">test_wrong_sample_weights(klass):</span>
    <span class="s4"># Test if ValueError is raised if sample_weight has wrong shape</span>
    <span class="s0">if </span><span class="s1">klass </span><span class="s0">in </span><span class="s1">[SGDClassifier</span><span class="s0">, </span><span class="s1">SparseSGDClassifier]:</span>
        <span class="s1">clf = klass(alpha=</span><span class="s3">0.1</span><span class="s0">, </span><span class="s1">max_iter=</span><span class="s3">1000</span><span class="s0">, </span><span class="s1">fit_intercept=</span><span class="s0">False</span><span class="s1">)</span>
    <span class="s0">elif </span><span class="s1">klass </span><span class="s0">in </span><span class="s1">[SGDOneClassSVM</span><span class="s0">, </span><span class="s1">SparseSGDOneClassSVM]:</span>
        <span class="s1">clf = klass(nu=</span><span class="s3">0.1</span><span class="s0">, </span><span class="s1">max_iter=</span><span class="s3">1000</span><span class="s0">, </span><span class="s1">fit_intercept=</span><span class="s0">False</span><span class="s1">)</span>
    <span class="s4"># provided sample_weight too long</span>
    <span class="s0">with </span><span class="s1">pytest.raises(ValueError):</span>
        <span class="s1">clf.fit(X</span><span class="s0">, </span><span class="s1">Y</span><span class="s0">, </span><span class="s1">sample_weight=np.arange(</span><span class="s3">7</span><span class="s1">))</span>


<span class="s1">@pytest.mark.parametrize(</span><span class="s2">&quot;klass&quot;</span><span class="s0">, </span><span class="s1">[SGDClassifier</span><span class="s0">, </span><span class="s1">SparseSGDClassifier])</span>
<span class="s0">def </span><span class="s1">test_partial_fit_exception(klass):</span>
    <span class="s1">clf = klass(alpha=</span><span class="s3">0.01</span><span class="s1">)</span>
    <span class="s4"># classes was not specified</span>
    <span class="s0">with </span><span class="s1">pytest.raises(ValueError):</span>
        <span class="s1">clf.partial_fit(X3</span><span class="s0">, </span><span class="s1">Y3)</span>


<span class="s1">@pytest.mark.parametrize(</span><span class="s2">&quot;klass&quot;</span><span class="s0">, </span><span class="s1">[SGDClassifier</span><span class="s0">, </span><span class="s1">SparseSGDClassifier])</span>
<span class="s0">def </span><span class="s1">test_partial_fit_binary(klass):</span>
    <span class="s1">third = X.shape[</span><span class="s3">0</span><span class="s1">] // </span><span class="s3">3</span>
    <span class="s1">clf = klass(alpha=</span><span class="s3">0.01</span><span class="s1">)</span>
    <span class="s1">classes = np.unique(Y)</span>

    <span class="s1">clf.partial_fit(X[:third]</span><span class="s0">, </span><span class="s1">Y[:third]</span><span class="s0">, </span><span class="s1">classes=classes)</span>
    <span class="s0">assert </span><span class="s1">clf.coef_.shape == (</span><span class="s3">1</span><span class="s0">, </span><span class="s1">X.shape[</span><span class="s3">1</span><span class="s1">])</span>
    <span class="s0">assert </span><span class="s1">clf.intercept_.shape == (</span><span class="s3">1</span><span class="s0">,</span><span class="s1">)</span>
    <span class="s0">assert </span><span class="s1">clf.decision_function([[</span><span class="s3">0</span><span class="s0">, </span><span class="s3">0</span><span class="s1">]]).shape == (</span><span class="s3">1</span><span class="s0">,</span><span class="s1">)</span>
    <span class="s1">id1 = id(clf.coef_.data)</span>

    <span class="s1">clf.partial_fit(X[third:]</span><span class="s0">, </span><span class="s1">Y[third:])</span>
    <span class="s1">id2 = id(clf.coef_.data)</span>
    <span class="s4"># check that coef_ haven't been re-allocated</span>
    <span class="s0">assert </span><span class="s1">id1</span><span class="s0">, </span><span class="s1">id2</span>

    <span class="s1">y_pred = clf.predict(T)</span>
    <span class="s1">assert_array_equal(y_pred</span><span class="s0">, </span><span class="s1">true_result)</span>


<span class="s1">@pytest.mark.parametrize(</span><span class="s2">&quot;klass&quot;</span><span class="s0">, </span><span class="s1">[SGDClassifier</span><span class="s0">, </span><span class="s1">SparseSGDClassifier])</span>
<span class="s0">def </span><span class="s1">test_partial_fit_multiclass(klass):</span>
    <span class="s1">third = X2.shape[</span><span class="s3">0</span><span class="s1">] // </span><span class="s3">3</span>
    <span class="s1">clf = klass(alpha=</span><span class="s3">0.01</span><span class="s1">)</span>
    <span class="s1">classes = np.unique(Y2)</span>

    <span class="s1">clf.partial_fit(X2[:third]</span><span class="s0">, </span><span class="s1">Y2[:third]</span><span class="s0">, </span><span class="s1">classes=classes)</span>
    <span class="s0">assert </span><span class="s1">clf.coef_.shape == (</span><span class="s3">3</span><span class="s0">, </span><span class="s1">X2.shape[</span><span class="s3">1</span><span class="s1">])</span>
    <span class="s0">assert </span><span class="s1">clf.intercept_.shape == (</span><span class="s3">3</span><span class="s0">,</span><span class="s1">)</span>
    <span class="s0">assert </span><span class="s1">clf.decision_function([[</span><span class="s3">0</span><span class="s0">, </span><span class="s3">0</span><span class="s1">]]).shape == (</span><span class="s3">1</span><span class="s0">, </span><span class="s3">3</span><span class="s1">)</span>
    <span class="s1">id1 = id(clf.coef_.data)</span>

    <span class="s1">clf.partial_fit(X2[third:]</span><span class="s0">, </span><span class="s1">Y2[third:])</span>
    <span class="s1">id2 = id(clf.coef_.data)</span>
    <span class="s4"># check that coef_ haven't been re-allocated</span>
    <span class="s0">assert </span><span class="s1">id1</span><span class="s0">, </span><span class="s1">id2</span>


<span class="s1">@pytest.mark.parametrize(</span><span class="s2">&quot;klass&quot;</span><span class="s0">, </span><span class="s1">[SGDClassifier</span><span class="s0">, </span><span class="s1">SparseSGDClassifier])</span>
<span class="s0">def </span><span class="s1">test_partial_fit_multiclass_average(klass):</span>
    <span class="s1">third = X2.shape[</span><span class="s3">0</span><span class="s1">] // </span><span class="s3">3</span>
    <span class="s1">clf = klass(alpha=</span><span class="s3">0.01</span><span class="s0">, </span><span class="s1">average=X2.shape[</span><span class="s3">0</span><span class="s1">])</span>
    <span class="s1">classes = np.unique(Y2)</span>

    <span class="s1">clf.partial_fit(X2[:third]</span><span class="s0">, </span><span class="s1">Y2[:third]</span><span class="s0">, </span><span class="s1">classes=classes)</span>
    <span class="s0">assert </span><span class="s1">clf.coef_.shape == (</span><span class="s3">3</span><span class="s0">, </span><span class="s1">X2.shape[</span><span class="s3">1</span><span class="s1">])</span>
    <span class="s0">assert </span><span class="s1">clf.intercept_.shape == (</span><span class="s3">3</span><span class="s0">,</span><span class="s1">)</span>

    <span class="s1">clf.partial_fit(X2[third:]</span><span class="s0">, </span><span class="s1">Y2[third:])</span>
    <span class="s0">assert </span><span class="s1">clf.coef_.shape == (</span><span class="s3">3</span><span class="s0">, </span><span class="s1">X2.shape[</span><span class="s3">1</span><span class="s1">])</span>
    <span class="s0">assert </span><span class="s1">clf.intercept_.shape == (</span><span class="s3">3</span><span class="s0">,</span><span class="s1">)</span>


<span class="s1">@pytest.mark.parametrize(</span><span class="s2">&quot;klass&quot;</span><span class="s0">, </span><span class="s1">[SGDClassifier</span><span class="s0">, </span><span class="s1">SparseSGDClassifier])</span>
<span class="s0">def </span><span class="s1">test_fit_then_partial_fit(klass):</span>
    <span class="s4"># Partial_fit should work after initial fit in the multiclass case.</span>
    <span class="s4"># Non-regression test for #2496; fit would previously produce a</span>
    <span class="s4"># Fortran-ordered coef_ that subsequent partial_fit couldn't handle.</span>
    <span class="s1">clf = klass()</span>
    <span class="s1">clf.fit(X2</span><span class="s0">, </span><span class="s1">Y2)</span>
    <span class="s1">clf.partial_fit(X2</span><span class="s0">, </span><span class="s1">Y2)  </span><span class="s4"># no exception here</span>


<span class="s1">@pytest.mark.parametrize(</span><span class="s2">&quot;klass&quot;</span><span class="s0">, </span><span class="s1">[SGDClassifier</span><span class="s0">, </span><span class="s1">SparseSGDClassifier])</span>
<span class="s1">@pytest.mark.parametrize(</span><span class="s2">&quot;lr&quot;</span><span class="s0">, </span><span class="s1">[</span><span class="s2">&quot;constant&quot;</span><span class="s0">, </span><span class="s2">&quot;optimal&quot;</span><span class="s0">, </span><span class="s2">&quot;invscaling&quot;</span><span class="s0">, </span><span class="s2">&quot;adaptive&quot;</span><span class="s1">])</span>
<span class="s0">def </span><span class="s1">test_partial_fit_equal_fit_classif(klass</span><span class="s0">, </span><span class="s1">lr):</span>
    <span class="s0">for </span><span class="s1">X_</span><span class="s0">, </span><span class="s1">Y_</span><span class="s0">, </span><span class="s1">T_ </span><span class="s0">in </span><span class="s1">((X</span><span class="s0">, </span><span class="s1">Y</span><span class="s0">, </span><span class="s1">T)</span><span class="s0">, </span><span class="s1">(X2</span><span class="s0">, </span><span class="s1">Y2</span><span class="s0">, </span><span class="s1">T2)):</span>
        <span class="s1">clf = klass(alpha=</span><span class="s3">0.01</span><span class="s0">, </span><span class="s1">eta0=</span><span class="s3">0.01</span><span class="s0">, </span><span class="s1">max_iter=</span><span class="s3">2</span><span class="s0">, </span><span class="s1">learning_rate=lr</span><span class="s0">, </span><span class="s1">shuffle=</span><span class="s0">False</span><span class="s1">)</span>
        <span class="s1">clf.fit(X_</span><span class="s0">, </span><span class="s1">Y_)</span>
        <span class="s1">y_pred = clf.decision_function(T_)</span>
        <span class="s1">t = clf.t_</span>

        <span class="s1">classes = np.unique(Y_)</span>
        <span class="s1">clf = klass(alpha=</span><span class="s3">0.01</span><span class="s0">, </span><span class="s1">eta0=</span><span class="s3">0.01</span><span class="s0">, </span><span class="s1">learning_rate=lr</span><span class="s0">, </span><span class="s1">shuffle=</span><span class="s0">False</span><span class="s1">)</span>
        <span class="s0">for </span><span class="s1">i </span><span class="s0">in </span><span class="s1">range(</span><span class="s3">2</span><span class="s1">):</span>
            <span class="s1">clf.partial_fit(X_</span><span class="s0">, </span><span class="s1">Y_</span><span class="s0">, </span><span class="s1">classes=classes)</span>
        <span class="s1">y_pred2 = clf.decision_function(T_)</span>

        <span class="s0">assert </span><span class="s1">clf.t_ == t</span>
        <span class="s1">assert_array_almost_equal(y_pred</span><span class="s0">, </span><span class="s1">y_pred2</span><span class="s0">, </span><span class="s1">decimal=</span><span class="s3">2</span><span class="s1">)</span>


<span class="s1">@pytest.mark.parametrize(</span><span class="s2">&quot;klass&quot;</span><span class="s0">, </span><span class="s1">[SGDClassifier</span><span class="s0">, </span><span class="s1">SparseSGDClassifier])</span>
<span class="s0">def </span><span class="s1">test_regression_losses(klass):</span>
    <span class="s1">random_state = np.random.RandomState(</span><span class="s3">1</span><span class="s1">)</span>
    <span class="s1">clf = klass(</span>
        <span class="s1">alpha=</span><span class="s3">0.01</span><span class="s0">,</span>
        <span class="s1">learning_rate=</span><span class="s2">&quot;constant&quot;</span><span class="s0">,</span>
        <span class="s1">eta0=</span><span class="s3">0.1</span><span class="s0">,</span>
        <span class="s1">loss=</span><span class="s2">&quot;epsilon_insensitive&quot;</span><span class="s0">,</span>
        <span class="s1">random_state=random_state</span><span class="s0">,</span>
    <span class="s1">)</span>
    <span class="s1">clf.fit(X</span><span class="s0">, </span><span class="s1">Y)</span>
    <span class="s0">assert </span><span class="s3">1.0 </span><span class="s1">== np.mean(clf.predict(X) == Y)</span>

    <span class="s1">clf = klass(</span>
        <span class="s1">alpha=</span><span class="s3">0.01</span><span class="s0">,</span>
        <span class="s1">learning_rate=</span><span class="s2">&quot;constant&quot;</span><span class="s0">,</span>
        <span class="s1">eta0=</span><span class="s3">0.1</span><span class="s0">,</span>
        <span class="s1">loss=</span><span class="s2">&quot;squared_epsilon_insensitive&quot;</span><span class="s0">,</span>
        <span class="s1">random_state=random_state</span><span class="s0">,</span>
    <span class="s1">)</span>
    <span class="s1">clf.fit(X</span><span class="s0">, </span><span class="s1">Y)</span>
    <span class="s0">assert </span><span class="s3">1.0 </span><span class="s1">== np.mean(clf.predict(X) == Y)</span>

    <span class="s1">clf = klass(alpha=</span><span class="s3">0.01</span><span class="s0">, </span><span class="s1">loss=</span><span class="s2">&quot;huber&quot;</span><span class="s0">, </span><span class="s1">random_state=random_state)</span>
    <span class="s1">clf.fit(X</span><span class="s0">, </span><span class="s1">Y)</span>
    <span class="s0">assert </span><span class="s3">1.0 </span><span class="s1">== np.mean(clf.predict(X) == Y)</span>

    <span class="s1">clf = klass(</span>
        <span class="s1">alpha=</span><span class="s3">0.01</span><span class="s0">,</span>
        <span class="s1">learning_rate=</span><span class="s2">&quot;constant&quot;</span><span class="s0">,</span>
        <span class="s1">eta0=</span><span class="s3">0.01</span><span class="s0">,</span>
        <span class="s1">loss=</span><span class="s2">&quot;squared_error&quot;</span><span class="s0">,</span>
        <span class="s1">random_state=random_state</span><span class="s0">,</span>
    <span class="s1">)</span>
    <span class="s1">clf.fit(X</span><span class="s0">, </span><span class="s1">Y)</span>
    <span class="s0">assert </span><span class="s3">1.0 </span><span class="s1">== np.mean(clf.predict(X) == Y)</span>


<span class="s1">@pytest.mark.parametrize(</span><span class="s2">&quot;klass&quot;</span><span class="s0">, </span><span class="s1">[SGDClassifier</span><span class="s0">, </span><span class="s1">SparseSGDClassifier])</span>
<span class="s0">def </span><span class="s1">test_warm_start_multiclass(klass):</span>
    <span class="s1">_test_warm_start(klass</span><span class="s0">, </span><span class="s1">X2</span><span class="s0">, </span><span class="s1">Y2</span><span class="s0">, </span><span class="s2">&quot;optimal&quot;</span><span class="s1">)</span>


<span class="s1">@pytest.mark.parametrize(</span><span class="s2">&quot;klass&quot;</span><span class="s0">, </span><span class="s1">[SGDClassifier</span><span class="s0">, </span><span class="s1">SparseSGDClassifier])</span>
<span class="s0">def </span><span class="s1">test_multiple_fit(klass):</span>
    <span class="s4"># Test multiple calls of fit w/ different shaped inputs.</span>
    <span class="s1">clf = klass(alpha=</span><span class="s3">0.01</span><span class="s0">, </span><span class="s1">shuffle=</span><span class="s0">False</span><span class="s1">)</span>
    <span class="s1">clf.fit(X</span><span class="s0">, </span><span class="s1">Y)</span>
    <span class="s0">assert </span><span class="s1">hasattr(clf</span><span class="s0">, </span><span class="s2">&quot;coef_&quot;</span><span class="s1">)</span>

    <span class="s4"># Non-regression test: try fitting with a different label set.</span>
    <span class="s1">y = [[</span><span class="s2">&quot;ham&quot;</span><span class="s0">, </span><span class="s2">&quot;spam&quot;</span><span class="s1">][i] </span><span class="s0">for </span><span class="s1">i </span><span class="s0">in </span><span class="s1">LabelEncoder().fit_transform(Y)]</span>
    <span class="s1">clf.fit(X[:</span><span class="s0">, </span><span class="s1">:-</span><span class="s3">1</span><span class="s1">]</span><span class="s0">, </span><span class="s1">y)</span>


<span class="s4">###############################################################################</span>
<span class="s4"># Regression Test Case</span>


<span class="s1">@pytest.mark.parametrize(</span><span class="s2">&quot;klass&quot;</span><span class="s0">, </span><span class="s1">[SGDRegressor</span><span class="s0">, </span><span class="s1">SparseSGDRegressor])</span>
<span class="s0">def </span><span class="s1">test_sgd_reg(klass):</span>
    <span class="s4"># Check that SGD gives any results.</span>
    <span class="s1">clf = klass(alpha=</span><span class="s3">0.1</span><span class="s0">, </span><span class="s1">max_iter=</span><span class="s3">2</span><span class="s0">, </span><span class="s1">fit_intercept=</span><span class="s0">False</span><span class="s1">)</span>
    <span class="s1">clf.fit([[</span><span class="s3">0</span><span class="s0">, </span><span class="s3">0</span><span class="s1">]</span><span class="s0">, </span><span class="s1">[</span><span class="s3">1</span><span class="s0">, </span><span class="s3">1</span><span class="s1">]</span><span class="s0">, </span><span class="s1">[</span><span class="s3">2</span><span class="s0">, </span><span class="s3">2</span><span class="s1">]]</span><span class="s0">, </span><span class="s1">[</span><span class="s3">0</span><span class="s0">, </span><span class="s3">1</span><span class="s0">, </span><span class="s3">2</span><span class="s1">])</span>
    <span class="s0">assert </span><span class="s1">clf.coef_[</span><span class="s3">0</span><span class="s1">] == clf.coef_[</span><span class="s3">1</span><span class="s1">]</span>


<span class="s1">@pytest.mark.parametrize(</span><span class="s2">&quot;klass&quot;</span><span class="s0">, </span><span class="s1">[SGDRegressor</span><span class="s0">, </span><span class="s1">SparseSGDRegressor])</span>
<span class="s0">def </span><span class="s1">test_sgd_averaged_computed_correctly(klass):</span>
    <span class="s4"># Tests the average regressor matches the naive implementation</span>

    <span class="s1">eta = </span><span class="s3">0.001</span>
    <span class="s1">alpha = </span><span class="s3">0.01</span>
    <span class="s1">n_samples = </span><span class="s3">20</span>
    <span class="s1">n_features = </span><span class="s3">10</span>
    <span class="s1">rng = np.random.RandomState(</span><span class="s3">0</span><span class="s1">)</span>
    <span class="s1">X = rng.normal(size=(n_samples</span><span class="s0">, </span><span class="s1">n_features))</span>
    <span class="s1">w = rng.normal(size=n_features)</span>

    <span class="s4"># simple linear function without noise</span>
    <span class="s1">y = np.dot(X</span><span class="s0">, </span><span class="s1">w)</span>

    <span class="s1">clf = klass(</span>
        <span class="s1">loss=</span><span class="s2">&quot;squared_error&quot;</span><span class="s0">,</span>
        <span class="s1">learning_rate=</span><span class="s2">&quot;constant&quot;</span><span class="s0">,</span>
        <span class="s1">eta0=eta</span><span class="s0">,</span>
        <span class="s1">alpha=alpha</span><span class="s0">,</span>
        <span class="s1">fit_intercept=</span><span class="s0">True,</span>
        <span class="s1">max_iter=</span><span class="s3">1</span><span class="s0">,</span>
        <span class="s1">average=</span><span class="s0">True,</span>
        <span class="s1">shuffle=</span><span class="s0">False,</span>
    <span class="s1">)</span>

    <span class="s1">clf.fit(X</span><span class="s0">, </span><span class="s1">y)</span>
    <span class="s1">average_weights</span><span class="s0">, </span><span class="s1">average_intercept = asgd(klass</span><span class="s0">, </span><span class="s1">X</span><span class="s0">, </span><span class="s1">y</span><span class="s0">, </span><span class="s1">eta</span><span class="s0">, </span><span class="s1">alpha)</span>

    <span class="s1">assert_array_almost_equal(clf.coef_</span><span class="s0">, </span><span class="s1">average_weights</span><span class="s0">, </span><span class="s1">decimal=</span><span class="s3">16</span><span class="s1">)</span>
    <span class="s1">assert_almost_equal(clf.intercept_</span><span class="s0">, </span><span class="s1">average_intercept</span><span class="s0">, </span><span class="s1">decimal=</span><span class="s3">16</span><span class="s1">)</span>


<span class="s1">@pytest.mark.parametrize(</span><span class="s2">&quot;klass&quot;</span><span class="s0">, </span><span class="s1">[SGDRegressor</span><span class="s0">, </span><span class="s1">SparseSGDRegressor])</span>
<span class="s0">def </span><span class="s1">test_sgd_averaged_partial_fit(klass):</span>
    <span class="s4"># Tests whether the partial fit yields the same average as the fit</span>
    <span class="s1">eta = </span><span class="s3">0.001</span>
    <span class="s1">alpha = </span><span class="s3">0.01</span>
    <span class="s1">n_samples = </span><span class="s3">20</span>
    <span class="s1">n_features = </span><span class="s3">10</span>
    <span class="s1">rng = np.random.RandomState(</span><span class="s3">0</span><span class="s1">)</span>
    <span class="s1">X = rng.normal(size=(n_samples</span><span class="s0">, </span><span class="s1">n_features))</span>
    <span class="s1">w = rng.normal(size=n_features)</span>

    <span class="s4"># simple linear function without noise</span>
    <span class="s1">y = np.dot(X</span><span class="s0">, </span><span class="s1">w)</span>

    <span class="s1">clf = klass(</span>
        <span class="s1">loss=</span><span class="s2">&quot;squared_error&quot;</span><span class="s0">,</span>
        <span class="s1">learning_rate=</span><span class="s2">&quot;constant&quot;</span><span class="s0">,</span>
        <span class="s1">eta0=eta</span><span class="s0">,</span>
        <span class="s1">alpha=alpha</span><span class="s0">,</span>
        <span class="s1">fit_intercept=</span><span class="s0">True,</span>
        <span class="s1">max_iter=</span><span class="s3">1</span><span class="s0">,</span>
        <span class="s1">average=</span><span class="s0">True,</span>
        <span class="s1">shuffle=</span><span class="s0">False,</span>
    <span class="s1">)</span>

    <span class="s1">clf.partial_fit(X[: int(n_samples / </span><span class="s3">2</span><span class="s1">)][:]</span><span class="s0">, </span><span class="s1">y[: int(n_samples / </span><span class="s3">2</span><span class="s1">)])</span>
    <span class="s1">clf.partial_fit(X[int(n_samples / </span><span class="s3">2</span><span class="s1">) :][:]</span><span class="s0">, </span><span class="s1">y[int(n_samples / </span><span class="s3">2</span><span class="s1">) :])</span>
    <span class="s1">average_weights</span><span class="s0">, </span><span class="s1">average_intercept = asgd(klass</span><span class="s0">, </span><span class="s1">X</span><span class="s0">, </span><span class="s1">y</span><span class="s0">, </span><span class="s1">eta</span><span class="s0">, </span><span class="s1">alpha)</span>

    <span class="s1">assert_array_almost_equal(clf.coef_</span><span class="s0">, </span><span class="s1">average_weights</span><span class="s0">, </span><span class="s1">decimal=</span><span class="s3">16</span><span class="s1">)</span>
    <span class="s1">assert_almost_equal(clf.intercept_[</span><span class="s3">0</span><span class="s1">]</span><span class="s0">, </span><span class="s1">average_intercept</span><span class="s0">, </span><span class="s1">decimal=</span><span class="s3">16</span><span class="s1">)</span>


<span class="s1">@pytest.mark.parametrize(</span><span class="s2">&quot;klass&quot;</span><span class="s0">, </span><span class="s1">[SGDRegressor</span><span class="s0">, </span><span class="s1">SparseSGDRegressor])</span>
<span class="s0">def </span><span class="s1">test_average_sparse(klass):</span>
    <span class="s4"># Checks the average weights on data with 0s</span>

    <span class="s1">eta = </span><span class="s3">0.001</span>
    <span class="s1">alpha = </span><span class="s3">0.01</span>
    <span class="s1">clf = klass(</span>
        <span class="s1">loss=</span><span class="s2">&quot;squared_error&quot;</span><span class="s0">,</span>
        <span class="s1">learning_rate=</span><span class="s2">&quot;constant&quot;</span><span class="s0">,</span>
        <span class="s1">eta0=eta</span><span class="s0">,</span>
        <span class="s1">alpha=alpha</span><span class="s0">,</span>
        <span class="s1">fit_intercept=</span><span class="s0">True,</span>
        <span class="s1">max_iter=</span><span class="s3">1</span><span class="s0">,</span>
        <span class="s1">average=</span><span class="s0">True,</span>
        <span class="s1">shuffle=</span><span class="s0">False,</span>
    <span class="s1">)</span>

    <span class="s1">n_samples = Y3.shape[</span><span class="s3">0</span><span class="s1">]</span>

    <span class="s1">clf.partial_fit(X3[: int(n_samples / </span><span class="s3">2</span><span class="s1">)][:]</span><span class="s0">, </span><span class="s1">Y3[: int(n_samples / </span><span class="s3">2</span><span class="s1">)])</span>
    <span class="s1">clf.partial_fit(X3[int(n_samples / </span><span class="s3">2</span><span class="s1">) :][:]</span><span class="s0">, </span><span class="s1">Y3[int(n_samples / </span><span class="s3">2</span><span class="s1">) :])</span>
    <span class="s1">average_weights</span><span class="s0">, </span><span class="s1">average_intercept = asgd(klass</span><span class="s0">, </span><span class="s1">X3</span><span class="s0">, </span><span class="s1">Y3</span><span class="s0">, </span><span class="s1">eta</span><span class="s0">, </span><span class="s1">alpha)</span>

    <span class="s1">assert_array_almost_equal(clf.coef_</span><span class="s0">, </span><span class="s1">average_weights</span><span class="s0">, </span><span class="s1">decimal=</span><span class="s3">16</span><span class="s1">)</span>
    <span class="s1">assert_almost_equal(clf.intercept_</span><span class="s0">, </span><span class="s1">average_intercept</span><span class="s0">, </span><span class="s1">decimal=</span><span class="s3">16</span><span class="s1">)</span>


<span class="s1">@pytest.mark.parametrize(</span><span class="s2">&quot;klass&quot;</span><span class="s0">, </span><span class="s1">[SGDRegressor</span><span class="s0">, </span><span class="s1">SparseSGDRegressor])</span>
<span class="s0">def </span><span class="s1">test_sgd_least_squares_fit(klass):</span>
    <span class="s1">xmin</span><span class="s0">, </span><span class="s1">xmax = -</span><span class="s3">5</span><span class="s0">, </span><span class="s3">5</span>
    <span class="s1">n_samples = </span><span class="s3">100</span>
    <span class="s1">rng = np.random.RandomState(</span><span class="s3">0</span><span class="s1">)</span>
    <span class="s1">X = np.linspace(xmin</span><span class="s0">, </span><span class="s1">xmax</span><span class="s0">, </span><span class="s1">n_samples).reshape(n_samples</span><span class="s0">, </span><span class="s3">1</span><span class="s1">)</span>

    <span class="s4"># simple linear function without noise</span>
    <span class="s1">y = </span><span class="s3">0.5 </span><span class="s1">* X.ravel()</span>

    <span class="s1">clf = klass(loss=</span><span class="s2">&quot;squared_error&quot;</span><span class="s0">, </span><span class="s1">alpha=</span><span class="s3">0.1</span><span class="s0">, </span><span class="s1">max_iter=</span><span class="s3">20</span><span class="s0">, </span><span class="s1">fit_intercept=</span><span class="s0">False</span><span class="s1">)</span>
    <span class="s1">clf.fit(X</span><span class="s0">, </span><span class="s1">y)</span>
    <span class="s1">score = clf.score(X</span><span class="s0">, </span><span class="s1">y)</span>
    <span class="s0">assert </span><span class="s1">score &gt; </span><span class="s3">0.99</span>

    <span class="s4"># simple linear function with noise</span>
    <span class="s1">y = </span><span class="s3">0.5 </span><span class="s1">* X.ravel() + rng.randn(n_samples</span><span class="s0">, </span><span class="s3">1</span><span class="s1">).ravel()</span>

    <span class="s1">clf = klass(loss=</span><span class="s2">&quot;squared_error&quot;</span><span class="s0">, </span><span class="s1">alpha=</span><span class="s3">0.1</span><span class="s0">, </span><span class="s1">max_iter=</span><span class="s3">20</span><span class="s0">, </span><span class="s1">fit_intercept=</span><span class="s0">False</span><span class="s1">)</span>
    <span class="s1">clf.fit(X</span><span class="s0">, </span><span class="s1">y)</span>
    <span class="s1">score = clf.score(X</span><span class="s0">, </span><span class="s1">y)</span>
    <span class="s0">assert </span><span class="s1">score &gt; </span><span class="s3">0.5</span>


<span class="s1">@pytest.mark.parametrize(</span><span class="s2">&quot;klass&quot;</span><span class="s0">, </span><span class="s1">[SGDRegressor</span><span class="s0">, </span><span class="s1">SparseSGDRegressor])</span>
<span class="s0">def </span><span class="s1">test_sgd_epsilon_insensitive(klass):</span>
    <span class="s1">xmin</span><span class="s0">, </span><span class="s1">xmax = -</span><span class="s3">5</span><span class="s0">, </span><span class="s3">5</span>
    <span class="s1">n_samples = </span><span class="s3">100</span>
    <span class="s1">rng = np.random.RandomState(</span><span class="s3">0</span><span class="s1">)</span>
    <span class="s1">X = np.linspace(xmin</span><span class="s0">, </span><span class="s1">xmax</span><span class="s0">, </span><span class="s1">n_samples).reshape(n_samples</span><span class="s0">, </span><span class="s3">1</span><span class="s1">)</span>

    <span class="s4"># simple linear function without noise</span>
    <span class="s1">y = </span><span class="s3">0.5 </span><span class="s1">* X.ravel()</span>

    <span class="s1">clf = klass(</span>
        <span class="s1">loss=</span><span class="s2">&quot;epsilon_insensitive&quot;</span><span class="s0">,</span>
        <span class="s1">epsilon=</span><span class="s3">0.01</span><span class="s0">,</span>
        <span class="s1">alpha=</span><span class="s3">0.1</span><span class="s0">,</span>
        <span class="s1">max_iter=</span><span class="s3">20</span><span class="s0">,</span>
        <span class="s1">fit_intercept=</span><span class="s0">False,</span>
    <span class="s1">)</span>
    <span class="s1">clf.fit(X</span><span class="s0">, </span><span class="s1">y)</span>
    <span class="s1">score = clf.score(X</span><span class="s0">, </span><span class="s1">y)</span>
    <span class="s0">assert </span><span class="s1">score &gt; </span><span class="s3">0.99</span>

    <span class="s4"># simple linear function with noise</span>
    <span class="s1">y = </span><span class="s3">0.5 </span><span class="s1">* X.ravel() + rng.randn(n_samples</span><span class="s0">, </span><span class="s3">1</span><span class="s1">).ravel()</span>

    <span class="s1">clf = klass(</span>
        <span class="s1">loss=</span><span class="s2">&quot;epsilon_insensitive&quot;</span><span class="s0">,</span>
        <span class="s1">epsilon=</span><span class="s3">0.01</span><span class="s0">,</span>
        <span class="s1">alpha=</span><span class="s3">0.1</span><span class="s0">,</span>
        <span class="s1">max_iter=</span><span class="s3">20</span><span class="s0">,</span>
        <span class="s1">fit_intercept=</span><span class="s0">False,</span>
    <span class="s1">)</span>
    <span class="s1">clf.fit(X</span><span class="s0">, </span><span class="s1">y)</span>
    <span class="s1">score = clf.score(X</span><span class="s0">, </span><span class="s1">y)</span>
    <span class="s0">assert </span><span class="s1">score &gt; </span><span class="s3">0.5</span>


<span class="s1">@pytest.mark.parametrize(</span><span class="s2">&quot;klass&quot;</span><span class="s0">, </span><span class="s1">[SGDRegressor</span><span class="s0">, </span><span class="s1">SparseSGDRegressor])</span>
<span class="s0">def </span><span class="s1">test_sgd_huber_fit(klass):</span>
    <span class="s1">xmin</span><span class="s0">, </span><span class="s1">xmax = -</span><span class="s3">5</span><span class="s0">, </span><span class="s3">5</span>
    <span class="s1">n_samples = </span><span class="s3">100</span>
    <span class="s1">rng = np.random.RandomState(</span><span class="s3">0</span><span class="s1">)</span>
    <span class="s1">X = np.linspace(xmin</span><span class="s0">, </span><span class="s1">xmax</span><span class="s0">, </span><span class="s1">n_samples).reshape(n_samples</span><span class="s0">, </span><span class="s3">1</span><span class="s1">)</span>

    <span class="s4"># simple linear function without noise</span>
    <span class="s1">y = </span><span class="s3">0.5 </span><span class="s1">* X.ravel()</span>

    <span class="s1">clf = klass(loss=</span><span class="s2">&quot;huber&quot;</span><span class="s0">, </span><span class="s1">epsilon=</span><span class="s3">0.1</span><span class="s0">, </span><span class="s1">alpha=</span><span class="s3">0.1</span><span class="s0">, </span><span class="s1">max_iter=</span><span class="s3">20</span><span class="s0">, </span><span class="s1">fit_intercept=</span><span class="s0">False</span><span class="s1">)</span>
    <span class="s1">clf.fit(X</span><span class="s0">, </span><span class="s1">y)</span>
    <span class="s1">score = clf.score(X</span><span class="s0">, </span><span class="s1">y)</span>
    <span class="s0">assert </span><span class="s1">score &gt; </span><span class="s3">0.99</span>

    <span class="s4"># simple linear function with noise</span>
    <span class="s1">y = </span><span class="s3">0.5 </span><span class="s1">* X.ravel() + rng.randn(n_samples</span><span class="s0">, </span><span class="s3">1</span><span class="s1">).ravel()</span>

    <span class="s1">clf = klass(loss=</span><span class="s2">&quot;huber&quot;</span><span class="s0">, </span><span class="s1">epsilon=</span><span class="s3">0.1</span><span class="s0">, </span><span class="s1">alpha=</span><span class="s3">0.1</span><span class="s0">, </span><span class="s1">max_iter=</span><span class="s3">20</span><span class="s0">, </span><span class="s1">fit_intercept=</span><span class="s0">False</span><span class="s1">)</span>
    <span class="s1">clf.fit(X</span><span class="s0">, </span><span class="s1">y)</span>
    <span class="s1">score = clf.score(X</span><span class="s0">, </span><span class="s1">y)</span>
    <span class="s0">assert </span><span class="s1">score &gt; </span><span class="s3">0.5</span>


<span class="s1">@pytest.mark.parametrize(</span><span class="s2">&quot;klass&quot;</span><span class="s0">, </span><span class="s1">[SGDRegressor</span><span class="s0">, </span><span class="s1">SparseSGDRegressor])</span>
<span class="s0">def </span><span class="s1">test_elasticnet_convergence(klass):</span>
    <span class="s4"># Check that the SGD output is consistent with coordinate descent</span>

    <span class="s1">n_samples</span><span class="s0">, </span><span class="s1">n_features = </span><span class="s3">1000</span><span class="s0">, </span><span class="s3">5</span>
    <span class="s1">rng = np.random.RandomState(</span><span class="s3">0</span><span class="s1">)</span>
    <span class="s1">X = rng.randn(n_samples</span><span class="s0">, </span><span class="s1">n_features)</span>
    <span class="s4"># ground_truth linear model that generate y from X and to which the</span>
    <span class="s4"># models should converge if the regularizer would be set to 0.0</span>
    <span class="s1">ground_truth_coef = rng.randn(n_features)</span>
    <span class="s1">y = np.dot(X</span><span class="s0">, </span><span class="s1">ground_truth_coef)</span>

    <span class="s4"># XXX: alpha = 0.1 seems to cause convergence problems</span>
    <span class="s0">for </span><span class="s1">alpha </span><span class="s0">in </span><span class="s1">[</span><span class="s3">0.01</span><span class="s0">, </span><span class="s3">0.001</span><span class="s1">]:</span>
        <span class="s0">for </span><span class="s1">l1_ratio </span><span class="s0">in </span><span class="s1">[</span><span class="s3">0.5</span><span class="s0">, </span><span class="s3">0.8</span><span class="s0">, </span><span class="s3">1.0</span><span class="s1">]:</span>
            <span class="s1">cd = linear_model.ElasticNet(</span>
                <span class="s1">alpha=alpha</span><span class="s0">, </span><span class="s1">l1_ratio=l1_ratio</span><span class="s0">, </span><span class="s1">fit_intercept=</span><span class="s0">False</span>
            <span class="s1">)</span>
            <span class="s1">cd.fit(X</span><span class="s0">, </span><span class="s1">y)</span>
            <span class="s1">sgd = klass(</span>
                <span class="s1">penalty=</span><span class="s2">&quot;elasticnet&quot;</span><span class="s0">,</span>
                <span class="s1">max_iter=</span><span class="s3">50</span><span class="s0">,</span>
                <span class="s1">alpha=alpha</span><span class="s0">,</span>
                <span class="s1">l1_ratio=l1_ratio</span><span class="s0">,</span>
                <span class="s1">fit_intercept=</span><span class="s0">False,</span>
            <span class="s1">)</span>
            <span class="s1">sgd.fit(X</span><span class="s0">, </span><span class="s1">y)</span>
            <span class="s1">err_msg = (</span>
                <span class="s2">&quot;cd and sgd did not converge to comparable &quot;</span>
                <span class="s2">&quot;results for alpha=%f and l1_ratio=%f&quot; </span><span class="s1">% (alpha</span><span class="s0">, </span><span class="s1">l1_ratio)</span>
            <span class="s1">)</span>
            <span class="s1">assert_almost_equal(cd.coef_</span><span class="s0">, </span><span class="s1">sgd.coef_</span><span class="s0">, </span><span class="s1">decimal=</span><span class="s3">2</span><span class="s0">, </span><span class="s1">err_msg=err_msg)</span>


<span class="s1">@ignore_warnings</span>
<span class="s1">@pytest.mark.parametrize(</span><span class="s2">&quot;klass&quot;</span><span class="s0">, </span><span class="s1">[SGDRegressor</span><span class="s0">, </span><span class="s1">SparseSGDRegressor])</span>
<span class="s0">def </span><span class="s1">test_partial_fit(klass):</span>
    <span class="s1">third = X.shape[</span><span class="s3">0</span><span class="s1">] // </span><span class="s3">3</span>
    <span class="s1">clf = klass(alpha=</span><span class="s3">0.01</span><span class="s1">)</span>

    <span class="s1">clf.partial_fit(X[:third]</span><span class="s0">, </span><span class="s1">Y[:third])</span>
    <span class="s0">assert </span><span class="s1">clf.coef_.shape == (X.shape[</span><span class="s3">1</span><span class="s1">]</span><span class="s0">,</span><span class="s1">)</span>
    <span class="s0">assert </span><span class="s1">clf.intercept_.shape == (</span><span class="s3">1</span><span class="s0">,</span><span class="s1">)</span>
    <span class="s0">assert </span><span class="s1">clf.predict([[</span><span class="s3">0</span><span class="s0">, </span><span class="s3">0</span><span class="s1">]]).shape == (</span><span class="s3">1</span><span class="s0">,</span><span class="s1">)</span>
    <span class="s1">id1 = id(clf.coef_.data)</span>

    <span class="s1">clf.partial_fit(X[third:]</span><span class="s0">, </span><span class="s1">Y[third:])</span>
    <span class="s1">id2 = id(clf.coef_.data)</span>
    <span class="s4"># check that coef_ haven't been re-allocated</span>
    <span class="s0">assert </span><span class="s1">id1</span><span class="s0">, </span><span class="s1">id2</span>


<span class="s1">@pytest.mark.parametrize(</span><span class="s2">&quot;klass&quot;</span><span class="s0">, </span><span class="s1">[SGDRegressor</span><span class="s0">, </span><span class="s1">SparseSGDRegressor])</span>
<span class="s1">@pytest.mark.parametrize(</span><span class="s2">&quot;lr&quot;</span><span class="s0">, </span><span class="s1">[</span><span class="s2">&quot;constant&quot;</span><span class="s0">, </span><span class="s2">&quot;optimal&quot;</span><span class="s0">, </span><span class="s2">&quot;invscaling&quot;</span><span class="s0">, </span><span class="s2">&quot;adaptive&quot;</span><span class="s1">])</span>
<span class="s0">def </span><span class="s1">test_partial_fit_equal_fit(klass</span><span class="s0">, </span><span class="s1">lr):</span>
    <span class="s1">clf = klass(alpha=</span><span class="s3">0.01</span><span class="s0">, </span><span class="s1">max_iter=</span><span class="s3">2</span><span class="s0">, </span><span class="s1">eta0=</span><span class="s3">0.01</span><span class="s0">, </span><span class="s1">learning_rate=lr</span><span class="s0">, </span><span class="s1">shuffle=</span><span class="s0">False</span><span class="s1">)</span>
    <span class="s1">clf.fit(X</span><span class="s0">, </span><span class="s1">Y)</span>
    <span class="s1">y_pred = clf.predict(T)</span>
    <span class="s1">t = clf.t_</span>

    <span class="s1">clf = klass(alpha=</span><span class="s3">0.01</span><span class="s0">, </span><span class="s1">eta0=</span><span class="s3">0.01</span><span class="s0">, </span><span class="s1">learning_rate=lr</span><span class="s0">, </span><span class="s1">shuffle=</span><span class="s0">False</span><span class="s1">)</span>
    <span class="s0">for </span><span class="s1">i </span><span class="s0">in </span><span class="s1">range(</span><span class="s3">2</span><span class="s1">):</span>
        <span class="s1">clf.partial_fit(X</span><span class="s0">, </span><span class="s1">Y)</span>
    <span class="s1">y_pred2 = clf.predict(T)</span>

    <span class="s0">assert </span><span class="s1">clf.t_ == t</span>
    <span class="s1">assert_array_almost_equal(y_pred</span><span class="s0">, </span><span class="s1">y_pred2</span><span class="s0">, </span><span class="s1">decimal=</span><span class="s3">2</span><span class="s1">)</span>


<span class="s1">@pytest.mark.parametrize(</span><span class="s2">&quot;klass&quot;</span><span class="s0">, </span><span class="s1">[SGDRegressor</span><span class="s0">, </span><span class="s1">SparseSGDRegressor])</span>
<span class="s0">def </span><span class="s1">test_loss_function_epsilon(klass):</span>
    <span class="s1">clf = klass(epsilon=</span><span class="s3">0.9</span><span class="s1">)</span>
    <span class="s1">clf.set_params(epsilon=</span><span class="s3">0.1</span><span class="s1">)</span>
    <span class="s0">assert </span><span class="s1">clf.loss_functions[</span><span class="s2">&quot;huber&quot;</span><span class="s1">][</span><span class="s3">1</span><span class="s1">] == </span><span class="s3">0.1</span>


<span class="s4">###############################################################################</span>
<span class="s4"># SGD One Class SVM Test Case</span>


<span class="s4"># a simple implementation of ASGD to use for testing SGDOneClassSVM</span>
<span class="s0">def </span><span class="s1">asgd_oneclass(klass</span><span class="s0">, </span><span class="s1">X</span><span class="s0">, </span><span class="s1">eta</span><span class="s0">, </span><span class="s1">nu</span><span class="s0">, </span><span class="s1">coef_init=</span><span class="s0">None, </span><span class="s1">offset_init=</span><span class="s3">0.0</span><span class="s1">):</span>
    <span class="s0">if </span><span class="s1">coef_init </span><span class="s0">is None</span><span class="s1">:</span>
        <span class="s1">coef = np.zeros(X.shape[</span><span class="s3">1</span><span class="s1">])</span>
    <span class="s0">else</span><span class="s1">:</span>
        <span class="s1">coef = coef_init</span>

    <span class="s1">average_coef = np.zeros(X.shape[</span><span class="s3">1</span><span class="s1">])</span>
    <span class="s1">offset = offset_init</span>
    <span class="s1">intercept = </span><span class="s3">1 </span><span class="s1">- offset</span>
    <span class="s1">average_intercept = </span><span class="s3">0.0</span>
    <span class="s1">decay = </span><span class="s3">1.0</span>

    <span class="s4"># sparse data has a fixed decay of .01</span>
    <span class="s0">if </span><span class="s1">klass == SparseSGDOneClassSVM:</span>
        <span class="s1">decay = </span><span class="s3">0.01</span>

    <span class="s0">for </span><span class="s1">i</span><span class="s0">, </span><span class="s1">entry </span><span class="s0">in </span><span class="s1">enumerate(X):</span>
        <span class="s1">p = np.dot(entry</span><span class="s0">, </span><span class="s1">coef)</span>
        <span class="s1">p += intercept</span>
        <span class="s0">if </span><span class="s1">p &lt;= </span><span class="s3">1.0</span><span class="s1">:</span>
            <span class="s1">gradient = -</span><span class="s3">1</span>
        <span class="s0">else</span><span class="s1">:</span>
            <span class="s1">gradient = </span><span class="s3">0</span>
        <span class="s1">coef *= max(</span><span class="s3">0</span><span class="s0">, </span><span class="s3">1.0 </span><span class="s1">- (eta * nu / </span><span class="s3">2</span><span class="s1">))</span>
        <span class="s1">coef += -(eta * gradient * entry)</span>
        <span class="s1">intercept += -(eta * (nu + gradient)) * decay</span>

        <span class="s1">average_coef *= i</span>
        <span class="s1">average_coef += coef</span>
        <span class="s1">average_coef /= i + </span><span class="s3">1.0</span>

        <span class="s1">average_intercept *= i</span>
        <span class="s1">average_intercept += intercept</span>
        <span class="s1">average_intercept /= i + </span><span class="s3">1.0</span>

    <span class="s0">return </span><span class="s1">average_coef</span><span class="s0">, </span><span class="s3">1 </span><span class="s1">- average_intercept</span>


<span class="s1">@pytest.mark.parametrize(</span><span class="s2">&quot;klass&quot;</span><span class="s0">, </span><span class="s1">[SGDOneClassSVM</span><span class="s0">, </span><span class="s1">SparseSGDOneClassSVM])</span>
<span class="s0">def </span><span class="s1">_test_warm_start_oneclass(klass</span><span class="s0">, </span><span class="s1">X</span><span class="s0">, </span><span class="s1">lr):</span>
    <span class="s4"># Test that explicit warm restart...</span>
    <span class="s1">clf = klass(nu=</span><span class="s3">0.5</span><span class="s0">, </span><span class="s1">eta0=</span><span class="s3">0.01</span><span class="s0">, </span><span class="s1">shuffle=</span><span class="s0">False, </span><span class="s1">learning_rate=lr)</span>
    <span class="s1">clf.fit(X)</span>

    <span class="s1">clf2 = klass(nu=</span><span class="s3">0.1</span><span class="s0">, </span><span class="s1">eta0=</span><span class="s3">0.01</span><span class="s0">, </span><span class="s1">shuffle=</span><span class="s0">False, </span><span class="s1">learning_rate=lr)</span>
    <span class="s1">clf2.fit(X</span><span class="s0">, </span><span class="s1">coef_init=clf.coef_.copy()</span><span class="s0">, </span><span class="s1">offset_init=clf.offset_.copy())</span>

    <span class="s4"># ... and implicit warm restart are equivalent.</span>
    <span class="s1">clf3 = klass(nu=</span><span class="s3">0.5</span><span class="s0">, </span><span class="s1">eta0=</span><span class="s3">0.01</span><span class="s0">, </span><span class="s1">shuffle=</span><span class="s0">False, </span><span class="s1">warm_start=</span><span class="s0">True, </span><span class="s1">learning_rate=lr)</span>
    <span class="s1">clf3.fit(X)</span>

    <span class="s0">assert </span><span class="s1">clf3.t_ == clf.t_</span>
    <span class="s1">assert_allclose(clf3.coef_</span><span class="s0">, </span><span class="s1">clf.coef_)</span>

    <span class="s1">clf3.set_params(nu=</span><span class="s3">0.1</span><span class="s1">)</span>
    <span class="s1">clf3.fit(X)</span>

    <span class="s0">assert </span><span class="s1">clf3.t_ == clf2.t_</span>
    <span class="s1">assert_allclose(clf3.coef_</span><span class="s0">, </span><span class="s1">clf2.coef_)</span>


<span class="s1">@pytest.mark.parametrize(</span><span class="s2">&quot;klass&quot;</span><span class="s0">, </span><span class="s1">[SGDOneClassSVM</span><span class="s0">, </span><span class="s1">SparseSGDOneClassSVM])</span>
<span class="s1">@pytest.mark.parametrize(</span><span class="s2">&quot;lr&quot;</span><span class="s0">, </span><span class="s1">[</span><span class="s2">&quot;constant&quot;</span><span class="s0">, </span><span class="s2">&quot;optimal&quot;</span><span class="s0">, </span><span class="s2">&quot;invscaling&quot;</span><span class="s0">, </span><span class="s2">&quot;adaptive&quot;</span><span class="s1">])</span>
<span class="s0">def </span><span class="s1">test_warm_start_oneclass(klass</span><span class="s0">, </span><span class="s1">lr):</span>
    <span class="s1">_test_warm_start_oneclass(klass</span><span class="s0">, </span><span class="s1">X</span><span class="s0">, </span><span class="s1">lr)</span>


<span class="s1">@pytest.mark.parametrize(</span><span class="s2">&quot;klass&quot;</span><span class="s0">, </span><span class="s1">[SGDOneClassSVM</span><span class="s0">, </span><span class="s1">SparseSGDOneClassSVM])</span>
<span class="s0">def </span><span class="s1">test_clone_oneclass(klass):</span>
    <span class="s4"># Test whether clone works ok.</span>
    <span class="s1">clf = klass(nu=</span><span class="s3">0.5</span><span class="s1">)</span>
    <span class="s1">clf = clone(clf)</span>
    <span class="s1">clf.set_params(nu=</span><span class="s3">0.1</span><span class="s1">)</span>
    <span class="s1">clf.fit(X)</span>

    <span class="s1">clf2 = klass(nu=</span><span class="s3">0.1</span><span class="s1">)</span>
    <span class="s1">clf2.fit(X)</span>

    <span class="s1">assert_array_equal(clf.coef_</span><span class="s0">, </span><span class="s1">clf2.coef_)</span>


<span class="s1">@pytest.mark.parametrize(</span><span class="s2">&quot;klass&quot;</span><span class="s0">, </span><span class="s1">[SGDOneClassSVM</span><span class="s0">, </span><span class="s1">SparseSGDOneClassSVM])</span>
<span class="s0">def </span><span class="s1">test_partial_fit_oneclass(klass):</span>
    <span class="s1">third = X.shape[</span><span class="s3">0</span><span class="s1">] // </span><span class="s3">3</span>
    <span class="s1">clf = klass(nu=</span><span class="s3">0.1</span><span class="s1">)</span>

    <span class="s1">clf.partial_fit(X[:third])</span>
    <span class="s0">assert </span><span class="s1">clf.coef_.shape == (X.shape[</span><span class="s3">1</span><span class="s1">]</span><span class="s0">,</span><span class="s1">)</span>
    <span class="s0">assert </span><span class="s1">clf.offset_.shape == (</span><span class="s3">1</span><span class="s0">,</span><span class="s1">)</span>
    <span class="s0">assert </span><span class="s1">clf.predict([[</span><span class="s3">0</span><span class="s0">, </span><span class="s3">0</span><span class="s1">]]).shape == (</span><span class="s3">1</span><span class="s0">,</span><span class="s1">)</span>
    <span class="s1">previous_coefs = clf.coef_</span>

    <span class="s1">clf.partial_fit(X[third:])</span>
    <span class="s4"># check that coef_ haven't been re-allocated</span>
    <span class="s0">assert </span><span class="s1">clf.coef_ </span><span class="s0">is </span><span class="s1">previous_coefs</span>

    <span class="s4"># raises ValueError if number of features does not match previous data</span>
    <span class="s0">with </span><span class="s1">pytest.raises(ValueError):</span>
        <span class="s1">clf.partial_fit(X[:</span><span class="s0">, </span><span class="s3">1</span><span class="s1">])</span>


<span class="s1">@pytest.mark.parametrize(</span><span class="s2">&quot;klass&quot;</span><span class="s0">, </span><span class="s1">[SGDOneClassSVM</span><span class="s0">, </span><span class="s1">SparseSGDOneClassSVM])</span>
<span class="s1">@pytest.mark.parametrize(</span><span class="s2">&quot;lr&quot;</span><span class="s0">, </span><span class="s1">[</span><span class="s2">&quot;constant&quot;</span><span class="s0">, </span><span class="s2">&quot;optimal&quot;</span><span class="s0">, </span><span class="s2">&quot;invscaling&quot;</span><span class="s0">, </span><span class="s2">&quot;adaptive&quot;</span><span class="s1">])</span>
<span class="s0">def </span><span class="s1">test_partial_fit_equal_fit_oneclass(klass</span><span class="s0">, </span><span class="s1">lr):</span>
    <span class="s1">clf = klass(nu=</span><span class="s3">0.05</span><span class="s0">, </span><span class="s1">max_iter=</span><span class="s3">2</span><span class="s0">, </span><span class="s1">eta0=</span><span class="s3">0.01</span><span class="s0">, </span><span class="s1">learning_rate=lr</span><span class="s0">, </span><span class="s1">shuffle=</span><span class="s0">False</span><span class="s1">)</span>
    <span class="s1">clf.fit(X)</span>
    <span class="s1">y_scores = clf.decision_function(T)</span>
    <span class="s1">t = clf.t_</span>
    <span class="s1">coef = clf.coef_</span>
    <span class="s1">offset = clf.offset_</span>

    <span class="s1">clf = klass(nu=</span><span class="s3">0.05</span><span class="s0">, </span><span class="s1">eta0=</span><span class="s3">0.01</span><span class="s0">, </span><span class="s1">max_iter=</span><span class="s3">1</span><span class="s0">, </span><span class="s1">learning_rate=lr</span><span class="s0">, </span><span class="s1">shuffle=</span><span class="s0">False</span><span class="s1">)</span>
    <span class="s0">for </span><span class="s1">_ </span><span class="s0">in </span><span class="s1">range(</span><span class="s3">2</span><span class="s1">):</span>
        <span class="s1">clf.partial_fit(X)</span>
    <span class="s1">y_scores2 = clf.decision_function(T)</span>

    <span class="s0">assert </span><span class="s1">clf.t_ == t</span>
    <span class="s1">assert_allclose(y_scores</span><span class="s0">, </span><span class="s1">y_scores2)</span>
    <span class="s1">assert_allclose(clf.coef_</span><span class="s0">, </span><span class="s1">coef)</span>
    <span class="s1">assert_allclose(clf.offset_</span><span class="s0">, </span><span class="s1">offset)</span>


<span class="s1">@pytest.mark.parametrize(</span><span class="s2">&quot;klass&quot;</span><span class="s0">, </span><span class="s1">[SGDOneClassSVM</span><span class="s0">, </span><span class="s1">SparseSGDOneClassSVM])</span>
<span class="s0">def </span><span class="s1">test_late_onset_averaging_reached_oneclass(klass):</span>
    <span class="s4"># Test average</span>
    <span class="s1">eta0 = </span><span class="s3">0.001</span>
    <span class="s1">nu = </span><span class="s3">0.05</span>

    <span class="s4"># 2 passes over the training set but average only at second pass</span>
    <span class="s1">clf1 = klass(</span>
        <span class="s1">average=</span><span class="s3">7</span><span class="s0">, </span><span class="s1">learning_rate=</span><span class="s2">&quot;constant&quot;</span><span class="s0">, </span><span class="s1">eta0=eta0</span><span class="s0">, </span><span class="s1">nu=nu</span><span class="s0">, </span><span class="s1">max_iter=</span><span class="s3">2</span><span class="s0">, </span><span class="s1">shuffle=</span><span class="s0">False</span>
    <span class="s1">)</span>
    <span class="s4"># 1 pass over the training set with no averaging</span>
    <span class="s1">clf2 = klass(</span>
        <span class="s1">average=</span><span class="s3">0</span><span class="s0">, </span><span class="s1">learning_rate=</span><span class="s2">&quot;constant&quot;</span><span class="s0">, </span><span class="s1">eta0=eta0</span><span class="s0">, </span><span class="s1">nu=nu</span><span class="s0">, </span><span class="s1">max_iter=</span><span class="s3">1</span><span class="s0">, </span><span class="s1">shuffle=</span><span class="s0">False</span>
    <span class="s1">)</span>

    <span class="s1">clf1.fit(X)</span>
    <span class="s1">clf2.fit(X)</span>

    <span class="s4"># Start from clf2 solution, compute averaging using asgd function and</span>
    <span class="s4"># compare with clf1 solution</span>
    <span class="s1">average_coef</span><span class="s0">, </span><span class="s1">average_offset = asgd_oneclass(</span>
        <span class="s1">klass</span><span class="s0">, </span><span class="s1">X</span><span class="s0">, </span><span class="s1">eta0</span><span class="s0">, </span><span class="s1">nu</span><span class="s0">, </span><span class="s1">coef_init=clf2.coef_.ravel()</span><span class="s0">, </span><span class="s1">offset_init=clf2.offset_</span>
    <span class="s1">)</span>

    <span class="s1">assert_allclose(clf1.coef_.ravel()</span><span class="s0">, </span><span class="s1">average_coef.ravel())</span>
    <span class="s1">assert_allclose(clf1.offset_</span><span class="s0">, </span><span class="s1">average_offset)</span>


<span class="s1">@pytest.mark.parametrize(</span><span class="s2">&quot;klass&quot;</span><span class="s0">, </span><span class="s1">[SGDOneClassSVM</span><span class="s0">, </span><span class="s1">SparseSGDOneClassSVM])</span>
<span class="s0">def </span><span class="s1">test_sgd_averaged_computed_correctly_oneclass(klass):</span>
    <span class="s4"># Tests the average SGD One-Class SVM matches the naive implementation</span>
    <span class="s1">eta = </span><span class="s3">0.001</span>
    <span class="s1">nu = </span><span class="s3">0.05</span>
    <span class="s1">n_samples = </span><span class="s3">20</span>
    <span class="s1">n_features = </span><span class="s3">10</span>
    <span class="s1">rng = np.random.RandomState(</span><span class="s3">0</span><span class="s1">)</span>
    <span class="s1">X = rng.normal(size=(n_samples</span><span class="s0">, </span><span class="s1">n_features))</span>

    <span class="s1">clf = klass(</span>
        <span class="s1">learning_rate=</span><span class="s2">&quot;constant&quot;</span><span class="s0">,</span>
        <span class="s1">eta0=eta</span><span class="s0">,</span>
        <span class="s1">nu=nu</span><span class="s0">,</span>
        <span class="s1">fit_intercept=</span><span class="s0">True,</span>
        <span class="s1">max_iter=</span><span class="s3">1</span><span class="s0">,</span>
        <span class="s1">average=</span><span class="s0">True,</span>
        <span class="s1">shuffle=</span><span class="s0">False,</span>
    <span class="s1">)</span>

    <span class="s1">clf.fit(X)</span>
    <span class="s1">average_coef</span><span class="s0">, </span><span class="s1">average_offset = asgd_oneclass(klass</span><span class="s0">, </span><span class="s1">X</span><span class="s0">, </span><span class="s1">eta</span><span class="s0">, </span><span class="s1">nu)</span>

    <span class="s1">assert_allclose(clf.coef_</span><span class="s0">, </span><span class="s1">average_coef)</span>
    <span class="s1">assert_allclose(clf.offset_</span><span class="s0">, </span><span class="s1">average_offset)</span>


<span class="s1">@pytest.mark.parametrize(</span><span class="s2">&quot;klass&quot;</span><span class="s0">, </span><span class="s1">[SGDOneClassSVM</span><span class="s0">, </span><span class="s1">SparseSGDOneClassSVM])</span>
<span class="s0">def </span><span class="s1">test_sgd_averaged_partial_fit_oneclass(klass):</span>
    <span class="s4"># Tests whether the partial fit yields the same average as the fit</span>
    <span class="s1">eta = </span><span class="s3">0.001</span>
    <span class="s1">nu = </span><span class="s3">0.05</span>
    <span class="s1">n_samples = </span><span class="s3">20</span>
    <span class="s1">n_features = </span><span class="s3">10</span>
    <span class="s1">rng = np.random.RandomState(</span><span class="s3">0</span><span class="s1">)</span>
    <span class="s1">X = rng.normal(size=(n_samples</span><span class="s0">, </span><span class="s1">n_features))</span>

    <span class="s1">clf = klass(</span>
        <span class="s1">learning_rate=</span><span class="s2">&quot;constant&quot;</span><span class="s0">,</span>
        <span class="s1">eta0=eta</span><span class="s0">,</span>
        <span class="s1">nu=nu</span><span class="s0">,</span>
        <span class="s1">fit_intercept=</span><span class="s0">True,</span>
        <span class="s1">max_iter=</span><span class="s3">1</span><span class="s0">,</span>
        <span class="s1">average=</span><span class="s0">True,</span>
        <span class="s1">shuffle=</span><span class="s0">False,</span>
    <span class="s1">)</span>

    <span class="s1">clf.partial_fit(X[: int(n_samples / </span><span class="s3">2</span><span class="s1">)][:])</span>
    <span class="s1">clf.partial_fit(X[int(n_samples / </span><span class="s3">2</span><span class="s1">) :][:])</span>
    <span class="s1">average_coef</span><span class="s0">, </span><span class="s1">average_offset = asgd_oneclass(klass</span><span class="s0">, </span><span class="s1">X</span><span class="s0">, </span><span class="s1">eta</span><span class="s0">, </span><span class="s1">nu)</span>

    <span class="s1">assert_allclose(clf.coef_</span><span class="s0">, </span><span class="s1">average_coef)</span>
    <span class="s1">assert_allclose(clf.offset_</span><span class="s0">, </span><span class="s1">average_offset)</span>


<span class="s1">@pytest.mark.parametrize(</span><span class="s2">&quot;klass&quot;</span><span class="s0">, </span><span class="s1">[SGDOneClassSVM</span><span class="s0">, </span><span class="s1">SparseSGDOneClassSVM])</span>
<span class="s0">def </span><span class="s1">test_average_sparse_oneclass(klass):</span>
    <span class="s4"># Checks the average coef on data with 0s</span>
    <span class="s1">eta = </span><span class="s3">0.001</span>
    <span class="s1">nu = </span><span class="s3">0.01</span>
    <span class="s1">clf = klass(</span>
        <span class="s1">learning_rate=</span><span class="s2">&quot;constant&quot;</span><span class="s0">,</span>
        <span class="s1">eta0=eta</span><span class="s0">,</span>
        <span class="s1">nu=nu</span><span class="s0">,</span>
        <span class="s1">fit_intercept=</span><span class="s0">True,</span>
        <span class="s1">max_iter=</span><span class="s3">1</span><span class="s0">,</span>
        <span class="s1">average=</span><span class="s0">True,</span>
        <span class="s1">shuffle=</span><span class="s0">False,</span>
    <span class="s1">)</span>

    <span class="s1">n_samples = X3.shape[</span><span class="s3">0</span><span class="s1">]</span>

    <span class="s1">clf.partial_fit(X3[: int(n_samples / </span><span class="s3">2</span><span class="s1">)])</span>
    <span class="s1">clf.partial_fit(X3[int(n_samples / </span><span class="s3">2</span><span class="s1">) :])</span>
    <span class="s1">average_coef</span><span class="s0">, </span><span class="s1">average_offset = asgd_oneclass(klass</span><span class="s0">, </span><span class="s1">X3</span><span class="s0">, </span><span class="s1">eta</span><span class="s0">, </span><span class="s1">nu)</span>

    <span class="s1">assert_allclose(clf.coef_</span><span class="s0">, </span><span class="s1">average_coef)</span>
    <span class="s1">assert_allclose(clf.offset_</span><span class="s0">, </span><span class="s1">average_offset)</span>


<span class="s0">def </span><span class="s1">test_sgd_oneclass():</span>
    <span class="s4"># Test fit, decision_function, predict and score_samples on a toy</span>
    <span class="s4"># dataset</span>
    <span class="s1">X_train = np.array([[-</span><span class="s3">2</span><span class="s0">, </span><span class="s1">-</span><span class="s3">1</span><span class="s1">]</span><span class="s0">, </span><span class="s1">[-</span><span class="s3">1</span><span class="s0">, </span><span class="s1">-</span><span class="s3">1</span><span class="s1">]</span><span class="s0">, </span><span class="s1">[</span><span class="s3">1</span><span class="s0">, </span><span class="s3">1</span><span class="s1">]])</span>
    <span class="s1">X_test = np.array([[</span><span class="s3">0.5</span><span class="s0">, </span><span class="s1">-</span><span class="s3">2</span><span class="s1">]</span><span class="s0">, </span><span class="s1">[</span><span class="s3">2</span><span class="s0">, </span><span class="s3">2</span><span class="s1">]])</span>
    <span class="s1">clf = SGDOneClassSVM(</span>
        <span class="s1">nu=</span><span class="s3">0.5</span><span class="s0">, </span><span class="s1">eta0=</span><span class="s3">1</span><span class="s0">, </span><span class="s1">learning_rate=</span><span class="s2">&quot;constant&quot;</span><span class="s0">, </span><span class="s1">shuffle=</span><span class="s0">False, </span><span class="s1">max_iter=</span><span class="s3">1</span>
    <span class="s1">)</span>
    <span class="s1">clf.fit(X_train)</span>
    <span class="s1">assert_allclose(clf.coef_</span><span class="s0">, </span><span class="s1">np.array([-</span><span class="s3">0.125</span><span class="s0">, </span><span class="s3">0.4375</span><span class="s1">]))</span>
    <span class="s0">assert </span><span class="s1">clf.offset_[</span><span class="s3">0</span><span class="s1">] == -</span><span class="s3">0.5</span>

    <span class="s1">scores = clf.score_samples(X_test)</span>
    <span class="s1">assert_allclose(scores</span><span class="s0">, </span><span class="s1">np.array([-</span><span class="s3">0.9375</span><span class="s0">, </span><span class="s3">0.625</span><span class="s1">]))</span>

    <span class="s1">dec = clf.score_samples(X_test) - clf.offset_</span>
    <span class="s1">assert_allclose(clf.decision_function(X_test)</span><span class="s0">, </span><span class="s1">dec)</span>

    <span class="s1">pred = clf.predict(X_test)</span>
    <span class="s1">assert_array_equal(pred</span><span class="s0">, </span><span class="s1">np.array([-</span><span class="s3">1</span><span class="s0">, </span><span class="s3">1</span><span class="s1">]))</span>


<span class="s0">def </span><span class="s1">test_ocsvm_vs_sgdocsvm():</span>
    <span class="s4"># Checks SGDOneClass SVM gives a good approximation of kernelized</span>
    <span class="s4"># One-Class SVM</span>
    <span class="s1">nu = </span><span class="s3">0.05</span>
    <span class="s1">gamma = </span><span class="s3">2.0</span>
    <span class="s1">random_state = </span><span class="s3">42</span>

    <span class="s4"># Generate train and test data</span>
    <span class="s1">rng = np.random.RandomState(random_state)</span>
    <span class="s1">X = </span><span class="s3">0.3 </span><span class="s1">* rng.randn(</span><span class="s3">500</span><span class="s0">, </span><span class="s3">2</span><span class="s1">)</span>
    <span class="s1">X_train = np.r_[X + </span><span class="s3">2</span><span class="s0">, </span><span class="s1">X - </span><span class="s3">2</span><span class="s1">]</span>
    <span class="s1">X = </span><span class="s3">0.3 </span><span class="s1">* rng.randn(</span><span class="s3">100</span><span class="s0">, </span><span class="s3">2</span><span class="s1">)</span>
    <span class="s1">X_test = np.r_[X + </span><span class="s3">2</span><span class="s0">, </span><span class="s1">X - </span><span class="s3">2</span><span class="s1">]</span>

    <span class="s4"># One-Class SVM</span>
    <span class="s1">clf = OneClassSVM(gamma=gamma</span><span class="s0">, </span><span class="s1">kernel=</span><span class="s2">&quot;rbf&quot;</span><span class="s0">, </span><span class="s1">nu=nu)</span>
    <span class="s1">clf.fit(X_train)</span>
    <span class="s1">y_pred_ocsvm = clf.predict(X_test)</span>
    <span class="s1">dec_ocsvm = clf.decision_function(X_test).reshape(</span><span class="s3">1</span><span class="s0">, </span><span class="s1">-</span><span class="s3">1</span><span class="s1">)</span>

    <span class="s4"># SGDOneClassSVM using kernel approximation</span>
    <span class="s1">max_iter = </span><span class="s3">15</span>
    <span class="s1">transform = Nystroem(gamma=gamma</span><span class="s0">, </span><span class="s1">random_state=random_state)</span>
    <span class="s1">clf_sgd = SGDOneClassSVM(</span>
        <span class="s1">nu=nu</span><span class="s0">,</span>
        <span class="s1">shuffle=</span><span class="s0">True,</span>
        <span class="s1">fit_intercept=</span><span class="s0">True,</span>
        <span class="s1">max_iter=max_iter</span><span class="s0">,</span>
        <span class="s1">random_state=random_state</span><span class="s0">,</span>
        <span class="s1">tol=</span><span class="s0">None,</span>
    <span class="s1">)</span>
    <span class="s1">pipe_sgd = make_pipeline(transform</span><span class="s0">, </span><span class="s1">clf_sgd)</span>
    <span class="s1">pipe_sgd.fit(X_train)</span>
    <span class="s1">y_pred_sgdocsvm = pipe_sgd.predict(X_test)</span>
    <span class="s1">dec_sgdocsvm = pipe_sgd.decision_function(X_test).reshape(</span><span class="s3">1</span><span class="s0">, </span><span class="s1">-</span><span class="s3">1</span><span class="s1">)</span>

    <span class="s0">assert </span><span class="s1">np.mean(y_pred_sgdocsvm == y_pred_ocsvm) &gt;= </span><span class="s3">0.99</span>
    <span class="s1">corrcoef = np.corrcoef(np.concatenate((dec_ocsvm</span><span class="s0">, </span><span class="s1">dec_sgdocsvm)))[</span><span class="s3">0</span><span class="s0">, </span><span class="s3">1</span><span class="s1">]</span>
    <span class="s0">assert </span><span class="s1">corrcoef &gt;= </span><span class="s3">0.9</span>


<span class="s0">def </span><span class="s1">test_l1_ratio():</span>
    <span class="s4"># Test if l1 ratio extremes match L1 and L2 penalty settings.</span>
    <span class="s1">X</span><span class="s0">, </span><span class="s1">y = datasets.make_classification(</span>
        <span class="s1">n_samples=</span><span class="s3">1000</span><span class="s0">, </span><span class="s1">n_features=</span><span class="s3">100</span><span class="s0">, </span><span class="s1">n_informative=</span><span class="s3">20</span><span class="s0">, </span><span class="s1">random_state=</span><span class="s3">1234</span>
    <span class="s1">)</span>

    <span class="s4"># test if elasticnet with l1_ratio near 1 gives same result as pure l1</span>
    <span class="s1">est_en = SGDClassifier(</span>
        <span class="s1">alpha=</span><span class="s3">0.001</span><span class="s0">,</span>
        <span class="s1">penalty=</span><span class="s2">&quot;elasticnet&quot;</span><span class="s0">,</span>
        <span class="s1">tol=</span><span class="s0">None,</span>
        <span class="s1">max_iter=</span><span class="s3">6</span><span class="s0">,</span>
        <span class="s1">l1_ratio=</span><span class="s3">0.9999999999</span><span class="s0">,</span>
        <span class="s1">random_state=</span><span class="s3">42</span><span class="s0">,</span>
    <span class="s1">).fit(X</span><span class="s0">, </span><span class="s1">y)</span>
    <span class="s1">est_l1 = SGDClassifier(</span>
        <span class="s1">alpha=</span><span class="s3">0.001</span><span class="s0">, </span><span class="s1">penalty=</span><span class="s2">&quot;l1&quot;</span><span class="s0">, </span><span class="s1">max_iter=</span><span class="s3">6</span><span class="s0">, </span><span class="s1">random_state=</span><span class="s3">42</span><span class="s0">, </span><span class="s1">tol=</span><span class="s0">None</span>
    <span class="s1">).fit(X</span><span class="s0">, </span><span class="s1">y)</span>
    <span class="s1">assert_array_almost_equal(est_en.coef_</span><span class="s0">, </span><span class="s1">est_l1.coef_)</span>

    <span class="s4"># test if elasticnet with l1_ratio near 0 gives same result as pure l2</span>
    <span class="s1">est_en = SGDClassifier(</span>
        <span class="s1">alpha=</span><span class="s3">0.001</span><span class="s0">,</span>
        <span class="s1">penalty=</span><span class="s2">&quot;elasticnet&quot;</span><span class="s0">,</span>
        <span class="s1">tol=</span><span class="s0">None,</span>
        <span class="s1">max_iter=</span><span class="s3">6</span><span class="s0">,</span>
        <span class="s1">l1_ratio=</span><span class="s3">0.0000000001</span><span class="s0">,</span>
        <span class="s1">random_state=</span><span class="s3">42</span><span class="s0">,</span>
    <span class="s1">).fit(X</span><span class="s0">, </span><span class="s1">y)</span>
    <span class="s1">est_l2 = SGDClassifier(</span>
        <span class="s1">alpha=</span><span class="s3">0.001</span><span class="s0">, </span><span class="s1">penalty=</span><span class="s2">&quot;l2&quot;</span><span class="s0">, </span><span class="s1">max_iter=</span><span class="s3">6</span><span class="s0">, </span><span class="s1">random_state=</span><span class="s3">42</span><span class="s0">, </span><span class="s1">tol=</span><span class="s0">None</span>
    <span class="s1">).fit(X</span><span class="s0">, </span><span class="s1">y)</span>
    <span class="s1">assert_array_almost_equal(est_en.coef_</span><span class="s0">, </span><span class="s1">est_l2.coef_)</span>


<span class="s0">def </span><span class="s1">test_underflow_or_overlow():</span>
    <span class="s0">with </span><span class="s1">np.errstate(all=</span><span class="s2">&quot;raise&quot;</span><span class="s1">):</span>
        <span class="s4"># Generate some weird data with hugely unscaled features</span>
        <span class="s1">rng = np.random.RandomState(</span><span class="s3">0</span><span class="s1">)</span>
        <span class="s1">n_samples = </span><span class="s3">100</span>
        <span class="s1">n_features = </span><span class="s3">10</span>

        <span class="s1">X = rng.normal(size=(n_samples</span><span class="s0">, </span><span class="s1">n_features))</span>
        <span class="s1">X[:</span><span class="s0">, </span><span class="s1">:</span><span class="s3">2</span><span class="s1">] *= </span><span class="s3">1e300</span>
        <span class="s0">assert </span><span class="s1">np.isfinite(X).all()</span>

        <span class="s4"># Use MinMaxScaler to scale the data without introducing a numerical</span>
        <span class="s4"># instability (computing the standard deviation naively is not possible</span>
        <span class="s4"># on this data)</span>
        <span class="s1">X_scaled = MinMaxScaler().fit_transform(X)</span>
        <span class="s0">assert </span><span class="s1">np.isfinite(X_scaled).all()</span>

        <span class="s4"># Define a ground truth on the scaled data</span>
        <span class="s1">ground_truth = rng.normal(size=n_features)</span>
        <span class="s1">y = (np.dot(X_scaled</span><span class="s0">, </span><span class="s1">ground_truth) &gt; </span><span class="s3">0.0</span><span class="s1">).astype(np.int32)</span>
        <span class="s1">assert_array_equal(np.unique(y)</span><span class="s0">, </span><span class="s1">[</span><span class="s3">0</span><span class="s0">, </span><span class="s3">1</span><span class="s1">])</span>

        <span class="s1">model = SGDClassifier(alpha=</span><span class="s3">0.1</span><span class="s0">, </span><span class="s1">loss=</span><span class="s2">&quot;squared_hinge&quot;</span><span class="s0">, </span><span class="s1">max_iter=</span><span class="s3">500</span><span class="s1">)</span>

        <span class="s4"># smoke test: model is stable on scaled data</span>
        <span class="s1">model.fit(X_scaled</span><span class="s0">, </span><span class="s1">y)</span>
        <span class="s0">assert </span><span class="s1">np.isfinite(model.coef_).all()</span>

        <span class="s4"># model is numerically unstable on unscaled data</span>
        <span class="s1">msg_regxp = (</span>
            <span class="s2">r&quot;Floating-point under-/overflow occurred at epoch #.*&quot;</span>
            <span class="s2">&quot; Scaling input data with StandardScaler or MinMaxScaler&quot;</span>
            <span class="s2">&quot; might help.&quot;</span>
        <span class="s1">)</span>
        <span class="s0">with </span><span class="s1">pytest.raises(ValueError</span><span class="s0">, </span><span class="s1">match=msg_regxp):</span>
            <span class="s1">model.fit(X</span><span class="s0">, </span><span class="s1">y)</span>


<span class="s0">def </span><span class="s1">test_numerical_stability_large_gradient():</span>
    <span class="s4"># Non regression test case for numerical stability on scaled problems</span>
    <span class="s4"># where the gradient can still explode with some losses</span>
    <span class="s1">model = SGDClassifier(</span>
        <span class="s1">loss=</span><span class="s2">&quot;squared_hinge&quot;</span><span class="s0">,</span>
        <span class="s1">max_iter=</span><span class="s3">10</span><span class="s0">,</span>
        <span class="s1">shuffle=</span><span class="s0">True,</span>
        <span class="s1">penalty=</span><span class="s2">&quot;elasticnet&quot;</span><span class="s0">,</span>
        <span class="s1">l1_ratio=</span><span class="s3">0.3</span><span class="s0">,</span>
        <span class="s1">alpha=</span><span class="s3">0.01</span><span class="s0">,</span>
        <span class="s1">eta0=</span><span class="s3">0.001</span><span class="s0">,</span>
        <span class="s1">random_state=</span><span class="s3">0</span><span class="s0">,</span>
        <span class="s1">tol=</span><span class="s0">None,</span>
    <span class="s1">)</span>
    <span class="s0">with </span><span class="s1">np.errstate(all=</span><span class="s2">&quot;raise&quot;</span><span class="s1">):</span>
        <span class="s1">model.fit(iris.data</span><span class="s0">, </span><span class="s1">iris.target)</span>
    <span class="s0">assert </span><span class="s1">np.isfinite(model.coef_).all()</span>


<span class="s1">@pytest.mark.parametrize(</span><span class="s2">&quot;penalty&quot;</span><span class="s0">, </span><span class="s1">[</span><span class="s2">&quot;l2&quot;</span><span class="s0">, </span><span class="s2">&quot;l1&quot;</span><span class="s0">, </span><span class="s2">&quot;elasticnet&quot;</span><span class="s1">])</span>
<span class="s0">def </span><span class="s1">test_large_regularization(penalty):</span>
    <span class="s4"># Non regression tests for numerical stability issues caused by large</span>
    <span class="s4"># regularization parameters</span>
    <span class="s1">model = SGDClassifier(</span>
        <span class="s1">alpha=</span><span class="s3">1e5</span><span class="s0">,</span>
        <span class="s1">learning_rate=</span><span class="s2">&quot;constant&quot;</span><span class="s0">,</span>
        <span class="s1">eta0=</span><span class="s3">0.1</span><span class="s0">,</span>
        <span class="s1">penalty=penalty</span><span class="s0">,</span>
        <span class="s1">shuffle=</span><span class="s0">False,</span>
        <span class="s1">tol=</span><span class="s0">None,</span>
        <span class="s1">max_iter=</span><span class="s3">6</span><span class="s0">,</span>
    <span class="s1">)</span>
    <span class="s0">with </span><span class="s1">np.errstate(all=</span><span class="s2">&quot;raise&quot;</span><span class="s1">):</span>
        <span class="s1">model.fit(iris.data</span><span class="s0">, </span><span class="s1">iris.target)</span>
    <span class="s1">assert_array_almost_equal(model.coef_</span><span class="s0">, </span><span class="s1">np.zeros_like(model.coef_))</span>


<span class="s0">def </span><span class="s1">test_tol_parameter():</span>
    <span class="s4"># Test that the tol parameter behaves as expected</span>
    <span class="s1">X = StandardScaler().fit_transform(iris.data)</span>
    <span class="s1">y = iris.target == </span><span class="s3">1</span>

    <span class="s4"># With tol is None, the number of iteration should be equal to max_iter</span>
    <span class="s1">max_iter = </span><span class="s3">42</span>
    <span class="s1">model_0 = SGDClassifier(tol=</span><span class="s0">None, </span><span class="s1">random_state=</span><span class="s3">0</span><span class="s0">, </span><span class="s1">max_iter=max_iter)</span>
    <span class="s1">model_0.fit(X</span><span class="s0">, </span><span class="s1">y)</span>
    <span class="s0">assert </span><span class="s1">max_iter == model_0.n_iter_</span>

    <span class="s4"># If tol is not None, the number of iteration should be less than max_iter</span>
    <span class="s1">max_iter = </span><span class="s3">2000</span>
    <span class="s1">model_1 = SGDClassifier(tol=</span><span class="s3">0</span><span class="s0">, </span><span class="s1">random_state=</span><span class="s3">0</span><span class="s0">, </span><span class="s1">max_iter=max_iter)</span>
    <span class="s1">model_1.fit(X</span><span class="s0">, </span><span class="s1">y)</span>
    <span class="s0">assert </span><span class="s1">max_iter &gt; model_1.n_iter_</span>
    <span class="s0">assert </span><span class="s1">model_1.n_iter_ &gt; </span><span class="s3">5</span>

    <span class="s4"># A larger tol should yield a smaller number of iteration</span>
    <span class="s1">model_2 = SGDClassifier(tol=</span><span class="s3">0.1</span><span class="s0">, </span><span class="s1">random_state=</span><span class="s3">0</span><span class="s0">, </span><span class="s1">max_iter=max_iter)</span>
    <span class="s1">model_2.fit(X</span><span class="s0">, </span><span class="s1">y)</span>
    <span class="s0">assert </span><span class="s1">model_1.n_iter_ &gt; model_2.n_iter_</span>
    <span class="s0">assert </span><span class="s1">model_2.n_iter_ &gt; </span><span class="s3">3</span>

    <span class="s4"># Strict tolerance and small max_iter should trigger a warning</span>
    <span class="s1">model_3 = SGDClassifier(max_iter=</span><span class="s3">3</span><span class="s0">, </span><span class="s1">tol=</span><span class="s3">1e-3</span><span class="s0">, </span><span class="s1">random_state=</span><span class="s3">0</span><span class="s1">)</span>
    <span class="s1">warning_message = (</span>
        <span class="s2">&quot;Maximum number of iteration reached before &quot;</span>
        <span class="s2">&quot;convergence. Consider increasing max_iter to &quot;</span>
        <span class="s2">&quot;improve the fit.&quot;</span>
    <span class="s1">)</span>
    <span class="s0">with </span><span class="s1">pytest.warns(ConvergenceWarning</span><span class="s0">, </span><span class="s1">match=warning_message):</span>
        <span class="s1">model_3.fit(X</span><span class="s0">, </span><span class="s1">y)</span>
    <span class="s0">assert </span><span class="s1">model_3.n_iter_ == </span><span class="s3">3</span>


<span class="s0">def </span><span class="s1">_test_loss_common(loss_function</span><span class="s0">, </span><span class="s1">cases):</span>
    <span class="s4"># Test the different loss functions</span>
    <span class="s4"># cases is a list of (p, y, expected)</span>
    <span class="s0">for </span><span class="s1">p</span><span class="s0">, </span><span class="s1">y</span><span class="s0">, </span><span class="s1">expected_loss</span><span class="s0">, </span><span class="s1">expected_dloss </span><span class="s0">in </span><span class="s1">cases:</span>
        <span class="s1">assert_almost_equal(loss_function.py_loss(p</span><span class="s0">, </span><span class="s1">y)</span><span class="s0">, </span><span class="s1">expected_loss)</span>
        <span class="s1">assert_almost_equal(loss_function.py_dloss(p</span><span class="s0">, </span><span class="s1">y)</span><span class="s0">, </span><span class="s1">expected_dloss)</span>


<span class="s0">def </span><span class="s1">test_loss_hinge():</span>
    <span class="s4"># Test Hinge (hinge / perceptron)</span>
    <span class="s4"># hinge</span>
    <span class="s1">loss = sgd_fast.Hinge(</span><span class="s3">1.0</span><span class="s1">)</span>
    <span class="s1">cases = [</span>
        <span class="s4"># (p, y, expected_loss, expected_dloss)</span>
        <span class="s1">(</span><span class="s3">1.1</span><span class="s0">, </span><span class="s3">1.0</span><span class="s0">, </span><span class="s3">0.0</span><span class="s0">, </span><span class="s3">0.0</span><span class="s1">)</span><span class="s0">,</span>
        <span class="s1">(-</span><span class="s3">2.0</span><span class="s0">, </span><span class="s1">-</span><span class="s3">1.0</span><span class="s0">, </span><span class="s3">0.0</span><span class="s0">, </span><span class="s3">0.0</span><span class="s1">)</span><span class="s0">,</span>
        <span class="s1">(</span><span class="s3">1.0</span><span class="s0">, </span><span class="s3">1.0</span><span class="s0">, </span><span class="s3">0.0</span><span class="s0">, </span><span class="s1">-</span><span class="s3">1.0</span><span class="s1">)</span><span class="s0">,</span>
        <span class="s1">(-</span><span class="s3">1.0</span><span class="s0">, </span><span class="s1">-</span><span class="s3">1.0</span><span class="s0">, </span><span class="s3">0.0</span><span class="s0">, </span><span class="s3">1.0</span><span class="s1">)</span><span class="s0">,</span>
        <span class="s1">(</span><span class="s3">0.5</span><span class="s0">, </span><span class="s3">1.0</span><span class="s0">, </span><span class="s3">0.5</span><span class="s0">, </span><span class="s1">-</span><span class="s3">1.0</span><span class="s1">)</span><span class="s0">,</span>
        <span class="s1">(</span><span class="s3">2.0</span><span class="s0">, </span><span class="s1">-</span><span class="s3">1.0</span><span class="s0">, </span><span class="s3">3.0</span><span class="s0">, </span><span class="s3">1.0</span><span class="s1">)</span><span class="s0">,</span>
        <span class="s1">(-</span><span class="s3">0.5</span><span class="s0">, </span><span class="s1">-</span><span class="s3">1.0</span><span class="s0">, </span><span class="s3">0.5</span><span class="s0">, </span><span class="s3">1.0</span><span class="s1">)</span><span class="s0">,</span>
        <span class="s1">(</span><span class="s3">0.0</span><span class="s0">, </span><span class="s3">1.0</span><span class="s0">, </span><span class="s3">1</span><span class="s0">, </span><span class="s1">-</span><span class="s3">1.0</span><span class="s1">)</span><span class="s0">,</span>
    <span class="s1">]</span>
    <span class="s1">_test_loss_common(loss</span><span class="s0">, </span><span class="s1">cases)</span>

    <span class="s4"># perceptron</span>
    <span class="s1">loss = sgd_fast.Hinge(</span><span class="s3">0.0</span><span class="s1">)</span>
    <span class="s1">cases = [</span>
        <span class="s4"># (p, y, expected_loss, expected_dloss)</span>
        <span class="s1">(</span><span class="s3">1.0</span><span class="s0">, </span><span class="s3">1.0</span><span class="s0">, </span><span class="s3">0.0</span><span class="s0">, </span><span class="s3">0.0</span><span class="s1">)</span><span class="s0">,</span>
        <span class="s1">(-</span><span class="s3">0.1</span><span class="s0">, </span><span class="s1">-</span><span class="s3">1.0</span><span class="s0">, </span><span class="s3">0.0</span><span class="s0">, </span><span class="s3">0.0</span><span class="s1">)</span><span class="s0">,</span>
        <span class="s1">(</span><span class="s3">0.0</span><span class="s0">, </span><span class="s3">1.0</span><span class="s0">, </span><span class="s3">0.0</span><span class="s0">, </span><span class="s1">-</span><span class="s3">1.0</span><span class="s1">)</span><span class="s0">,</span>
        <span class="s1">(</span><span class="s3">0.0</span><span class="s0">, </span><span class="s1">-</span><span class="s3">1.0</span><span class="s0">, </span><span class="s3">0.0</span><span class="s0">, </span><span class="s3">1.0</span><span class="s1">)</span><span class="s0">,</span>
        <span class="s1">(</span><span class="s3">0.5</span><span class="s0">, </span><span class="s1">-</span><span class="s3">1.0</span><span class="s0">, </span><span class="s3">0.5</span><span class="s0">, </span><span class="s3">1.0</span><span class="s1">)</span><span class="s0">,</span>
        <span class="s1">(</span><span class="s3">2.0</span><span class="s0">, </span><span class="s1">-</span><span class="s3">1.0</span><span class="s0">, </span><span class="s3">2.0</span><span class="s0">, </span><span class="s3">1.0</span><span class="s1">)</span><span class="s0">,</span>
        <span class="s1">(-</span><span class="s3">0.5</span><span class="s0">, </span><span class="s3">1.0</span><span class="s0">, </span><span class="s3">0.5</span><span class="s0">, </span><span class="s1">-</span><span class="s3">1.0</span><span class="s1">)</span><span class="s0">,</span>
        <span class="s1">(-</span><span class="s3">1.0</span><span class="s0">, </span><span class="s3">1.0</span><span class="s0">, </span><span class="s3">1.0</span><span class="s0">, </span><span class="s1">-</span><span class="s3">1.0</span><span class="s1">)</span><span class="s0">,</span>
    <span class="s1">]</span>
    <span class="s1">_test_loss_common(loss</span><span class="s0">, </span><span class="s1">cases)</span>


<span class="s0">def </span><span class="s1">test_gradient_squared_hinge():</span>
    <span class="s4"># Test SquaredHinge</span>
    <span class="s1">loss = sgd_fast.SquaredHinge(</span><span class="s3">1.0</span><span class="s1">)</span>
    <span class="s1">cases = [</span>
        <span class="s4"># (p, y, expected_loss, expected_dloss)</span>
        <span class="s1">(</span><span class="s3">1.0</span><span class="s0">, </span><span class="s3">1.0</span><span class="s0">, </span><span class="s3">0.0</span><span class="s0">, </span><span class="s3">0.0</span><span class="s1">)</span><span class="s0">,</span>
        <span class="s1">(-</span><span class="s3">2.0</span><span class="s0">, </span><span class="s1">-</span><span class="s3">1.0</span><span class="s0">, </span><span class="s3">0.0</span><span class="s0">, </span><span class="s3">0.0</span><span class="s1">)</span><span class="s0">,</span>
        <span class="s1">(</span><span class="s3">1.0</span><span class="s0">, </span><span class="s1">-</span><span class="s3">1.0</span><span class="s0">, </span><span class="s3">4.0</span><span class="s0">, </span><span class="s3">4.0</span><span class="s1">)</span><span class="s0">,</span>
        <span class="s1">(-</span><span class="s3">1.0</span><span class="s0">, </span><span class="s3">1.0</span><span class="s0">, </span><span class="s3">4.0</span><span class="s0">, </span><span class="s1">-</span><span class="s3">4.0</span><span class="s1">)</span><span class="s0">,</span>
        <span class="s1">(</span><span class="s3">0.5</span><span class="s0">, </span><span class="s3">1.0</span><span class="s0">, </span><span class="s3">0.25</span><span class="s0">, </span><span class="s1">-</span><span class="s3">1.0</span><span class="s1">)</span><span class="s0">,</span>
        <span class="s1">(</span><span class="s3">0.5</span><span class="s0">, </span><span class="s1">-</span><span class="s3">1.0</span><span class="s0">, </span><span class="s3">2.25</span><span class="s0">, </span><span class="s3">3.0</span><span class="s1">)</span><span class="s0">,</span>
    <span class="s1">]</span>
    <span class="s1">_test_loss_common(loss</span><span class="s0">, </span><span class="s1">cases)</span>


<span class="s0">def </span><span class="s1">test_loss_log():</span>
    <span class="s4"># Test Log (logistic loss)</span>
    <span class="s1">loss = sgd_fast.Log()</span>
    <span class="s1">cases = [</span>
        <span class="s4"># (p, y, expected_loss, expected_dloss)</span>
        <span class="s1">(</span><span class="s3">1.0</span><span class="s0">, </span><span class="s3">1.0</span><span class="s0">, </span><span class="s1">np.log(</span><span class="s3">1.0 </span><span class="s1">+ np.exp(-</span><span class="s3">1.0</span><span class="s1">))</span><span class="s0">, </span><span class="s1">-</span><span class="s3">1.0 </span><span class="s1">/ (np.exp(</span><span class="s3">1.0</span><span class="s1">) + </span><span class="s3">1.0</span><span class="s1">))</span><span class="s0">,</span>
        <span class="s1">(</span><span class="s3">1.0</span><span class="s0">, </span><span class="s1">-</span><span class="s3">1.0</span><span class="s0">, </span><span class="s1">np.log(</span><span class="s3">1.0 </span><span class="s1">+ np.exp(</span><span class="s3">1.0</span><span class="s1">))</span><span class="s0">, </span><span class="s3">1.0 </span><span class="s1">/ (np.exp(-</span><span class="s3">1.0</span><span class="s1">) + </span><span class="s3">1.0</span><span class="s1">))</span><span class="s0">,</span>
        <span class="s1">(-</span><span class="s3">1.0</span><span class="s0">, </span><span class="s1">-</span><span class="s3">1.0</span><span class="s0">, </span><span class="s1">np.log(</span><span class="s3">1.0 </span><span class="s1">+ np.exp(-</span><span class="s3">1.0</span><span class="s1">))</span><span class="s0">, </span><span class="s3">1.0 </span><span class="s1">/ (np.exp(</span><span class="s3">1.0</span><span class="s1">) + </span><span class="s3">1.0</span><span class="s1">))</span><span class="s0">,</span>
        <span class="s1">(-</span><span class="s3">1.0</span><span class="s0">, </span><span class="s3">1.0</span><span class="s0">, </span><span class="s1">np.log(</span><span class="s3">1.0 </span><span class="s1">+ np.exp(</span><span class="s3">1.0</span><span class="s1">))</span><span class="s0">, </span><span class="s1">-</span><span class="s3">1.0 </span><span class="s1">/ (np.exp(-</span><span class="s3">1.0</span><span class="s1">) + </span><span class="s3">1.0</span><span class="s1">))</span><span class="s0">,</span>
        <span class="s1">(</span><span class="s3">0.0</span><span class="s0">, </span><span class="s3">1.0</span><span class="s0">, </span><span class="s1">np.log(</span><span class="s3">2</span><span class="s1">)</span><span class="s0">, </span><span class="s1">-</span><span class="s3">0.5</span><span class="s1">)</span><span class="s0">,</span>
        <span class="s1">(</span><span class="s3">0.0</span><span class="s0">, </span><span class="s1">-</span><span class="s3">1.0</span><span class="s0">, </span><span class="s1">np.log(</span><span class="s3">2</span><span class="s1">)</span><span class="s0">, </span><span class="s3">0.5</span><span class="s1">)</span><span class="s0">,</span>
        <span class="s1">(</span><span class="s3">17.9</span><span class="s0">, </span><span class="s1">-</span><span class="s3">1.0</span><span class="s0">, </span><span class="s3">17.9</span><span class="s0">, </span><span class="s3">1.0</span><span class="s1">)</span><span class="s0">,</span>
        <span class="s1">(-</span><span class="s3">17.9</span><span class="s0">, </span><span class="s3">1.0</span><span class="s0">, </span><span class="s3">17.9</span><span class="s0">, </span><span class="s1">-</span><span class="s3">1.0</span><span class="s1">)</span><span class="s0">,</span>
    <span class="s1">]</span>
    <span class="s1">_test_loss_common(loss</span><span class="s0">, </span><span class="s1">cases)</span>
    <span class="s1">assert_almost_equal(loss.py_dloss(</span><span class="s3">18.1</span><span class="s0">, </span><span class="s3">1.0</span><span class="s1">)</span><span class="s0">, </span><span class="s1">np.exp(-</span><span class="s3">18.1</span><span class="s1">) * -</span><span class="s3">1.0</span><span class="s0">, </span><span class="s3">16</span><span class="s1">)</span>
    <span class="s1">assert_almost_equal(loss.py_loss(</span><span class="s3">18.1</span><span class="s0">, </span><span class="s3">1.0</span><span class="s1">)</span><span class="s0">, </span><span class="s1">np.exp(-</span><span class="s3">18.1</span><span class="s1">)</span><span class="s0">, </span><span class="s3">16</span><span class="s1">)</span>
    <span class="s1">assert_almost_equal(loss.py_dloss(-</span><span class="s3">18.1</span><span class="s0">, </span><span class="s1">-</span><span class="s3">1.0</span><span class="s1">)</span><span class="s0">, </span><span class="s1">np.exp(-</span><span class="s3">18.1</span><span class="s1">) * </span><span class="s3">1.0</span><span class="s0">, </span><span class="s3">16</span><span class="s1">)</span>
    <span class="s1">assert_almost_equal(loss.py_loss(-</span><span class="s3">18.1</span><span class="s0">, </span><span class="s3">1.0</span><span class="s1">)</span><span class="s0">, </span><span class="s3">18.1</span><span class="s0">, </span><span class="s3">16</span><span class="s1">)</span>


<span class="s0">def </span><span class="s1">test_loss_squared_loss():</span>
    <span class="s4"># Test SquaredLoss</span>
    <span class="s1">loss = sgd_fast.SquaredLoss()</span>
    <span class="s1">cases = [</span>
        <span class="s4"># (p, y, expected_loss, expected_dloss)</span>
        <span class="s1">(</span><span class="s3">0.0</span><span class="s0">, </span><span class="s3">0.0</span><span class="s0">, </span><span class="s3">0.0</span><span class="s0">, </span><span class="s3">0.0</span><span class="s1">)</span><span class="s0">,</span>
        <span class="s1">(</span><span class="s3">1.0</span><span class="s0">, </span><span class="s3">1.0</span><span class="s0">, </span><span class="s3">0.0</span><span class="s0">, </span><span class="s3">0.0</span><span class="s1">)</span><span class="s0">,</span>
        <span class="s1">(</span><span class="s3">1.0</span><span class="s0">, </span><span class="s3">0.0</span><span class="s0">, </span><span class="s3">0.5</span><span class="s0">, </span><span class="s3">1.0</span><span class="s1">)</span><span class="s0">,</span>
        <span class="s1">(</span><span class="s3">0.5</span><span class="s0">, </span><span class="s1">-</span><span class="s3">1.0</span><span class="s0">, </span><span class="s3">1.125</span><span class="s0">, </span><span class="s3">1.5</span><span class="s1">)</span><span class="s0">,</span>
        <span class="s1">(-</span><span class="s3">2.5</span><span class="s0">, </span><span class="s3">2.0</span><span class="s0">, </span><span class="s3">10.125</span><span class="s0">, </span><span class="s1">-</span><span class="s3">4.5</span><span class="s1">)</span><span class="s0">,</span>
    <span class="s1">]</span>
    <span class="s1">_test_loss_common(loss</span><span class="s0">, </span><span class="s1">cases)</span>


<span class="s0">def </span><span class="s1">test_loss_huber():</span>
    <span class="s4"># Test Huber</span>
    <span class="s1">loss = sgd_fast.Huber(</span><span class="s3">0.1</span><span class="s1">)</span>
    <span class="s1">cases = [</span>
        <span class="s4"># (p, y, expected_loss, expected_dloss)</span>
        <span class="s1">(</span><span class="s3">0.0</span><span class="s0">, </span><span class="s3">0.0</span><span class="s0">, </span><span class="s3">0.0</span><span class="s0">, </span><span class="s3">0.0</span><span class="s1">)</span><span class="s0">,</span>
        <span class="s1">(</span><span class="s3">0.1</span><span class="s0">, </span><span class="s3">0.0</span><span class="s0">, </span><span class="s3">0.005</span><span class="s0">, </span><span class="s3">0.1</span><span class="s1">)</span><span class="s0">,</span>
        <span class="s1">(</span><span class="s3">0.0</span><span class="s0">, </span><span class="s3">0.1</span><span class="s0">, </span><span class="s3">0.005</span><span class="s0">, </span><span class="s1">-</span><span class="s3">0.1</span><span class="s1">)</span><span class="s0">,</span>
        <span class="s1">(</span><span class="s3">3.95</span><span class="s0">, </span><span class="s3">4.0</span><span class="s0">, </span><span class="s3">0.00125</span><span class="s0">, </span><span class="s1">-</span><span class="s3">0.05</span><span class="s1">)</span><span class="s0">,</span>
        <span class="s1">(</span><span class="s3">5.0</span><span class="s0">, </span><span class="s3">2.0</span><span class="s0">, </span><span class="s3">0.295</span><span class="s0">, </span><span class="s3">0.1</span><span class="s1">)</span><span class="s0">,</span>
        <span class="s1">(-</span><span class="s3">1.0</span><span class="s0">, </span><span class="s3">5.0</span><span class="s0">, </span><span class="s3">0.595</span><span class="s0">, </span><span class="s1">-</span><span class="s3">0.1</span><span class="s1">)</span><span class="s0">,</span>
    <span class="s1">]</span>
    <span class="s1">_test_loss_common(loss</span><span class="s0">, </span><span class="s1">cases)</span>


<span class="s0">def </span><span class="s1">test_loss_modified_huber():</span>
    <span class="s4"># (p, y, expected_loss, expected_dloss)</span>
    <span class="s1">loss = sgd_fast.ModifiedHuber()</span>
    <span class="s1">cases = [</span>
        <span class="s4"># (p, y, expected_loss, expected_dloss)</span>
        <span class="s1">(</span><span class="s3">1.0</span><span class="s0">, </span><span class="s3">1.0</span><span class="s0">, </span><span class="s3">0.0</span><span class="s0">, </span><span class="s3">0.0</span><span class="s1">)</span><span class="s0">,</span>
        <span class="s1">(-</span><span class="s3">1.0</span><span class="s0">, </span><span class="s1">-</span><span class="s3">1.0</span><span class="s0">, </span><span class="s3">0.0</span><span class="s0">, </span><span class="s3">0.0</span><span class="s1">)</span><span class="s0">,</span>
        <span class="s1">(</span><span class="s3">2.0</span><span class="s0">, </span><span class="s3">1.0</span><span class="s0">, </span><span class="s3">0.0</span><span class="s0">, </span><span class="s3">0.0</span><span class="s1">)</span><span class="s0">,</span>
        <span class="s1">(</span><span class="s3">0.0</span><span class="s0">, </span><span class="s3">1.0</span><span class="s0">, </span><span class="s3">1.0</span><span class="s0">, </span><span class="s1">-</span><span class="s3">2.0</span><span class="s1">)</span><span class="s0">,</span>
        <span class="s1">(-</span><span class="s3">1.0</span><span class="s0">, </span><span class="s3">1.0</span><span class="s0">, </span><span class="s3">4.0</span><span class="s0">, </span><span class="s1">-</span><span class="s3">4.0</span><span class="s1">)</span><span class="s0">,</span>
        <span class="s1">(</span><span class="s3">0.5</span><span class="s0">, </span><span class="s1">-</span><span class="s3">1.0</span><span class="s0">, </span><span class="s3">2.25</span><span class="s0">, </span><span class="s3">3.0</span><span class="s1">)</span><span class="s0">,</span>
        <span class="s1">(-</span><span class="s3">2.0</span><span class="s0">, </span><span class="s3">1.0</span><span class="s0">, </span><span class="s3">8</span><span class="s0">, </span><span class="s1">-</span><span class="s3">4.0</span><span class="s1">)</span><span class="s0">,</span>
        <span class="s1">(-</span><span class="s3">3.0</span><span class="s0">, </span><span class="s3">1.0</span><span class="s0">, </span><span class="s3">12</span><span class="s0">, </span><span class="s1">-</span><span class="s3">4.0</span><span class="s1">)</span><span class="s0">,</span>
    <span class="s1">]</span>
    <span class="s1">_test_loss_common(loss</span><span class="s0">, </span><span class="s1">cases)</span>


<span class="s0">def </span><span class="s1">test_loss_epsilon_insensitive():</span>
    <span class="s4"># Test EpsilonInsensitive</span>
    <span class="s1">loss = sgd_fast.EpsilonInsensitive(</span><span class="s3">0.1</span><span class="s1">)</span>
    <span class="s1">cases = [</span>
        <span class="s4"># (p, y, expected_loss, expected_dloss)</span>
        <span class="s1">(</span><span class="s3">0.0</span><span class="s0">, </span><span class="s3">0.0</span><span class="s0">, </span><span class="s3">0.0</span><span class="s0">, </span><span class="s3">0.0</span><span class="s1">)</span><span class="s0">,</span>
        <span class="s1">(</span><span class="s3">0.1</span><span class="s0">, </span><span class="s3">0.0</span><span class="s0">, </span><span class="s3">0.0</span><span class="s0">, </span><span class="s3">0.0</span><span class="s1">)</span><span class="s0">,</span>
        <span class="s1">(-</span><span class="s3">2.05</span><span class="s0">, </span><span class="s1">-</span><span class="s3">2.0</span><span class="s0">, </span><span class="s3">0.0</span><span class="s0">, </span><span class="s3">0.0</span><span class="s1">)</span><span class="s0">,</span>
        <span class="s1">(</span><span class="s3">3.05</span><span class="s0">, </span><span class="s3">3.0</span><span class="s0">, </span><span class="s3">0.0</span><span class="s0">, </span><span class="s3">0.0</span><span class="s1">)</span><span class="s0">,</span>
        <span class="s1">(</span><span class="s3">2.2</span><span class="s0">, </span><span class="s3">2.0</span><span class="s0">, </span><span class="s3">0.1</span><span class="s0">, </span><span class="s3">1.0</span><span class="s1">)</span><span class="s0">,</span>
        <span class="s1">(</span><span class="s3">2.0</span><span class="s0">, </span><span class="s1">-</span><span class="s3">1.0</span><span class="s0">, </span><span class="s3">2.9</span><span class="s0">, </span><span class="s3">1.0</span><span class="s1">)</span><span class="s0">,</span>
        <span class="s1">(</span><span class="s3">2.0</span><span class="s0">, </span><span class="s3">2.2</span><span class="s0">, </span><span class="s3">0.1</span><span class="s0">, </span><span class="s1">-</span><span class="s3">1.0</span><span class="s1">)</span><span class="s0">,</span>
        <span class="s1">(-</span><span class="s3">2.0</span><span class="s0">, </span><span class="s3">1.0</span><span class="s0">, </span><span class="s3">2.9</span><span class="s0">, </span><span class="s1">-</span><span class="s3">1.0</span><span class="s1">)</span><span class="s0">,</span>
    <span class="s1">]</span>
    <span class="s1">_test_loss_common(loss</span><span class="s0">, </span><span class="s1">cases)</span>


<span class="s0">def </span><span class="s1">test_loss_squared_epsilon_insensitive():</span>
    <span class="s4"># Test SquaredEpsilonInsensitive</span>
    <span class="s1">loss = sgd_fast.SquaredEpsilonInsensitive(</span><span class="s3">0.1</span><span class="s1">)</span>
    <span class="s1">cases = [</span>
        <span class="s4"># (p, y, expected_loss, expected_dloss)</span>
        <span class="s1">(</span><span class="s3">0.0</span><span class="s0">, </span><span class="s3">0.0</span><span class="s0">, </span><span class="s3">0.0</span><span class="s0">, </span><span class="s3">0.0</span><span class="s1">)</span><span class="s0">,</span>
        <span class="s1">(</span><span class="s3">0.1</span><span class="s0">, </span><span class="s3">0.0</span><span class="s0">, </span><span class="s3">0.0</span><span class="s0">, </span><span class="s3">0.0</span><span class="s1">)</span><span class="s0">,</span>
        <span class="s1">(-</span><span class="s3">2.05</span><span class="s0">, </span><span class="s1">-</span><span class="s3">2.0</span><span class="s0">, </span><span class="s3">0.0</span><span class="s0">, </span><span class="s3">0.0</span><span class="s1">)</span><span class="s0">,</span>
        <span class="s1">(</span><span class="s3">3.05</span><span class="s0">, </span><span class="s3">3.0</span><span class="s0">, </span><span class="s3">0.0</span><span class="s0">, </span><span class="s3">0.0</span><span class="s1">)</span><span class="s0">,</span>
        <span class="s1">(</span><span class="s3">2.2</span><span class="s0">, </span><span class="s3">2.0</span><span class="s0">, </span><span class="s3">0.01</span><span class="s0">, </span><span class="s3">0.2</span><span class="s1">)</span><span class="s0">,</span>
        <span class="s1">(</span><span class="s3">2.0</span><span class="s0">, </span><span class="s1">-</span><span class="s3">1.0</span><span class="s0">, </span><span class="s3">8.41</span><span class="s0">, </span><span class="s3">5.8</span><span class="s1">)</span><span class="s0">,</span>
        <span class="s1">(</span><span class="s3">2.0</span><span class="s0">, </span><span class="s3">2.2</span><span class="s0">, </span><span class="s3">0.01</span><span class="s0">, </span><span class="s1">-</span><span class="s3">0.2</span><span class="s1">)</span><span class="s0">,</span>
        <span class="s1">(-</span><span class="s3">2.0</span><span class="s0">, </span><span class="s3">1.0</span><span class="s0">, </span><span class="s3">8.41</span><span class="s0">, </span><span class="s1">-</span><span class="s3">5.8</span><span class="s1">)</span><span class="s0">,</span>
    <span class="s1">]</span>
    <span class="s1">_test_loss_common(loss</span><span class="s0">, </span><span class="s1">cases)</span>


<span class="s0">def </span><span class="s1">test_multi_thread_multi_class_and_early_stopping():</span>
    <span class="s4"># This is a non-regression test for a bad interaction between</span>
    <span class="s4"># early stopping internal attribute and thread-based parallelism.</span>
    <span class="s1">clf = SGDClassifier(</span>
        <span class="s1">alpha=</span><span class="s3">1e-3</span><span class="s0">,</span>
        <span class="s1">tol=</span><span class="s3">1e-3</span><span class="s0">,</span>
        <span class="s1">max_iter=</span><span class="s3">1000</span><span class="s0">,</span>
        <span class="s1">early_stopping=</span><span class="s0">True,</span>
        <span class="s1">n_iter_no_change=</span><span class="s3">100</span><span class="s0">,</span>
        <span class="s1">random_state=</span><span class="s3">0</span><span class="s0">,</span>
        <span class="s1">n_jobs=</span><span class="s3">2</span><span class="s0">,</span>
    <span class="s1">)</span>
    <span class="s1">clf.fit(iris.data</span><span class="s0">, </span><span class="s1">iris.target)</span>
    <span class="s0">assert </span><span class="s1">clf.n_iter_ &gt; clf.n_iter_no_change</span>
    <span class="s0">assert </span><span class="s1">clf.n_iter_ &lt; clf.n_iter_no_change + </span><span class="s3">20</span>
    <span class="s0">assert </span><span class="s1">clf.score(iris.data</span><span class="s0">, </span><span class="s1">iris.target) &gt; </span><span class="s3">0.8</span>


<span class="s0">def </span><span class="s1">test_multi_core_gridsearch_and_early_stopping():</span>
    <span class="s4"># This is a non-regression test for a bad interaction between</span>
    <span class="s4"># early stopping internal attribute and process-based multi-core</span>
    <span class="s4"># parallelism.</span>
    <span class="s1">param_grid = {</span>
        <span class="s2">&quot;alpha&quot;</span><span class="s1">: np.logspace(-</span><span class="s3">4</span><span class="s0">, </span><span class="s3">4</span><span class="s0">, </span><span class="s3">9</span><span class="s1">)</span><span class="s0">,</span>
        <span class="s2">&quot;n_iter_no_change&quot;</span><span class="s1">: [</span><span class="s3">5</span><span class="s0">, </span><span class="s3">10</span><span class="s0">, </span><span class="s3">50</span><span class="s1">]</span><span class="s0">,</span>
    <span class="s1">}</span>

    <span class="s1">clf = SGDClassifier(tol=</span><span class="s3">1e-2</span><span class="s0">, </span><span class="s1">max_iter=</span><span class="s3">1000</span><span class="s0">, </span><span class="s1">early_stopping=</span><span class="s0">True, </span><span class="s1">random_state=</span><span class="s3">0</span><span class="s1">)</span>
    <span class="s1">search = RandomizedSearchCV(clf</span><span class="s0">, </span><span class="s1">param_grid</span><span class="s0">, </span><span class="s1">n_iter=</span><span class="s3">5</span><span class="s0">, </span><span class="s1">n_jobs=</span><span class="s3">2</span><span class="s0">, </span><span class="s1">random_state=</span><span class="s3">0</span><span class="s1">)</span>
    <span class="s1">search.fit(iris.data</span><span class="s0">, </span><span class="s1">iris.target)</span>
    <span class="s0">assert </span><span class="s1">search.best_score_ &gt; </span><span class="s3">0.8</span>


<span class="s1">@pytest.mark.parametrize(</span><span class="s2">&quot;backend&quot;</span><span class="s0">, </span><span class="s1">[</span><span class="s2">&quot;loky&quot;</span><span class="s0">, </span><span class="s2">&quot;multiprocessing&quot;</span><span class="s0">, </span><span class="s2">&quot;threading&quot;</span><span class="s1">])</span>
<span class="s0">def </span><span class="s1">test_SGDClassifier_fit_for_all_backends(backend):</span>
    <span class="s4"># This is a non-regression smoke test. In the multi-class case,</span>
    <span class="s4"># SGDClassifier.fit fits each class in a one-versus-all fashion using</span>
    <span class="s4"># joblib.Parallel.  However, each OvA step updates the coef_ attribute of</span>
    <span class="s4"># the estimator in-place. Internally, SGDClassifier calls Parallel using</span>
    <span class="s4"># require='sharedmem'. This test makes sure SGDClassifier.fit works</span>
    <span class="s4"># consistently even when the user asks for a backend that does not provide</span>
    <span class="s4"># sharedmem semantics.</span>

    <span class="s4"># We further test a case where memmapping would have been used if</span>
    <span class="s4"># SGDClassifier.fit was called from a loky or multiprocessing backend. In</span>
    <span class="s4"># this specific case, in-place modification of clf.coef_ would have caused</span>
    <span class="s4"># a segmentation fault when trying to write in a readonly memory mapped</span>
    <span class="s4"># buffer.</span>

    <span class="s1">random_state = np.random.RandomState(</span><span class="s3">42</span><span class="s1">)</span>

    <span class="s4"># Create a classification problem with 50000 features and 20 classes. Using</span>
    <span class="s4"># loky or multiprocessing this make the clf.coef_ exceed the threshold</span>
    <span class="s4"># above which memmaping is used in joblib and loky (1MB as of 2018/11/1).</span>
    <span class="s1">X = sp.random(</span><span class="s3">500</span><span class="s0">, </span><span class="s3">2000</span><span class="s0">, </span><span class="s1">density=</span><span class="s3">0.02</span><span class="s0">, </span><span class="s1">format=</span><span class="s2">&quot;csr&quot;</span><span class="s0">, </span><span class="s1">random_state=random_state)</span>
    <span class="s1">y = random_state.choice(</span><span class="s3">20</span><span class="s0">, </span><span class="s3">500</span><span class="s1">)</span>

    <span class="s4"># Begin by fitting a SGD classifier sequentially</span>
    <span class="s1">clf_sequential = SGDClassifier(max_iter=</span><span class="s3">1000</span><span class="s0">, </span><span class="s1">n_jobs=</span><span class="s3">1</span><span class="s0">, </span><span class="s1">random_state=</span><span class="s3">42</span><span class="s1">)</span>
    <span class="s1">clf_sequential.fit(X</span><span class="s0">, </span><span class="s1">y)</span>

    <span class="s4"># Fit a SGDClassifier using the specified backend, and make sure the</span>
    <span class="s4"># coefficients are equal to those obtained using a sequential fit</span>
    <span class="s1">clf_parallel = SGDClassifier(max_iter=</span><span class="s3">1000</span><span class="s0">, </span><span class="s1">n_jobs=</span><span class="s3">4</span><span class="s0">, </span><span class="s1">random_state=</span><span class="s3">42</span><span class="s1">)</span>
    <span class="s0">with </span><span class="s1">joblib.parallel_backend(backend=backend):</span>
        <span class="s1">clf_parallel.fit(X</span><span class="s0">, </span><span class="s1">y)</span>
    <span class="s1">assert_array_almost_equal(clf_sequential.coef_</span><span class="s0">, </span><span class="s1">clf_parallel.coef_)</span>


<span class="s1">@pytest.mark.parametrize(</span>
    <span class="s2">&quot;Estimator&quot;</span><span class="s0">, </span><span class="s1">[linear_model.SGDClassifier</span><span class="s0">, </span><span class="s1">linear_model.SGDRegressor]</span>
<span class="s1">)</span>
<span class="s0">def </span><span class="s1">test_sgd_random_state(Estimator</span><span class="s0">, </span><span class="s1">global_random_seed):</span>
    <span class="s4"># Train the same model on the same data without converging and check that we</span>
    <span class="s4"># get reproducible results by fixing the random seed.</span>
    <span class="s0">if </span><span class="s1">Estimator == linear_model.SGDRegressor:</span>
        <span class="s1">X</span><span class="s0">, </span><span class="s1">y = datasets.make_regression(random_state=global_random_seed)</span>
    <span class="s0">else</span><span class="s1">:</span>
        <span class="s1">X</span><span class="s0">, </span><span class="s1">y = datasets.make_classification(random_state=global_random_seed)</span>

    <span class="s4"># Fitting twice a model with the same hyper-parameters on the same training</span>
    <span class="s4"># set with the same seed leads to the same results deterministically.</span>

    <span class="s1">est = Estimator(random_state=global_random_seed</span><span class="s0">, </span><span class="s1">max_iter=</span><span class="s3">1</span><span class="s1">)</span>
    <span class="s0">with </span><span class="s1">pytest.warns(ConvergenceWarning):</span>
        <span class="s1">coef_same_seed_a = est.fit(X</span><span class="s0">, </span><span class="s1">y).coef_</span>
        <span class="s0">assert </span><span class="s1">est.n_iter_ == </span><span class="s3">1</span>

    <span class="s1">est = Estimator(random_state=global_random_seed</span><span class="s0">, </span><span class="s1">max_iter=</span><span class="s3">1</span><span class="s1">)</span>
    <span class="s0">with </span><span class="s1">pytest.warns(ConvergenceWarning):</span>
        <span class="s1">coef_same_seed_b = est.fit(X</span><span class="s0">, </span><span class="s1">y).coef_</span>
        <span class="s0">assert </span><span class="s1">est.n_iter_ == </span><span class="s3">1</span>

    <span class="s1">assert_allclose(coef_same_seed_a</span><span class="s0">, </span><span class="s1">coef_same_seed_b)</span>

    <span class="s4"># Fitting twice a model with the same hyper-parameters on the same training</span>
    <span class="s4"># set but with different random seed leads to different results after one</span>
    <span class="s4"># epoch because of the random shuffling of the dataset.</span>

    <span class="s1">est = Estimator(random_state=global_random_seed + </span><span class="s3">1</span><span class="s0">, </span><span class="s1">max_iter=</span><span class="s3">1</span><span class="s1">)</span>
    <span class="s0">with </span><span class="s1">pytest.warns(ConvergenceWarning):</span>
        <span class="s1">coef_other_seed = est.fit(X</span><span class="s0">, </span><span class="s1">y).coef_</span>
        <span class="s0">assert </span><span class="s1">est.n_iter_ == </span><span class="s3">1</span>

    <span class="s0">assert </span><span class="s1">np.abs(coef_same_seed_a - coef_other_seed).max() &gt; </span><span class="s3">1.0</span>


<span class="s0">def </span><span class="s1">test_validation_mask_correctly_subsets(monkeypatch):</span>
    <span class="s5">&quot;&quot;&quot;Test that data passed to validation callback correctly subsets. 
 
    Non-regression test for #23255. 
    &quot;&quot;&quot;</span>
    <span class="s1">X</span><span class="s0">, </span><span class="s1">Y = iris.data</span><span class="s0">, </span><span class="s1">iris.target</span>
    <span class="s1">n_samples = X.shape[</span><span class="s3">0</span><span class="s1">]</span>
    <span class="s1">validation_fraction = </span><span class="s3">0.2</span>
    <span class="s1">clf = linear_model.SGDClassifier(</span>
        <span class="s1">early_stopping=</span><span class="s0">True,</span>
        <span class="s1">tol=</span><span class="s3">1e-3</span><span class="s0">,</span>
        <span class="s1">max_iter=</span><span class="s3">1000</span><span class="s0">,</span>
        <span class="s1">validation_fraction=validation_fraction</span><span class="s0">,</span>
    <span class="s1">)</span>

    <span class="s1">mock = Mock(side_effect=_stochastic_gradient._ValidationScoreCallback)</span>
    <span class="s1">monkeypatch.setattr(_stochastic_gradient</span><span class="s0">, </span><span class="s2">&quot;_ValidationScoreCallback&quot;</span><span class="s0">, </span><span class="s1">mock)</span>
    <span class="s1">clf.fit(X</span><span class="s0">, </span><span class="s1">Y)</span>

    <span class="s1">X_val</span><span class="s0">, </span><span class="s1">y_val = mock.call_args[</span><span class="s3">0</span><span class="s1">][</span><span class="s3">1</span><span class="s1">:</span><span class="s3">3</span><span class="s1">]</span>
    <span class="s0">assert </span><span class="s1">X_val.shape[</span><span class="s3">0</span><span class="s1">] == int(n_samples * validation_fraction)</span>
    <span class="s0">assert </span><span class="s1">y_val.shape[</span><span class="s3">0</span><span class="s1">] == int(n_samples * validation_fraction)</span>


<span class="s0">def </span><span class="s1">test_sgd_error_on_zero_validation_weight():</span>
    <span class="s4"># Test that SGDClassifier raises error when all the validation samples</span>
    <span class="s4"># have zero sample_weight. Non-regression test for #17229.</span>
    <span class="s1">X</span><span class="s0">, </span><span class="s1">Y = iris.data</span><span class="s0">, </span><span class="s1">iris.target</span>
    <span class="s1">sample_weight = np.zeros_like(Y)</span>
    <span class="s1">validation_fraction = </span><span class="s3">0.4</span>

    <span class="s1">clf = linear_model.SGDClassifier(</span>
        <span class="s1">early_stopping=</span><span class="s0">True, </span><span class="s1">validation_fraction=validation_fraction</span><span class="s0">, </span><span class="s1">random_state=</span><span class="s3">0</span>
    <span class="s1">)</span>

    <span class="s1">error_message = (</span>
        <span class="s2">&quot;The sample weights for validation set are all zero, consider using a&quot;</span>
        <span class="s2">&quot; different random state.&quot;</span>
    <span class="s1">)</span>
    <span class="s0">with </span><span class="s1">pytest.raises(ValueError</span><span class="s0">, </span><span class="s1">match=error_message):</span>
        <span class="s1">clf.fit(X</span><span class="s0">, </span><span class="s1">Y</span><span class="s0">, </span><span class="s1">sample_weight=sample_weight)</span>


<span class="s1">@pytest.mark.parametrize(</span><span class="s2">&quot;Estimator&quot;</span><span class="s0">, </span><span class="s1">[SGDClassifier</span><span class="s0">, </span><span class="s1">SGDRegressor])</span>
<span class="s0">def </span><span class="s1">test_sgd_verbose(Estimator):</span>
    <span class="s5">&quot;&quot;&quot;non-regression test for gh #25249&quot;&quot;&quot;</span>
    <span class="s1">Estimator(verbose=</span><span class="s3">1</span><span class="s1">).fit(X</span><span class="s0">, </span><span class="s1">Y)</span>


<span class="s1">@pytest.mark.parametrize(</span>
    <span class="s2">&quot;SGDEstimator&quot;</span><span class="s0">,</span>
    <span class="s1">[</span>
        <span class="s1">SGDClassifier</span><span class="s0">,</span>
        <span class="s1">SparseSGDClassifier</span><span class="s0">,</span>
        <span class="s1">SGDRegressor</span><span class="s0">,</span>
        <span class="s1">SparseSGDRegressor</span><span class="s0">,</span>
        <span class="s1">SGDOneClassSVM</span><span class="s0">,</span>
        <span class="s1">SparseSGDOneClassSVM</span><span class="s0">,</span>
    <span class="s1">]</span><span class="s0">,</span>
<span class="s1">)</span>
<span class="s1">@pytest.mark.parametrize(</span><span class="s2">&quot;data_type&quot;</span><span class="s0">, </span><span class="s1">(np.float32</span><span class="s0">, </span><span class="s1">np.float64))</span>
<span class="s0">def </span><span class="s1">test_sgd_dtype_match(SGDEstimator</span><span class="s0">, </span><span class="s1">data_type):</span>
    <span class="s1">_X = X.astype(data_type)</span>
    <span class="s1">_Y = np.array(Y</span><span class="s0">, </span><span class="s1">dtype=data_type)</span>
    <span class="s1">sgd_model = SGDEstimator()</span>
    <span class="s1">sgd_model.fit(_X</span><span class="s0">, </span><span class="s1">_Y)</span>
    <span class="s0">assert </span><span class="s1">sgd_model.coef_.dtype == data_type</span>


<span class="s1">@pytest.mark.parametrize(</span>
    <span class="s2">&quot;SGDEstimator&quot;</span><span class="s0">,</span>
    <span class="s1">[</span>
        <span class="s1">SGDClassifier</span><span class="s0">,</span>
        <span class="s1">SparseSGDClassifier</span><span class="s0">,</span>
        <span class="s1">SGDRegressor</span><span class="s0">,</span>
        <span class="s1">SparseSGDRegressor</span><span class="s0">,</span>
        <span class="s1">SGDOneClassSVM</span><span class="s0">,</span>
        <span class="s1">SparseSGDOneClassSVM</span><span class="s0">,</span>
    <span class="s1">]</span><span class="s0">,</span>
<span class="s1">)</span>
<span class="s0">def </span><span class="s1">test_sgd_numerical_consistency(SGDEstimator):</span>
    <span class="s1">X_64 = X.astype(dtype=np.float64)</span>
    <span class="s1">Y_64 = np.array(Y</span><span class="s0">, </span><span class="s1">dtype=np.float64)</span>

    <span class="s1">X_32 = X.astype(dtype=np.float32)</span>
    <span class="s1">Y_32 = np.array(Y</span><span class="s0">, </span><span class="s1">dtype=np.float32)</span>

    <span class="s1">sgd_64 = SGDEstimator(max_iter=</span><span class="s3">20</span><span class="s1">)</span>
    <span class="s1">sgd_64.fit(X_64</span><span class="s0">, </span><span class="s1">Y_64)</span>

    <span class="s1">sgd_32 = SGDEstimator(max_iter=</span><span class="s3">20</span><span class="s1">)</span>
    <span class="s1">sgd_32.fit(X_32</span><span class="s0">, </span><span class="s1">Y_32)</span>

    <span class="s1">assert_allclose(sgd_64.coef_</span><span class="s0">, </span><span class="s1">sgd_32.coef_)</span>
</pre>
</body>
</html>