<html>
<head>
<title>test_layers.py</title>
<meta http-equiv="Content-Type" content="text/html; charset=utf-8">
<style type="text/css">
.s0 { color: #cc7832;}
.s1 { color: #a9b7c6;}
.s2 { color: #6a8759;}
.s3 { color: #808080;}
.s4 { color: #629755; font-style: italic;}
.s5 { color: #6897bb;}
</style>
</head>
<body bgcolor="#2b2b2b">
<table CELLSPACING=0 CELLPADDING=5 COLS=1 WIDTH="100%" BGCOLOR="#606060" >
<tr><td><center>
<font face="Arial, Helvetica" color="#000000">
test_layers.py</font>
</center></td></tr></table>
<pre><span class="s0">from </span><span class="s1">__future__ </span><span class="s0">import </span><span class="s1">annotations</span>

<span class="s0">import </span><span class="s1">os</span>

<span class="s0">import </span><span class="s1">pytest</span>

<span class="s1">pytest.importorskip(</span><span class="s2">&quot;distributed&quot;</span><span class="s1">)</span>

<span class="s0">import </span><span class="s1">sys</span>
<span class="s0">from </span><span class="s1">operator </span><span class="s0">import </span><span class="s1">getitem</span>

<span class="s0">from </span><span class="s1">distributed </span><span class="s0">import </span><span class="s1">Client</span><span class="s0">, </span><span class="s1">SchedulerPlugin</span>
<span class="s0">from </span><span class="s1">distributed.utils_test </span><span class="s0">import </span><span class="s1">cluster</span><span class="s0">, </span><span class="s1">loop  </span><span class="s3"># noqa F401</span>

<span class="s0">from </span><span class="s1">dask.highlevelgraph </span><span class="s0">import </span><span class="s1">HighLevelGraph</span>
<span class="s0">from </span><span class="s1">dask.layers </span><span class="s0">import </span><span class="s1">ArrayChunkShapeDep</span><span class="s0">, </span><span class="s1">ArraySliceDep</span><span class="s0">, </span><span class="s1">fractional_slice</span>


<span class="s0">class </span><span class="s1">SchedulerImportCheck(SchedulerPlugin):</span>
    <span class="s4">&quot;&quot;&quot;Plugin to help record which modules are imported on the scheduler&quot;&quot;&quot;</span>

    <span class="s1">name = </span><span class="s2">&quot;import-check&quot;</span>

    <span class="s0">def </span><span class="s1">__init__(self</span><span class="s0">, </span><span class="s1">pattern):</span>
        <span class="s1">self.pattern = pattern</span>

    <span class="s0">async def </span><span class="s1">start(self</span><span class="s0">, </span><span class="s1">scheduler):</span>
        <span class="s3"># Record the modules that have been imported when the scheduler starts</span>
        <span class="s1">self.start_modules = set()</span>
        <span class="s0">for </span><span class="s1">mod </span><span class="s0">in </span><span class="s1">set(sys.modules):</span>
            <span class="s0">if not </span><span class="s1">mod.startswith(self.pattern):</span>
                <span class="s1">self.start_modules.add(mod)</span>
            <span class="s0">else</span><span class="s1">:</span>
                <span class="s3"># Maually remove the target library</span>
                <span class="s1">sys.modules.pop(mod)</span>


<span class="s0">def </span><span class="s1">test_array_chunk_shape_dep():</span>
    <span class="s1">dac = pytest.importorskip(</span><span class="s2">&quot;dask.array.core&quot;</span><span class="s1">)</span>
    <span class="s1">d = </span><span class="s5">2  </span><span class="s3"># number of chunks in x,y</span>
    <span class="s1">chunk = (</span><span class="s5">2</span><span class="s0">, </span><span class="s5">3</span><span class="s1">)  </span><span class="s3"># chunk shape</span>
    <span class="s1">shape = tuple(d * n </span><span class="s0">for </span><span class="s1">n </span><span class="s0">in </span><span class="s1">chunk)  </span><span class="s3"># array shape</span>
    <span class="s1">chunks = dac.normalize_chunks(chunk</span><span class="s0">, </span><span class="s1">shape)</span>
    <span class="s1">array_deps = ArrayChunkShapeDep(chunks)</span>

    <span class="s0">def </span><span class="s1">check(i</span><span class="s0">, </span><span class="s1">j):</span>
        <span class="s1">chunk_shape = array_deps[(i</span><span class="s0">, </span><span class="s1">j)]</span>
        <span class="s0">assert </span><span class="s1">chunk_shape == chunk</span>

    <span class="s0">for </span><span class="s1">i </span><span class="s0">in </span><span class="s1">range(d):</span>
        <span class="s0">for </span><span class="s1">j </span><span class="s0">in </span><span class="s1">range(d):</span>
            <span class="s1">check(i</span><span class="s0">, </span><span class="s1">j)</span>


<span class="s0">def </span><span class="s1">test_array_slice_deps():</span>
    <span class="s1">dac = pytest.importorskip(</span><span class="s2">&quot;dask.array.core&quot;</span><span class="s1">)</span>
    <span class="s1">d = </span><span class="s5">2  </span><span class="s3"># number of chunks in x,y</span>
    <span class="s1">chunk = (</span><span class="s5">2</span><span class="s0">, </span><span class="s5">3</span><span class="s1">)  </span><span class="s3"># chunk shape</span>
    <span class="s1">shape = tuple(d * n </span><span class="s0">for </span><span class="s1">n </span><span class="s0">in </span><span class="s1">chunk)  </span><span class="s3"># array shape</span>
    <span class="s1">chunks = dac.normalize_chunks(chunk</span><span class="s0">, </span><span class="s1">shape)</span>
    <span class="s1">array_deps = ArraySliceDep(chunks)</span>

    <span class="s0">def </span><span class="s1">check(i</span><span class="s0">, </span><span class="s1">j):</span>
        <span class="s1">slices = array_deps[(i</span><span class="s0">, </span><span class="s1">j)]</span>
        <span class="s0">assert </span><span class="s1">slices == (</span>
            <span class="s1">slice(chunk[</span><span class="s5">0</span><span class="s1">] * i</span><span class="s0">, </span><span class="s1">chunk[</span><span class="s5">0</span><span class="s1">] * (i + </span><span class="s5">1</span><span class="s1">)</span><span class="s0">, None</span><span class="s1">)</span><span class="s0">,</span>
            <span class="s1">slice(chunk[</span><span class="s5">1</span><span class="s1">] * j</span><span class="s0">, </span><span class="s1">chunk[</span><span class="s5">1</span><span class="s1">] * (j + </span><span class="s5">1</span><span class="s1">)</span><span class="s0">, None</span><span class="s1">)</span><span class="s0">,</span>
        <span class="s1">)</span>

    <span class="s0">for </span><span class="s1">i </span><span class="s0">in </span><span class="s1">range(d):</span>
        <span class="s0">for </span><span class="s1">j </span><span class="s0">in </span><span class="s1">range(d):</span>
            <span class="s1">check(i</span><span class="s0">, </span><span class="s1">j)</span>


<span class="s0">def </span><span class="s1">_dataframe_shuffle(tmpdir):</span>
    <span class="s1">pd = pytest.importorskip(</span><span class="s2">&quot;pandas&quot;</span><span class="s1">)</span>
    <span class="s1">dd = pytest.importorskip(</span><span class="s2">&quot;dask.dataframe&quot;</span><span class="s1">)</span>

    <span class="s3"># Perform a computation using an HLG-based shuffle</span>
    <span class="s1">df = pd.DataFrame({</span><span class="s2">&quot;a&quot;</span><span class="s1">: range(</span><span class="s5">10</span><span class="s1">)</span><span class="s0">, </span><span class="s2">&quot;b&quot;</span><span class="s1">: range(</span><span class="s5">10</span><span class="s0">, </span><span class="s5">20</span><span class="s1">)})</span>
    <span class="s0">return </span><span class="s1">dd.from_pandas(df</span><span class="s0">, </span><span class="s1">npartitions=</span><span class="s5">2</span><span class="s1">).shuffle(</span><span class="s2">&quot;a&quot;</span><span class="s0">, </span><span class="s1">shuffle=</span><span class="s2">&quot;tasks&quot;</span><span class="s1">)</span>


<span class="s0">def </span><span class="s1">_dataframe_tree_reduction(tmpdir):</span>
    <span class="s1">pd = pytest.importorskip(</span><span class="s2">&quot;pandas&quot;</span><span class="s1">)</span>
    <span class="s1">dd = pytest.importorskip(</span><span class="s2">&quot;dask.dataframe&quot;</span><span class="s1">)</span>

    <span class="s3"># Perform a computation using an HLG-based tree reduction</span>
    <span class="s1">df = pd.DataFrame({</span><span class="s2">&quot;a&quot;</span><span class="s1">: range(</span><span class="s5">10</span><span class="s1">)</span><span class="s0">, </span><span class="s2">&quot;b&quot;</span><span class="s1">: range(</span><span class="s5">10</span><span class="s0">, </span><span class="s5">20</span><span class="s1">)})</span>
    <span class="s0">return </span><span class="s1">dd.from_pandas(df</span><span class="s0">, </span><span class="s1">npartitions=</span><span class="s5">2</span><span class="s1">).mean()</span>


<span class="s0">def </span><span class="s1">_dataframe_broadcast_join(tmpdir):</span>
    <span class="s1">pd = pytest.importorskip(</span><span class="s2">&quot;pandas&quot;</span><span class="s1">)</span>
    <span class="s1">dd = pytest.importorskip(</span><span class="s2">&quot;dask.dataframe&quot;</span><span class="s1">)</span>

    <span class="s3"># Perform a computation using an HLG-based broadcast join</span>
    <span class="s1">df = pd.DataFrame({</span><span class="s2">&quot;a&quot;</span><span class="s1">: range(</span><span class="s5">10</span><span class="s1">)</span><span class="s0">, </span><span class="s2">&quot;b&quot;</span><span class="s1">: range(</span><span class="s5">10</span><span class="s0">, </span><span class="s5">20</span><span class="s1">)})</span>
    <span class="s1">ddf1 = dd.from_pandas(df</span><span class="s0">, </span><span class="s1">npartitions=</span><span class="s5">4</span><span class="s1">)</span>
    <span class="s1">ddf2 = dd.from_pandas(df</span><span class="s0">, </span><span class="s1">npartitions=</span><span class="s5">1</span><span class="s1">)</span>
    <span class="s0">return </span><span class="s1">ddf1.merge(ddf2</span><span class="s0">, </span><span class="s1">how=</span><span class="s2">&quot;left&quot;</span><span class="s0">, </span><span class="s1">broadcast=</span><span class="s0">True, </span><span class="s1">shuffle=</span><span class="s2">&quot;tasks&quot;</span><span class="s1">)</span>


<span class="s0">def </span><span class="s1">_array_creation(tmpdir):</span>
    <span class="s1">da = pytest.importorskip(</span><span class="s2">&quot;dask.array&quot;</span><span class="s1">)</span>

    <span class="s3"># Perform a computation using HLG-based array creation</span>
    <span class="s0">return </span><span class="s1">da.ones((</span><span class="s5">100</span><span class="s0">,</span><span class="s1">)) + da.zeros((</span><span class="s5">100</span><span class="s0">,</span><span class="s1">))</span>


<span class="s0">def </span><span class="s1">_array_map_overlap(tmpdir):</span>
    <span class="s1">da = pytest.importorskip(</span><span class="s2">&quot;dask.array&quot;</span><span class="s1">)</span>
    <span class="s1">array = da.ones((</span><span class="s5">100</span><span class="s0">,</span><span class="s1">))</span>
    <span class="s0">return </span><span class="s1">array.map_overlap(</span><span class="s0">lambda </span><span class="s1">x: x</span><span class="s0">, </span><span class="s1">depth=</span><span class="s5">1</span><span class="s0">, </span><span class="s1">boundary=</span><span class="s2">&quot;none&quot;</span><span class="s1">)</span>


<span class="s0">def </span><span class="s1">test_fractional_slice():</span>
    <span class="s0">assert </span><span class="s1">fractional_slice((</span><span class="s2">&quot;x&quot;</span><span class="s0">, </span><span class="s5">4.9</span><span class="s1">)</span><span class="s0">, </span><span class="s1">{</span><span class="s5">0</span><span class="s1">: </span><span class="s5">2</span><span class="s1">}) == (getitem</span><span class="s0">, </span><span class="s1">(</span><span class="s2">&quot;x&quot;</span><span class="s0">, </span><span class="s5">5</span><span class="s1">)</span><span class="s0">, </span><span class="s1">(slice(</span><span class="s5">0</span><span class="s0">, </span><span class="s5">2</span><span class="s1">)</span><span class="s0">,</span><span class="s1">))</span>

    <span class="s0">assert </span><span class="s1">fractional_slice((</span><span class="s2">&quot;x&quot;</span><span class="s0">, </span><span class="s5">3</span><span class="s0">, </span><span class="s5">5.1</span><span class="s1">)</span><span class="s0">, </span><span class="s1">{</span><span class="s5">0</span><span class="s1">: </span><span class="s5">2</span><span class="s0">, </span><span class="s5">1</span><span class="s1">: </span><span class="s5">3</span><span class="s1">}) == (</span>
        <span class="s1">getitem</span><span class="s0">,</span>
        <span class="s1">(</span><span class="s2">&quot;x&quot;</span><span class="s0">, </span><span class="s5">3</span><span class="s0">, </span><span class="s5">5</span><span class="s1">)</span><span class="s0">,</span>
        <span class="s1">(slice(</span><span class="s0">None, None, None</span><span class="s1">)</span><span class="s0">, </span><span class="s1">slice(-</span><span class="s5">3</span><span class="s0">, None</span><span class="s1">))</span><span class="s0">,</span>
    <span class="s1">)</span>

    <span class="s0">assert </span><span class="s1">fractional_slice((</span><span class="s2">&quot;x&quot;</span><span class="s0">, </span><span class="s5">2.9</span><span class="s0">, </span><span class="s5">5.1</span><span class="s1">)</span><span class="s0">, </span><span class="s1">{</span><span class="s5">0</span><span class="s1">: </span><span class="s5">2</span><span class="s0">, </span><span class="s5">1</span><span class="s1">: </span><span class="s5">3</span><span class="s1">}) == (</span>
        <span class="s1">getitem</span><span class="s0">,</span>
        <span class="s1">(</span><span class="s2">&quot;x&quot;</span><span class="s0">, </span><span class="s5">3</span><span class="s0">, </span><span class="s5">5</span><span class="s1">)</span><span class="s0">,</span>
        <span class="s1">(slice(</span><span class="s5">0</span><span class="s0">, </span><span class="s5">2</span><span class="s1">)</span><span class="s0">, </span><span class="s1">slice(-</span><span class="s5">3</span><span class="s0">, None</span><span class="s1">))</span><span class="s0">,</span>
    <span class="s1">)</span>

    <span class="s1">fs = fractional_slice((</span><span class="s2">&quot;x&quot;</span><span class="s0">, </span><span class="s5">4.9</span><span class="s1">)</span><span class="s0">, </span><span class="s1">{</span><span class="s5">0</span><span class="s1">: </span><span class="s5">2</span><span class="s1">})</span>
    <span class="s0">assert </span><span class="s1">isinstance(fs[</span><span class="s5">1</span><span class="s1">][</span><span class="s5">1</span><span class="s1">]</span><span class="s0">, </span><span class="s1">int)</span>


<span class="s0">def </span><span class="s1">_pq_pyarrow(tmpdir):</span>
    <span class="s1">pytest.importorskip(</span><span class="s2">&quot;pyarrow.parquet&quot;</span><span class="s1">)</span>
    <span class="s1">pd = pytest.importorskip(</span><span class="s2">&quot;pandas&quot;</span><span class="s1">)</span>
    <span class="s1">dd = pytest.importorskip(</span><span class="s2">&quot;dask.dataframe&quot;</span><span class="s1">)</span>

    <span class="s0">try</span><span class="s1">:</span>
        <span class="s0">import </span><span class="s1">pyarrow.dataset </span><span class="s0">as </span><span class="s1">pa_ds</span>
    <span class="s0">except </span><span class="s1">ImportError:</span>
        <span class="s3"># PyArrow version too old for Dataset API</span>
        <span class="s1">pa_ds = </span><span class="s0">None</span>

    <span class="s1">dd.from_pandas(pd.DataFrame({</span><span class="s2">&quot;a&quot;</span><span class="s1">: range(</span><span class="s5">10</span><span class="s1">)})</span><span class="s0">, </span><span class="s1">npartitions=</span><span class="s5">2</span><span class="s1">).to_parquet(</span>
        <span class="s1">str(tmpdir)</span><span class="s0">,</span>
        <span class="s1">engine=</span><span class="s2">&quot;pyarrow&quot;</span><span class="s0">,</span>
    <span class="s1">)</span>
    <span class="s1">filters = [((</span><span class="s2">&quot;a&quot;</span><span class="s0">, </span><span class="s2">&quot;&lt;=&quot;</span><span class="s0">, </span><span class="s5">2</span><span class="s1">))]</span>

    <span class="s1">ddf1 = dd.read_parquet(str(tmpdir)</span><span class="s0">, </span><span class="s1">engine=</span><span class="s2">&quot;pyarrow&quot;</span><span class="s0">, </span><span class="s1">filters=filters)</span>
    <span class="s0">if </span><span class="s1">pa_ds:</span>
        <span class="s3"># Need to test that layer serialization succeeds</span>
        <span class="s3"># with &quot;pyarrow-dataset&quot; filtering</span>
        <span class="s1">ddf2 = dd.read_parquet(</span>
            <span class="s1">str(tmpdir)</span><span class="s0">,</span>
            <span class="s1">engine=</span><span class="s2">&quot;pyarrow-dataset&quot;</span><span class="s0">,</span>
            <span class="s1">filters=filters</span><span class="s0">,</span>
        <span class="s1">)</span>
        <span class="s0">return </span><span class="s1">(ddf1</span><span class="s0">, </span><span class="s1">ddf2)</span>
    <span class="s0">else</span><span class="s1">:</span>
        <span class="s0">return </span><span class="s1">ddf1</span>


<span class="s0">def </span><span class="s1">_pq_fastparquet(tmpdir):</span>
    <span class="s1">pytest.importorskip(</span><span class="s2">&quot;fastparquet&quot;</span><span class="s1">)</span>
    <span class="s1">pd = pytest.importorskip(</span><span class="s2">&quot;pandas&quot;</span><span class="s1">)</span>
    <span class="s1">dd = pytest.importorskip(</span><span class="s2">&quot;dask.dataframe&quot;</span><span class="s1">)</span>

    <span class="s1">dd.from_pandas(pd.DataFrame({</span><span class="s2">&quot;a&quot;</span><span class="s1">: range(</span><span class="s5">10</span><span class="s1">)})</span><span class="s0">, </span><span class="s1">npartitions=</span><span class="s5">2</span><span class="s1">).to_parquet(</span>
        <span class="s1">str(tmpdir)</span><span class="s0">,</span>
        <span class="s1">engine=</span><span class="s2">&quot;fastparquet&quot;</span><span class="s0">,</span>
    <span class="s1">)</span>
    <span class="s0">return </span><span class="s1">dd.read_parquet(str(tmpdir)</span><span class="s0">, </span><span class="s1">engine=</span><span class="s2">&quot;fastparquet&quot;</span><span class="s1">)</span>


<span class="s0">def </span><span class="s1">_read_csv(tmpdir):</span>
    <span class="s1">pd = pytest.importorskip(</span><span class="s2">&quot;pandas&quot;</span><span class="s1">)</span>
    <span class="s1">dd = pytest.importorskip(</span><span class="s2">&quot;dask.dataframe&quot;</span><span class="s1">)</span>

    <span class="s1">dd.from_pandas(</span>
        <span class="s1">pd.DataFrame({</span><span class="s2">&quot;a&quot;</span><span class="s1">: range(</span><span class="s5">10</span><span class="s1">)})</span><span class="s0">,</span>
        <span class="s1">npartitions=</span><span class="s5">2</span><span class="s0">,</span>
    <span class="s1">).to_csv(str(tmpdir))</span>
    <span class="s0">return </span><span class="s1">dd.read_csv(os.path.join(str(tmpdir)</span><span class="s0">, </span><span class="s2">&quot;*&quot;</span><span class="s1">))</span>


<span class="s1">@pytest.mark.xfail(reason=</span><span class="s2">&quot;#8480&quot;</span><span class="s1">)</span>
<span class="s1">@pytest.mark.parametrize(</span>
    <span class="s2">&quot;op,lib&quot;</span><span class="s0">,</span>
    <span class="s1">[</span>
        <span class="s1">(_dataframe_shuffle</span><span class="s0">, </span><span class="s2">&quot;pandas.&quot;</span><span class="s1">)</span><span class="s0">,</span>
        <span class="s1">(_dataframe_tree_reduction</span><span class="s0">, </span><span class="s2">&quot;pandas.&quot;</span><span class="s1">)</span><span class="s0">,</span>
        <span class="s1">(_dataframe_broadcast_join</span><span class="s0">, </span><span class="s2">&quot;pandas.&quot;</span><span class="s1">)</span><span class="s0">,</span>
        <span class="s1">(_pq_pyarrow</span><span class="s0">, </span><span class="s2">&quot;pandas.&quot;</span><span class="s1">)</span><span class="s0">,</span>
        <span class="s1">(_pq_fastparquet</span><span class="s0">, </span><span class="s2">&quot;pandas.&quot;</span><span class="s1">)</span><span class="s0">,</span>
        <span class="s1">(_read_csv</span><span class="s0">, </span><span class="s2">&quot;pandas.&quot;</span><span class="s1">)</span><span class="s0">,</span>
        <span class="s1">(_array_creation</span><span class="s0">, </span><span class="s2">&quot;numpy.&quot;</span><span class="s1">)</span><span class="s0">,</span>
        <span class="s1">(_array_map_overlap</span><span class="s0">, </span><span class="s2">&quot;numpy.&quot;</span><span class="s1">)</span><span class="s0">,</span>
    <span class="s1">]</span><span class="s0">,</span>
<span class="s1">)</span>
<span class="s1">@pytest.mark.parametrize(</span><span class="s2">&quot;optimize_graph&quot;</span><span class="s0">, </span><span class="s1">[</span><span class="s0">True, False</span><span class="s1">])</span>
<span class="s0">def </span><span class="s1">test_scheduler_highlevel_graph_unpack_import(op</span><span class="s0">, </span><span class="s1">lib</span><span class="s0">, </span><span class="s1">optimize_graph</span><span class="s0">, </span><span class="s1">loop</span><span class="s0">, </span><span class="s1">tmpdir):</span>
    <span class="s3"># Test that array/dataframe-specific modules are not imported</span>
    <span class="s3"># on the scheduler when an HLG layers are unpacked/materialized.</span>

    <span class="s0">with </span><span class="s1">cluster(scheduler_kwargs={</span><span class="s2">&quot;plugins&quot;</span><span class="s1">: [SchedulerImportCheck(lib)]}) </span><span class="s0">as </span><span class="s1">(</span>
        <span class="s1">scheduler</span><span class="s0">,</span>
        <span class="s1">workers</span><span class="s0">,</span>
    <span class="s1">):</span>
        <span class="s0">with </span><span class="s1">Client(scheduler[</span><span class="s2">&quot;address&quot;</span><span class="s1">]</span><span class="s0">, </span><span class="s1">loop=loop) </span><span class="s0">as </span><span class="s1">c:</span>
            <span class="s3"># Perform a computation using a HighLevelGraph Layer</span>
            <span class="s1">c.compute(op(tmpdir)</span><span class="s0">, </span><span class="s1">optimize_graph=optimize_graph)</span>

            <span class="s3"># Get the new modules which were imported on the scheduler during the computation</span>
            <span class="s1">end_modules = c.run_on_scheduler(</span><span class="s0">lambda</span><span class="s1">: set(sys.modules))</span>
            <span class="s1">start_modules = c.run_on_scheduler(</span>
                <span class="s0">lambda </span><span class="s1">dask_scheduler: dask_scheduler.plugins[</span>
                    <span class="s1">SchedulerImportCheck.name</span>
                <span class="s1">].start_modules</span>
            <span class="s1">)</span>
            <span class="s1">new_modules = end_modules - start_modules</span>

            <span class="s3"># Check that the scheduler didn't start with `lib`</span>
            <span class="s3"># (otherwise we arent testing anything)</span>
            <span class="s0">assert not </span><span class="s1">any(module.startswith(lib) </span><span class="s0">for </span><span class="s1">module </span><span class="s0">in </span><span class="s1">start_modules)</span>

            <span class="s3"># Check whether we imported `lib` on the scheduler</span>
            <span class="s0">assert not </span><span class="s1">any(module.startswith(lib) </span><span class="s0">for </span><span class="s1">module </span><span class="s0">in </span><span class="s1">new_modules)</span>


<span class="s0">def </span><span class="s1">_shuffle_op(ddf):</span>
    <span class="s0">return </span><span class="s1">ddf.shuffle(</span><span class="s2">&quot;x&quot;</span><span class="s0">, </span><span class="s1">shuffle=</span><span class="s2">&quot;tasks&quot;</span><span class="s1">)</span>


<span class="s0">def </span><span class="s1">_groupby_op(ddf):</span>
    <span class="s0">return </span><span class="s1">ddf.groupby(</span><span class="s2">&quot;name&quot;</span><span class="s1">).agg({</span><span class="s2">&quot;x&quot;</span><span class="s1">: </span><span class="s2">&quot;mean&quot;</span><span class="s1">})</span>


<span class="s1">@pytest.mark.parametrize(</span><span class="s2">&quot;op&quot;</span><span class="s0">, </span><span class="s1">[_shuffle_op</span><span class="s0">, </span><span class="s1">_groupby_op])</span>
<span class="s0">def </span><span class="s1">test_dataframe_cull_key_dependencies(op):</span>
    <span class="s3"># Test that HighLevelGraph.cull does not populate the</span>
    <span class="s3"># output graph with incorrect key_dependencies for</span>
    <span class="s3"># &quot;complex&quot; DataFrame Layers</span>
    <span class="s3"># See: https://github.com/dask/dask/pull/9267</span>

    <span class="s1">pytest.importorskip(</span><span class="s2">&quot;dask.dataframe&quot;</span><span class="s1">)</span>
    <span class="s1">datasets = pytest.importorskip(</span><span class="s2">&quot;dask.datasets&quot;</span><span class="s1">)</span>

    <span class="s1">result = op(datasets.timeseries(end=</span><span class="s2">&quot;2000-01-15&quot;</span><span class="s1">)).count()</span>
    <span class="s1">graph = result.dask</span>
    <span class="s1">culled_graph = graph.cull(result.__dask_keys__())</span>

    <span class="s0">assert </span><span class="s1">graph.get_all_dependencies() == culled_graph.get_all_dependencies()</span>


<span class="s0">def </span><span class="s1">test_dataframe_cull_key_dependencies_materialized():</span>
    <span class="s3"># Test that caching of MaterializedLayer</span>
    <span class="s3"># dependencies during culling doesn't break</span>
    <span class="s3"># the result of ``get_all_dependencies``</span>

    <span class="s1">datasets = pytest.importorskip(</span><span class="s2">&quot;dask.datasets&quot;</span><span class="s1">)</span>
    <span class="s1">dd = pytest.importorskip(</span><span class="s2">&quot;dask.dataframe&quot;</span><span class="s1">)</span>

    <span class="s1">ddf = datasets.timeseries(end=</span><span class="s2">&quot;2000-01-15&quot;</span><span class="s1">)</span>

    <span class="s3"># Build a custom layer to ensure</span>
    <span class="s3"># MaterializedLayer is used</span>
    <span class="s1">name = </span><span class="s2">&quot;custom_graph_test&quot;</span>
    <span class="s1">name_0 = </span><span class="s2">&quot;custom_graph_test_0&quot;</span>
    <span class="s1">dsk = {}</span>
    <span class="s0">for </span><span class="s1">i </span><span class="s0">in </span><span class="s1">range(ddf.npartitions):</span>
        <span class="s1">dsk[(name_0</span><span class="s0">, </span><span class="s1">i)] = (</span><span class="s0">lambda </span><span class="s1">x: x</span><span class="s0">, </span><span class="s1">(ddf._name</span><span class="s0">, </span><span class="s1">i))</span>
        <span class="s1">dsk[(name</span><span class="s0">, </span><span class="s1">i)] = (</span><span class="s0">lambda </span><span class="s1">x: x</span><span class="s0">, </span><span class="s1">(name_0</span><span class="s0">, </span><span class="s1">i))</span>
    <span class="s1">dsk = HighLevelGraph.from_collections(name</span><span class="s0">, </span><span class="s1">dsk</span><span class="s0">, </span><span class="s1">dependencies=[ddf])</span>
    <span class="s1">result = dd.core.new_dd_object(dsk</span><span class="s0">, </span><span class="s1">name</span><span class="s0">, </span><span class="s1">ddf._meta</span><span class="s0">, </span><span class="s1">ddf.divisions)</span>
    <span class="s1">graph = result.dask</span>

    <span class="s3"># HLG cull</span>
    <span class="s1">culled_keys = [k </span><span class="s0">for </span><span class="s1">k </span><span class="s0">in </span><span class="s1">result.__dask_keys__() </span><span class="s0">if </span><span class="s1">k != (name</span><span class="s0">, </span><span class="s5">0</span><span class="s1">)]</span>
    <span class="s1">culled_graph = graph.cull(culled_keys)</span>

    <span class="s3"># Check that culled_deps are cached</span>
    <span class="s3"># See: https://github.com/dask/dask/issues/9389</span>
    <span class="s1">cached_deps = culled_graph.key_dependencies.copy()</span>
    <span class="s1">deps = culled_graph.get_all_dependencies()</span>
    <span class="s0">assert </span><span class="s1">cached_deps == deps</span>

    <span class="s3"># Manual cull</span>
    <span class="s1">deps0 = graph.get_all_dependencies()</span>
    <span class="s0">for </span><span class="s1">name</span><span class="s0">, </span><span class="s1">i </span><span class="s0">in </span><span class="s1">list(deps0.keys()):</span>
        <span class="s0">if </span><span class="s1">i == </span><span class="s5">0</span><span class="s1">:</span>
            <span class="s1">deps0.pop((name</span><span class="s0">, </span><span class="s1">i))</span>

    <span class="s3"># Check that get_all_dependencies results match</span>
    <span class="s0">assert </span><span class="s1">deps0 == deps</span>
</pre>
</body>
</html>