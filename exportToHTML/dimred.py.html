<html>
<head>
<title>dimred.py</title>
<meta http-equiv="Content-Type" content="text/html; charset=utf-8">
<style type="text/css">
.s0 { color: #cc7832;}
.s1 { color: #a9b7c6;}
.s2 { color: #629755; font-style: italic;}
.s3 { color: #808080;}
.s4 { color: #6897bb;}
.s5 { color: #6a8759;}
</style>
</head>
<body bgcolor="#2b2b2b">
<table CELLSPACING=0 CELLPADDING=5 COLS=1 WIDTH="100%" BGCOLOR="#606060" >
<tr><td><center>
<font face="Arial, Helvetica" color="#000000">
dimred.py</font>
</center></td></tr></table>
<pre><span class="s0">import </span><span class="s1">warnings</span>

<span class="s0">import </span><span class="s1">numpy </span><span class="s0">as </span><span class="s1">np</span>
<span class="s0">import </span><span class="s1">pandas </span><span class="s0">as </span><span class="s1">pd</span>

<span class="s0">from </span><span class="s1">statsmodels.base </span><span class="s0">import </span><span class="s1">model</span>
<span class="s0">import </span><span class="s1">statsmodels.base.wrapper </span><span class="s0">as </span><span class="s1">wrap</span>
<span class="s0">from </span><span class="s1">statsmodels.tools.sm_exceptions </span><span class="s0">import </span><span class="s1">ConvergenceWarning</span>


<span class="s0">class </span><span class="s1">_DimReductionRegression(model.Model):</span>
    <span class="s2">&quot;&quot;&quot; 
    A base class for dimension reduction regression methods. 
    &quot;&quot;&quot;</span>

    <span class="s0">def </span><span class="s1">__init__(self</span><span class="s0">, </span><span class="s1">endog</span><span class="s0">, </span><span class="s1">exog</span><span class="s0">, </span><span class="s1">**kwargs):</span>
        <span class="s1">super(_DimReductionRegression</span><span class="s0">, </span><span class="s1">self).__init__(endog</span><span class="s0">, </span><span class="s1">exog</span><span class="s0">, </span><span class="s1">**kwargs)</span>

    <span class="s0">def </span><span class="s1">_prep(self</span><span class="s0">, </span><span class="s1">n_slice):</span>

        <span class="s3"># Sort the data by endog</span>
        <span class="s1">ii = np.argsort(self.endog)</span>
        <span class="s1">x = self.exog[ii</span><span class="s0">, </span><span class="s1">:]</span>

        <span class="s3"># Whiten the data</span>
        <span class="s1">x -= x.mean(</span><span class="s4">0</span><span class="s1">)</span>
        <span class="s1">covx = np.dot(x.T</span><span class="s0">, </span><span class="s1">x) / x.shape[</span><span class="s4">0</span><span class="s1">]</span>
        <span class="s1">covxr = np.linalg.cholesky(covx)</span>
        <span class="s1">x = np.linalg.solve(covxr</span><span class="s0">, </span><span class="s1">x.T).T</span>
        <span class="s1">self.wexog = x</span>
        <span class="s1">self._covxr = covxr</span>

        <span class="s3"># Split the data into slices</span>
        <span class="s1">self._split_wexog = np.array_split(x</span><span class="s0">, </span><span class="s1">n_slice)</span>


<span class="s0">class </span><span class="s1">SlicedInverseReg(_DimReductionRegression):</span>
    <span class="s2">&quot;&quot;&quot; 
    Sliced Inverse Regression (SIR) 
 
    Parameters 
    ---------- 
    endog : array_like (1d) 
        The dependent variable 
    exog : array_like (2d) 
        The covariates 
 
    References 
    ---------- 
    KC Li (1991).  Sliced inverse regression for dimension reduction. 
    JASA 86, 316-342. 
    &quot;&quot;&quot;</span>

    <span class="s0">def </span><span class="s1">fit(self</span><span class="s0">, </span><span class="s1">slice_n=</span><span class="s4">20</span><span class="s0">, </span><span class="s1">**kwargs):</span>
        <span class="s2">&quot;&quot;&quot; 
        Estimate the EDR space using Sliced Inverse Regression. 
 
        Parameters 
        ---------- 
        slice_n : int, optional 
            Target number of observations per slice 
        &quot;&quot;&quot;</span>

        <span class="s3"># Sample size per slice</span>
        <span class="s0">if </span><span class="s1">len(kwargs) &gt; </span><span class="s4">0</span><span class="s1">:</span>
            <span class="s1">msg = </span><span class="s5">&quot;SIR.fit does not take any extra keyword arguments&quot;</span>
            <span class="s1">warnings.warn(msg)</span>

        <span class="s3"># Number of slices</span>
        <span class="s1">n_slice = self.exog.shape[</span><span class="s4">0</span><span class="s1">] // slice_n</span>

        <span class="s1">self._prep(n_slice)</span>

        <span class="s1">mn = [z.mean(</span><span class="s4">0</span><span class="s1">) </span><span class="s0">for </span><span class="s1">z </span><span class="s0">in </span><span class="s1">self._split_wexog]</span>
        <span class="s1">n = [z.shape[</span><span class="s4">0</span><span class="s1">] </span><span class="s0">for </span><span class="s1">z </span><span class="s0">in </span><span class="s1">self._split_wexog]</span>
        <span class="s1">mn = np.asarray(mn)</span>
        <span class="s1">n = np.asarray(n)</span>

        <span class="s3"># Estimate Cov E[X | Y=y]</span>
        <span class="s1">mnc = np.dot(mn.T</span><span class="s0">, </span><span class="s1">n[:</span><span class="s0">, None</span><span class="s1">] * mn) / n.sum()</span>

        <span class="s1">a</span><span class="s0">, </span><span class="s1">b = np.linalg.eigh(mnc)</span>
        <span class="s1">jj = np.argsort(-a)</span>
        <span class="s1">a = a[jj]</span>
        <span class="s1">b = b[:</span><span class="s0">, </span><span class="s1">jj]</span>
        <span class="s1">params = np.linalg.solve(self._covxr.T</span><span class="s0">, </span><span class="s1">b)</span>

        <span class="s1">results = DimReductionResults(self</span><span class="s0">, </span><span class="s1">params</span><span class="s0">, </span><span class="s1">eigs=a)</span>
        <span class="s0">return </span><span class="s1">DimReductionResultsWrapper(results)</span>

    <span class="s0">def </span><span class="s1">_regularized_objective(self</span><span class="s0">, </span><span class="s1">A):</span>
        <span class="s3"># The objective function for regularized SIR</span>

        <span class="s1">p = self.k_vars</span>
        <span class="s1">covx = self._covx</span>
        <span class="s1">mn = self._slice_means</span>
        <span class="s1">ph = self._slice_props</span>
        <span class="s1">v = </span><span class="s4">0</span>
        <span class="s1">A = np.reshape(A</span><span class="s0">, </span><span class="s1">(p</span><span class="s0">, </span><span class="s1">self.ndim))</span>

        <span class="s3"># The penalty</span>
        <span class="s0">for </span><span class="s1">k </span><span class="s0">in </span><span class="s1">range(self.ndim):</span>
            <span class="s1">u = np.dot(self.pen_mat</span><span class="s0">, </span><span class="s1">A[:</span><span class="s0">, </span><span class="s1">k])</span>
            <span class="s1">v += np.sum(u * u)</span>

        <span class="s3"># The SIR objective function</span>
        <span class="s1">covxa = np.dot(covx</span><span class="s0">, </span><span class="s1">A)</span>
        <span class="s1">q</span><span class="s0">, </span><span class="s1">_ = np.linalg.qr(covxa)</span>
        <span class="s1">qd = np.dot(q</span><span class="s0">, </span><span class="s1">np.dot(q.T</span><span class="s0">, </span><span class="s1">mn.T))</span>
        <span class="s1">qu = mn.T - qd</span>
        <span class="s1">v += np.dot(ph</span><span class="s0">, </span><span class="s1">(qu * qu).sum(</span><span class="s4">0</span><span class="s1">))</span>

        <span class="s0">return </span><span class="s1">v</span>

    <span class="s0">def </span><span class="s1">_regularized_grad(self</span><span class="s0">, </span><span class="s1">A):</span>
        <span class="s3"># The gradient of the objective function for regularized SIR</span>

        <span class="s1">p = self.k_vars</span>
        <span class="s1">ndim = self.ndim</span>
        <span class="s1">covx = self._covx</span>
        <span class="s1">n_slice = self.n_slice</span>
        <span class="s1">mn = self._slice_means</span>
        <span class="s1">ph = self._slice_props</span>
        <span class="s1">A = A.reshape((p</span><span class="s0">, </span><span class="s1">ndim))</span>

        <span class="s3"># Penalty gradient</span>
        <span class="s1">gr = </span><span class="s4">2 </span><span class="s1">* np.dot(self.pen_mat.T</span><span class="s0">, </span><span class="s1">np.dot(self.pen_mat</span><span class="s0">, </span><span class="s1">A))</span>

        <span class="s1">A = A.reshape((p</span><span class="s0">, </span><span class="s1">ndim))</span>
        <span class="s1">covxa = np.dot(covx</span><span class="s0">, </span><span class="s1">A)</span>
        <span class="s1">covx2a = np.dot(covx</span><span class="s0">, </span><span class="s1">covxa)</span>
        <span class="s1">Q = np.dot(covxa.T</span><span class="s0">, </span><span class="s1">covxa)</span>
        <span class="s1">Qi = np.linalg.inv(Q)</span>
        <span class="s1">jm = np.zeros((p</span><span class="s0">, </span><span class="s1">ndim))</span>
        <span class="s1">qcv = np.linalg.solve(Q</span><span class="s0">, </span><span class="s1">covxa.T)</span>

        <span class="s1">ft = [</span><span class="s0">None</span><span class="s1">] * (p * ndim)</span>
        <span class="s0">for </span><span class="s1">q </span><span class="s0">in </span><span class="s1">range(p):</span>
            <span class="s0">for </span><span class="s1">r </span><span class="s0">in </span><span class="s1">range(ndim):</span>
                <span class="s1">jm *= </span><span class="s4">0</span>
                <span class="s1">jm[q</span><span class="s0">, </span><span class="s1">r] = </span><span class="s4">1</span>
                <span class="s1">umat = np.dot(covx2a.T</span><span class="s0">, </span><span class="s1">jm)</span>
                <span class="s1">umat += umat.T</span>
                <span class="s1">umat = -np.dot(Qi</span><span class="s0">, </span><span class="s1">np.dot(umat</span><span class="s0">, </span><span class="s1">Qi))</span>
                <span class="s1">fmat = np.dot(np.dot(covx</span><span class="s0">, </span><span class="s1">jm)</span><span class="s0">, </span><span class="s1">qcv)</span>
                <span class="s1">fmat += np.dot(covxa</span><span class="s0">, </span><span class="s1">np.dot(umat</span><span class="s0">, </span><span class="s1">covxa.T))</span>
                <span class="s1">fmat += np.dot(covxa</span><span class="s0">, </span><span class="s1">np.linalg.solve(Q</span><span class="s0">, </span><span class="s1">np.dot(jm.T</span><span class="s0">, </span><span class="s1">covx)))</span>
                <span class="s1">ft[q*ndim + r] = fmat</span>

        <span class="s1">ch = np.linalg.solve(Q</span><span class="s0">, </span><span class="s1">np.dot(covxa.T</span><span class="s0">, </span><span class="s1">mn.T))</span>
        <span class="s1">cu = mn - np.dot(covxa</span><span class="s0">, </span><span class="s1">ch).T</span>
        <span class="s0">for </span><span class="s1">i </span><span class="s0">in </span><span class="s1">range(n_slice):</span>
            <span class="s1">u = cu[i</span><span class="s0">, </span><span class="s1">:]</span>
            <span class="s1">v = mn[i</span><span class="s0">, </span><span class="s1">:]</span>
            <span class="s0">for </span><span class="s1">q </span><span class="s0">in </span><span class="s1">range(p):</span>
                <span class="s0">for </span><span class="s1">r </span><span class="s0">in </span><span class="s1">range(ndim):</span>
                    <span class="s1">f = np.dot(u</span><span class="s0">, </span><span class="s1">np.dot(ft[q*ndim + r]</span><span class="s0">, </span><span class="s1">v))</span>
                    <span class="s1">gr[q</span><span class="s0">, </span><span class="s1">r] -= </span><span class="s4">2 </span><span class="s1">* ph[i] * f</span>

        <span class="s0">return </span><span class="s1">gr.ravel()</span>

    <span class="s0">def </span><span class="s1">fit_regularized(self</span><span class="s0">, </span><span class="s1">ndim=</span><span class="s4">1</span><span class="s0">, </span><span class="s1">pen_mat=</span><span class="s0">None, </span><span class="s1">slice_n=</span><span class="s4">20</span><span class="s0">, </span><span class="s1">maxiter=</span><span class="s4">100</span><span class="s0">,</span>
                        <span class="s1">gtol=</span><span class="s4">1e-3</span><span class="s0">, </span><span class="s1">**kwargs):</span>
        <span class="s2">&quot;&quot;&quot; 
        Estimate the EDR space using regularized SIR. 
 
        Parameters 
        ---------- 
        ndim : int 
            The number of EDR directions to estimate 
        pen_mat : array_like 
            A 2d array such that the squared Frobenius norm of 
            `dot(pen_mat, dirs)`` is added to the objective function, 
            where `dirs` is an orthogonal array whose columns span 
            the estimated EDR space. 
        slice_n : int, optional 
            Target number of observations per slice 
        maxiter :int 
            The maximum number of iterations for estimating the EDR 
            space. 
        gtol : float 
            If the norm of the gradient of the objective function 
            falls below this value, the algorithm has converged. 
 
        Returns 
        ------- 
        A results class instance. 
 
        Notes 
        ----- 
        If each row of `exog` can be viewed as containing the values of a 
        function evaluated at equally-spaced locations, then setting the 
        rows of `pen_mat` to [[1, -2, 1, ...], [0, 1, -2, 1, ..], ...] 
        will give smooth EDR coefficients.  This is a form of &quot;functional 
        SIR&quot; using the squared second derivative as a penalty. 
 
        References 
        ---------- 
        L. Ferre, A.F. Yao (2003).  Functional sliced inverse regression 
        analysis.  Statistics: a journal of theoretical and applied 
        statistics 37(6) 475-488. 
        &quot;&quot;&quot;</span>

        <span class="s0">if </span><span class="s1">len(kwargs) &gt; </span><span class="s4">0</span><span class="s1">:</span>
            <span class="s1">msg = </span><span class="s5">&quot;SIR.fit_regularized does not take keyword arguments&quot;</span>
            <span class="s1">warnings.warn(msg)</span>

        <span class="s0">if </span><span class="s1">pen_mat </span><span class="s0">is None</span><span class="s1">:</span>
            <span class="s0">raise </span><span class="s1">ValueError(</span><span class="s5">&quot;pen_mat is a required argument&quot;</span><span class="s1">)</span>

        <span class="s1">start_params = kwargs.get(</span><span class="s5">&quot;start_params&quot;</span><span class="s0">, None</span><span class="s1">)</span>

        <span class="s3"># Sample size per slice</span>
        <span class="s1">slice_n = kwargs.get(</span><span class="s5">&quot;slice_n&quot;</span><span class="s0">, </span><span class="s4">20</span><span class="s1">)</span>

        <span class="s3"># Number of slices</span>
        <span class="s1">n_slice = self.exog.shape[</span><span class="s4">0</span><span class="s1">] // slice_n</span>

        <span class="s3"># Sort the data by endog</span>
        <span class="s1">ii = np.argsort(self.endog)</span>
        <span class="s1">x = self.exog[ii</span><span class="s0">, </span><span class="s1">:]</span>
        <span class="s1">x -= x.mean(</span><span class="s4">0</span><span class="s1">)</span>

        <span class="s1">covx = np.cov(x.T)</span>

        <span class="s3"># Split the data into slices</span>
        <span class="s1">split_exog = np.array_split(x</span><span class="s0">, </span><span class="s1">n_slice)</span>

        <span class="s1">mn = [z.mean(</span><span class="s4">0</span><span class="s1">) </span><span class="s0">for </span><span class="s1">z </span><span class="s0">in </span><span class="s1">split_exog]</span>
        <span class="s1">n = [z.shape[</span><span class="s4">0</span><span class="s1">] </span><span class="s0">for </span><span class="s1">z </span><span class="s0">in </span><span class="s1">split_exog]</span>
        <span class="s1">mn = np.asarray(mn)</span>
        <span class="s1">n = np.asarray(n)</span>
        <span class="s1">self._slice_props = n / n.sum()</span>
        <span class="s1">self.ndim = ndim</span>
        <span class="s1">self.k_vars = covx.shape[</span><span class="s4">0</span><span class="s1">]</span>
        <span class="s1">self.pen_mat = pen_mat</span>
        <span class="s1">self._covx = covx</span>
        <span class="s1">self.n_slice = n_slice</span>
        <span class="s1">self._slice_means = mn</span>

        <span class="s0">if </span><span class="s1">start_params </span><span class="s0">is None</span><span class="s1">:</span>
            <span class="s1">params = np.zeros((self.k_vars</span><span class="s0">, </span><span class="s1">ndim))</span>
            <span class="s1">params[</span><span class="s4">0</span><span class="s1">:ndim</span><span class="s0">, </span><span class="s4">0</span><span class="s1">:ndim] = np.eye(ndim)</span>
            <span class="s1">params = params</span>
        <span class="s0">else</span><span class="s1">:</span>
            <span class="s0">if </span><span class="s1">start_params.shape[</span><span class="s4">1</span><span class="s1">] != ndim:</span>
                <span class="s1">msg = </span><span class="s5">&quot;Shape of start_params is not compatible with ndim&quot;</span>
                <span class="s0">raise </span><span class="s1">ValueError(msg)</span>
            <span class="s1">params = start_params</span>

        <span class="s1">params</span><span class="s0">, </span><span class="s1">_</span><span class="s0">, </span><span class="s1">cnvrg = _grass_opt(params</span><span class="s0">, </span><span class="s1">self._regularized_objective</span><span class="s0">,</span>
                                      <span class="s1">self._regularized_grad</span><span class="s0">, </span><span class="s1">maxiter</span><span class="s0">, </span><span class="s1">gtol)</span>

        <span class="s0">if not </span><span class="s1">cnvrg:</span>
            <span class="s1">g = self._regularized_grad(params.ravel())</span>
            <span class="s1">gn = np.sqrt(np.dot(g</span><span class="s0">, </span><span class="s1">g))</span>
            <span class="s1">msg = </span><span class="s5">&quot;SIR.fit_regularized did not converge, |g|=%f&quot; </span><span class="s1">% gn</span>
            <span class="s1">warnings.warn(msg)</span>

        <span class="s1">results = DimReductionResults(self</span><span class="s0">, </span><span class="s1">params</span><span class="s0">, </span><span class="s1">eigs=</span><span class="s0">None</span><span class="s1">)</span>
        <span class="s0">return </span><span class="s1">DimReductionResultsWrapper(results)</span>


<span class="s0">class </span><span class="s1">PrincipalHessianDirections(_DimReductionRegression):</span>
    <span class="s2">&quot;&quot;&quot; 
    Principal Hessian Directions (PHD) 
 
    Parameters 
    ---------- 
    endog : array_like (1d) 
        The dependent variable 
    exog : array_like (2d) 
        The covariates 
 
    Returns 
    ------- 
    A model instance.  Call `fit` to obtain a results instance, 
    from which the estimated parameters can be obtained. 
 
    References 
    ---------- 
    KC Li (1992).  On Principal Hessian Directions for Data 
    Visualization and Dimension Reduction: Another application 
    of Stein's lemma. JASA 87:420. 
    &quot;&quot;&quot;</span>

    <span class="s0">def </span><span class="s1">fit(self</span><span class="s0">, </span><span class="s1">**kwargs):</span>
        <span class="s2">&quot;&quot;&quot; 
        Estimate the EDR space using PHD. 
 
        Parameters 
        ---------- 
        resid : bool, optional 
            If True, use least squares regression to remove the 
            linear relationship between each covariate and the 
            response, before conducting PHD. 
 
        Returns 
        ------- 
        A results instance which can be used to access the estimated 
        parameters. 
        &quot;&quot;&quot;</span>

        <span class="s1">resid = kwargs.get(</span><span class="s5">&quot;resid&quot;</span><span class="s0">, False</span><span class="s1">)</span>

        <span class="s1">y = self.endog - self.endog.mean()</span>
        <span class="s1">x = self.exog - self.exog.mean(</span><span class="s4">0</span><span class="s1">)</span>

        <span class="s0">if </span><span class="s1">resid:</span>
            <span class="s0">from </span><span class="s1">statsmodels.regression.linear_model </span><span class="s0">import </span><span class="s1">OLS</span>
            <span class="s1">r = OLS(y</span><span class="s0">, </span><span class="s1">x).fit()</span>
            <span class="s1">y = r.resid</span>

        <span class="s1">cm = np.einsum(</span><span class="s5">'i,ij,ik-&gt;jk'</span><span class="s0">, </span><span class="s1">y</span><span class="s0">, </span><span class="s1">x</span><span class="s0">, </span><span class="s1">x)</span>
        <span class="s1">cm /= len(y)</span>

        <span class="s1">cx = np.cov(x.T)</span>
        <span class="s1">cb = np.linalg.solve(cx</span><span class="s0">, </span><span class="s1">cm)</span>

        <span class="s1">a</span><span class="s0">, </span><span class="s1">b = np.linalg.eig(cb)</span>
        <span class="s1">jj = np.argsort(-np.abs(a))</span>
        <span class="s1">a = a[jj]</span>
        <span class="s1">params = b[:</span><span class="s0">, </span><span class="s1">jj]</span>

        <span class="s1">results = DimReductionResults(self</span><span class="s0">, </span><span class="s1">params</span><span class="s0">, </span><span class="s1">eigs=a)</span>
        <span class="s0">return </span><span class="s1">DimReductionResultsWrapper(results)</span>


<span class="s0">class </span><span class="s1">SlicedAverageVarianceEstimation(_DimReductionRegression):</span>
    <span class="s2">&quot;&quot;&quot; 
    Sliced Average Variance Estimation (SAVE) 
 
    Parameters 
    ---------- 
    endog : array_like (1d) 
        The dependent variable 
    exog : array_like (2d) 
        The covariates 
    bc : bool, optional 
        If True, use the bias-corrected CSAVE method of Li and Zhu. 
 
    References 
    ---------- 
    RD Cook.  SAVE: A method for dimension reduction and graphics 
    in regression. 
    http://www.stat.umn.edu/RegGraph/RecentDev/save.pdf 
 
    Y Li, L-X Zhu (2007). Asymptotics for sliced average 
    variance estimation.  The Annals of Statistics. 
    https://arxiv.org/pdf/0708.0462.pdf 
    &quot;&quot;&quot;</span>

    <span class="s0">def </span><span class="s1">__init__(self</span><span class="s0">, </span><span class="s1">endog</span><span class="s0">, </span><span class="s1">exog</span><span class="s0">, </span><span class="s1">**kwargs):</span>
        <span class="s1">super(SAVE</span><span class="s0">, </span><span class="s1">self).__init__(endog</span><span class="s0">, </span><span class="s1">exog</span><span class="s0">, </span><span class="s1">**kwargs)</span>

        <span class="s1">self.bc = </span><span class="s0">False</span>
        <span class="s0">if </span><span class="s5">&quot;bc&quot; </span><span class="s0">in </span><span class="s1">kwargs </span><span class="s0">and </span><span class="s1">kwargs[</span><span class="s5">&quot;bc&quot;</span><span class="s1">] </span><span class="s0">is True</span><span class="s1">:</span>
            <span class="s1">self.bc = </span><span class="s0">True</span>

    <span class="s0">def </span><span class="s1">fit(self</span><span class="s0">, </span><span class="s1">**kwargs):</span>
        <span class="s2">&quot;&quot;&quot; 
        Estimate the EDR space. 
 
        Parameters 
        ---------- 
        slice_n : int 
            Number of observations per slice 
        &quot;&quot;&quot;</span>

        <span class="s3"># Sample size per slice</span>
        <span class="s1">slice_n = kwargs.get(</span><span class="s5">&quot;slice_n&quot;</span><span class="s0">, </span><span class="s4">50</span><span class="s1">)</span>

        <span class="s3"># Number of slices</span>
        <span class="s1">n_slice = self.exog.shape[</span><span class="s4">0</span><span class="s1">] // slice_n</span>

        <span class="s1">self._prep(n_slice)</span>

        <span class="s1">cv = [np.cov(z.T) </span><span class="s0">for </span><span class="s1">z </span><span class="s0">in </span><span class="s1">self._split_wexog]</span>
        <span class="s1">ns = [z.shape[</span><span class="s4">0</span><span class="s1">] </span><span class="s0">for </span><span class="s1">z </span><span class="s0">in </span><span class="s1">self._split_wexog]</span>

        <span class="s1">p = self.wexog.shape[</span><span class="s4">1</span><span class="s1">]</span>

        <span class="s0">if not </span><span class="s1">self.bc:</span>
            <span class="s3"># Cook's original approach</span>
            <span class="s1">vm = </span><span class="s4">0</span>
            <span class="s0">for </span><span class="s1">w</span><span class="s0">, </span><span class="s1">cvx </span><span class="s0">in </span><span class="s1">zip(ns</span><span class="s0">, </span><span class="s1">cv):</span>
                <span class="s1">icv = np.eye(p) - cvx</span>
                <span class="s1">vm += w * np.dot(icv</span><span class="s0">, </span><span class="s1">icv)</span>
            <span class="s1">vm /= len(cv)</span>
        <span class="s0">else</span><span class="s1">:</span>
            <span class="s3"># The bias-corrected approach of Li and Zhu</span>

            <span class="s3"># \Lambda_n in Li, Zhu</span>
            <span class="s1">av = </span><span class="s4">0</span>
            <span class="s0">for </span><span class="s1">c </span><span class="s0">in </span><span class="s1">cv:</span>
                <span class="s1">av += np.dot(c</span><span class="s0">, </span><span class="s1">c)</span>
            <span class="s1">av /= len(cv)</span>

            <span class="s3"># V_n in Li, Zhu</span>
            <span class="s1">vn = </span><span class="s4">0</span>
            <span class="s0">for </span><span class="s1">x </span><span class="s0">in </span><span class="s1">self._split_wexog:</span>
                <span class="s1">r = x - x.mean(</span><span class="s4">0</span><span class="s1">)</span>
                <span class="s0">for </span><span class="s1">i </span><span class="s0">in </span><span class="s1">range(r.shape[</span><span class="s4">0</span><span class="s1">]):</span>
                    <span class="s1">u = r[i</span><span class="s0">, </span><span class="s1">:]</span>
                    <span class="s1">m = np.outer(u</span><span class="s0">, </span><span class="s1">u)</span>
                    <span class="s1">vn += np.dot(m</span><span class="s0">, </span><span class="s1">m)</span>
            <span class="s1">vn /= self.exog.shape[</span><span class="s4">0</span><span class="s1">]</span>

            <span class="s1">c = np.mean(ns)</span>
            <span class="s1">k1 = c * (c - </span><span class="s4">1</span><span class="s1">) / ((c - </span><span class="s4">1</span><span class="s1">)**</span><span class="s4">2 </span><span class="s1">+ </span><span class="s4">1</span><span class="s1">)</span>
            <span class="s1">k2 = (c - </span><span class="s4">1</span><span class="s1">) / ((c - </span><span class="s4">1</span><span class="s1">)**</span><span class="s4">2 </span><span class="s1">+ </span><span class="s4">1</span><span class="s1">)</span>
            <span class="s1">av2 = k1 * av - k2 * vn</span>

            <span class="s1">vm = np.eye(p) - </span><span class="s4">2 </span><span class="s1">* sum(cv) / len(cv) + av2</span>

        <span class="s1">a</span><span class="s0">, </span><span class="s1">b = np.linalg.eigh(vm)</span>
        <span class="s1">jj = np.argsort(-a)</span>
        <span class="s1">a = a[jj]</span>
        <span class="s1">b = b[:</span><span class="s0">, </span><span class="s1">jj]</span>
        <span class="s1">params = np.linalg.solve(self._covxr.T</span><span class="s0">, </span><span class="s1">b)</span>

        <span class="s1">results = DimReductionResults(self</span><span class="s0">, </span><span class="s1">params</span><span class="s0">, </span><span class="s1">eigs=a)</span>
        <span class="s0">return </span><span class="s1">DimReductionResultsWrapper(results)</span>


<span class="s0">class </span><span class="s1">DimReductionResults(model.Results):</span>
    <span class="s2">&quot;&quot;&quot; 
    Results class for a dimension reduction regression. 
 
    Notes 
    ----- 
    The `params` attribute is a matrix whose columns span 
    the effective dimension reduction (EDR) space.  Some 
    methods produce a corresponding set of eigenvalues 
    (`eigs`) that indicate how much information is contained 
    in each basis direction. 
    &quot;&quot;&quot;</span>

    <span class="s0">def </span><span class="s1">__init__(self</span><span class="s0">, </span><span class="s1">model</span><span class="s0">, </span><span class="s1">params</span><span class="s0">, </span><span class="s1">eigs):</span>
        <span class="s1">super(DimReductionResults</span><span class="s0">, </span><span class="s1">self).__init__(</span>
              <span class="s1">model</span><span class="s0">, </span><span class="s1">params)</span>
        <span class="s1">self.eigs = eigs</span>


<span class="s0">class </span><span class="s1">DimReductionResultsWrapper(wrap.ResultsWrapper):</span>
    <span class="s1">_attrs = {</span>
        <span class="s5">'params'</span><span class="s1">: </span><span class="s5">'columns'</span><span class="s0">,</span>
    <span class="s1">}</span>
    <span class="s1">_wrap_attrs = _attrs</span>

<span class="s1">wrap.populate_wrapper(DimReductionResultsWrapper</span><span class="s0">,  </span><span class="s3"># noqa:E305</span>
                      <span class="s1">DimReductionResults)</span>


<span class="s0">def </span><span class="s1">_grass_opt(params</span><span class="s0">, </span><span class="s1">fun</span><span class="s0">, </span><span class="s1">grad</span><span class="s0">, </span><span class="s1">maxiter</span><span class="s0">, </span><span class="s1">gtol):</span>
    <span class="s2">&quot;&quot;&quot; 
    Minimize a function on a Grassmann manifold. 
 
    Parameters 
    ---------- 
    params : array_like 
        Starting value for the optimization. 
    fun : function 
        The function to be minimized. 
    grad : function 
        The gradient of fun. 
    maxiter : int 
        The maximum number of iterations. 
    gtol : float 
        Convergence occurs when the gradient norm falls below this value. 
 
    Returns 
    ------- 
    params : array_like 
        The minimizing value for the objective function. 
    fval : float 
        The smallest achieved value of the objective function. 
    cnvrg : bool 
        True if the algorithm converged to a limit point. 
 
    Notes 
    ----- 
    `params` is 2-d, but `fun` and `grad` should take 1-d arrays 
    `params.ravel()` as arguments. 
 
    Reference 
    --------- 
    A Edelman, TA Arias, ST Smith (1998).  The geometry of algorithms with 
    orthogonality constraints. SIAM J Matrix Anal Appl. 
    http://math.mit.edu/~edelman/publications/geometry_of_algorithms.pdf 
    &quot;&quot;&quot;</span>

    <span class="s1">p</span><span class="s0">, </span><span class="s1">d = params.shape</span>
    <span class="s1">params = params.ravel()</span>

    <span class="s1">f0 = fun(params)</span>
    <span class="s1">cnvrg = </span><span class="s0">False</span>

    <span class="s0">for </span><span class="s1">_ </span><span class="s0">in </span><span class="s1">range(maxiter):</span>

        <span class="s3"># Project the gradient to the tangent space</span>
        <span class="s1">g = grad(params)</span>
        <span class="s1">g -= np.dot(g</span><span class="s0">, </span><span class="s1">params) * params / np.dot(params</span><span class="s0">, </span><span class="s1">params)</span>

        <span class="s0">if </span><span class="s1">np.sqrt(np.sum(g * g)) &lt; gtol:</span>
            <span class="s1">cnvrg = </span><span class="s0">True</span>
            <span class="s0">break</span>

        <span class="s1">gm = g.reshape((p</span><span class="s0">, </span><span class="s1">d))</span>
        <span class="s1">u</span><span class="s0">, </span><span class="s1">s</span><span class="s0">, </span><span class="s1">vt = np.linalg.svd(gm</span><span class="s0">, </span><span class="s4">0</span><span class="s1">)</span>

        <span class="s1">paramsm = params.reshape((p</span><span class="s0">, </span><span class="s1">d))</span>
        <span class="s1">pa0 = np.dot(paramsm</span><span class="s0">, </span><span class="s1">vt.T)</span>

        <span class="s0">def </span><span class="s1">geo(t):</span>
            <span class="s3"># Parameterize the geodesic path in the direction</span>
            <span class="s3"># of the gradient as a function of a real value t.</span>
            <span class="s1">pa = pa0 * np.cos(s * t) + u * np.sin(s * t)</span>
            <span class="s0">return </span><span class="s1">np.dot(pa</span><span class="s0">, </span><span class="s1">vt).ravel()</span>

        <span class="s3"># Try to find a downhill step along the geodesic path.</span>
        <span class="s1">step = </span><span class="s4">2.</span>
        <span class="s0">while </span><span class="s1">step &gt; </span><span class="s4">1e-10</span><span class="s1">:</span>
            <span class="s1">pa = geo(-step)</span>
            <span class="s1">f1 = fun(pa)</span>
            <span class="s0">if </span><span class="s1">f1 &lt; f0:</span>
                <span class="s1">params = pa</span>
                <span class="s1">f0 = f1</span>
                <span class="s0">break</span>
            <span class="s1">step /= </span><span class="s4">2</span>

    <span class="s1">params = params.reshape((p</span><span class="s0">, </span><span class="s1">d))</span>
    <span class="s0">return </span><span class="s1">params</span><span class="s0">, </span><span class="s1">f0</span><span class="s0">, </span><span class="s1">cnvrg</span>


<span class="s0">class </span><span class="s1">CovarianceReduction(_DimReductionRegression):</span>
    <span class="s2">&quot;&quot;&quot; 
    Dimension reduction for covariance matrices (CORE). 
 
    Parameters 
    ---------- 
    endog : array_like 
        The dependent variable, treated as group labels 
    exog : array_like 
        The independent variables. 
    dim : int 
        The dimension of the subspace onto which the covariance 
        matrices are projected. 
 
    Returns 
    ------- 
    A model instance.  Call `fit` on the model instance to obtain 
    a results instance, which contains the fitted model parameters. 
 
    Notes 
    ----- 
    This is a likelihood-based dimension reduction procedure based 
    on Wishart models for sample covariance matrices.  The goal 
    is to find a projection matrix P so that C_i | P'C_iP and 
    C_j | P'C_jP are equal in distribution for all i, j, where 
    the C_i are the within-group covariance matrices. 
 
    The model and methodology are as described in Cook and Forzani. 
    The optimization method follows Edelman et. al. 
 
    References 
    ---------- 
    DR Cook, L Forzani (2008).  Covariance reducing models: an alternative 
    to spectral modeling of covariance matrices.  Biometrika 95:4. 
 
    A Edelman, TA Arias, ST Smith (1998).  The geometry of algorithms with 
    orthogonality constraints. SIAM J Matrix Anal Appl. 
    http://math.mit.edu/~edelman/publications/geometry_of_algorithms.pdf 
    &quot;&quot;&quot;</span>

    <span class="s0">def </span><span class="s1">__init__(self</span><span class="s0">, </span><span class="s1">endog</span><span class="s0">, </span><span class="s1">exog</span><span class="s0">, </span><span class="s1">dim):</span>

        <span class="s1">super(CovarianceReduction</span><span class="s0">, </span><span class="s1">self).__init__(endog</span><span class="s0">, </span><span class="s1">exog)</span>

        <span class="s1">covs</span><span class="s0">, </span><span class="s1">ns = []</span><span class="s0">, </span><span class="s1">[]</span>
        <span class="s1">df = pd.DataFrame(self.exog</span><span class="s0">, </span><span class="s1">index=self.endog)</span>
        <span class="s0">for </span><span class="s1">_</span><span class="s0">, </span><span class="s1">v </span><span class="s0">in </span><span class="s1">df.groupby(df.index):</span>
            <span class="s1">covs.append(v.cov().values)</span>
            <span class="s1">ns.append(v.shape[</span><span class="s4">0</span><span class="s1">])</span>

        <span class="s1">self.nobs = len(endog)</span>

        <span class="s3"># The marginal covariance</span>
        <span class="s1">covm = </span><span class="s4">0</span>
        <span class="s0">for </span><span class="s1">i</span><span class="s0">, </span><span class="s1">_ </span><span class="s0">in </span><span class="s1">enumerate(covs):</span>
            <span class="s1">covm += covs[i] * ns[i]</span>
        <span class="s1">covm /= self.nobs</span>
        <span class="s1">self.covm = covm</span>

        <span class="s1">self.covs = covs</span>
        <span class="s1">self.ns = ns</span>
        <span class="s1">self.dim = dim</span>

    <span class="s0">def </span><span class="s1">loglike(self</span><span class="s0">, </span><span class="s1">params):</span>
        <span class="s2">&quot;&quot;&quot; 
        Evaluate the log-likelihood 
 
        Parameters 
        ---------- 
        params : array_like 
            The projection matrix used to reduce the covariances, flattened 
            to 1d. 
 
        Returns the log-likelihood. 
        &quot;&quot;&quot;</span>

        <span class="s1">p = self.covm.shape[</span><span class="s4">0</span><span class="s1">]</span>
        <span class="s1">proj = params.reshape((p</span><span class="s0">, </span><span class="s1">self.dim))</span>

        <span class="s1">c = np.dot(proj.T</span><span class="s0">, </span><span class="s1">np.dot(self.covm</span><span class="s0">, </span><span class="s1">proj))</span>
        <span class="s1">_</span><span class="s0">, </span><span class="s1">ldet = np.linalg.slogdet(c)</span>
        <span class="s1">f = self.nobs * ldet / </span><span class="s4">2</span>

        <span class="s0">for </span><span class="s1">j</span><span class="s0">, </span><span class="s1">c </span><span class="s0">in </span><span class="s1">enumerate(self.covs):</span>
            <span class="s1">c = np.dot(proj.T</span><span class="s0">, </span><span class="s1">np.dot(c</span><span class="s0">, </span><span class="s1">proj))</span>
            <span class="s1">_</span><span class="s0">, </span><span class="s1">ldet = np.linalg.slogdet(c)</span>
            <span class="s1">f -= self.ns[j] * ldet / </span><span class="s4">2</span>

        <span class="s0">return </span><span class="s1">f</span>

    <span class="s0">def </span><span class="s1">score(self</span><span class="s0">, </span><span class="s1">params):</span>
        <span class="s2">&quot;&quot;&quot; 
        Evaluate the score function. 
 
        Parameters 
        ---------- 
        params : array_like 
            The projection matrix used to reduce the covariances, 
            flattened to 1d. 
 
        Returns the score function evaluated at 'params'. 
        &quot;&quot;&quot;</span>

        <span class="s1">p = self.covm.shape[</span><span class="s4">0</span><span class="s1">]</span>
        <span class="s1">proj = params.reshape((p</span><span class="s0">, </span><span class="s1">self.dim))</span>

        <span class="s1">c0 = np.dot(proj.T</span><span class="s0">, </span><span class="s1">np.dot(self.covm</span><span class="s0">, </span><span class="s1">proj))</span>
        <span class="s1">cP = np.dot(self.covm</span><span class="s0">, </span><span class="s1">proj)</span>
        <span class="s1">g = self.nobs * np.linalg.solve(c0</span><span class="s0">, </span><span class="s1">cP.T).T</span>

        <span class="s0">for </span><span class="s1">j</span><span class="s0">, </span><span class="s1">c </span><span class="s0">in </span><span class="s1">enumerate(self.covs):</span>
            <span class="s1">c0 = np.dot(proj.T</span><span class="s0">, </span><span class="s1">np.dot(c</span><span class="s0">, </span><span class="s1">proj))</span>
            <span class="s1">cP = np.dot(c</span><span class="s0">, </span><span class="s1">proj)</span>
            <span class="s1">g -= self.ns[j] * np.linalg.solve(c0</span><span class="s0">, </span><span class="s1">cP.T).T</span>

        <span class="s0">return </span><span class="s1">g.ravel()</span>

    <span class="s0">def </span><span class="s1">fit(self</span><span class="s0">, </span><span class="s1">start_params=</span><span class="s0">None, </span><span class="s1">maxiter=</span><span class="s4">200</span><span class="s0">, </span><span class="s1">gtol=</span><span class="s4">1e-4</span><span class="s1">):</span>
        <span class="s2">&quot;&quot;&quot; 
        Fit the covariance reduction model. 
 
        Parameters 
        ---------- 
        start_params : array_like 
            Starting value for the projection matrix. May be 
            rectangular, or flattened. 
        maxiter : int 
            The maximum number of gradient steps to take. 
        gtol : float 
            Convergence criterion for the gradient norm. 
 
        Returns 
        ------- 
        A results instance that can be used to access the 
        fitted parameters. 
        &quot;&quot;&quot;</span>

        <span class="s1">p = self.covm.shape[</span><span class="s4">0</span><span class="s1">]</span>
        <span class="s1">d = self.dim</span>

        <span class="s3"># Starting value for params</span>
        <span class="s0">if </span><span class="s1">start_params </span><span class="s0">is None</span><span class="s1">:</span>
            <span class="s1">params = np.zeros((p</span><span class="s0">, </span><span class="s1">d))</span>
            <span class="s1">params[</span><span class="s4">0</span><span class="s1">:d</span><span class="s0">, </span><span class="s4">0</span><span class="s1">:d] = np.eye(d)</span>
            <span class="s1">params = params</span>
        <span class="s0">else</span><span class="s1">:</span>
            <span class="s1">params = start_params</span>

        <span class="s3"># _grass_opt is designed for minimization, we are doing maximization</span>
        <span class="s3"># here so everything needs to be flipped.</span>
        <span class="s1">params</span><span class="s0">, </span><span class="s1">llf</span><span class="s0">, </span><span class="s1">cnvrg = _grass_opt(params</span><span class="s0">, lambda </span><span class="s1">x: -self.loglike(x)</span><span class="s0">,</span>
                                        <span class="s0">lambda </span><span class="s1">x: -self.score(x)</span><span class="s0">, </span><span class="s1">maxiter</span><span class="s0">,</span>
                                        <span class="s1">gtol)</span>
        <span class="s1">llf *= -</span><span class="s4">1</span>
        <span class="s0">if not </span><span class="s1">cnvrg:</span>
            <span class="s1">g = self.score(params.ravel())</span>
            <span class="s1">gn = np.sqrt(np.sum(g * g))</span>
            <span class="s1">msg = </span><span class="s5">&quot;CovReduce optimization did not converge, |g|=%f&quot; </span><span class="s1">% gn</span>
            <span class="s1">warnings.warn(msg</span><span class="s0">, </span><span class="s1">ConvergenceWarning)</span>

        <span class="s1">results = DimReductionResults(self</span><span class="s0">, </span><span class="s1">params</span><span class="s0">, </span><span class="s1">eigs=</span><span class="s0">None</span><span class="s1">)</span>
        <span class="s1">results.llf = llf</span>
        <span class="s0">return </span><span class="s1">DimReductionResultsWrapper(results)</span>


<span class="s3"># aliases for expert users</span>
<span class="s1">SIR = SlicedInverseReg</span>
<span class="s1">PHD = PrincipalHessianDirections</span>
<span class="s1">SAVE = SlicedAverageVarianceEstimation</span>
<span class="s1">CORE = CovarianceReduction</span>
</pre>
</body>
</html>