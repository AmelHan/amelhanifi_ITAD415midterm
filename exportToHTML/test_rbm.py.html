<html>
<head>
<title>test_rbm.py</title>
<meta http-equiv="Content-Type" content="text/html; charset=utf-8">
<style type="text/css">
.s0 { color: #cc7832;}
.s1 { color: #a9b7c6;}
.s2 { color: #6897bb;}
.s3 { color: #808080;}
.s4 { color: #6a8759;}
.s5 { color: #629755; font-style: italic;}
</style>
</head>
<body bgcolor="#2b2b2b">
<table CELLSPACING=0 CELLPADDING=5 COLS=1 WIDTH="100%" BGCOLOR="#606060" >
<tr><td><center>
<font face="Arial, Helvetica" color="#000000">
test_rbm.py</font>
</center></td></tr></table>
<pre><span class="s0">import </span><span class="s1">re</span>
<span class="s0">import </span><span class="s1">sys</span>
<span class="s0">from </span><span class="s1">io </span><span class="s0">import </span><span class="s1">StringIO</span>

<span class="s0">import </span><span class="s1">numpy </span><span class="s0">as </span><span class="s1">np</span>
<span class="s0">import </span><span class="s1">pytest</span>
<span class="s0">from </span><span class="s1">scipy.sparse </span><span class="s0">import </span><span class="s1">csc_matrix</span><span class="s0">, </span><span class="s1">csr_matrix</span><span class="s0">, </span><span class="s1">lil_matrix</span>

<span class="s0">from </span><span class="s1">sklearn.datasets </span><span class="s0">import </span><span class="s1">load_digits</span>
<span class="s0">from </span><span class="s1">sklearn.neural_network </span><span class="s0">import </span><span class="s1">BernoulliRBM</span>
<span class="s0">from </span><span class="s1">sklearn.utils._testing </span><span class="s0">import </span><span class="s1">(</span>
    <span class="s1">assert_allclose</span><span class="s0">,</span>
    <span class="s1">assert_almost_equal</span><span class="s0">,</span>
    <span class="s1">assert_array_equal</span><span class="s0">,</span>
<span class="s1">)</span>
<span class="s0">from </span><span class="s1">sklearn.utils.validation </span><span class="s0">import </span><span class="s1">assert_all_finite</span>

<span class="s1">Xdigits</span><span class="s0">, </span><span class="s1">_ = load_digits(return_X_y=</span><span class="s0">True</span><span class="s1">)</span>
<span class="s1">Xdigits -= Xdigits.min()</span>
<span class="s1">Xdigits /= Xdigits.max()</span>


<span class="s0">def </span><span class="s1">test_fit():</span>
    <span class="s1">X = Xdigits.copy()</span>

    <span class="s1">rbm = BernoulliRBM(</span>
        <span class="s1">n_components=</span><span class="s2">64</span><span class="s0">, </span><span class="s1">learning_rate=</span><span class="s2">0.1</span><span class="s0">, </span><span class="s1">batch_size=</span><span class="s2">10</span><span class="s0">, </span><span class="s1">n_iter=</span><span class="s2">7</span><span class="s0">, </span><span class="s1">random_state=</span><span class="s2">9</span>
    <span class="s1">)</span>
    <span class="s1">rbm.fit(X)</span>

    <span class="s1">assert_almost_equal(rbm.score_samples(X).mean()</span><span class="s0">, </span><span class="s1">-</span><span class="s2">21.0</span><span class="s0">, </span><span class="s1">decimal=</span><span class="s2">0</span><span class="s1">)</span>

    <span class="s3"># in-place tricks shouldn't have modified X</span>
    <span class="s1">assert_array_equal(X</span><span class="s0">, </span><span class="s1">Xdigits)</span>


<span class="s0">def </span><span class="s1">test_partial_fit():</span>
    <span class="s1">X = Xdigits.copy()</span>
    <span class="s1">rbm = BernoulliRBM(</span>
        <span class="s1">n_components=</span><span class="s2">64</span><span class="s0">, </span><span class="s1">learning_rate=</span><span class="s2">0.1</span><span class="s0">, </span><span class="s1">batch_size=</span><span class="s2">20</span><span class="s0">, </span><span class="s1">random_state=</span><span class="s2">9</span>
    <span class="s1">)</span>
    <span class="s1">n_samples = X.shape[</span><span class="s2">0</span><span class="s1">]</span>
    <span class="s1">n_batches = int(np.ceil(float(n_samples) / rbm.batch_size))</span>
    <span class="s1">batch_slices = np.array_split(X</span><span class="s0">, </span><span class="s1">n_batches)</span>

    <span class="s0">for </span><span class="s1">i </span><span class="s0">in </span><span class="s1">range(</span><span class="s2">7</span><span class="s1">):</span>
        <span class="s0">for </span><span class="s1">batch </span><span class="s0">in </span><span class="s1">batch_slices:</span>
            <span class="s1">rbm.partial_fit(batch)</span>

    <span class="s1">assert_almost_equal(rbm.score_samples(X).mean()</span><span class="s0">, </span><span class="s1">-</span><span class="s2">21.0</span><span class="s0">, </span><span class="s1">decimal=</span><span class="s2">0</span><span class="s1">)</span>
    <span class="s1">assert_array_equal(X</span><span class="s0">, </span><span class="s1">Xdigits)</span>


<span class="s0">def </span><span class="s1">test_transform():</span>
    <span class="s1">X = Xdigits[:</span><span class="s2">100</span><span class="s1">]</span>
    <span class="s1">rbm1 = BernoulliRBM(n_components=</span><span class="s2">16</span><span class="s0">, </span><span class="s1">batch_size=</span><span class="s2">5</span><span class="s0">, </span><span class="s1">n_iter=</span><span class="s2">5</span><span class="s0">, </span><span class="s1">random_state=</span><span class="s2">42</span><span class="s1">)</span>
    <span class="s1">rbm1.fit(X)</span>

    <span class="s1">Xt1 = rbm1.transform(X)</span>
    <span class="s1">Xt2 = rbm1._mean_hiddens(X)</span>

    <span class="s1">assert_array_equal(Xt1</span><span class="s0">, </span><span class="s1">Xt2)</span>


<span class="s0">def </span><span class="s1">test_small_sparse():</span>
    <span class="s3"># BernoulliRBM should work on small sparse matrices.</span>
    <span class="s1">X = csr_matrix(Xdigits[:</span><span class="s2">4</span><span class="s1">])</span>
    <span class="s1">BernoulliRBM().fit(X)  </span><span class="s3"># no exception</span>


<span class="s0">def </span><span class="s1">test_small_sparse_partial_fit():</span>
    <span class="s0">for </span><span class="s1">sparse </span><span class="s0">in </span><span class="s1">[csc_matrix</span><span class="s0">, </span><span class="s1">csr_matrix]:</span>
        <span class="s1">X_sparse = sparse(Xdigits[:</span><span class="s2">100</span><span class="s1">])</span>
        <span class="s1">X = Xdigits[:</span><span class="s2">100</span><span class="s1">].copy()</span>

        <span class="s1">rbm1 = BernoulliRBM(</span>
            <span class="s1">n_components=</span><span class="s2">64</span><span class="s0">, </span><span class="s1">learning_rate=</span><span class="s2">0.1</span><span class="s0">, </span><span class="s1">batch_size=</span><span class="s2">10</span><span class="s0">, </span><span class="s1">random_state=</span><span class="s2">9</span>
        <span class="s1">)</span>
        <span class="s1">rbm2 = BernoulliRBM(</span>
            <span class="s1">n_components=</span><span class="s2">64</span><span class="s0">, </span><span class="s1">learning_rate=</span><span class="s2">0.1</span><span class="s0">, </span><span class="s1">batch_size=</span><span class="s2">10</span><span class="s0">, </span><span class="s1">random_state=</span><span class="s2">9</span>
        <span class="s1">)</span>

        <span class="s1">rbm1.partial_fit(X_sparse)</span>
        <span class="s1">rbm2.partial_fit(X)</span>

        <span class="s1">assert_almost_equal(</span>
            <span class="s1">rbm1.score_samples(X).mean()</span><span class="s0">, </span><span class="s1">rbm2.score_samples(X).mean()</span><span class="s0">, </span><span class="s1">decimal=</span><span class="s2">0</span>
        <span class="s1">)</span>


<span class="s0">def </span><span class="s1">test_sample_hiddens():</span>
    <span class="s1">rng = np.random.RandomState(</span><span class="s2">0</span><span class="s1">)</span>
    <span class="s1">X = Xdigits[:</span><span class="s2">100</span><span class="s1">]</span>
    <span class="s1">rbm1 = BernoulliRBM(n_components=</span><span class="s2">2</span><span class="s0">, </span><span class="s1">batch_size=</span><span class="s2">5</span><span class="s0">, </span><span class="s1">n_iter=</span><span class="s2">5</span><span class="s0">, </span><span class="s1">random_state=</span><span class="s2">42</span><span class="s1">)</span>
    <span class="s1">rbm1.fit(X)</span>

    <span class="s1">h = rbm1._mean_hiddens(X[</span><span class="s2">0</span><span class="s1">])</span>
    <span class="s1">hs = np.mean([rbm1._sample_hiddens(X[</span><span class="s2">0</span><span class="s1">]</span><span class="s0">, </span><span class="s1">rng) </span><span class="s0">for </span><span class="s1">i </span><span class="s0">in </span><span class="s1">range(</span><span class="s2">100</span><span class="s1">)]</span><span class="s0">, </span><span class="s2">0</span><span class="s1">)</span>

    <span class="s1">assert_almost_equal(h</span><span class="s0">, </span><span class="s1">hs</span><span class="s0">, </span><span class="s1">decimal=</span><span class="s2">1</span><span class="s1">)</span>


<span class="s0">def </span><span class="s1">test_fit_gibbs():</span>
    <span class="s3"># XXX: this test is very seed-dependent! It probably needs to be rewritten.</span>

    <span class="s3"># Gibbs on the RBM hidden layer should be able to recreate [[0], [1]]</span>
    <span class="s3"># from the same input</span>
    <span class="s1">rng = np.random.RandomState(</span><span class="s2">42</span><span class="s1">)</span>
    <span class="s1">X = np.array([[</span><span class="s2">0.0</span><span class="s1">]</span><span class="s0">, </span><span class="s1">[</span><span class="s2">1.0</span><span class="s1">]])</span>
    <span class="s1">rbm1 = BernoulliRBM(n_components=</span><span class="s2">2</span><span class="s0">, </span><span class="s1">batch_size=</span><span class="s2">2</span><span class="s0">, </span><span class="s1">n_iter=</span><span class="s2">42</span><span class="s0">, </span><span class="s1">random_state=rng)</span>
    <span class="s3"># you need that much iters</span>
    <span class="s1">rbm1.fit(X)</span>
    <span class="s1">assert_almost_equal(</span>
        <span class="s1">rbm1.components_</span><span class="s0">, </span><span class="s1">np.array([[</span><span class="s2">0.02649814</span><span class="s1">]</span><span class="s0">, </span><span class="s1">[</span><span class="s2">0.02009084</span><span class="s1">]])</span><span class="s0">, </span><span class="s1">decimal=</span><span class="s2">4</span>
    <span class="s1">)</span>
    <span class="s1">assert_almost_equal(rbm1.gibbs(X)</span><span class="s0">, </span><span class="s1">X)</span>

    <span class="s3"># Gibbs on the RBM hidden layer should be able to recreate [[0], [1]] from</span>
    <span class="s3"># the same input even when the input is sparse, and test against non-sparse</span>
    <span class="s1">rng = np.random.RandomState(</span><span class="s2">42</span><span class="s1">)</span>
    <span class="s1">X = csc_matrix([[</span><span class="s2">0.0</span><span class="s1">]</span><span class="s0">, </span><span class="s1">[</span><span class="s2">1.0</span><span class="s1">]])</span>
    <span class="s1">rbm2 = BernoulliRBM(n_components=</span><span class="s2">2</span><span class="s0">, </span><span class="s1">batch_size=</span><span class="s2">2</span><span class="s0">, </span><span class="s1">n_iter=</span><span class="s2">42</span><span class="s0">, </span><span class="s1">random_state=rng)</span>
    <span class="s1">rbm2.fit(X)</span>
    <span class="s1">assert_almost_equal(</span>
        <span class="s1">rbm2.components_</span><span class="s0">, </span><span class="s1">np.array([[</span><span class="s2">0.02649814</span><span class="s1">]</span><span class="s0">, </span><span class="s1">[</span><span class="s2">0.02009084</span><span class="s1">]])</span><span class="s0">, </span><span class="s1">decimal=</span><span class="s2">4</span>
    <span class="s1">)</span>
    <span class="s1">assert_almost_equal(rbm2.gibbs(X)</span><span class="s0">, </span><span class="s1">X.toarray())</span>
    <span class="s1">assert_almost_equal(rbm1.components_</span><span class="s0">, </span><span class="s1">rbm2.components_)</span>


<span class="s0">def </span><span class="s1">test_gibbs_smoke():</span>
    <span class="s3"># Check if we don't get NaNs sampling the full digits dataset.</span>
    <span class="s3"># Also check that sampling again will yield different results.</span>
    <span class="s1">X = Xdigits</span>
    <span class="s1">rbm1 = BernoulliRBM(n_components=</span><span class="s2">42</span><span class="s0">, </span><span class="s1">batch_size=</span><span class="s2">40</span><span class="s0">, </span><span class="s1">n_iter=</span><span class="s2">20</span><span class="s0">, </span><span class="s1">random_state=</span><span class="s2">42</span><span class="s1">)</span>
    <span class="s1">rbm1.fit(X)</span>
    <span class="s1">X_sampled = rbm1.gibbs(X)</span>
    <span class="s1">assert_all_finite(X_sampled)</span>
    <span class="s1">X_sampled2 = rbm1.gibbs(X)</span>
    <span class="s0">assert </span><span class="s1">np.all((X_sampled != X_sampled2).max(axis=</span><span class="s2">1</span><span class="s1">))</span>


<span class="s0">def </span><span class="s1">test_score_samples():</span>
    <span class="s3"># Test score_samples (pseudo-likelihood) method.</span>
    <span class="s3"># Assert that pseudo-likelihood is computed without clipping.</span>
    <span class="s3"># See Fabian's blog, http://bit.ly/1iYefRk</span>
    <span class="s1">rng = np.random.RandomState(</span><span class="s2">42</span><span class="s1">)</span>
    <span class="s1">X = np.vstack([np.zeros(</span><span class="s2">1000</span><span class="s1">)</span><span class="s0">, </span><span class="s1">np.ones(</span><span class="s2">1000</span><span class="s1">)])</span>
    <span class="s1">rbm1 = BernoulliRBM(n_components=</span><span class="s2">10</span><span class="s0">, </span><span class="s1">batch_size=</span><span class="s2">2</span><span class="s0">, </span><span class="s1">n_iter=</span><span class="s2">10</span><span class="s0">, </span><span class="s1">random_state=rng)</span>
    <span class="s1">rbm1.fit(X)</span>
    <span class="s0">assert </span><span class="s1">(rbm1.score_samples(X) &lt; -</span><span class="s2">300</span><span class="s1">).all()</span>

    <span class="s3"># Sparse vs. dense should not affect the output. Also test sparse input</span>
    <span class="s3"># validation.</span>
    <span class="s1">rbm1.random_state = </span><span class="s2">42</span>
    <span class="s1">d_score = rbm1.score_samples(X)</span>
    <span class="s1">rbm1.random_state = </span><span class="s2">42</span>
    <span class="s1">s_score = rbm1.score_samples(lil_matrix(X))</span>
    <span class="s1">assert_almost_equal(d_score</span><span class="s0">, </span><span class="s1">s_score)</span>

    <span class="s3"># Test numerical stability (#2785): would previously generate infinities</span>
    <span class="s3"># and crash with an exception.</span>
    <span class="s0">with </span><span class="s1">np.errstate(under=</span><span class="s4">&quot;ignore&quot;</span><span class="s1">):</span>
        <span class="s1">rbm1.score_samples([np.arange(</span><span class="s2">1000</span><span class="s1">) * </span><span class="s2">100</span><span class="s1">])</span>


<span class="s0">def </span><span class="s1">test_rbm_verbose():</span>
    <span class="s1">rbm = BernoulliRBM(n_iter=</span><span class="s2">2</span><span class="s0">, </span><span class="s1">verbose=</span><span class="s2">10</span><span class="s1">)</span>
    <span class="s1">old_stdout = sys.stdout</span>
    <span class="s1">sys.stdout = StringIO()</span>
    <span class="s0">try</span><span class="s1">:</span>
        <span class="s1">rbm.fit(Xdigits)</span>
    <span class="s0">finally</span><span class="s1">:</span>
        <span class="s1">sys.stdout = old_stdout</span>


<span class="s0">def </span><span class="s1">test_sparse_and_verbose():</span>
    <span class="s3"># Make sure RBM works with sparse input when verbose=True</span>
    <span class="s1">old_stdout = sys.stdout</span>
    <span class="s1">sys.stdout = StringIO()</span>
    <span class="s0">from </span><span class="s1">scipy.sparse </span><span class="s0">import </span><span class="s1">csc_matrix</span>

    <span class="s1">X = csc_matrix([[</span><span class="s2">0.0</span><span class="s1">]</span><span class="s0">, </span><span class="s1">[</span><span class="s2">1.0</span><span class="s1">]])</span>
    <span class="s1">rbm = BernoulliRBM(</span>
        <span class="s1">n_components=</span><span class="s2">2</span><span class="s0">, </span><span class="s1">batch_size=</span><span class="s2">2</span><span class="s0">, </span><span class="s1">n_iter=</span><span class="s2">1</span><span class="s0">, </span><span class="s1">random_state=</span><span class="s2">42</span><span class="s0">, </span><span class="s1">verbose=</span><span class="s0">True</span>
    <span class="s1">)</span>
    <span class="s0">try</span><span class="s1">:</span>
        <span class="s1">rbm.fit(X)</span>
        <span class="s1">s = sys.stdout.getvalue()</span>
        <span class="s3"># make sure output is sound</span>
        <span class="s0">assert </span><span class="s1">re.match(</span>
            <span class="s4">r&quot;\[BernoulliRBM\] Iteration 1,&quot;</span>
            <span class="s4">r&quot; pseudo-likelihood = -?(\d)+(\.\d+)?,&quot;</span>
            <span class="s4">r&quot; time = (\d|\.)+s&quot;</span><span class="s0">,</span>
            <span class="s1">s</span><span class="s0">,</span>
        <span class="s1">)</span>
    <span class="s0">finally</span><span class="s1">:</span>
        <span class="s1">sys.stdout = old_stdout</span>


<span class="s1">@pytest.mark.parametrize(</span>
    <span class="s4">&quot;dtype_in, dtype_out&quot;</span><span class="s0">,</span>
    <span class="s1">[(np.float32</span><span class="s0">, </span><span class="s1">np.float32)</span><span class="s0">, </span><span class="s1">(np.float64</span><span class="s0">, </span><span class="s1">np.float64)</span><span class="s0">, </span><span class="s1">(int</span><span class="s0">, </span><span class="s1">np.float64)]</span><span class="s0">,</span>
<span class="s1">)</span>
<span class="s0">def </span><span class="s1">test_transformer_dtypes_casting(dtype_in</span><span class="s0">, </span><span class="s1">dtype_out):</span>
    <span class="s1">X = Xdigits[:</span><span class="s2">100</span><span class="s1">].astype(dtype_in)</span>
    <span class="s1">rbm = BernoulliRBM(n_components=</span><span class="s2">16</span><span class="s0">, </span><span class="s1">batch_size=</span><span class="s2">5</span><span class="s0">, </span><span class="s1">n_iter=</span><span class="s2">5</span><span class="s0">, </span><span class="s1">random_state=</span><span class="s2">42</span><span class="s1">)</span>
    <span class="s1">Xt = rbm.fit_transform(X)</span>

    <span class="s3"># dtype_in and dtype_out should be consistent</span>
    <span class="s0">assert </span><span class="s1">Xt.dtype == dtype_out</span><span class="s0">, </span><span class="s4">&quot;transform dtype: {} - original dtype: {}&quot;</span><span class="s1">.format(</span>
        <span class="s1">Xt.dtype</span><span class="s0">, </span><span class="s1">X.dtype</span>
    <span class="s1">)</span>


<span class="s0">def </span><span class="s1">test_convergence_dtype_consistency():</span>
    <span class="s3"># float 64 transformer</span>
    <span class="s1">X_64 = Xdigits[:</span><span class="s2">100</span><span class="s1">].astype(np.float64)</span>
    <span class="s1">rbm_64 = BernoulliRBM(n_components=</span><span class="s2">16</span><span class="s0">, </span><span class="s1">batch_size=</span><span class="s2">5</span><span class="s0">, </span><span class="s1">n_iter=</span><span class="s2">5</span><span class="s0">, </span><span class="s1">random_state=</span><span class="s2">42</span><span class="s1">)</span>
    <span class="s1">Xt_64 = rbm_64.fit_transform(X_64)</span>

    <span class="s3"># float 32 transformer</span>
    <span class="s1">X_32 = Xdigits[:</span><span class="s2">100</span><span class="s1">].astype(np.float32)</span>
    <span class="s1">rbm_32 = BernoulliRBM(n_components=</span><span class="s2">16</span><span class="s0">, </span><span class="s1">batch_size=</span><span class="s2">5</span><span class="s0">, </span><span class="s1">n_iter=</span><span class="s2">5</span><span class="s0">, </span><span class="s1">random_state=</span><span class="s2">42</span><span class="s1">)</span>
    <span class="s1">Xt_32 = rbm_32.fit_transform(X_32)</span>

    <span class="s3"># results and attributes should be close enough in 32 bit and 64 bit</span>
    <span class="s1">assert_allclose(Xt_64</span><span class="s0">, </span><span class="s1">Xt_32</span><span class="s0">, </span><span class="s1">rtol=</span><span class="s2">1e-06</span><span class="s0">, </span><span class="s1">atol=</span><span class="s2">0</span><span class="s1">)</span>
    <span class="s1">assert_allclose(</span>
        <span class="s1">rbm_64.intercept_hidden_</span><span class="s0">, </span><span class="s1">rbm_32.intercept_hidden_</span><span class="s0">, </span><span class="s1">rtol=</span><span class="s2">1e-06</span><span class="s0">, </span><span class="s1">atol=</span><span class="s2">0</span>
    <span class="s1">)</span>
    <span class="s1">assert_allclose(</span>
        <span class="s1">rbm_64.intercept_visible_</span><span class="s0">, </span><span class="s1">rbm_32.intercept_visible_</span><span class="s0">, </span><span class="s1">rtol=</span><span class="s2">1e-05</span><span class="s0">, </span><span class="s1">atol=</span><span class="s2">0</span>
    <span class="s1">)</span>
    <span class="s1">assert_allclose(rbm_64.components_</span><span class="s0">, </span><span class="s1">rbm_32.components_</span><span class="s0">, </span><span class="s1">rtol=</span><span class="s2">1e-03</span><span class="s0">, </span><span class="s1">atol=</span><span class="s2">0</span><span class="s1">)</span>
    <span class="s1">assert_allclose(rbm_64.h_samples_</span><span class="s0">, </span><span class="s1">rbm_32.h_samples_)</span>


<span class="s1">@pytest.mark.parametrize(</span><span class="s4">&quot;method&quot;</span><span class="s0">, </span><span class="s1">[</span><span class="s4">&quot;fit&quot;</span><span class="s0">, </span><span class="s4">&quot;partial_fit&quot;</span><span class="s1">])</span>
<span class="s0">def </span><span class="s1">test_feature_names_out(method):</span>
    <span class="s5">&quot;&quot;&quot;Check `get_feature_names_out` for `BernoulliRBM`.&quot;&quot;&quot;</span>
    <span class="s1">n_components = </span><span class="s2">10</span>
    <span class="s1">rbm = BernoulliRBM(n_components=n_components)</span>
    <span class="s1">getattr(rbm</span><span class="s0">, </span><span class="s1">method)(Xdigits)</span>

    <span class="s1">names = rbm.get_feature_names_out()</span>
    <span class="s1">expected_names = [</span><span class="s4">f&quot;bernoullirbm</span><span class="s0">{</span><span class="s1">i</span><span class="s0">}</span><span class="s4">&quot; </span><span class="s0">for </span><span class="s1">i </span><span class="s0">in </span><span class="s1">range(n_components)]</span>
    <span class="s1">assert_array_equal(expected_names</span><span class="s0">, </span><span class="s1">names)</span>
</pre>
</body>
</html>