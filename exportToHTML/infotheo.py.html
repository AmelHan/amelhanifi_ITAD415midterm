<html>
<head>
<title>infotheo.py</title>
<meta http-equiv="Content-Type" content="text/html; charset=utf-8">
<style type="text/css">
.s0 { color: #629755; font-style: italic;}
.s1 { color: #a9b7c6;}
.s2 { color: #808080;}
.s3 { color: #cc7832;}
.s4 { color: #6897bb;}
.s5 { color: #6a8759;}
</style>
</head>
<body bgcolor="#2b2b2b">
<table CELLSPACING=0 CELLPADDING=5 COLS=1 WIDTH="100%" BGCOLOR="#606060" >
<tr><td><center>
<font face="Arial, Helvetica" color="#000000">
infotheo.py</font>
</center></td></tr></table>
<pre><span class="s0">&quot;&quot;&quot; 
Information Theoretic and Entropy Measures 
 
References 
---------- 
Golan, As. 2008. &quot;Information and Entropy Econometrics -- A Review and 
    Synthesis.&quot; Foundations And Trends in Econometrics 2(1-2), 1-145. 
 
Golan, A., Judge, G., and Miller, D.  1996.  Maximum Entropy Econometrics. 
    Wiley &amp; Sons, Chichester. 
&quot;&quot;&quot;</span>
<span class="s2">#For MillerMadow correction</span>
<span class="s2">#Miller, G. 1955. Note on the bias of information estimates. Info. Theory</span>
<span class="s2">#    Psychol. Prob. Methods II-B:95-100.</span>

<span class="s2">#For ChaoShen method</span>
<span class="s2">#Chao, A., and T.-J. Shen. 2003. Nonparametric estimation of Shannon's index of diversity when</span>
<span class="s2">#there are unseen species in sample. Environ. Ecol. Stat. 10:429-443.</span>
<span class="s2">#Good, I. J. 1953. The population frequencies of species and the estimation of population parameters.</span>
<span class="s2">#Biometrika 40:237-264.</span>
<span class="s2">#Horvitz, D.G., and D. J. Thompson. 1952. A generalization of sampling without replacement from a finute universe. J. Am. Stat. Assoc. 47:663-685.</span>

<span class="s2">#For NSB method</span>
<span class="s2">#Nemenman, I., F. Shafee, and W. Bialek. 2002. Entropy and inference, revisited. In: Dietterich, T.,</span>
<span class="s2">#S. Becker, Z. Gharamani, eds. Advances in Neural Information Processing Systems 14: 471-478.</span>
<span class="s2">#Cambridge (Massachusetts): MIT Press.</span>

<span class="s2">#For shrinkage method</span>
<span class="s2">#Dougherty, J., Kohavi, R., and Sahami, M. (1995). Supervised and unsupervised discretization of</span>
<span class="s2">#continuous features. In International Conference on Machine Learning.</span>
<span class="s2">#Yang, Y. and Webb, G. I. (2003). Discretization for naive-bayes learning: managing discretization</span>
<span class="s2">#bias and variance. Technical Report 2003/131 School of Computer Science and Software Engineer-</span>
<span class="s2">#ing, Monash University.</span>

<span class="s3">from </span><span class="s1">statsmodels.compat.python </span><span class="s3">import </span><span class="s1">lzip</span><span class="s3">, </span><span class="s1">lmap</span>
<span class="s3">from </span><span class="s1">scipy </span><span class="s3">import </span><span class="s1">stats</span>
<span class="s3">import </span><span class="s1">numpy </span><span class="s3">as </span><span class="s1">np</span>
<span class="s3">from </span><span class="s1">matplotlib </span><span class="s3">import </span><span class="s1">pyplot </span><span class="s3">as </span><span class="s1">plt</span>
<span class="s3">from </span><span class="s1">scipy.special </span><span class="s3">import </span><span class="s1">logsumexp </span><span class="s3">as </span><span class="s1">sp_logsumexp</span>

<span class="s2">#TODO: change these to use maxentutils so that over/underflow is handled</span>
<span class="s2">#with the logsumexp.</span>


<span class="s3">def </span><span class="s1">logsumexp(a</span><span class="s3">, </span><span class="s1">axis=</span><span class="s3">None</span><span class="s1">):</span>
    <span class="s0">&quot;&quot;&quot; 
    Compute the log of the sum of exponentials log(e^{a_1}+...e^{a_n}) of a 
 
    Avoids numerical overflow. 
 
    Parameters 
    ---------- 
    a : array_like 
        The vector to exponentiate and sum 
    axis : int, optional 
        The axis along which to apply the operation.  Defaults is None. 
 
    Returns 
    ------- 
    sum(log(exp(a))) 
 
    Notes 
    ----- 
    This function was taken from the mailing list 
    http://mail.scipy.org/pipermail/scipy-user/2009-October/022931.html 
 
    This should be superceded by the ufunc when it is finished. 
    &quot;&quot;&quot;</span>
    <span class="s3">if </span><span class="s1">axis </span><span class="s3">is None</span><span class="s1">:</span>
        <span class="s2"># Use the scipy.maxentropy version.</span>
        <span class="s3">return </span><span class="s1">sp_logsumexp(a)</span>
    <span class="s1">a = np.asarray(a)</span>
    <span class="s1">shp = list(a.shape)</span>
    <span class="s1">shp[axis] = </span><span class="s4">1</span>
    <span class="s1">a_max = a.max(axis=axis)</span>
    <span class="s1">s = np.log(np.exp(a - a_max.reshape(shp)).sum(axis=axis))</span>
    <span class="s1">lse  = a_max + s</span>
    <span class="s3">return </span><span class="s1">lse</span>


<span class="s3">def </span><span class="s1">_isproperdist(X):</span>
    <span class="s0">&quot;&quot;&quot; 
    Checks to see if `X` is a proper probability distribution 
    &quot;&quot;&quot;</span>
    <span class="s1">X = np.asarray(X)</span>
    <span class="s3">if not </span><span class="s1">np.allclose(np.sum(X)</span><span class="s3">, </span><span class="s4">1</span><span class="s1">) </span><span class="s3">or not </span><span class="s1">np.all(X&gt;=</span><span class="s4">0</span><span class="s1">) </span><span class="s3">or not </span><span class="s1">np.all(X&lt;=</span><span class="s4">1</span><span class="s1">):</span>
        <span class="s3">return False</span>
    <span class="s3">else</span><span class="s1">:</span>
        <span class="s3">return True</span>

<span class="s3">def </span><span class="s1">discretize(X</span><span class="s3">, </span><span class="s1">method=</span><span class="s5">&quot;ef&quot;</span><span class="s3">, </span><span class="s1">nbins=</span><span class="s3">None</span><span class="s1">):</span>
    <span class="s0">&quot;&quot;&quot; 
    Discretize `X` 
 
    Parameters 
    ---------- 
    bins : int, optional 
        Number of bins.  Default is floor(sqrt(N)) 
    method : str 
        &quot;ef&quot; is equal-frequency binning 
        &quot;ew&quot; is equal-width binning 
 
    Examples 
    -------- 
    &quot;&quot;&quot;</span>
    <span class="s1">nobs = len(X)</span>
    <span class="s3">if </span><span class="s1">nbins </span><span class="s3">is None</span><span class="s1">:</span>
        <span class="s1">nbins = np.floor(np.sqrt(nobs))</span>
    <span class="s3">if </span><span class="s1">method == </span><span class="s5">&quot;ef&quot;</span><span class="s1">:</span>
        <span class="s1">discrete = np.ceil(nbins * stats.rankdata(X)/nobs)</span>
    <span class="s3">if </span><span class="s1">method == </span><span class="s5">&quot;ew&quot;</span><span class="s1">:</span>
        <span class="s1">width = np.max(X) - np.min(X)</span>
        <span class="s1">width = np.floor(width/nbins)</span>
        <span class="s1">svec</span><span class="s3">, </span><span class="s1">ivec = stats.fastsort(X)</span>
        <span class="s1">discrete = np.zeros(nobs)</span>
        <span class="s1">binnum = </span><span class="s4">1</span>
        <span class="s1">base = svec[</span><span class="s4">0</span><span class="s1">]</span>
        <span class="s1">discrete[ivec[</span><span class="s4">0</span><span class="s1">]] = binnum</span>
        <span class="s3">for </span><span class="s1">i </span><span class="s3">in </span><span class="s1">range(</span><span class="s4">1</span><span class="s3">,</span><span class="s1">nobs):</span>
            <span class="s3">if </span><span class="s1">svec[i] &lt; base + width:</span>
                <span class="s1">discrete[ivec[i]] = binnum</span>
            <span class="s3">else</span><span class="s1">:</span>
                <span class="s1">base = svec[i]</span>
                <span class="s1">binnum += </span><span class="s4">1</span>
                <span class="s1">discrete[ivec[i]] = binnum</span>
    <span class="s3">return </span><span class="s1">discrete</span>
<span class="s2">#TODO: looks okay but needs more robust tests for corner cases</span>



<span class="s3">def </span><span class="s1">logbasechange(a</span><span class="s3">,</span><span class="s1">b):</span>
    <span class="s0">&quot;&quot;&quot; 
    There is a one-to-one transformation of the entropy value from 
    a log base b to a log base a : 
 
    H_{b}(X)=log_{b}(a)[H_{a}(X)] 
 
    Returns 
    ------- 
    log_{b}(a) 
    &quot;&quot;&quot;</span>
    <span class="s3">return </span><span class="s1">np.log(b)/np.log(a)</span>

<span class="s3">def </span><span class="s1">natstobits(X):</span>
    <span class="s0">&quot;&quot;&quot; 
    Converts from nats to bits 
    &quot;&quot;&quot;</span>
    <span class="s3">return </span><span class="s1">logbasechange(np.e</span><span class="s3">, </span><span class="s4">2</span><span class="s1">) * X</span>

<span class="s3">def </span><span class="s1">bitstonats(X):</span>
    <span class="s0">&quot;&quot;&quot; 
    Converts from bits to nats 
    &quot;&quot;&quot;</span>
    <span class="s3">return </span><span class="s1">logbasechange(</span><span class="s4">2</span><span class="s3">, </span><span class="s1">np.e) * X</span>

<span class="s2">#TODO: make this entropy, and then have different measures as</span>
<span class="s2">#a method</span>
<span class="s3">def </span><span class="s1">shannonentropy(px</span><span class="s3">, </span><span class="s1">logbase=</span><span class="s4">2</span><span class="s1">):</span>
    <span class="s0">&quot;&quot;&quot; 
    This is Shannon's entropy 
 
    Parameters 
    ---------- 
    logbase, int or np.e 
        The base of the log 
    px : 1d or 2d array_like 
        Can be a discrete probability distribution, a 2d joint distribution, 
        or a sequence of probabilities. 
 
    Returns 
    ----- 
    For log base 2 (bits) given a discrete distribution 
        H(p) = sum(px * log2(1/px) = -sum(pk*log2(px)) = E[log2(1/p(X))] 
 
    For log base 2 (bits) given a joint distribution 
        H(px,py) = -sum_{k,j}*w_{kj}log2(w_{kj}) 
 
    Notes 
    ----- 
    shannonentropy(0) is defined as 0 
    &quot;&quot;&quot;</span>
<span class="s2">#TODO: have not defined the px,py case?</span>
    <span class="s1">px = np.asarray(px)</span>
    <span class="s3">if not </span><span class="s1">np.all(px &lt;= </span><span class="s4">1</span><span class="s1">) </span><span class="s3">or not </span><span class="s1">np.all(px &gt;= </span><span class="s4">0</span><span class="s1">):</span>
        <span class="s3">raise </span><span class="s1">ValueError(</span><span class="s5">&quot;px does not define proper distribution&quot;</span><span class="s1">)</span>
    <span class="s1">entropy = -np.sum(np.nan_to_num(px*np.log2(px)))</span>
    <span class="s3">if </span><span class="s1">logbase != </span><span class="s4">2</span><span class="s1">:</span>
        <span class="s3">return </span><span class="s1">logbasechange(</span><span class="s4">2</span><span class="s3">,</span><span class="s1">logbase) * entropy</span>
    <span class="s3">else</span><span class="s1">:</span>
        <span class="s3">return </span><span class="s1">entropy</span>

<span class="s2"># Shannon's information content</span>
<span class="s3">def </span><span class="s1">shannoninfo(px</span><span class="s3">, </span><span class="s1">logbase=</span><span class="s4">2</span><span class="s1">):</span>
    <span class="s0">&quot;&quot;&quot; 
    Shannon's information 
 
    Parameters 
    ---------- 
    px : float or array_like 
        `px` is a discrete probability distribution 
 
    Returns 
    ------- 
    For logbase = 2 
    np.log2(px) 
    &quot;&quot;&quot;</span>
    <span class="s1">px = np.asarray(px)</span>
    <span class="s3">if not </span><span class="s1">np.all(px &lt;= </span><span class="s4">1</span><span class="s1">) </span><span class="s3">or not </span><span class="s1">np.all(px &gt;= </span><span class="s4">0</span><span class="s1">):</span>
        <span class="s3">raise </span><span class="s1">ValueError(</span><span class="s5">&quot;px does not define proper distribution&quot;</span><span class="s1">)</span>
    <span class="s3">if </span><span class="s1">logbase != </span><span class="s4">2</span><span class="s1">:</span>
        <span class="s3">return </span><span class="s1">- logbasechange(</span><span class="s4">2</span><span class="s3">,</span><span class="s1">logbase) * np.log2(px)</span>
    <span class="s3">else</span><span class="s1">:</span>
        <span class="s3">return </span><span class="s1">- np.log2(px)</span>

<span class="s3">def </span><span class="s1">condentropy(px</span><span class="s3">, </span><span class="s1">py</span><span class="s3">, </span><span class="s1">pxpy=</span><span class="s3">None, </span><span class="s1">logbase=</span><span class="s4">2</span><span class="s1">):</span>
    <span class="s0">&quot;&quot;&quot; 
    Return the conditional entropy of X given Y. 
 
    Parameters 
    ---------- 
    px : array_like 
    py : array_like 
    pxpy : array_like, optional 
        If pxpy is None, the distributions are assumed to be independent 
        and conendtropy(px,py) = shannonentropy(px) 
    logbase : int or np.e 
 
    Returns 
    ------- 
    sum_{kj}log(q_{j}/w_{kj} 
 
    where q_{j} = Y[j] 
    and w_kj = X[k,j] 
    &quot;&quot;&quot;</span>
    <span class="s3">if not </span><span class="s1">_isproperdist(px) </span><span class="s3">or not </span><span class="s1">_isproperdist(py):</span>
        <span class="s3">raise </span><span class="s1">ValueError(</span><span class="s5">&quot;px or py is not a proper probability distribution&quot;</span><span class="s1">)</span>
    <span class="s3">if </span><span class="s1">pxpy </span><span class="s3">is not None and not </span><span class="s1">_isproperdist(pxpy):</span>
        <span class="s3">raise </span><span class="s1">ValueError(</span><span class="s5">&quot;pxpy is not a proper joint distribtion&quot;</span><span class="s1">)</span>
    <span class="s3">if </span><span class="s1">pxpy </span><span class="s3">is None</span><span class="s1">:</span>
        <span class="s1">pxpy = np.outer(py</span><span class="s3">,</span><span class="s1">px)</span>
    <span class="s1">condent = np.sum(pxpy * np.nan_to_num(np.log2(py/pxpy)))</span>
    <span class="s3">if </span><span class="s1">logbase == </span><span class="s4">2</span><span class="s1">:</span>
        <span class="s3">return </span><span class="s1">condent</span>
    <span class="s3">else</span><span class="s1">:</span>
        <span class="s3">return </span><span class="s1">logbasechange(</span><span class="s4">2</span><span class="s3">, </span><span class="s1">logbase) * condent</span>

<span class="s3">def </span><span class="s1">mutualinfo(px</span><span class="s3">,</span><span class="s1">py</span><span class="s3">,</span><span class="s1">pxpy</span><span class="s3">, </span><span class="s1">logbase=</span><span class="s4">2</span><span class="s1">):</span>
    <span class="s0">&quot;&quot;&quot; 
    Returns the mutual information between X and Y. 
 
    Parameters 
    ---------- 
    px : array_like 
        Discrete probability distribution of random variable X 
    py : array_like 
        Discrete probability distribution of random variable Y 
    pxpy : 2d array_like 
        The joint probability distribution of random variables X and Y. 
        Note that if X and Y are independent then the mutual information 
        is zero. 
    logbase : int or np.e, optional 
        Default is 2 (bits) 
 
    Returns 
    ------- 
    shannonentropy(px) - condentropy(px,py,pxpy) 
    &quot;&quot;&quot;</span>
    <span class="s3">if not </span><span class="s1">_isproperdist(px) </span><span class="s3">or not </span><span class="s1">_isproperdist(py):</span>
        <span class="s3">raise </span><span class="s1">ValueError(</span><span class="s5">&quot;px or py is not a proper probability distribution&quot;</span><span class="s1">)</span>
    <span class="s3">if </span><span class="s1">pxpy </span><span class="s3">is not None and not </span><span class="s1">_isproperdist(pxpy):</span>
        <span class="s3">raise </span><span class="s1">ValueError(</span><span class="s5">&quot;pxpy is not a proper joint distribtion&quot;</span><span class="s1">)</span>
    <span class="s3">if </span><span class="s1">pxpy </span><span class="s3">is None</span><span class="s1">:</span>
        <span class="s1">pxpy = np.outer(py</span><span class="s3">,</span><span class="s1">px)</span>
    <span class="s3">return </span><span class="s1">shannonentropy(px</span><span class="s3">, </span><span class="s1">logbase=logbase) - condentropy(px</span><span class="s3">,</span><span class="s1">py</span><span class="s3">,</span><span class="s1">pxpy</span><span class="s3">,</span>
            <span class="s1">logbase=logbase)</span>

<span class="s3">def </span><span class="s1">corrent(px</span><span class="s3">,</span><span class="s1">py</span><span class="s3">,</span><span class="s1">pxpy</span><span class="s3">,</span><span class="s1">logbase=</span><span class="s4">2</span><span class="s1">):</span>
    <span class="s0">&quot;&quot;&quot; 
    An information theoretic correlation measure. 
 
    Reflects linear and nonlinear correlation between two random variables 
    X and Y, characterized by the discrete probability distributions px and py 
    respectively. 
 
    Parameters 
    ---------- 
    px : array_like 
        Discrete probability distribution of random variable X 
    py : array_like 
        Discrete probability distribution of random variable Y 
    pxpy : 2d array_like, optional 
        Joint probability distribution of X and Y.  If pxpy is None, X and Y 
        are assumed to be independent. 
    logbase : int or np.e, optional 
        Default is 2 (bits) 
 
    Returns 
    ------- 
    mutualinfo(px,py,pxpy,logbase=logbase)/shannonentropy(py,logbase=logbase) 
 
    Notes 
    ----- 
    This is also equivalent to 
 
    corrent(px,py,pxpy) = 1 - condent(px,py,pxpy)/shannonentropy(py) 
    &quot;&quot;&quot;</span>
    <span class="s3">if not </span><span class="s1">_isproperdist(px) </span><span class="s3">or not </span><span class="s1">_isproperdist(py):</span>
        <span class="s3">raise </span><span class="s1">ValueError(</span><span class="s5">&quot;px or py is not a proper probability distribution&quot;</span><span class="s1">)</span>
    <span class="s3">if </span><span class="s1">pxpy </span><span class="s3">is not None and not </span><span class="s1">_isproperdist(pxpy):</span>
        <span class="s3">raise </span><span class="s1">ValueError(</span><span class="s5">&quot;pxpy is not a proper joint distribtion&quot;</span><span class="s1">)</span>
    <span class="s3">if </span><span class="s1">pxpy </span><span class="s3">is None</span><span class="s1">:</span>
        <span class="s1">pxpy = np.outer(py</span><span class="s3">,</span><span class="s1">px)</span>

    <span class="s3">return </span><span class="s1">mutualinfo(px</span><span class="s3">,</span><span class="s1">py</span><span class="s3">,</span><span class="s1">pxpy</span><span class="s3">,</span><span class="s1">logbase=logbase)/shannonentropy(py</span><span class="s3">,</span>
            <span class="s1">logbase=logbase)</span>

<span class="s3">def </span><span class="s1">covent(px</span><span class="s3">,</span><span class="s1">py</span><span class="s3">,</span><span class="s1">pxpy</span><span class="s3">,</span><span class="s1">logbase=</span><span class="s4">2</span><span class="s1">):</span>
    <span class="s0">&quot;&quot;&quot; 
    An information theoretic covariance measure. 
 
    Reflects linear and nonlinear correlation between two random variables 
    X and Y, characterized by the discrete probability distributions px and py 
    respectively. 
 
    Parameters 
    ---------- 
    px : array_like 
        Discrete probability distribution of random variable X 
    py : array_like 
        Discrete probability distribution of random variable Y 
    pxpy : 2d array_like, optional 
        Joint probability distribution of X and Y.  If pxpy is None, X and Y 
        are assumed to be independent. 
    logbase : int or np.e, optional 
        Default is 2 (bits) 
 
    Returns 
    ------- 
    condent(px,py,pxpy,logbase=logbase) + condent(py,px,pxpy, 
            logbase=logbase) 
 
    Notes 
    ----- 
    This is also equivalent to 
 
    covent(px,py,pxpy) = condent(px,py,pxpy) + condent(py,px,pxpy) 
    &quot;&quot;&quot;</span>
    <span class="s3">if not </span><span class="s1">_isproperdist(px) </span><span class="s3">or not </span><span class="s1">_isproperdist(py):</span>
        <span class="s3">raise </span><span class="s1">ValueError(</span><span class="s5">&quot;px or py is not a proper probability distribution&quot;</span><span class="s1">)</span>
    <span class="s3">if </span><span class="s1">pxpy </span><span class="s3">is not None and not </span><span class="s1">_isproperdist(pxpy):</span>
        <span class="s3">raise </span><span class="s1">ValueError(</span><span class="s5">&quot;pxpy is not a proper joint distribtion&quot;</span><span class="s1">)</span>
    <span class="s3">if </span><span class="s1">pxpy </span><span class="s3">is None</span><span class="s1">:</span>
        <span class="s1">pxpy = np.outer(py</span><span class="s3">,</span><span class="s1">px)</span>

    <span class="s2"># FIXME: these should be `condentropy`, not `condent`</span>
    <span class="s3">return </span><span class="s1">(condent(px</span><span class="s3">, </span><span class="s1">py</span><span class="s3">, </span><span class="s1">pxpy</span><span class="s3">, </span><span class="s1">logbase=logbase)  </span><span class="s2"># noqa:F821  See GH#5756</span>
            <span class="s1">+ condent(py</span><span class="s3">, </span><span class="s1">px</span><span class="s3">, </span><span class="s1">pxpy</span><span class="s3">, </span><span class="s1">logbase=logbase))  </span><span class="s2"># noqa:F821  See GH#5756</span>



<span class="s2">#### Generalized Entropies ####</span>

<span class="s3">def </span><span class="s1">renyientropy(px</span><span class="s3">,</span><span class="s1">alpha=</span><span class="s4">1</span><span class="s3">,</span><span class="s1">logbase=</span><span class="s4">2</span><span class="s3">,</span><span class="s1">measure=</span><span class="s5">'R'</span><span class="s1">):</span>
    <span class="s0">&quot;&quot;&quot; 
    Renyi's generalized entropy 
 
    Parameters 
    ---------- 
    px : array_like 
        Discrete probability distribution of random variable X.  Note that 
        px is assumed to be a proper probability distribution. 
    logbase : int or np.e, optional 
        Default is 2 (bits) 
    alpha : float or inf 
        The order of the entropy.  The default is 1, which in the limit 
        is just Shannon's entropy.  2 is Renyi (Collision) entropy.  If 
        the string &quot;inf&quot; or numpy.inf is specified the min-entropy is returned. 
    measure : str, optional 
        The type of entropy measure desired.  'R' returns Renyi entropy 
        measure.  'T' returns the Tsallis entropy measure. 
 
    Returns 
    ------- 
    1/(1-alpha)*log(sum(px**alpha)) 
 
    In the limit as alpha -&gt; 1, Shannon's entropy is returned. 
 
    In the limit as alpha -&gt; inf, min-entropy is returned. 
    &quot;&quot;&quot;</span>
<span class="s2">#TODO:finish returns</span>
<span class="s2">#TODO:add checks for measure</span>
    <span class="s3">if not </span><span class="s1">_isproperdist(px):</span>
        <span class="s3">raise </span><span class="s1">ValueError(</span><span class="s5">&quot;px is not a proper probability distribution&quot;</span><span class="s1">)</span>
    <span class="s1">alpha = float(alpha)</span>
    <span class="s3">if </span><span class="s1">alpha == </span><span class="s4">1</span><span class="s1">:</span>
        <span class="s1">genent = shannonentropy(px)</span>
        <span class="s3">if </span><span class="s1">logbase != </span><span class="s4">2</span><span class="s1">:</span>
            <span class="s3">return </span><span class="s1">logbasechange(</span><span class="s4">2</span><span class="s3">, </span><span class="s1">logbase) * genent</span>
        <span class="s3">return </span><span class="s1">genent</span>
    <span class="s3">elif </span><span class="s5">'inf' </span><span class="s3">in </span><span class="s1">str(alpha).lower() </span><span class="s3">or </span><span class="s1">alpha == np.inf:</span>
        <span class="s3">return </span><span class="s1">-np.log(np.max(px))</span>

    <span class="s2"># gets here if alpha != (1 or inf)</span>
    <span class="s1">px = px**alpha</span>
    <span class="s1">genent = np.log(px.sum())</span>
    <span class="s3">if </span><span class="s1">logbase == </span><span class="s4">2</span><span class="s1">:</span>
        <span class="s3">return </span><span class="s4">1</span><span class="s1">/(</span><span class="s4">1</span><span class="s1">-alpha) * genent</span>
    <span class="s3">else</span><span class="s1">:</span>
        <span class="s3">return </span><span class="s4">1</span><span class="s1">/(</span><span class="s4">1</span><span class="s1">-alpha) * logbasechange(</span><span class="s4">2</span><span class="s3">, </span><span class="s1">logbase) * genent</span>

<span class="s2">#TODO: before completing this, need to rethink the organization of</span>
<span class="s2"># (relative) entropy measures, ie., all put into one function</span>
<span class="s2"># and have kwdargs, etc.?</span>
<span class="s3">def </span><span class="s1">gencrossentropy(px</span><span class="s3">,</span><span class="s1">py</span><span class="s3">,</span><span class="s1">pxpy</span><span class="s3">,</span><span class="s1">alpha=</span><span class="s4">1</span><span class="s3">,</span><span class="s1">logbase=</span><span class="s4">2</span><span class="s3">, </span><span class="s1">measure=</span><span class="s5">'T'</span><span class="s1">):</span>
    <span class="s0">&quot;&quot;&quot; 
    Generalized cross-entropy measures. 
 
    Parameters 
    ---------- 
    px : array_like 
        Discrete probability distribution of random variable X 
    py : array_like 
        Discrete probability distribution of random variable Y 
    pxpy : 2d array_like, optional 
        Joint probability distribution of X and Y.  If pxpy is None, X and Y 
        are assumed to be independent. 
    logbase : int or np.e, optional 
        Default is 2 (bits) 
    measure : str, optional 
        The measure is the type of generalized cross-entropy desired. 'T' is 
        the cross-entropy version of the Tsallis measure.  'CR' is Cressie-Read 
        measure. 
    &quot;&quot;&quot;</span>


<span class="s3">if </span><span class="s1">__name__ == </span><span class="s5">&quot;__main__&quot;</span><span class="s1">:</span>
    <span class="s1">print(</span><span class="s5">&quot;From Golan (2008) </span><span class="s3">\&quot;</span><span class="s5">Information and Entropy Econometrics -- A Review </span><span class="s3">\ 
</span><span class="s5">and Synthesis&quot;</span><span class="s1">)</span>
    <span class="s1">print(</span><span class="s5">&quot;Table 3.1&quot;</span><span class="s1">)</span>
    <span class="s2"># Examples from Golan (2008)</span>

    <span class="s1">X = [</span><span class="s4">.2</span><span class="s3">,</span><span class="s4">.2</span><span class="s3">,</span><span class="s4">.2</span><span class="s3">,</span><span class="s4">.2</span><span class="s3">,</span><span class="s4">.2</span><span class="s1">]</span>
    <span class="s1">Y = [</span><span class="s4">.322</span><span class="s3">,</span><span class="s4">.072</span><span class="s3">,</span><span class="s4">.511</span><span class="s3">,</span><span class="s4">.091</span><span class="s3">,</span><span class="s4">.004</span><span class="s1">]</span>

    <span class="s3">for </span><span class="s1">i </span><span class="s3">in </span><span class="s1">X:</span>
        <span class="s1">print(shannoninfo(i))</span>
    <span class="s3">for </span><span class="s1">i </span><span class="s3">in </span><span class="s1">Y:</span>
        <span class="s1">print(shannoninfo(i))</span>
    <span class="s1">print(shannonentropy(X))</span>
    <span class="s1">print(shannonentropy(Y))</span>

    <span class="s1">p = [</span><span class="s4">1e-5</span><span class="s3">,</span><span class="s4">1e-4</span><span class="s3">,</span><span class="s4">.001</span><span class="s3">,</span><span class="s4">.01</span><span class="s3">,</span><span class="s4">.1</span><span class="s3">,</span><span class="s4">.15</span><span class="s3">,</span><span class="s4">.2</span><span class="s3">,</span><span class="s4">.25</span><span class="s3">,</span><span class="s4">.3</span><span class="s3">,</span><span class="s4">.35</span><span class="s3">,</span><span class="s4">.4</span><span class="s3">,</span><span class="s4">.45</span><span class="s3">,</span><span class="s4">.5</span><span class="s1">]</span>

    <span class="s1">plt.subplot(</span><span class="s4">111</span><span class="s1">)</span>
    <span class="s1">plt.ylabel(</span><span class="s5">&quot;Information&quot;</span><span class="s1">)</span>
    <span class="s1">plt.xlabel(</span><span class="s5">&quot;Probability&quot;</span><span class="s1">)</span>
    <span class="s1">x = np.linspace(</span><span class="s4">0</span><span class="s3">,</span><span class="s4">1</span><span class="s3">,</span><span class="s4">100001</span><span class="s1">)</span>
    <span class="s1">plt.plot(x</span><span class="s3">, </span><span class="s1">shannoninfo(x))</span>
<span class="s2">#    plt.show()</span>

    <span class="s1">plt.subplot(</span><span class="s4">111</span><span class="s1">)</span>
    <span class="s1">plt.ylabel(</span><span class="s5">&quot;Entropy&quot;</span><span class="s1">)</span>
    <span class="s1">plt.xlabel(</span><span class="s5">&quot;Probability&quot;</span><span class="s1">)</span>
    <span class="s1">x = np.linspace(</span><span class="s4">0</span><span class="s3">,</span><span class="s4">1</span><span class="s3">,</span><span class="s4">101</span><span class="s1">)</span>
    <span class="s1">plt.plot(x</span><span class="s3">, </span><span class="s1">lmap(shannonentropy</span><span class="s3">, </span><span class="s1">lzip(x</span><span class="s3">,</span><span class="s4">1</span><span class="s1">-x)))</span>
<span class="s2">#    plt.show()</span>

    <span class="s2"># define a joint probability distribution</span>
    <span class="s2"># from Golan (2008) table 3.3</span>
    <span class="s1">w = np.array([[</span><span class="s4">0</span><span class="s3">,</span><span class="s4">0</span><span class="s3">,</span><span class="s4">1.</span><span class="s1">/</span><span class="s4">3</span><span class="s1">]</span><span class="s3">,</span><span class="s1">[</span><span class="s4">1</span><span class="s1">/</span><span class="s4">9.</span><span class="s3">,</span><span class="s4">1</span><span class="s1">/</span><span class="s4">9.</span><span class="s3">,</span><span class="s4">1</span><span class="s1">/</span><span class="s4">9.</span><span class="s1">]</span><span class="s3">,</span><span class="s1">[</span><span class="s4">1</span><span class="s1">/</span><span class="s4">18.</span><span class="s3">,</span><span class="s4">1</span><span class="s1">/</span><span class="s4">9.</span><span class="s3">,</span><span class="s4">1</span><span class="s1">/</span><span class="s4">6.</span><span class="s1">]])</span>
    <span class="s2"># table 3.4</span>
    <span class="s1">px = w.sum(</span><span class="s4">0</span><span class="s1">)</span>
    <span class="s1">py = w.sum(</span><span class="s4">1</span><span class="s1">)</span>
    <span class="s1">H_X = shannonentropy(px)</span>
    <span class="s1">H_Y = shannonentropy(py)</span>
    <span class="s1">H_XY = shannonentropy(w)</span>
    <span class="s1">H_XgivenY = condentropy(px</span><span class="s3">,</span><span class="s1">py</span><span class="s3">,</span><span class="s1">w)</span>
    <span class="s1">H_YgivenX = condentropy(py</span><span class="s3">,</span><span class="s1">px</span><span class="s3">,</span><span class="s1">w)</span>
<span class="s2"># note that cross-entropy is not a distance measure as the following shows</span>
    <span class="s1">D_YX = logbasechange(</span><span class="s4">2</span><span class="s3">,</span><span class="s1">np.e)*stats.entropy(px</span><span class="s3">, </span><span class="s1">py)</span>
    <span class="s1">D_XY = logbasechange(</span><span class="s4">2</span><span class="s3">,</span><span class="s1">np.e)*stats.entropy(py</span><span class="s3">, </span><span class="s1">px)</span>
    <span class="s1">I_XY = mutualinfo(px</span><span class="s3">,</span><span class="s1">py</span><span class="s3">,</span><span class="s1">w)</span>
    <span class="s1">print(</span><span class="s5">&quot;Table 3.3&quot;</span><span class="s1">)</span>
    <span class="s1">print(H_X</span><span class="s3">,</span><span class="s1">H_Y</span><span class="s3">, </span><span class="s1">H_XY</span><span class="s3">, </span><span class="s1">H_XgivenY</span><span class="s3">, </span><span class="s1">H_YgivenX</span><span class="s3">, </span><span class="s1">D_YX</span><span class="s3">, </span><span class="s1">D_XY</span><span class="s3">, </span><span class="s1">I_XY)</span>

    <span class="s1">print(</span><span class="s5">&quot;discretize functions&quot;</span><span class="s1">)</span>
    <span class="s1">X=np.array([</span><span class="s4">21.2</span><span class="s3">,</span><span class="s4">44.5</span><span class="s3">,</span><span class="s4">31.0</span><span class="s3">,</span><span class="s4">19.5</span><span class="s3">,</span><span class="s4">40.6</span><span class="s3">,</span><span class="s4">38.7</span><span class="s3">,</span><span class="s4">11.1</span><span class="s3">,</span><span class="s4">15.8</span><span class="s3">,</span><span class="s4">31.9</span><span class="s3">,</span><span class="s4">25.8</span><span class="s3">,</span><span class="s4">20.2</span><span class="s3">,</span><span class="s4">14.2</span><span class="s3">,</span>
        <span class="s4">24.0</span><span class="s3">,</span><span class="s4">21.0</span><span class="s3">,</span><span class="s4">11.3</span><span class="s3">,</span><span class="s4">18.0</span><span class="s3">,</span><span class="s4">16.3</span><span class="s3">,</span><span class="s4">22.2</span><span class="s3">,</span><span class="s4">7.8</span><span class="s3">,</span><span class="s4">27.8</span><span class="s3">,</span><span class="s4">16.3</span><span class="s3">,</span><span class="s4">35.1</span><span class="s3">,</span><span class="s4">14.9</span><span class="s3">,</span><span class="s4">17.1</span><span class="s3">,</span><span class="s4">28.2</span><span class="s3">,</span><span class="s4">16.4</span><span class="s3">,</span>
        <span class="s4">16.5</span><span class="s3">,</span><span class="s4">46.0</span><span class="s3">,</span><span class="s4">9.5</span><span class="s3">,</span><span class="s4">18.8</span><span class="s3">,</span><span class="s4">32.1</span><span class="s3">,</span><span class="s4">26.1</span><span class="s3">,</span><span class="s4">16.1</span><span class="s3">,</span><span class="s4">7.3</span><span class="s3">,</span><span class="s4">21.4</span><span class="s3">,</span><span class="s4">20.0</span><span class="s3">,</span><span class="s4">29.3</span><span class="s3">,</span><span class="s4">14.9</span><span class="s3">,</span><span class="s4">8.3</span><span class="s3">,</span><span class="s4">22.5</span><span class="s3">,</span>
        <span class="s4">12.8</span><span class="s3">,</span><span class="s4">26.9</span><span class="s3">,</span><span class="s4">25.5</span><span class="s3">,</span><span class="s4">22.9</span><span class="s3">,</span><span class="s4">11.2</span><span class="s3">,</span><span class="s4">20.7</span><span class="s3">,</span><span class="s4">26.2</span><span class="s3">,</span><span class="s4">9.3</span><span class="s3">,</span><span class="s4">10.8</span><span class="s3">,</span><span class="s4">15.6</span><span class="s1">])</span>
    <span class="s1">discX = discretize(X)</span>
    <span class="s2">#CF: R's infotheo</span>
<span class="s2">#TODO: compare to pyentropy quantize?</span>
    <span class="s1">print</span>
    <span class="s1">print(</span><span class="s5">&quot;Example in section 3.6 of Golan, using table 3.3&quot;</span><span class="s1">)</span>
    <span class="s1">print(</span><span class="s5">&quot;Bounding errors using Fano's inequality&quot;</span><span class="s1">)</span>
    <span class="s1">print(</span><span class="s5">&quot;H(P_{e}) + P_{e}log(K-1) &gt;= H(X|Y)&quot;</span><span class="s1">)</span>
    <span class="s1">print(</span><span class="s5">&quot;or, a weaker inequality&quot;</span><span class="s1">)</span>
    <span class="s1">print(</span><span class="s5">&quot;P_{e} &gt;= [H(X|Y) - 1]/log(K)&quot;</span><span class="s1">)</span>
    <span class="s1">print(</span><span class="s5">&quot;P(x) = %s&quot; </span><span class="s1">% px)</span>
    <span class="s1">print(</span><span class="s5">&quot;X = 3 has the highest probability, so this is the estimate Xhat&quot;</span><span class="s1">)</span>
    <span class="s1">pe = </span><span class="s4">1 </span><span class="s1">- px[</span><span class="s4">2</span><span class="s1">]</span>
    <span class="s1">print(</span><span class="s5">&quot;The probability of error Pe is 1 - p(X=3) = %0.4g&quot; </span><span class="s1">% pe)</span>
    <span class="s1">H_pe = shannonentropy([pe</span><span class="s3">,</span><span class="s4">1</span><span class="s1">-pe])</span>
    <span class="s1">print(</span><span class="s5">&quot;H(Pe) = %0.4g and K=3&quot; </span><span class="s1">% H_pe)</span>
    <span class="s1">print(</span><span class="s5">&quot;H(Pe) + Pe*log(K-1) = %0.4g &gt;= H(X|Y) = %0.4g&quot; </span><span class="s1">% \</span>
            <span class="s1">(H_pe+pe*np.log2(</span><span class="s4">2</span><span class="s1">)</span><span class="s3">, </span><span class="s1">H_XgivenY))</span>
    <span class="s1">print(</span><span class="s5">&quot;or using the weaker inequality&quot;</span><span class="s1">)</span>
    <span class="s1">print(</span><span class="s5">&quot;Pe = %0.4g &gt;= [H(X) - 1]/log(K) = %0.4g&quot; </span><span class="s1">% (pe</span><span class="s3">, </span><span class="s1">(H_X - </span><span class="s4">1</span><span class="s1">)/np.log2(</span><span class="s4">3</span><span class="s1">)))</span>
    <span class="s1">print(</span><span class="s5">&quot;Consider now, table 3.5, where there is additional information&quot;</span><span class="s1">)</span>
    <span class="s1">print(</span><span class="s5">&quot;The conditional probabilities of P(X|Y=y) are &quot;</span><span class="s1">)</span>
    <span class="s1">w2 = np.array([[</span><span class="s4">0.</span><span class="s3">,</span><span class="s4">0.</span><span class="s3">,</span><span class="s4">1.</span><span class="s1">]</span><span class="s3">,</span><span class="s1">[</span><span class="s4">1</span><span class="s1">/</span><span class="s4">3.</span><span class="s3">,</span><span class="s4">1</span><span class="s1">/</span><span class="s4">3.</span><span class="s3">,</span><span class="s4">1</span><span class="s1">/</span><span class="s4">3.</span><span class="s1">]</span><span class="s3">,</span><span class="s1">[</span><span class="s4">1</span><span class="s1">/</span><span class="s4">6.</span><span class="s3">,</span><span class="s4">1</span><span class="s1">/</span><span class="s4">3.</span><span class="s3">,</span><span class="s4">1</span><span class="s1">/</span><span class="s4">2.</span><span class="s1">]])</span>
    <span class="s1">print(w2)</span>
<span class="s2"># not a proper distribution?</span>
    <span class="s1">print(</span><span class="s5">&quot;The probability of error given this information is&quot;</span><span class="s1">)</span>
    <span class="s1">print(</span><span class="s5">&quot;Pe = [H(X|Y) -1]/log(K) = %0.4g&quot; </span><span class="s1">% ((np.mean([</span><span class="s4">0</span><span class="s3">,</span><span class="s1">shannonentropy(w2[</span><span class="s4">1</span><span class="s1">])</span><span class="s3">,</span><span class="s1">shannonentropy(w2[</span><span class="s4">2</span><span class="s1">])])-</span><span class="s4">1</span><span class="s1">)/np.log2(</span><span class="s4">3</span><span class="s1">)))</span>
    <span class="s1">print(</span><span class="s5">&quot;such that more information lowers the error&quot;</span><span class="s1">)</span>

<span class="s2">### Stochastic processes</span>
    <span class="s1">markovchain = np.array([[</span><span class="s4">.553</span><span class="s3">,</span><span class="s4">.284</span><span class="s3">,</span><span class="s4">.163</span><span class="s1">]</span><span class="s3">,</span><span class="s1">[</span><span class="s4">.465</span><span class="s3">,</span><span class="s4">.312</span><span class="s3">,</span><span class="s4">.223</span><span class="s1">]</span><span class="s3">,</span><span class="s1">[</span><span class="s4">.420</span><span class="s3">,</span><span class="s4">.322</span><span class="s3">,</span><span class="s4">.258</span><span class="s1">]])</span>
</pre>
</body>
</html>