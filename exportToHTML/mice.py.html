<html>
<head>
<title>mice.py</title>
<meta http-equiv="Content-Type" content="text/html; charset=utf-8">
<style type="text/css">
.s0 { color: #629755; font-style: italic;}
.s1 { color: #a9b7c6;}
.s2 { color: #cc7832;}
.s3 { color: #6a8759;}
.s4 { color: #6897bb;}
.s5 { color: #808080;}
</style>
</head>
<body bgcolor="#2b2b2b">
<table CELLSPACING=0 CELLPADDING=5 COLS=1 WIDTH="100%" BGCOLOR="#606060" >
<tr><td><center>
<font face="Arial, Helvetica" color="#000000">
mice.py</font>
</center></td></tr></table>
<pre><span class="s0">&quot;&quot;&quot; 
Overview 
-------- 
 
This module implements the Multiple Imputation through Chained 
Equations (MICE) approach to handling missing data in statistical data 
analyses. The approach has the following steps: 
 
0. Impute each missing value with the mean of the observed values of 
the same variable. 
 
1. For each variable in the data set with missing values (termed the 
'focus variable'), do the following: 
 
1a. Fit an 'imputation model', which is a regression model for the 
focus variable, regressed on the observed and (current) imputed values 
of some or all of the other variables. 
 
1b. Impute the missing values for the focus variable.  Currently this 
imputation must use the 'predictive mean matching' (pmm) procedure. 
 
2. Once all variables have been imputed, fit the 'analysis model' to 
the data set. 
 
3. Repeat steps 1-2 multiple times and combine the results using a 
'combining rule' to produce point estimates of all parameters in the 
analysis model and standard errors for them. 
 
The imputations for each variable are based on an imputation model 
that is specified via a model class and a formula for the regression 
relationship.  The default model is OLS, with a formula specifying 
main effects for all other variables. 
 
The MICE procedure can be used in one of two ways: 
 
* If the goal is only to produce imputed data sets, the MICEData class 
can be used to wrap a data frame, providing facilities for doing the 
imputation.  Summary plots are available for assessing the performance 
of the imputation. 
 
* If the imputed data sets are to be used to fit an additional 
'analysis model', a MICE instance can be used.  After specifying the 
MICE instance and running it, the results are combined using the 
`combine` method.  Results and various summary plots are then 
available. 
 
Terminology 
----------- 
 
The primary goal of the analysis is usually to fit and perform 
inference using an 'analysis model'. If an analysis model is not 
specified, then imputed datasets are produced for later use. 
 
The MICE procedure involves a family of imputation models.  There is 
one imputation model for each variable with missing values.  An 
imputation model may be conditioned on all or a subset of the 
remaining variables, using main effects, transformations, 
interactions, etc. as desired. 
 
A 'perturbation method' is a method for setting the parameter estimate 
in an imputation model.  The 'gaussian' perturbation method first fits 
the model (usually using maximum likelihood, but it could use any 
statsmodels fit procedure), then sets the parameter vector equal to a 
draw from the Gaussian approximation to the sampling distribution for 
the fit.  The 'bootstrap' perturbation method sets the parameter 
vector equal to a fitted parameter vector obtained when fitting the 
conditional model to a bootstrapped version of the data set. 
 
Class structure 
--------------- 
 
There are two main classes in the module: 
 
* 'MICEData' wraps a Pandas dataframe, incorporating information about 
  the imputation model for each variable with missing values. It can 
  be used to produce multiply imputed data sets that are to be further 
  processed or distributed to other researchers.  A number of plotting 
  procedures are provided to visualize the imputation results and 
  missing data patterns.  The `history_func` hook allows any features 
  of interest of the imputed data sets to be saved for further 
  analysis. 
 
* 'MICE' takes both a 'MICEData' object and an analysis model 
  specification.  It runs the multiple imputation, fits the analysis 
  models, and combines the results to produce a `MICEResults` object. 
  The summary method of this results object can be used to see the key 
  estimands and inferential quantities. 
 
Notes 
----- 
 
By default, to conserve memory 'MICEData' saves very little 
information from one iteration to the next.  The data set passed by 
the user is copied on entry, but then is over-written each time new 
imputations are produced.  If using 'MICE', the fitted 
analysis models and results are saved.  MICEData includes a 
`history_callback` hook that allows arbitrary information from the 
intermediate datasets to be saved for future use. 
 
References 
---------- 
 
JL Schafer: 'Multiple Imputation: A Primer', Stat Methods Med Res, 
1999. 
 
TE Raghunathan et al.: 'A Multivariate Technique for Multiply 
Imputing Missing Values Using a Sequence of Regression Models', Survey 
Methodology, 2001. 
 
SAS Institute: 'Predictive Mean Matching Method for Monotone Missing 
Data', SAS 9.2 User's Guide, 2014. 
 
A Gelman et al.: 'Multiple Imputation with Diagnostics (mi) in R: 
Opening Windows into the Black Box', Journal of Statistical Software, 
2009. 
&quot;&quot;&quot;</span>

<span class="s2">import </span><span class="s1">pandas </span><span class="s2">as </span><span class="s1">pd</span>
<span class="s2">import </span><span class="s1">numpy </span><span class="s2">as </span><span class="s1">np</span>
<span class="s2">import </span><span class="s1">patsy</span>
<span class="s2">from </span><span class="s1">statsmodels.base.model </span><span class="s2">import </span><span class="s1">LikelihoodModelResults</span>
<span class="s2">from </span><span class="s1">statsmodels.regression.linear_model </span><span class="s2">import </span><span class="s1">OLS</span>
<span class="s2">from </span><span class="s1">collections </span><span class="s2">import </span><span class="s1">defaultdict</span>


<span class="s1">_mice_data_example_1 = </span><span class="s3">&quot;&quot;&quot; 
    &gt;&gt;&gt; imp = mice.MICEData(data) 
    &gt;&gt;&gt; imp.set_imputer('x1', formula='x2 + np.square(x2) + x3') 
    &gt;&gt;&gt; for j in range(20): 
    ...     imp.update_all() 
    ...     imp.data.to_csv('data%02d.csv' % j)&quot;&quot;&quot;</span>


<span class="s2">class </span><span class="s1">PatsyFormula:</span>
    <span class="s0">&quot;&quot;&quot; 
    A simple wrapper for a string to be interpreted as a Patsy formula. 
    &quot;&quot;&quot;</span>
    <span class="s2">def </span><span class="s1">__init__(self</span><span class="s2">, </span><span class="s1">formula):</span>
        <span class="s1">self.formula = </span><span class="s3">&quot;0 + &quot; </span><span class="s1">+ formula</span>


<span class="s2">class </span><span class="s1">MICEData:</span>

    <span class="s1">__doc__ = </span><span class="s3">&quot;&quot;&quot;</span><span class="s2">\ 
    </span><span class="s3">Wrap a data set to allow missing data handling with MICE. 
 
    Parameters 
    ---------- 
    data : Pandas data frame 
        The data set, which is copied internally. 
    perturbation_method : str 
        The default perturbation method 
    k_pmm : int 
        The number of nearest neighbors to use during predictive mean 
        matching.  Can also be specified in `fit`. 
    history_callback : function 
        A function that is called after each complete imputation 
        cycle.  The return value is appended to `history`.  The 
        MICEData object is passed as the sole argument to 
        `history_callback`. 
 
    Notes 
    ----- 
    Allowed perturbation methods are 'gaussian' (the model parameters 
    are set to a draw from the Gaussian approximation to the posterior 
    distribution), and 'boot' (the model parameters are set to the 
    estimated values obtained when fitting a bootstrapped version of 
    the data set). 
 
    `history_callback` can be implemented to have side effects such as 
    saving the current imputed data set to disk. 
 
    Examples 
    -------- 
    Draw 20 imputations from a data set called `data` and save them in 
    separate files with filename pattern `dataXX.csv`.  The variables 
    other than `x1` are imputed using linear models fit with OLS, with 
    mean structures containing main effects of all other variables in 
    `data`.  The variable named `x1` has a conditional mean structure 
    that includes an additional term for x2^2. 
    %(_mice_data_example_1)s 
    &quot;&quot;&quot; </span><span class="s1">% {</span><span class="s3">'_mice_data_example_1'</span><span class="s1">: _mice_data_example_1}</span>

    <span class="s2">def </span><span class="s1">__init__(self</span><span class="s2">, </span><span class="s1">data</span><span class="s2">, </span><span class="s1">perturbation_method=</span><span class="s3">'gaussian'</span><span class="s2">,</span>
                 <span class="s1">k_pmm=</span><span class="s4">20</span><span class="s2">, </span><span class="s1">history_callback=</span><span class="s2">None</span><span class="s1">):</span>

        <span class="s2">if </span><span class="s1">data.columns.dtype != np.dtype(</span><span class="s3">'O'</span><span class="s1">):</span>
            <span class="s1">msg = </span><span class="s3">&quot;MICEData data column names should be string type&quot;</span>
            <span class="s2">raise </span><span class="s1">ValueError(msg)</span>

        <span class="s1">self.regularized = dict()</span>

        <span class="s5"># Drop observations where all variables are missing.  This</span>
        <span class="s5"># also has the effect of copying the data frame.</span>
        <span class="s1">self.data = data.dropna(how=</span><span class="s3">'all'</span><span class="s1">).reset_index(drop=</span><span class="s2">True</span><span class="s1">)</span>

        <span class="s1">self.history_callback = history_callback</span>
        <span class="s1">self.history = []</span>
        <span class="s1">self.predict_kwds = {}</span>

        <span class="s5"># Assign the same perturbation method for all variables.</span>
        <span class="s5"># Can be overridden when calling 'set_imputer'.</span>
        <span class="s1">self.perturbation_method = defaultdict(</span><span class="s2">lambda</span><span class="s1">:</span>
                                               <span class="s1">perturbation_method)</span>

        <span class="s5"># Map from variable name to indices of observed/missing</span>
        <span class="s5"># values.</span>
        <span class="s1">self.ix_obs = {}</span>
        <span class="s1">self.ix_miss = {}</span>
        <span class="s2">for </span><span class="s1">col </span><span class="s2">in </span><span class="s1">self.data.columns:</span>
            <span class="s1">ix_obs</span><span class="s2">, </span><span class="s1">ix_miss = self._split_indices(self.data[col])</span>
            <span class="s1">self.ix_obs[col] = ix_obs</span>
            <span class="s1">self.ix_miss[col] = ix_miss</span>

        <span class="s5"># Most recent model instance and results instance for each variable.</span>
        <span class="s1">self.models = {}</span>
        <span class="s1">self.results = {}</span>

        <span class="s5"># Map from variable names to the conditional formula.</span>
        <span class="s1">self.conditional_formula = {}</span>

        <span class="s5"># Map from variable names to init/fit args of the conditional</span>
        <span class="s5"># models.</span>
        <span class="s1">self.init_kwds = defaultdict(dict)</span>
        <span class="s1">self.fit_kwds = defaultdict(dict)</span>

        <span class="s5"># Map from variable names to the model class.</span>
        <span class="s1">self.model_class = {}</span>

        <span class="s5"># Map from variable names to most recent params update.</span>
        <span class="s1">self.params = {}</span>

        <span class="s5"># Set default imputers.</span>
        <span class="s2">for </span><span class="s1">vname </span><span class="s2">in </span><span class="s1">data.columns:</span>
            <span class="s1">self.set_imputer(vname)</span>

        <span class="s5"># The order in which variables are imputed in each cycle.</span>
        <span class="s5"># Impute variables with the fewest missing values first.</span>
        <span class="s1">vnames = list(data.columns)</span>
        <span class="s1">nmiss = [len(self.ix_miss[v]) </span><span class="s2">for </span><span class="s1">v </span><span class="s2">in </span><span class="s1">vnames]</span>
        <span class="s1">nmiss = np.asarray(nmiss)</span>
        <span class="s1">ii = np.argsort(nmiss)</span>
        <span class="s1">ii = ii[sum(nmiss == </span><span class="s4">0</span><span class="s1">):]</span>
        <span class="s1">self._cycle_order = [vnames[i] </span><span class="s2">for </span><span class="s1">i </span><span class="s2">in </span><span class="s1">ii]</span>

        <span class="s1">self._initial_imputation()</span>

        <span class="s1">self.k_pmm = k_pmm</span>

    <span class="s2">def </span><span class="s1">next_sample(self):</span>
        <span class="s0">&quot;&quot;&quot; 
        Returns the next imputed dataset in the imputation process. 
 
        Returns 
        ------- 
        data : array_like 
            An imputed dataset from the MICE chain. 
 
        Notes 
        ----- 
        `MICEData` does not have a `skip` parameter.  Consecutive 
        values returned by `next_sample` are immediately consecutive 
        in the imputation chain. 
 
        The returned value is a reference to the data attribute of 
        the class and should be copied before making any changes. 
        &quot;&quot;&quot;</span>

        <span class="s1">self.update_all(</span><span class="s4">1</span><span class="s1">)</span>
        <span class="s2">return </span><span class="s1">self.data</span>

    <span class="s2">def </span><span class="s1">_initial_imputation(self):</span>
        <span class="s0">&quot;&quot;&quot; 
        Use a PMM-like procedure for initial imputed values. 
 
        For each variable, missing values are imputed as the observed 
        value that is closest to the mean over all observed values. 
        &quot;&quot;&quot;</span>
        <span class="s5"># Changed for pandas 2.0 copy-on-write behavior to use a single</span>
        <span class="s5"># in-place fill</span>
        <span class="s1">imp_values = {}</span>
        <span class="s2">for </span><span class="s1">col </span><span class="s2">in </span><span class="s1">self.data.columns:</span>
            <span class="s1">di = self.data[col] - self.data[col].mean()</span>
            <span class="s1">di = np.abs(di)</span>
            <span class="s1">ix = di.idxmin()</span>
            <span class="s1">imp_values[col] = self.data[col].loc[ix]</span>
        <span class="s1">self.data.fillna(imp_values</span><span class="s2">, </span><span class="s1">inplace=</span><span class="s2">True</span><span class="s1">)</span>

    <span class="s2">def </span><span class="s1">_split_indices(self</span><span class="s2">, </span><span class="s1">vec):</span>
        <span class="s1">null = pd.isnull(vec)</span>
        <span class="s1">ix_obs = np.flatnonzero(~null)</span>
        <span class="s1">ix_miss = np.flatnonzero(null)</span>
        <span class="s2">if </span><span class="s1">len(ix_obs) == </span><span class="s4">0</span><span class="s1">:</span>
            <span class="s2">raise </span><span class="s1">ValueError(</span><span class="s3">&quot;variable to be imputed has no observed values&quot;</span><span class="s1">)</span>
        <span class="s2">return </span><span class="s1">ix_obs</span><span class="s2">, </span><span class="s1">ix_miss</span>

    <span class="s2">def </span><span class="s1">set_imputer(self</span><span class="s2">, </span><span class="s1">endog_name</span><span class="s2">, </span><span class="s1">formula=</span><span class="s2">None, </span><span class="s1">model_class=</span><span class="s2">None,</span>
                    <span class="s1">init_kwds=</span><span class="s2">None, </span><span class="s1">fit_kwds=</span><span class="s2">None, </span><span class="s1">predict_kwds=</span><span class="s2">None,</span>
                    <span class="s1">k_pmm=</span><span class="s4">20</span><span class="s2">, </span><span class="s1">perturbation_method=</span><span class="s2">None, </span><span class="s1">regularized=</span><span class="s2">False</span><span class="s1">):</span>
        <span class="s0">&quot;&quot;&quot; 
        Specify the imputation process for a single variable. 
 
        Parameters 
        ---------- 
        endog_name : str 
            Name of the variable to be imputed. 
        formula : str 
            Conditional formula for imputation. Defaults to a formula 
            with main effects for all other variables in dataset.  The 
            formula should only include an expression for the mean 
            structure, e.g. use 'x1 + x2' not 'x4 ~ x1 + x2'. 
        model_class : statsmodels model 
            Conditional model for imputation. Defaults to OLS.  See below 
            for more information. 
        init_kwds : dit-like 
            Keyword arguments passed to the model init method. 
        fit_kwds : dict-like 
            Keyword arguments passed to the model fit method. 
        predict_kwds : dict-like 
            Keyword arguments passed to the model predict method. 
        k_pmm : int 
            Determines number of neighboring observations from which 
            to randomly sample when using predictive mean matching. 
        perturbation_method : str 
            Either 'gaussian' or 'bootstrap'. Determines the method 
            for perturbing parameters in the imputation model.  If 
            None, uses the default specified at class initialization. 
        regularized : dict 
            If regularized[name]=True, `fit_regularized` rather than 
            `fit` is called when fitting imputation models for this 
            variable.  When regularized[name]=True for any variable, 
            perturbation_method must be set to boot. 
 
        Notes 
        ----- 
        The model class must meet the following conditions: 
            * A model must have a 'fit' method that returns an object. 
            * The object returned from `fit` must have a `params` attribute 
              that is an array-like object. 
            * The object returned from `fit` must have a cov_params method 
              that returns a square array-like object. 
            * The model must have a `predict` method. 
        &quot;&quot;&quot;</span>

        <span class="s2">if </span><span class="s1">formula </span><span class="s2">is None</span><span class="s1">:</span>
            <span class="s1">main_effects = [x </span><span class="s2">for </span><span class="s1">x </span><span class="s2">in </span><span class="s1">self.data.columns</span>
                            <span class="s2">if </span><span class="s1">x != endog_name]</span>
            <span class="s1">fml = endog_name + </span><span class="s3">&quot; ~ &quot; </span><span class="s1">+ </span><span class="s3">&quot; + &quot;</span><span class="s1">.join(main_effects)</span>
            <span class="s1">self.conditional_formula[endog_name] = fml</span>
        <span class="s2">else</span><span class="s1">:</span>
            <span class="s1">fml = endog_name + </span><span class="s3">&quot; ~ &quot; </span><span class="s1">+ formula</span>
            <span class="s1">self.conditional_formula[endog_name] = fml</span>

        <span class="s2">if </span><span class="s1">model_class </span><span class="s2">is None</span><span class="s1">:</span>
            <span class="s1">self.model_class[endog_name] = OLS</span>
        <span class="s2">else</span><span class="s1">:</span>
            <span class="s1">self.model_class[endog_name] = model_class</span>

        <span class="s2">if </span><span class="s1">init_kwds </span><span class="s2">is not None</span><span class="s1">:</span>
            <span class="s1">self.init_kwds[endog_name] = init_kwds</span>

        <span class="s2">if </span><span class="s1">fit_kwds </span><span class="s2">is not None</span><span class="s1">:</span>
            <span class="s1">self.fit_kwds[endog_name] = fit_kwds</span>

        <span class="s2">if </span><span class="s1">predict_kwds </span><span class="s2">is not None</span><span class="s1">:</span>
            <span class="s1">self.predict_kwds[endog_name] = predict_kwds</span>

        <span class="s2">if </span><span class="s1">perturbation_method </span><span class="s2">is not None</span><span class="s1">:</span>
            <span class="s1">self.perturbation_method[endog_name] = perturbation_method</span>

        <span class="s1">self.k_pmm = k_pmm</span>
        <span class="s1">self.regularized[endog_name] = regularized</span>

    <span class="s2">def </span><span class="s1">_store_changes(self</span><span class="s2">, </span><span class="s1">col</span><span class="s2">, </span><span class="s1">vals):</span>
        <span class="s0">&quot;&quot;&quot; 
        Fill in dataset with imputed values. 
 
        Parameters 
        ---------- 
        col : str 
            Name of variable to be filled in. 
        vals : ndarray 
            Array of imputed values to use for filling-in missing values. 
        &quot;&quot;&quot;</span>

        <span class="s1">ix = self.ix_miss[col]</span>
        <span class="s2">if </span><span class="s1">len(ix) &gt; </span><span class="s4">0</span><span class="s1">:</span>
            <span class="s1">self.data.iloc[ix</span><span class="s2">, </span><span class="s1">self.data.columns.get_loc(col)] = np.atleast_1d(vals)</span>

    <span class="s2">def </span><span class="s1">update_all(self</span><span class="s2">, </span><span class="s1">n_iter=</span><span class="s4">1</span><span class="s1">):</span>
        <span class="s0">&quot;&quot;&quot; 
        Perform a specified number of MICE iterations. 
 
        Parameters 
        ---------- 
        n_iter : int 
            The number of updates to perform.  Only the result of the 
            final update will be available. 
 
        Notes 
        ----- 
        The imputed values are stored in the class attribute `self.data`. 
        &quot;&quot;&quot;</span>

        <span class="s2">for </span><span class="s1">k </span><span class="s2">in </span><span class="s1">range(n_iter):</span>
            <span class="s2">for </span><span class="s1">vname </span><span class="s2">in </span><span class="s1">self._cycle_order:</span>
                <span class="s1">self.update(vname)</span>

        <span class="s2">if </span><span class="s1">self.history_callback </span><span class="s2">is not None</span><span class="s1">:</span>
            <span class="s1">hv = self.history_callback(self)</span>
            <span class="s1">self.history.append(hv)</span>

    <span class="s2">def </span><span class="s1">get_split_data(self</span><span class="s2">, </span><span class="s1">vname):</span>
        <span class="s0">&quot;&quot;&quot; 
        Return endog and exog for imputation of a given variable. 
 
        Parameters 
        ---------- 
        vname : str 
           The variable for which the split data is returned. 
 
        Returns 
        ------- 
        endog_obs : DataFrame 
            Observed values of the variable to be imputed. 
        exog_obs : DataFrame 
            Current values of the predictors where the variable to be 
            imputed is observed. 
        exog_miss : DataFrame 
            Current values of the predictors where the variable to be 
            Imputed is missing. 
        init_kwds : dict-like 
            The init keyword arguments for `vname`, processed through Patsy 
            as required. 
        fit_kwds : dict-like 
            The fit keyword arguments for `vname`, processed through Patsy 
            as required. 
        &quot;&quot;&quot;</span>

        <span class="s1">formula = self.conditional_formula[vname]</span>
        <span class="s1">endog</span><span class="s2">, </span><span class="s1">exog = patsy.dmatrices(formula</span><span class="s2">, </span><span class="s1">self.data</span><span class="s2">,</span>
                                      <span class="s1">return_type=</span><span class="s3">&quot;dataframe&quot;</span><span class="s1">)</span>

        <span class="s5"># Rows with observed endog</span>
        <span class="s1">ixo = self.ix_obs[vname]</span>
        <span class="s1">endog_obs = np.require(endog.iloc[ixo]</span><span class="s2">, </span><span class="s1">requirements=</span><span class="s3">&quot;W&quot;</span><span class="s1">)</span>
        <span class="s1">exog_obs = np.require(exog.iloc[ixo</span><span class="s2">, </span><span class="s1">:]</span><span class="s2">, </span><span class="s1">requirements=</span><span class="s3">&quot;W&quot;</span><span class="s1">)</span>

        <span class="s5"># Rows with missing endog</span>
        <span class="s1">ixm = self.ix_miss[vname]</span>
        <span class="s1">exog_miss = np.require(exog.iloc[ixm</span><span class="s2">, </span><span class="s1">:]</span><span class="s2">, </span><span class="s1">requirements=</span><span class="s3">&quot;W&quot;</span><span class="s1">)</span>

        <span class="s1">predict_obs_kwds = {}</span>
        <span class="s2">if </span><span class="s1">vname </span><span class="s2">in </span><span class="s1">self.predict_kwds:</span>
            <span class="s1">kwds = self.predict_kwds[vname]</span>
            <span class="s1">predict_obs_kwds = self._process_kwds(kwds</span><span class="s2">, </span><span class="s1">ixo)</span>

        <span class="s1">predict_miss_kwds = {}</span>
        <span class="s2">if </span><span class="s1">vname </span><span class="s2">in </span><span class="s1">self.predict_kwds:</span>
            <span class="s1">kwds = self.predict_kwds[vname]</span>
            <span class="s1">predict_miss_kwds = self._process_kwds(kwds</span><span class="s2">, </span><span class="s1">ixo)</span>

        <span class="s2">return </span><span class="s1">(endog_obs</span><span class="s2">, </span><span class="s1">exog_obs</span><span class="s2">, </span><span class="s1">exog_miss</span><span class="s2">, </span><span class="s1">predict_obs_kwds</span><span class="s2">,</span>
                <span class="s1">predict_miss_kwds)</span>

    <span class="s2">def </span><span class="s1">_process_kwds(self</span><span class="s2">, </span><span class="s1">kwds</span><span class="s2">, </span><span class="s1">ix):</span>
        <span class="s1">kwds = kwds.copy()</span>
        <span class="s2">for </span><span class="s1">k </span><span class="s2">in </span><span class="s1">kwds:</span>
            <span class="s1">v = kwds[k]</span>
            <span class="s2">if </span><span class="s1">isinstance(v</span><span class="s2">, </span><span class="s1">PatsyFormula):</span>
                <span class="s1">mat = patsy.dmatrix(v.formula</span><span class="s2">, </span><span class="s1">self.data</span><span class="s2">,</span>
                                    <span class="s1">return_type=</span><span class="s3">&quot;dataframe&quot;</span><span class="s1">)</span>
                <span class="s1">mat = np.require(mat</span><span class="s2">, </span><span class="s1">requirements=</span><span class="s3">&quot;W&quot;</span><span class="s1">)[ix</span><span class="s2">, </span><span class="s1">:]</span>
                <span class="s2">if </span><span class="s1">mat.shape[</span><span class="s4">1</span><span class="s1">] == </span><span class="s4">1</span><span class="s1">:</span>
                    <span class="s1">mat = mat[:</span><span class="s2">, </span><span class="s4">0</span><span class="s1">]</span>
                <span class="s1">kwds[k] = mat</span>
        <span class="s2">return </span><span class="s1">kwds</span>

    <span class="s2">def </span><span class="s1">get_fitting_data(self</span><span class="s2">, </span><span class="s1">vname):</span>
        <span class="s0">&quot;&quot;&quot; 
        Return the data needed to fit a model for imputation. 
 
        The data is used to impute variable `vname`, and therefore 
        only includes cases for which `vname` is observed. 
 
        Values of type `PatsyFormula` in `init_kwds` or `fit_kwds` are 
        processed through Patsy and subset to align with the model's 
        endog and exog. 
 
        Parameters 
        ---------- 
        vname : str 
           The variable for which the fitting data is returned. 
 
        Returns 
        ------- 
        endog : DataFrame 
            Observed values of `vname`. 
        exog : DataFrame 
            Regression design matrix for imputing `vname`. 
        init_kwds : dict-like 
            The init keyword arguments for `vname`, processed through Patsy 
            as required. 
        fit_kwds : dict-like 
            The fit keyword arguments for `vname`, processed through Patsy 
            as required. 
        &quot;&quot;&quot;</span>

        <span class="s5"># Rows with observed endog</span>
        <span class="s1">ix = self.ix_obs[vname]</span>

        <span class="s1">formula = self.conditional_formula[vname]</span>
        <span class="s1">endog</span><span class="s2">, </span><span class="s1">exog = patsy.dmatrices(formula</span><span class="s2">, </span><span class="s1">self.data</span><span class="s2">,</span>
                                      <span class="s1">return_type=</span><span class="s3">&quot;dataframe&quot;</span><span class="s1">)</span>

        <span class="s1">endog = np.require(endog.iloc[ix</span><span class="s2">, </span><span class="s4">0</span><span class="s1">]</span><span class="s2">, </span><span class="s1">requirements=</span><span class="s3">&quot;W&quot;</span><span class="s1">)</span>
        <span class="s1">exog = np.require(exog.iloc[ix</span><span class="s2">, </span><span class="s1">:]</span><span class="s2">, </span><span class="s1">requirements=</span><span class="s3">&quot;W&quot;</span><span class="s1">)</span>

        <span class="s1">init_kwds = self._process_kwds(self.init_kwds[vname]</span><span class="s2">, </span><span class="s1">ix)</span>
        <span class="s1">fit_kwds = self._process_kwds(self.fit_kwds[vname]</span><span class="s2">, </span><span class="s1">ix)</span>

        <span class="s2">return </span><span class="s1">endog</span><span class="s2">, </span><span class="s1">exog</span><span class="s2">, </span><span class="s1">init_kwds</span><span class="s2">, </span><span class="s1">fit_kwds</span>

    <span class="s2">def </span><span class="s1">plot_missing_pattern(self</span><span class="s2">, </span><span class="s1">ax=</span><span class="s2">None, </span><span class="s1">row_order=</span><span class="s3">&quot;pattern&quot;</span><span class="s2">,</span>
                             <span class="s1">column_order=</span><span class="s3">&quot;pattern&quot;</span><span class="s2">,</span>
                             <span class="s1">hide_complete_rows=</span><span class="s2">False,</span>
                             <span class="s1">hide_complete_columns=</span><span class="s2">False,</span>
                             <span class="s1">color_row_patterns=</span><span class="s2">True</span><span class="s1">):</span>
        <span class="s0">&quot;&quot;&quot; 
        Generate an image showing the missing data pattern. 
 
        Parameters 
        ---------- 
        ax : AxesSubplot 
            Axes on which to draw the plot. 
        row_order : str 
            The method for ordering the rows.  Must be one of 'pattern', 
            'proportion', or 'raw'. 
        column_order : str 
            The method for ordering the columns.  Must be one of 'pattern', 
            'proportion', or 'raw'. 
        hide_complete_rows : bool 
            If True, rows with no missing values are not drawn. 
        hide_complete_columns : bool 
            If True, columns with no missing values are not drawn. 
        color_row_patterns : bool 
            If True, color the unique row patterns, otherwise use grey 
            and white as colors. 
 
        Returns 
        ------- 
        A figure containing a plot of the missing data pattern. 
        &quot;&quot;&quot;</span>

        <span class="s5"># Create an indicator matrix for missing values.</span>
        <span class="s1">miss = np.zeros(self.data.shape)</span>
        <span class="s1">cols = self.data.columns</span>
        <span class="s2">for </span><span class="s1">j</span><span class="s2">, </span><span class="s1">col </span><span class="s2">in </span><span class="s1">enumerate(cols):</span>
            <span class="s1">ix = self.ix_miss[col]</span>
            <span class="s1">miss[ix</span><span class="s2">, </span><span class="s1">j] = </span><span class="s4">1</span>

        <span class="s5"># Order the columns as requested</span>
        <span class="s2">if </span><span class="s1">column_order == </span><span class="s3">&quot;proportion&quot;</span><span class="s1">:</span>
            <span class="s1">ix = np.argsort(miss.mean(</span><span class="s4">0</span><span class="s1">))</span>
        <span class="s2">elif </span><span class="s1">column_order == </span><span class="s3">&quot;pattern&quot;</span><span class="s1">:</span>
            <span class="s1">cv = np.cov(miss.T)</span>
            <span class="s1">u</span><span class="s2">, </span><span class="s1">s</span><span class="s2">, </span><span class="s1">vt = np.linalg.svd(cv</span><span class="s2">, </span><span class="s4">0</span><span class="s1">)</span>
            <span class="s1">ix = np.argsort(cv[:</span><span class="s2">, </span><span class="s4">0</span><span class="s1">])</span>
        <span class="s2">elif </span><span class="s1">column_order == </span><span class="s3">&quot;raw&quot;</span><span class="s1">:</span>
            <span class="s1">ix = np.arange(len(cols))</span>
        <span class="s2">else</span><span class="s1">:</span>
            <span class="s2">raise </span><span class="s1">ValueError(</span>
                <span class="s1">column_order + </span><span class="s3">&quot; is not an allowed value for `column_order`.&quot;</span><span class="s1">)</span>
        <span class="s1">miss = miss[:</span><span class="s2">, </span><span class="s1">ix]</span>
        <span class="s1">cols = [cols[i] </span><span class="s2">for </span><span class="s1">i </span><span class="s2">in </span><span class="s1">ix]</span>

        <span class="s5"># Order the rows as requested</span>
        <span class="s2">if </span><span class="s1">row_order == </span><span class="s3">&quot;proportion&quot;</span><span class="s1">:</span>
            <span class="s1">ix = np.argsort(miss.mean(</span><span class="s4">1</span><span class="s1">))</span>
        <span class="s2">elif </span><span class="s1">row_order == </span><span class="s3">&quot;pattern&quot;</span><span class="s1">:</span>
            <span class="s1">x = </span><span class="s4">2</span><span class="s1">**np.arange(miss.shape[</span><span class="s4">1</span><span class="s1">])</span>
            <span class="s1">rky = np.dot(miss</span><span class="s2">, </span><span class="s1">x)</span>
            <span class="s1">ix = np.argsort(rky)</span>
        <span class="s2">elif </span><span class="s1">row_order == </span><span class="s3">&quot;raw&quot;</span><span class="s1">:</span>
            <span class="s1">ix = np.arange(miss.shape[</span><span class="s4">0</span><span class="s1">])</span>
        <span class="s2">else</span><span class="s1">:</span>
            <span class="s2">raise </span><span class="s1">ValueError(</span>
                <span class="s1">row_order + </span><span class="s3">&quot; is not an allowed value for `row_order`.&quot;</span><span class="s1">)</span>
        <span class="s1">miss = miss[ix</span><span class="s2">, </span><span class="s1">:]</span>

        <span class="s2">if </span><span class="s1">hide_complete_rows:</span>
            <span class="s1">ix = np.flatnonzero((miss == </span><span class="s4">1</span><span class="s1">).any(</span><span class="s4">1</span><span class="s1">))</span>
            <span class="s1">miss = miss[ix</span><span class="s2">, </span><span class="s1">:]</span>

        <span class="s2">if </span><span class="s1">hide_complete_columns:</span>
            <span class="s1">ix = np.flatnonzero((miss == </span><span class="s4">1</span><span class="s1">).any(</span><span class="s4">0</span><span class="s1">))</span>
            <span class="s1">miss = miss[:</span><span class="s2">, </span><span class="s1">ix]</span>
            <span class="s1">cols = [cols[i] </span><span class="s2">for </span><span class="s1">i </span><span class="s2">in </span><span class="s1">ix]</span>

        <span class="s2">from </span><span class="s1">statsmodels.graphics </span><span class="s2">import </span><span class="s1">utils </span><span class="s2">as </span><span class="s1">gutils</span>
        <span class="s2">from </span><span class="s1">matplotlib.colors </span><span class="s2">import </span><span class="s1">LinearSegmentedColormap</span>

        <span class="s2">if </span><span class="s1">ax </span><span class="s2">is None</span><span class="s1">:</span>
            <span class="s1">fig</span><span class="s2">, </span><span class="s1">ax = gutils.create_mpl_ax(ax)</span>
        <span class="s2">else</span><span class="s1">:</span>
            <span class="s1">fig = ax.get_figure()</span>

        <span class="s2">if </span><span class="s1">color_row_patterns:</span>
            <span class="s1">x = </span><span class="s4">2</span><span class="s1">**np.arange(miss.shape[</span><span class="s4">1</span><span class="s1">])</span>
            <span class="s1">rky = np.dot(miss</span><span class="s2">, </span><span class="s1">x)</span>
            <span class="s1">_</span><span class="s2">, </span><span class="s1">rcol = np.unique(rky</span><span class="s2">, </span><span class="s1">return_inverse=</span><span class="s2">True</span><span class="s1">)</span>
            <span class="s1">miss *= </span><span class="s4">1 </span><span class="s1">+ rcol[:</span><span class="s2">, None</span><span class="s1">]</span>
            <span class="s1">ax.imshow(miss</span><span class="s2">, </span><span class="s1">aspect=</span><span class="s3">&quot;auto&quot;</span><span class="s2">, </span><span class="s1">interpolation=</span><span class="s3">&quot;nearest&quot;</span><span class="s2">,</span>
                      <span class="s1">cmap=</span><span class="s3">'gist_ncar_r'</span><span class="s1">)</span>
        <span class="s2">else</span><span class="s1">:</span>
            <span class="s1">cmap = LinearSegmentedColormap.from_list(</span><span class="s3">&quot;_&quot;</span><span class="s2">,</span>
                                                     <span class="s1">[</span><span class="s3">&quot;white&quot;</span><span class="s2">, </span><span class="s3">&quot;darkgrey&quot;</span><span class="s1">])</span>
            <span class="s1">ax.imshow(miss</span><span class="s2">, </span><span class="s1">aspect=</span><span class="s3">&quot;auto&quot;</span><span class="s2">, </span><span class="s1">interpolation=</span><span class="s3">&quot;nearest&quot;</span><span class="s2">,</span>
                      <span class="s1">cmap=cmap)</span>

        <span class="s1">ax.set_ylabel(</span><span class="s3">&quot;Cases&quot;</span><span class="s1">)</span>
        <span class="s1">ax.set_xticks(range(len(cols)))</span>
        <span class="s1">ax.set_xticklabels(cols</span><span class="s2">, </span><span class="s1">rotation=</span><span class="s4">90</span><span class="s1">)</span>

        <span class="s2">return </span><span class="s1">fig</span>

    <span class="s2">def </span><span class="s1">plot_bivariate(self</span><span class="s2">, </span><span class="s1">col1_name</span><span class="s2">, </span><span class="s1">col2_name</span><span class="s2">,</span>
                       <span class="s1">lowess_args=</span><span class="s2">None, </span><span class="s1">lowess_min_n=</span><span class="s4">40</span><span class="s2">,</span>
                       <span class="s1">jitter=</span><span class="s2">None, </span><span class="s1">plot_points=</span><span class="s2">True, </span><span class="s1">ax=</span><span class="s2">None</span><span class="s1">):</span>
        <span class="s0">&quot;&quot;&quot; 
        Plot observed and imputed values for two variables. 
 
        Displays a scatterplot of one variable against another.  The 
        points are colored according to whether the values are 
        observed or imputed. 
 
        Parameters 
        ---------- 
        col1_name : str 
            The variable to be plotted on the horizontal axis. 
        col2_name : str 
            The variable to be plotted on the vertical axis. 
        lowess_args : dictionary 
            A dictionary of dictionaries, keys are 'ii', 'io', 'oi' 
            and 'oo', where 'o' denotes 'observed' and 'i' denotes 
            imputed.  See Notes for details. 
        lowess_min_n : int 
            Minimum sample size to plot a lowess fit 
        jitter : float or tuple 
            Standard deviation for jittering points in the plot. 
            Either a single scalar applied to both axes, or a tuple 
            containing x-axis jitter and y-axis jitter, respectively. 
        plot_points : bool 
            If True, the data points are plotted. 
        ax : AxesSubplot 
            Axes on which to plot, created if not provided. 
 
        Returns 
        ------- 
        The matplotlib figure on which the plot id drawn. 
        &quot;&quot;&quot;</span>

        <span class="s2">from </span><span class="s1">statsmodels.graphics </span><span class="s2">import </span><span class="s1">utils </span><span class="s2">as </span><span class="s1">gutils</span>
        <span class="s2">from </span><span class="s1">statsmodels.nonparametric.smoothers_lowess </span><span class="s2">import </span><span class="s1">lowess</span>

        <span class="s2">if </span><span class="s1">lowess_args </span><span class="s2">is None</span><span class="s1">:</span>
            <span class="s1">lowess_args = {}</span>

        <span class="s2">if </span><span class="s1">ax </span><span class="s2">is None</span><span class="s1">:</span>
            <span class="s1">fig</span><span class="s2">, </span><span class="s1">ax = gutils.create_mpl_ax(ax)</span>
        <span class="s2">else</span><span class="s1">:</span>
            <span class="s1">fig = ax.get_figure()</span>

        <span class="s1">ax.set_position([</span><span class="s4">0.1</span><span class="s2">, </span><span class="s4">0.1</span><span class="s2">, </span><span class="s4">0.7</span><span class="s2">, </span><span class="s4">0.8</span><span class="s1">])</span>

        <span class="s1">ix1i = self.ix_miss[col1_name]</span>
        <span class="s1">ix1o = self.ix_obs[col1_name]</span>
        <span class="s1">ix2i = self.ix_miss[col2_name]</span>
        <span class="s1">ix2o = self.ix_obs[col2_name]</span>

        <span class="s1">ix_ii = np.intersect1d(ix1i</span><span class="s2">, </span><span class="s1">ix2i)</span>
        <span class="s1">ix_io = np.intersect1d(ix1i</span><span class="s2">, </span><span class="s1">ix2o)</span>
        <span class="s1">ix_oi = np.intersect1d(ix1o</span><span class="s2">, </span><span class="s1">ix2i)</span>
        <span class="s1">ix_oo = np.intersect1d(ix1o</span><span class="s2">, </span><span class="s1">ix2o)</span>

        <span class="s1">vec1 = np.require(self.data[col1_name]</span><span class="s2">, </span><span class="s1">requirements=</span><span class="s3">&quot;W&quot;</span><span class="s1">)</span>
        <span class="s1">vec2 = np.require(self.data[col2_name]</span><span class="s2">, </span><span class="s1">requirements=</span><span class="s3">&quot;W&quot;</span><span class="s1">)</span>

        <span class="s2">if </span><span class="s1">jitter </span><span class="s2">is not None</span><span class="s1">:</span>
            <span class="s2">if </span><span class="s1">np.isscalar(jitter):</span>
                <span class="s1">jitter = (jitter</span><span class="s2">, </span><span class="s1">jitter)</span>
            <span class="s1">vec1 += jitter[</span><span class="s4">0</span><span class="s1">] * np.random.normal(size=len(vec1))</span>
            <span class="s1">vec2 += jitter[</span><span class="s4">1</span><span class="s1">] * np.random.normal(size=len(vec2))</span>

        <span class="s5"># Plot the points</span>
        <span class="s1">keys = [</span><span class="s3">'oo'</span><span class="s2">, </span><span class="s3">'io'</span><span class="s2">, </span><span class="s3">'oi'</span><span class="s2">, </span><span class="s3">'ii'</span><span class="s1">]</span>
        <span class="s1">lak = {</span><span class="s3">'i'</span><span class="s1">: </span><span class="s3">'imp'</span><span class="s2">, </span><span class="s3">'o'</span><span class="s1">: </span><span class="s3">'obs'</span><span class="s1">}</span>
        <span class="s1">ixs = {</span><span class="s3">'ii'</span><span class="s1">: ix_ii</span><span class="s2">, </span><span class="s3">'io'</span><span class="s1">: ix_io</span><span class="s2">, </span><span class="s3">'oi'</span><span class="s1">: ix_oi</span><span class="s2">, </span><span class="s3">'oo'</span><span class="s1">: ix_oo}</span>
        <span class="s1">color = {</span><span class="s3">'oo'</span><span class="s1">: </span><span class="s3">'grey'</span><span class="s2">, </span><span class="s3">'ii'</span><span class="s1">: </span><span class="s3">'red'</span><span class="s2">, </span><span class="s3">'io'</span><span class="s1">: </span><span class="s3">'orange'</span><span class="s2">,</span>
                 <span class="s3">'oi'</span><span class="s1">: </span><span class="s3">'lime'</span><span class="s1">}</span>
        <span class="s2">if </span><span class="s1">plot_points:</span>
            <span class="s2">for </span><span class="s1">ky </span><span class="s2">in </span><span class="s1">keys:</span>
                <span class="s1">ix = ixs[ky]</span>
                <span class="s1">lab = lak[ky[</span><span class="s4">0</span><span class="s1">]] + </span><span class="s3">&quot;/&quot; </span><span class="s1">+ lak[ky[</span><span class="s4">1</span><span class="s1">]]</span>
                <span class="s1">ax.plot(vec1[ix]</span><span class="s2">, </span><span class="s1">vec2[ix]</span><span class="s2">, </span><span class="s3">'o'</span><span class="s2">, </span><span class="s1">color=color[ky]</span><span class="s2">,</span>
                        <span class="s1">label=lab</span><span class="s2">, </span><span class="s1">alpha=</span><span class="s4">0.6</span><span class="s1">)</span>

        <span class="s5"># Plot the lowess fits</span>
        <span class="s2">for </span><span class="s1">ky </span><span class="s2">in </span><span class="s1">keys:</span>
            <span class="s1">ix = ixs[ky]</span>
            <span class="s2">if </span><span class="s1">len(ix) &lt; lowess_min_n:</span>
                <span class="s2">continue</span>
            <span class="s2">if </span><span class="s1">ky </span><span class="s2">in </span><span class="s1">lowess_args:</span>
                <span class="s1">la = lowess_args[ky]</span>
            <span class="s2">else</span><span class="s1">:</span>
                <span class="s1">la = {}</span>
            <span class="s1">ix = ixs[ky]</span>
            <span class="s1">lfit = lowess(vec2[ix]</span><span class="s2">, </span><span class="s1">vec1[ix]</span><span class="s2">, </span><span class="s1">**la)</span>
            <span class="s2">if </span><span class="s1">plot_points:</span>
                <span class="s1">ax.plot(lfit[:</span><span class="s2">, </span><span class="s4">0</span><span class="s1">]</span><span class="s2">, </span><span class="s1">lfit[:</span><span class="s2">, </span><span class="s4">1</span><span class="s1">]</span><span class="s2">, </span><span class="s3">'-'</span><span class="s2">, </span><span class="s1">color=color[ky]</span><span class="s2">,</span>
                        <span class="s1">alpha=</span><span class="s4">0.6</span><span class="s2">, </span><span class="s1">lw=</span><span class="s4">4</span><span class="s1">)</span>
            <span class="s2">else</span><span class="s1">:</span>
                <span class="s1">lab = lak[ky[</span><span class="s4">0</span><span class="s1">]] + </span><span class="s3">&quot;/&quot; </span><span class="s1">+ lak[ky[</span><span class="s4">1</span><span class="s1">]]</span>
                <span class="s1">ax.plot(lfit[:</span><span class="s2">, </span><span class="s4">0</span><span class="s1">]</span><span class="s2">, </span><span class="s1">lfit[:</span><span class="s2">, </span><span class="s4">1</span><span class="s1">]</span><span class="s2">, </span><span class="s3">'-'</span><span class="s2">, </span><span class="s1">color=color[ky]</span><span class="s2">,</span>
                        <span class="s1">alpha=</span><span class="s4">0.6</span><span class="s2">, </span><span class="s1">lw=</span><span class="s4">4</span><span class="s2">, </span><span class="s1">label=lab)</span>

        <span class="s1">ha</span><span class="s2">, </span><span class="s1">la = ax.get_legend_handles_labels()</span>
        <span class="s1">pad = </span><span class="s4">0.0001 </span><span class="s2">if </span><span class="s1">plot_points </span><span class="s2">else </span><span class="s4">0.5</span>
        <span class="s1">leg = fig.legend(ha</span><span class="s2">, </span><span class="s1">la</span><span class="s2">, </span><span class="s1">loc=</span><span class="s3">'center right'</span><span class="s2">, </span><span class="s1">numpoints=</span><span class="s4">1</span><span class="s2">,</span>
                         <span class="s1">handletextpad=pad)</span>
        <span class="s1">leg.draw_frame(</span><span class="s2">False</span><span class="s1">)</span>

        <span class="s1">ax.set_xlabel(col1_name)</span>
        <span class="s1">ax.set_ylabel(col2_name)</span>

        <span class="s2">return </span><span class="s1">fig</span>

    <span class="s2">def </span><span class="s1">plot_fit_obs(self</span><span class="s2">, </span><span class="s1">col_name</span><span class="s2">, </span><span class="s1">lowess_args=</span><span class="s2">None,</span>
                     <span class="s1">lowess_min_n=</span><span class="s4">40</span><span class="s2">, </span><span class="s1">jitter=</span><span class="s2">None,</span>
                     <span class="s1">plot_points=</span><span class="s2">True, </span><span class="s1">ax=</span><span class="s2">None</span><span class="s1">):</span>
        <span class="s0">&quot;&quot;&quot; 
        Plot fitted versus imputed or observed values as a scatterplot. 
 
        Parameters 
        ---------- 
        col_name : str 
            The variable to be plotted on the horizontal axis. 
        lowess_args : dict-like 
            Keyword arguments passed to lowess fit.  A dictionary of 
            dictionaries, keys are 'o' and 'i' denoting 'observed' and 
            'imputed', respectively. 
        lowess_min_n : int 
            Minimum sample size to plot a lowess fit 
        jitter : float or tuple 
            Standard deviation for jittering points in the plot. 
            Either a single scalar applied to both axes, or a tuple 
            containing x-axis jitter and y-axis jitter, respectively. 
        plot_points : bool 
            If True, the data points are plotted. 
        ax : AxesSubplot 
            Axes on which to plot, created if not provided. 
 
        Returns 
        ------- 
        The matplotlib figure on which the plot is drawn. 
        &quot;&quot;&quot;</span>

        <span class="s2">from </span><span class="s1">statsmodels.graphics </span><span class="s2">import </span><span class="s1">utils </span><span class="s2">as </span><span class="s1">gutils</span>
        <span class="s2">from </span><span class="s1">statsmodels.nonparametric.smoothers_lowess </span><span class="s2">import </span><span class="s1">lowess</span>

        <span class="s2">if </span><span class="s1">lowess_args </span><span class="s2">is None</span><span class="s1">:</span>
            <span class="s1">lowess_args = {}</span>

        <span class="s2">if </span><span class="s1">ax </span><span class="s2">is None</span><span class="s1">:</span>
            <span class="s1">fig</span><span class="s2">, </span><span class="s1">ax = gutils.create_mpl_ax(ax)</span>
        <span class="s2">else</span><span class="s1">:</span>
            <span class="s1">fig = ax.get_figure()</span>

        <span class="s1">ax.set_position([</span><span class="s4">0.1</span><span class="s2">, </span><span class="s4">0.1</span><span class="s2">, </span><span class="s4">0.7</span><span class="s2">, </span><span class="s4">0.8</span><span class="s1">])</span>

        <span class="s1">ixi = self.ix_miss[col_name]</span>
        <span class="s1">ixo = self.ix_obs[col_name]</span>

        <span class="s1">vec1 = np.require(self.data[col_name]</span><span class="s2">, </span><span class="s1">requirements=</span><span class="s3">&quot;W&quot;</span><span class="s1">)</span>

        <span class="s5"># Fitted values</span>
        <span class="s1">formula = self.conditional_formula[col_name]</span>
        <span class="s1">endog</span><span class="s2">, </span><span class="s1">exog = patsy.dmatrices(formula</span><span class="s2">, </span><span class="s1">self.data</span><span class="s2">,</span>
                                      <span class="s1">return_type=</span><span class="s3">&quot;dataframe&quot;</span><span class="s1">)</span>
        <span class="s1">results = self.results[col_name]</span>
        <span class="s1">vec2 = results.predict(exog=exog)</span>
        <span class="s1">vec2 = self._get_predicted(vec2)</span>

        <span class="s2">if </span><span class="s1">jitter </span><span class="s2">is not None</span><span class="s1">:</span>
            <span class="s2">if </span><span class="s1">np.isscalar(jitter):</span>
                <span class="s1">jitter = (jitter</span><span class="s2">, </span><span class="s1">jitter)</span>
            <span class="s1">vec1 += jitter[</span><span class="s4">0</span><span class="s1">] * np.random.normal(size=len(vec1))</span>
            <span class="s1">vec2 += jitter[</span><span class="s4">1</span><span class="s1">] * np.random.normal(size=len(vec2))</span>

        <span class="s5"># Plot the points</span>
        <span class="s1">keys = [</span><span class="s3">'o'</span><span class="s2">, </span><span class="s3">'i'</span><span class="s1">]</span>
        <span class="s1">ixs = {</span><span class="s3">'o'</span><span class="s1">: ixo</span><span class="s2">, </span><span class="s3">'i'</span><span class="s1">: ixi}</span>
        <span class="s1">lak = {</span><span class="s3">'o'</span><span class="s1">: </span><span class="s3">'obs'</span><span class="s2">, </span><span class="s3">'i'</span><span class="s1">: </span><span class="s3">'imp'</span><span class="s1">}</span>
        <span class="s1">color = {</span><span class="s3">'o'</span><span class="s1">: </span><span class="s3">'orange'</span><span class="s2">, </span><span class="s3">'i'</span><span class="s1">: </span><span class="s3">'lime'</span><span class="s1">}</span>
        <span class="s2">if </span><span class="s1">plot_points:</span>
            <span class="s2">for </span><span class="s1">ky </span><span class="s2">in </span><span class="s1">keys:</span>
                <span class="s1">ix = ixs[ky]</span>
                <span class="s1">ax.plot(vec1[ix]</span><span class="s2">, </span><span class="s1">vec2[ix]</span><span class="s2">, </span><span class="s3">'o'</span><span class="s2">, </span><span class="s1">color=color[ky]</span><span class="s2">,</span>
                        <span class="s1">label=lak[ky]</span><span class="s2">, </span><span class="s1">alpha=</span><span class="s4">0.6</span><span class="s1">)</span>

        <span class="s5"># Plot the lowess fits</span>
        <span class="s2">for </span><span class="s1">ky </span><span class="s2">in </span><span class="s1">keys:</span>
            <span class="s1">ix = ixs[ky]</span>
            <span class="s2">if </span><span class="s1">len(ix) &lt; lowess_min_n:</span>
                <span class="s2">continue</span>
            <span class="s2">if </span><span class="s1">ky </span><span class="s2">in </span><span class="s1">lowess_args:</span>
                <span class="s1">la = lowess_args[ky]</span>
            <span class="s2">else</span><span class="s1">:</span>
                <span class="s1">la = {}</span>
            <span class="s1">ix = ixs[ky]</span>
            <span class="s1">lfit = lowess(vec2[ix]</span><span class="s2">, </span><span class="s1">vec1[ix]</span><span class="s2">, </span><span class="s1">**la)</span>
            <span class="s1">ax.plot(lfit[:</span><span class="s2">, </span><span class="s4">0</span><span class="s1">]</span><span class="s2">, </span><span class="s1">lfit[:</span><span class="s2">, </span><span class="s4">1</span><span class="s1">]</span><span class="s2">, </span><span class="s3">'-'</span><span class="s2">, </span><span class="s1">color=color[ky]</span><span class="s2">,</span>
                    <span class="s1">alpha=</span><span class="s4">0.6</span><span class="s2">, </span><span class="s1">lw=</span><span class="s4">4</span><span class="s2">, </span><span class="s1">label=lak[ky])</span>

        <span class="s1">ha</span><span class="s2">, </span><span class="s1">la = ax.get_legend_handles_labels()</span>
        <span class="s1">leg = fig.legend(ha</span><span class="s2">, </span><span class="s1">la</span><span class="s2">, </span><span class="s1">loc=</span><span class="s3">'center right'</span><span class="s2">, </span><span class="s1">numpoints=</span><span class="s4">1</span><span class="s1">)</span>
        <span class="s1">leg.draw_frame(</span><span class="s2">False</span><span class="s1">)</span>

        <span class="s1">ax.set_xlabel(col_name + </span><span class="s3">&quot; observed or imputed&quot;</span><span class="s1">)</span>
        <span class="s1">ax.set_ylabel(col_name + </span><span class="s3">&quot; fitted&quot;</span><span class="s1">)</span>

        <span class="s2">return </span><span class="s1">fig</span>

    <span class="s2">def </span><span class="s1">plot_imputed_hist(self</span><span class="s2">, </span><span class="s1">col_name</span><span class="s2">, </span><span class="s1">ax=</span><span class="s2">None, </span><span class="s1">imp_hist_args=</span><span class="s2">None,</span>
                          <span class="s1">obs_hist_args=</span><span class="s2">None, </span><span class="s1">all_hist_args=</span><span class="s2">None</span><span class="s1">):</span>
        <span class="s0">&quot;&quot;&quot; 
        Display imputed values for one variable as a histogram. 
 
        Parameters 
        ---------- 
        col_name : str 
            The name of the variable to be plotted. 
        ax : AxesSubplot 
            An axes on which to draw the histograms.  If not provided, 
            one is created. 
        imp_hist_args : dict 
            Keyword arguments to be passed to pyplot.hist when 
            creating the histogram for imputed values. 
        obs_hist_args : dict 
            Keyword arguments to be passed to pyplot.hist when 
            creating the histogram for observed values. 
        all_hist_args : dict 
            Keyword arguments to be passed to pyplot.hist when 
            creating the histogram for all values. 
 
        Returns 
        ------- 
        The matplotlib figure on which the histograms were drawn 
        &quot;&quot;&quot;</span>

        <span class="s2">from </span><span class="s1">statsmodels.graphics </span><span class="s2">import </span><span class="s1">utils </span><span class="s2">as </span><span class="s1">gutils</span>

        <span class="s2">if </span><span class="s1">imp_hist_args </span><span class="s2">is None</span><span class="s1">:</span>
            <span class="s1">imp_hist_args = {}</span>
        <span class="s2">if </span><span class="s1">obs_hist_args </span><span class="s2">is None</span><span class="s1">:</span>
            <span class="s1">obs_hist_args = {}</span>
        <span class="s2">if </span><span class="s1">all_hist_args </span><span class="s2">is None</span><span class="s1">:</span>
            <span class="s1">all_hist_args = {}</span>

        <span class="s2">if </span><span class="s1">ax </span><span class="s2">is None</span><span class="s1">:</span>
            <span class="s1">fig</span><span class="s2">, </span><span class="s1">ax = gutils.create_mpl_ax(ax)</span>
        <span class="s2">else</span><span class="s1">:</span>
            <span class="s1">fig = ax.get_figure()</span>

        <span class="s1">ax.set_position([</span><span class="s4">0.1</span><span class="s2">, </span><span class="s4">0.1</span><span class="s2">, </span><span class="s4">0.7</span><span class="s2">, </span><span class="s4">0.8</span><span class="s1">])</span>

        <span class="s1">ixm = self.ix_miss[col_name]</span>
        <span class="s1">ixo = self.ix_obs[col_name]</span>

        <span class="s1">imp = self.data[col_name].iloc[ixm]</span>
        <span class="s1">obs = self.data[col_name].iloc[ixo]</span>

        <span class="s2">for </span><span class="s1">di </span><span class="s2">in </span><span class="s1">imp_hist_args</span><span class="s2">, </span><span class="s1">obs_hist_args</span><span class="s2">, </span><span class="s1">all_hist_args:</span>
            <span class="s2">if </span><span class="s3">'histtype' </span><span class="s2">not in </span><span class="s1">di:</span>
                <span class="s1">di[</span><span class="s3">'histtype'</span><span class="s1">] = </span><span class="s3">'step'</span>

        <span class="s1">ha</span><span class="s2">, </span><span class="s1">la = []</span><span class="s2">, </span><span class="s1">[]</span>
        <span class="s2">if </span><span class="s1">len(imp) &gt; </span><span class="s4">0</span><span class="s1">:</span>
            <span class="s1">h = ax.hist(np.asarray(imp)</span><span class="s2">, </span><span class="s1">**imp_hist_args)</span>
            <span class="s1">ha.append(h[-</span><span class="s4">1</span><span class="s1">][</span><span class="s4">0</span><span class="s1">])</span>
            <span class="s1">la.append(</span><span class="s3">&quot;Imp&quot;</span><span class="s1">)</span>
        <span class="s1">h1 = ax.hist(np.asarray(obs)</span><span class="s2">, </span><span class="s1">**obs_hist_args)</span>
        <span class="s1">h2 = ax.hist(np.asarray(self.data[col_name])</span><span class="s2">, </span><span class="s1">**all_hist_args)</span>
        <span class="s1">ha.extend([h1[-</span><span class="s4">1</span><span class="s1">][</span><span class="s4">0</span><span class="s1">]</span><span class="s2">, </span><span class="s1">h2[-</span><span class="s4">1</span><span class="s1">][</span><span class="s4">0</span><span class="s1">]])</span>
        <span class="s1">la.extend([</span><span class="s3">&quot;Obs&quot;</span><span class="s2">, </span><span class="s3">&quot;All&quot;</span><span class="s1">])</span>

        <span class="s1">leg = fig.legend(ha</span><span class="s2">, </span><span class="s1">la</span><span class="s2">, </span><span class="s1">loc=</span><span class="s3">'center right'</span><span class="s2">, </span><span class="s1">numpoints=</span><span class="s4">1</span><span class="s1">)</span>
        <span class="s1">leg.draw_frame(</span><span class="s2">False</span><span class="s1">)</span>

        <span class="s1">ax.set_xlabel(col_name)</span>
        <span class="s1">ax.set_ylabel(</span><span class="s3">&quot;Frequency&quot;</span><span class="s1">)</span>

        <span class="s2">return </span><span class="s1">fig</span>

    <span class="s5"># Try to identify any auxiliary arrays (e.g. status vector in</span>
    <span class="s5"># PHReg) that need to be bootstrapped along with exog and endog.</span>
    <span class="s2">def </span><span class="s1">_boot_kwds(self</span><span class="s2">, </span><span class="s1">kwds</span><span class="s2">, </span><span class="s1">rix):</span>

        <span class="s2">for </span><span class="s1">k </span><span class="s2">in </span><span class="s1">kwds:</span>
            <span class="s1">v = kwds[k]</span>

            <span class="s5"># This is only relevant for ndarrays</span>
            <span class="s2">if not </span><span class="s1">isinstance(v</span><span class="s2">, </span><span class="s1">np.ndarray):</span>
                <span class="s2">continue</span>

            <span class="s5"># Handle 1d vectors</span>
            <span class="s2">if </span><span class="s1">(v.ndim == </span><span class="s4">1</span><span class="s1">) </span><span class="s2">and </span><span class="s1">(v.shape[</span><span class="s4">0</span><span class="s1">] == len(rix)):</span>
                <span class="s1">kwds[k] = v[rix]</span>

            <span class="s5"># Handle 2d arrays</span>
            <span class="s2">if </span><span class="s1">(v.ndim == </span><span class="s4">2</span><span class="s1">) </span><span class="s2">and </span><span class="s1">(v.shape[</span><span class="s4">0</span><span class="s1">] == len(rix)):</span>
                <span class="s1">kwds[k] = v[rix</span><span class="s2">, </span><span class="s1">:]</span>

        <span class="s2">return </span><span class="s1">kwds</span>

    <span class="s2">def </span><span class="s1">_perturb_bootstrap(self</span><span class="s2">, </span><span class="s1">vname):</span>
        <span class="s0">&quot;&quot;&quot; 
        Perturbs the model's parameters using a bootstrap. 
        &quot;&quot;&quot;</span>

        <span class="s1">endog</span><span class="s2">, </span><span class="s1">exog</span><span class="s2">, </span><span class="s1">init_kwds</span><span class="s2">, </span><span class="s1">fit_kwds = self.get_fitting_data(vname)</span>

        <span class="s1">m = len(endog)</span>
        <span class="s1">rix = np.random.randint(</span><span class="s4">0</span><span class="s2">, </span><span class="s1">m</span><span class="s2">, </span><span class="s1">m)</span>
        <span class="s1">endog = endog[rix]</span>
        <span class="s1">exog = exog[rix</span><span class="s2">, </span><span class="s1">:]</span>

        <span class="s1">init_kwds = self._boot_kwds(init_kwds</span><span class="s2">, </span><span class="s1">rix)</span>
        <span class="s1">fit_kwds = self._boot_kwds(fit_kwds</span><span class="s2">, </span><span class="s1">rix)</span>

        <span class="s1">klass = self.model_class[vname]</span>
        <span class="s1">self.models[vname] = klass(endog</span><span class="s2">, </span><span class="s1">exog</span><span class="s2">, </span><span class="s1">**init_kwds)</span>

        <span class="s2">if </span><span class="s1">vname </span><span class="s2">in </span><span class="s1">self.regularized </span><span class="s2">and </span><span class="s1">self.regularized[vname]:</span>
            <span class="s1">self.results[vname] = (</span>
                <span class="s1">self.models[vname].fit_regularized(**fit_kwds))</span>
        <span class="s2">else</span><span class="s1">:</span>
            <span class="s1">self.results[vname] = self.models[vname].fit(**fit_kwds)</span>

        <span class="s1">self.params[vname] = self.results[vname].params</span>

    <span class="s2">def </span><span class="s1">_perturb_gaussian(self</span><span class="s2">, </span><span class="s1">vname):</span>
        <span class="s0">&quot;&quot;&quot; 
        Gaussian perturbation of model parameters. 
 
        The normal approximation to the sampling distribution of the 
        parameter estimates is used to define the mean and covariance 
        structure of the perturbation distribution. 
        &quot;&quot;&quot;</span>

        <span class="s1">endog</span><span class="s2">, </span><span class="s1">exog</span><span class="s2">, </span><span class="s1">init_kwds</span><span class="s2">, </span><span class="s1">fit_kwds = self.get_fitting_data(vname)</span>

        <span class="s1">klass = self.model_class[vname]</span>
        <span class="s1">self.models[vname] = klass(endog</span><span class="s2">, </span><span class="s1">exog</span><span class="s2">, </span><span class="s1">**init_kwds)</span>
        <span class="s1">self.results[vname] = self.models[vname].fit(**fit_kwds)</span>

        <span class="s1">cov = self.results[vname].cov_params()</span>
        <span class="s1">mu = self.results[vname].params</span>
        <span class="s1">self.params[vname] = np.random.multivariate_normal(mean=mu</span><span class="s2">, </span><span class="s1">cov=cov)</span>

    <span class="s2">def </span><span class="s1">perturb_params(self</span><span class="s2">, </span><span class="s1">vname):</span>

        <span class="s2">if </span><span class="s1">self.perturbation_method[vname] == </span><span class="s3">&quot;gaussian&quot;</span><span class="s1">:</span>
            <span class="s1">self._perturb_gaussian(vname)</span>
        <span class="s2">elif </span><span class="s1">self.perturbation_method[vname] == </span><span class="s3">&quot;boot&quot;</span><span class="s1">:</span>
            <span class="s1">self._perturb_bootstrap(vname)</span>
        <span class="s2">else</span><span class="s1">:</span>
            <span class="s2">raise </span><span class="s1">ValueError(</span><span class="s3">&quot;unknown perturbation method&quot;</span><span class="s1">)</span>

    <span class="s2">def </span><span class="s1">impute(self</span><span class="s2">, </span><span class="s1">vname):</span>
        <span class="s5"># Wrap this in case we later add additional imputation</span>
        <span class="s5"># methods.</span>
        <span class="s1">self.impute_pmm(vname)</span>

    <span class="s2">def </span><span class="s1">update(self</span><span class="s2">, </span><span class="s1">vname):</span>
        <span class="s0">&quot;&quot;&quot; 
        Impute missing values for a single variable. 
 
        This is a two-step process in which first the parameters are 
        perturbed, then the missing values are re-imputed. 
 
        Parameters 
        ---------- 
        vname : str 
            The name of the variable to be updated. 
        &quot;&quot;&quot;</span>

        <span class="s1">self.perturb_params(vname)</span>
        <span class="s1">self.impute(vname)</span>

    <span class="s5"># work-around for inconsistent predict return values</span>
    <span class="s2">def </span><span class="s1">_get_predicted(self</span><span class="s2">, </span><span class="s1">obj):</span>

        <span class="s2">if </span><span class="s1">isinstance(obj</span><span class="s2">, </span><span class="s1">np.ndarray):</span>
            <span class="s2">return </span><span class="s1">obj</span>
        <span class="s2">elif </span><span class="s1">isinstance(obj</span><span class="s2">, </span><span class="s1">pd.Series):</span>
            <span class="s2">return </span><span class="s1">obj.values</span>
        <span class="s2">elif </span><span class="s1">hasattr(obj</span><span class="s2">, </span><span class="s3">'predicted_values'</span><span class="s1">):</span>
            <span class="s2">return </span><span class="s1">obj.predicted_values</span>
        <span class="s2">else</span><span class="s1">:</span>
            <span class="s2">raise </span><span class="s1">ValueError(</span>
                <span class="s3">&quot;cannot obtain predicted values from %s&quot; </span><span class="s1">% obj.__class__)</span>

    <span class="s2">def </span><span class="s1">impute_pmm(self</span><span class="s2">, </span><span class="s1">vname):</span>
        <span class="s0">&quot;&quot;&quot; 
        Use predictive mean matching to impute missing values. 
 
        Notes 
        ----- 
        The `perturb_params` method must be called first to define the 
        model. 
        &quot;&quot;&quot;</span>

        <span class="s1">k_pmm = self.k_pmm</span>

        <span class="s1">endog_obs</span><span class="s2">, </span><span class="s1">exog_obs</span><span class="s2">, </span><span class="s1">exog_miss</span><span class="s2">, </span><span class="s1">predict_obs_kwds</span><span class="s2">, </span><span class="s1">predict_miss_kwds = (</span>
            <span class="s1">self.get_split_data(vname))</span>

        <span class="s5"># Predict imputed variable for both missing and non-missing</span>
        <span class="s5"># observations</span>
        <span class="s1">model = self.models[vname]</span>
        <span class="s1">pendog_obs = model.predict(self.params[vname]</span><span class="s2">, </span><span class="s1">exog_obs</span><span class="s2">,</span>
                                   <span class="s1">**predict_obs_kwds)</span>
        <span class="s1">pendog_miss = model.predict(self.params[vname]</span><span class="s2">, </span><span class="s1">exog_miss</span><span class="s2">,</span>
                                    <span class="s1">**predict_miss_kwds)</span>

        <span class="s1">pendog_obs = self._get_predicted(pendog_obs)</span>
        <span class="s1">pendog_miss = self._get_predicted(pendog_miss)</span>

        <span class="s5"># Jointly sort the observed and predicted endog values for the</span>
        <span class="s5"># cases with observed values.</span>
        <span class="s1">ii = np.argsort(pendog_obs)</span>
        <span class="s1">endog_obs = endog_obs[ii]</span>
        <span class="s1">pendog_obs = pendog_obs[ii]</span>

        <span class="s5"># Find the closest match to the predicted endog values for</span>
        <span class="s5"># cases with missing endog values.</span>
        <span class="s1">ix = np.searchsorted(pendog_obs</span><span class="s2">, </span><span class="s1">pendog_miss)</span>

        <span class="s5"># Get the indices for the closest k_pmm values on</span>
        <span class="s5"># either side of the closest index.</span>
        <span class="s1">ixm = ix[:</span><span class="s2">, None</span><span class="s1">] + np.arange(-k_pmm</span><span class="s2">, </span><span class="s1">k_pmm)[</span><span class="s2">None, </span><span class="s1">:]</span>

        <span class="s5"># Account for boundary effects</span>
        <span class="s1">msk = np.nonzero((ixm &lt; </span><span class="s4">0</span><span class="s1">) | (ixm &gt; len(endog_obs) - </span><span class="s4">1</span><span class="s1">))</span>
        <span class="s1">ixm = np.clip(ixm</span><span class="s2">, </span><span class="s4">0</span><span class="s2">, </span><span class="s1">len(endog_obs) - </span><span class="s4">1</span><span class="s1">)</span>

        <span class="s5"># Get the distances</span>
        <span class="s1">dx = pendog_miss[:</span><span class="s2">, None</span><span class="s1">] - pendog_obs[ixm]</span>
        <span class="s1">dx = np.abs(dx)</span>
        <span class="s1">dx[msk] = np.inf</span>

        <span class="s5"># Closest positions in ix, row-wise.</span>
        <span class="s1">dxi = np.argsort(dx</span><span class="s2">, </span><span class="s4">1</span><span class="s1">)[:</span><span class="s2">, </span><span class="s4">0</span><span class="s1">:k_pmm]</span>

        <span class="s5"># Choose a column for each row.</span>
        <span class="s1">ir = np.random.randint(</span><span class="s4">0</span><span class="s2">, </span><span class="s1">k_pmm</span><span class="s2">, </span><span class="s1">len(pendog_miss))</span>

        <span class="s5"># Unwind the indices</span>
        <span class="s1">jj = np.arange(dxi.shape[</span><span class="s4">0</span><span class="s1">])</span>
        <span class="s1">ix = dxi[(jj</span><span class="s2">, </span><span class="s1">ir)]</span>
        <span class="s1">iz = ixm[(jj</span><span class="s2">, </span><span class="s1">ix)]</span>

        <span class="s1">imputed_miss = np.array(endog_obs[iz]).squeeze()</span>
        <span class="s1">self._store_changes(vname</span><span class="s2">, </span><span class="s1">imputed_miss)</span>


<span class="s1">_mice_example_1 = </span><span class="s3">&quot;&quot;&quot; 
    &gt;&gt;&gt; imp = mice.MICEData(data) 
    &gt;&gt;&gt; fml = 'y ~ x1 + x2 + x3 + x4' 
    &gt;&gt;&gt; mice = mice.MICE(fml, sm.OLS, imp) 
    &gt;&gt;&gt; results = mice.fit(10, 10) 
    &gt;&gt;&gt; print(results.summary()) 
 
    .. literalinclude:: ../plots/mice_example_1.txt 
    &quot;&quot;&quot;</span>

<span class="s1">_mice_example_2 = </span><span class="s3">&quot;&quot;&quot; 
    &gt;&gt;&gt; imp = mice.MICEData(data) 
    &gt;&gt;&gt; fml = 'y ~ x1 + x2 + x3 + x4' 
    &gt;&gt;&gt; mice = mice.MICE(fml, sm.OLS, imp) 
    &gt;&gt;&gt; results = [] 
    &gt;&gt;&gt; for k in range(10): 
    &gt;&gt;&gt;     x = mice.next_sample() 
    &gt;&gt;&gt;     results.append(x) 
    &quot;&quot;&quot;</span>


<span class="s2">class </span><span class="s1">MICE:</span>

    <span class="s1">__doc__ = </span><span class="s3">&quot;&quot;&quot;</span><span class="s2">\ 
    </span><span class="s3">Multiple Imputation with Chained Equations. 
 
    This class can be used to fit most statsmodels models to data sets 
    with missing values using the 'multiple imputation with chained 
    equations' (MICE) approach.. 
 
    Parameters 
    ---------- 
    model_formula : str 
        The model formula to be fit to the imputed data sets.  This 
        formula is for the 'analysis model'. 
    model_class : statsmodels model 
        The model to be fit to the imputed data sets.  This model 
        class if for the 'analysis model'. 
    data : MICEData instance 
        MICEData object containing the data set for which 
        missing values will be imputed 
    n_skip : int 
        The number of imputed datasets to skip between consecutive 
        imputed datasets that are used for analysis. 
    init_kwds : dict-like 
        Dictionary of keyword arguments passed to the init method 
        of the analysis model. 
    fit_kwds : dict-like 
        Dictionary of keyword arguments passed to the fit method 
        of the analysis model. 
 
    Examples 
    -------- 
    Run all MICE steps and obtain results: 
    %(mice_example_1)s 
 
    Obtain a sequence of fitted analysis models without combining 
    to obtain summary:: 
    %(mice_example_2)s 
    &quot;&quot;&quot; </span><span class="s1">% {</span><span class="s3">'mice_example_1'</span><span class="s1">: _mice_example_1</span><span class="s2">,</span>
           <span class="s3">'mice_example_2'</span><span class="s1">: _mice_example_2}</span>

    <span class="s2">def </span><span class="s1">__init__(self</span><span class="s2">, </span><span class="s1">model_formula</span><span class="s2">, </span><span class="s1">model_class</span><span class="s2">, </span><span class="s1">data</span><span class="s2">, </span><span class="s1">n_skip=</span><span class="s4">3</span><span class="s2">,</span>
                 <span class="s1">init_kwds=</span><span class="s2">None, </span><span class="s1">fit_kwds=</span><span class="s2">None</span><span class="s1">):</span>

        <span class="s1">self.model_formula = model_formula</span>
        <span class="s1">self.model_class = model_class</span>
        <span class="s1">self.n_skip = n_skip</span>
        <span class="s1">self.data = data</span>
        <span class="s1">self.results_list = []</span>

        <span class="s1">self.init_kwds = init_kwds </span><span class="s2">if </span><span class="s1">init_kwds </span><span class="s2">is not None else </span><span class="s1">{}</span>
        <span class="s1">self.fit_kwds = fit_kwds </span><span class="s2">if </span><span class="s1">fit_kwds </span><span class="s2">is not None else </span><span class="s1">{}</span>

    <span class="s2">def </span><span class="s1">next_sample(self):</span>
        <span class="s0">&quot;&quot;&quot; 
        Perform one complete MICE iteration. 
 
        A single MICE iteration updates all missing values using their 
        respective imputation models, then fits the analysis model to 
        the imputed data. 
 
        Returns 
        ------- 
        params : array_like 
            The model parameters for the analysis model. 
 
        Notes 
        ----- 
        This function fits the analysis model and returns its 
        parameter estimate.  The parameter vector is not stored by the 
        class and is not used in any subsequent calls to `combine`. 
        Use `fit` to run all MICE steps together and obtain summary 
        results. 
 
        The complete cycle of missing value imputation followed by 
        fitting the analysis model is repeated `n_skip + 1` times and 
        the analysis model parameters from the final fit are returned. 
        &quot;&quot;&quot;</span>

        <span class="s5"># Impute missing values</span>
        <span class="s1">self.data.update_all(self.n_skip + </span><span class="s4">1</span><span class="s1">)</span>
        <span class="s1">start_params = </span><span class="s2">None</span>
        <span class="s2">if </span><span class="s1">len(self.results_list) &gt; </span><span class="s4">0</span><span class="s1">:</span>
            <span class="s1">start_params = self.results_list[-</span><span class="s4">1</span><span class="s1">].params</span>

        <span class="s5"># Fit the analysis model.</span>
        <span class="s1">model = self.model_class.from_formula(self.model_formula</span><span class="s2">,</span>
                                              <span class="s1">self.data.data</span><span class="s2">,</span>
                                              <span class="s1">**self.init_kwds)</span>
        <span class="s1">self.fit_kwds.update({</span><span class="s3">&quot;start_params&quot;</span><span class="s1">: start_params})</span>
        <span class="s1">result = model.fit(**self.fit_kwds)</span>

        <span class="s2">return </span><span class="s1">result</span>

    <span class="s2">def </span><span class="s1">fit(self</span><span class="s2">, </span><span class="s1">n_burnin=</span><span class="s4">10</span><span class="s2">, </span><span class="s1">n_imputations=</span><span class="s4">10</span><span class="s1">):</span>
        <span class="s0">&quot;&quot;&quot; 
        Fit a model using MICE. 
 
        Parameters 
        ---------- 
        n_burnin : int 
            The number of burn-in cycles to skip. 
        n_imputations : int 
            The number of data sets to impute 
        &quot;&quot;&quot;</span>

        <span class="s5"># Run without fitting the analysis model</span>
        <span class="s1">self.data.update_all(n_burnin)</span>

        <span class="s2">for </span><span class="s1">j </span><span class="s2">in </span><span class="s1">range(n_imputations):</span>
            <span class="s1">result = self.next_sample()</span>
            <span class="s1">self.results_list.append(result)</span>

        <span class="s1">self.endog_names = result.model.endog_names</span>
        <span class="s1">self.exog_names = result.model.exog_names</span>

        <span class="s2">return </span><span class="s1">self.combine()</span>

    <span class="s2">def </span><span class="s1">combine(self):</span>
        <span class="s0">&quot;&quot;&quot; 
        Pools MICE imputation results. 
 
        This method can only be used after the `run` method has been 
        called.  Returns estimates and standard errors of the analysis 
        model parameters. 
 
        Returns a MICEResults instance. 
        &quot;&quot;&quot;</span>

        <span class="s5"># Extract a few things from the models that were fit to</span>
        <span class="s5"># imputed data sets.</span>
        <span class="s1">params_list = []</span>
        <span class="s1">cov_within = </span><span class="s4">0.</span>
        <span class="s1">scale_list = []</span>
        <span class="s2">for </span><span class="s1">results </span><span class="s2">in </span><span class="s1">self.results_list:</span>
            <span class="s1">results_uw = results._results</span>
            <span class="s1">params_list.append(results_uw.params)</span>
            <span class="s1">cov_within += results_uw.cov_params()</span>
            <span class="s1">scale_list.append(results.scale)</span>
        <span class="s1">params_list = np.asarray(params_list)</span>
        <span class="s1">scale_list = np.asarray(scale_list)</span>

        <span class="s5"># The estimated parameters for the MICE analysis</span>
        <span class="s1">params = params_list.mean(</span><span class="s4">0</span><span class="s1">)</span>

        <span class="s5"># The average of the within-imputation covariances</span>
        <span class="s1">cov_within /= len(self.results_list)</span>

        <span class="s5"># The between-imputation covariance</span>
        <span class="s1">cov_between = np.cov(params_list.T)</span>

        <span class="s5"># The estimated covariance matrix for the MICE analysis</span>
        <span class="s1">f = </span><span class="s4">1 </span><span class="s1">+ </span><span class="s4">1 </span><span class="s1">/ float(len(self.results_list))</span>
        <span class="s1">cov_params = cov_within + f * cov_between</span>

        <span class="s5"># Fraction of missing information</span>
        <span class="s1">fmi = f * np.diag(cov_between) / np.diag(cov_params)</span>

        <span class="s5"># Set up a results instance</span>
        <span class="s1">scale = np.mean(scale_list)</span>
        <span class="s1">results = MICEResults(self</span><span class="s2">, </span><span class="s1">params</span><span class="s2">, </span><span class="s1">cov_params / scale)</span>
        <span class="s1">results.scale = scale</span>
        <span class="s1">results.frac_miss_info = fmi</span>
        <span class="s1">results.exog_names = self.exog_names</span>
        <span class="s1">results.endog_names = self.endog_names</span>
        <span class="s1">results.model_class = self.model_class</span>

        <span class="s2">return </span><span class="s1">results</span>


<span class="s2">class </span><span class="s1">MICEResults(LikelihoodModelResults):</span>

    <span class="s2">def </span><span class="s1">__init__(self</span><span class="s2">, </span><span class="s1">model</span><span class="s2">, </span><span class="s1">params</span><span class="s2">, </span><span class="s1">normalized_cov_params):</span>

        <span class="s1">super(MICEResults</span><span class="s2">, </span><span class="s1">self).__init__(model</span><span class="s2">, </span><span class="s1">params</span><span class="s2">,</span>
                                          <span class="s1">normalized_cov_params)</span>

    <span class="s2">def </span><span class="s1">summary(self</span><span class="s2">, </span><span class="s1">title=</span><span class="s2">None, </span><span class="s1">alpha=</span><span class="s4">.05</span><span class="s1">):</span>
        <span class="s0">&quot;&quot;&quot; 
        Summarize the results of running MICE. 
 
        Parameters 
        ---------- 
        title : str, optional 
            Title for the top table. If not None, then this replaces 
            the default title 
        alpha : float 
            Significance level for the confidence intervals 
 
        Returns 
        ------- 
        smry : Summary instance 
            This holds the summary tables and text, which can be 
            printed or converted to various output formats. 
        &quot;&quot;&quot;</span>

        <span class="s2">from </span><span class="s1">statsmodels.iolib </span><span class="s2">import </span><span class="s1">summary2</span>

        <span class="s1">smry = summary2.Summary()</span>
        <span class="s1">float_format = </span><span class="s3">&quot;%8.3f&quot;</span>

        <span class="s1">info = {}</span>
        <span class="s1">info[</span><span class="s3">&quot;Method:&quot;</span><span class="s1">] = </span><span class="s3">&quot;MICE&quot;</span>
        <span class="s1">info[</span><span class="s3">&quot;Model:&quot;</span><span class="s1">] = self.model_class.__name__</span>
        <span class="s1">info[</span><span class="s3">&quot;Dependent variable:&quot;</span><span class="s1">] = self.endog_names</span>
        <span class="s1">info[</span><span class="s3">&quot;Sample size:&quot;</span><span class="s1">] = </span><span class="s3">&quot;%d&quot; </span><span class="s1">% self.model.data.data.shape[</span><span class="s4">0</span><span class="s1">]</span>
        <span class="s1">info[</span><span class="s3">&quot;Scale&quot;</span><span class="s1">] = </span><span class="s3">&quot;%.2f&quot; </span><span class="s1">% self.scale</span>
        <span class="s1">info[</span><span class="s3">&quot;Num. imputations&quot;</span><span class="s1">] = </span><span class="s3">&quot;%d&quot; </span><span class="s1">% len(self.model.results_list)</span>

        <span class="s1">smry.add_dict(info</span><span class="s2">, </span><span class="s1">align=</span><span class="s3">'l'</span><span class="s2">, </span><span class="s1">float_format=float_format)</span>

        <span class="s1">param = summary2.summary_params(self</span><span class="s2">, </span><span class="s1">alpha=alpha)</span>
        <span class="s1">param[</span><span class="s3">&quot;FMI&quot;</span><span class="s1">] = self.frac_miss_info</span>

        <span class="s1">smry.add_df(param</span><span class="s2">, </span><span class="s1">float_format=float_format)</span>
        <span class="s1">smry.add_title(title=title</span><span class="s2">, </span><span class="s1">results=self)</span>

        <span class="s2">return </span><span class="s1">smry</span>
</pre>
</body>
</html>