<html>
<head>
<title>test_gaussian_mixture.py</title>
<meta http-equiv="Content-Type" content="text/html; charset=utf-8">
<style type="text/css">
.s0 { color: #808080;}
.s1 { color: #a9b7c6;}
.s2 { color: #cc7832;}
.s3 { color: #6a8759;}
.s4 { color: #6897bb;}
.s5 { color: #629755; font-style: italic;}
</style>
</head>
<body bgcolor="#2b2b2b">
<table CELLSPACING=0 CELLPADDING=5 COLS=1 WIDTH="100%" BGCOLOR="#606060" >
<tr><td><center>
<font face="Arial, Helvetica" color="#000000">
test_gaussian_mixture.py</font>
</center></td></tr></table>
<pre><span class="s0"># Author: Wei Xue &lt;xuewei4d@gmail.com&gt;</span>
<span class="s0">#         Thierry Guillemot &lt;thierry.guillemot.work@gmail.com&gt;</span>
<span class="s0"># License: BSD 3 clause</span>

<span class="s2">import </span><span class="s1">copy</span>
<span class="s2">import </span><span class="s1">itertools</span>
<span class="s2">import </span><span class="s1">re</span>
<span class="s2">import </span><span class="s1">sys</span>
<span class="s2">import </span><span class="s1">warnings</span>
<span class="s2">from </span><span class="s1">io </span><span class="s2">import </span><span class="s1">StringIO</span>

<span class="s2">import </span><span class="s1">numpy </span><span class="s2">as </span><span class="s1">np</span>
<span class="s2">import </span><span class="s1">pytest</span>
<span class="s2">from </span><span class="s1">scipy </span><span class="s2">import </span><span class="s1">linalg</span><span class="s2">, </span><span class="s1">stats</span>

<span class="s2">from </span><span class="s1">sklearn.cluster </span><span class="s2">import </span><span class="s1">KMeans</span>
<span class="s2">from </span><span class="s1">sklearn.covariance </span><span class="s2">import </span><span class="s1">EmpiricalCovariance</span>
<span class="s2">from </span><span class="s1">sklearn.datasets </span><span class="s2">import </span><span class="s1">make_spd_matrix</span>
<span class="s2">from </span><span class="s1">sklearn.exceptions </span><span class="s2">import </span><span class="s1">ConvergenceWarning</span><span class="s2">, </span><span class="s1">NotFittedError</span>
<span class="s2">from </span><span class="s1">sklearn.metrics.cluster </span><span class="s2">import </span><span class="s1">adjusted_rand_score</span>
<span class="s2">from </span><span class="s1">sklearn.mixture </span><span class="s2">import </span><span class="s1">GaussianMixture</span>
<span class="s2">from </span><span class="s1">sklearn.mixture._gaussian_mixture </span><span class="s2">import </span><span class="s1">(</span>
    <span class="s1">_compute_log_det_cholesky</span><span class="s2">,</span>
    <span class="s1">_compute_precision_cholesky</span><span class="s2">,</span>
    <span class="s1">_estimate_gaussian_covariances_diag</span><span class="s2">,</span>
    <span class="s1">_estimate_gaussian_covariances_full</span><span class="s2">,</span>
    <span class="s1">_estimate_gaussian_covariances_spherical</span><span class="s2">,</span>
    <span class="s1">_estimate_gaussian_covariances_tied</span><span class="s2">,</span>
    <span class="s1">_estimate_gaussian_parameters</span><span class="s2">,</span>
<span class="s1">)</span>
<span class="s2">from </span><span class="s1">sklearn.utils._testing </span><span class="s2">import </span><span class="s1">(</span>
    <span class="s1">assert_allclose</span><span class="s2">,</span>
    <span class="s1">assert_almost_equal</span><span class="s2">,</span>
    <span class="s1">assert_array_almost_equal</span><span class="s2">,</span>
    <span class="s1">assert_array_equal</span><span class="s2">,</span>
    <span class="s1">ignore_warnings</span><span class="s2">,</span>
<span class="s1">)</span>
<span class="s2">from </span><span class="s1">sklearn.utils.extmath </span><span class="s2">import </span><span class="s1">fast_logdet</span>

<span class="s1">COVARIANCE_TYPE = [</span><span class="s3">&quot;full&quot;</span><span class="s2">, </span><span class="s3">&quot;tied&quot;</span><span class="s2">, </span><span class="s3">&quot;diag&quot;</span><span class="s2">, </span><span class="s3">&quot;spherical&quot;</span><span class="s1">]</span>


<span class="s2">def </span><span class="s1">generate_data(n_samples</span><span class="s2">, </span><span class="s1">n_features</span><span class="s2">, </span><span class="s1">weights</span><span class="s2">, </span><span class="s1">means</span><span class="s2">, </span><span class="s1">precisions</span><span class="s2">, </span><span class="s1">covariance_type):</span>
    <span class="s1">rng = np.random.RandomState(</span><span class="s4">0</span><span class="s1">)</span>

    <span class="s1">X = []</span>
    <span class="s2">if </span><span class="s1">covariance_type == </span><span class="s3">&quot;spherical&quot;</span><span class="s1">:</span>
        <span class="s2">for </span><span class="s1">_</span><span class="s2">, </span><span class="s1">(w</span><span class="s2">, </span><span class="s1">m</span><span class="s2">, </span><span class="s1">c) </span><span class="s2">in </span><span class="s1">enumerate(zip(weights</span><span class="s2">, </span><span class="s1">means</span><span class="s2">, </span><span class="s1">precisions[</span><span class="s3">&quot;spherical&quot;</span><span class="s1">])):</span>
            <span class="s1">X.append(</span>
                <span class="s1">rng.multivariate_normal(</span>
                    <span class="s1">m</span><span class="s2">, </span><span class="s1">c * np.eye(n_features)</span><span class="s2">, </span><span class="s1">int(np.round(w * n_samples))</span>
                <span class="s1">)</span>
            <span class="s1">)</span>
    <span class="s2">if </span><span class="s1">covariance_type == </span><span class="s3">&quot;diag&quot;</span><span class="s1">:</span>
        <span class="s2">for </span><span class="s1">_</span><span class="s2">, </span><span class="s1">(w</span><span class="s2">, </span><span class="s1">m</span><span class="s2">, </span><span class="s1">c) </span><span class="s2">in </span><span class="s1">enumerate(zip(weights</span><span class="s2">, </span><span class="s1">means</span><span class="s2">, </span><span class="s1">precisions[</span><span class="s3">&quot;diag&quot;</span><span class="s1">])):</span>
            <span class="s1">X.append(</span>
                <span class="s1">rng.multivariate_normal(m</span><span class="s2">, </span><span class="s1">np.diag(c)</span><span class="s2">, </span><span class="s1">int(np.round(w * n_samples)))</span>
            <span class="s1">)</span>
    <span class="s2">if </span><span class="s1">covariance_type == </span><span class="s3">&quot;tied&quot;</span><span class="s1">:</span>
        <span class="s2">for </span><span class="s1">_</span><span class="s2">, </span><span class="s1">(w</span><span class="s2">, </span><span class="s1">m) </span><span class="s2">in </span><span class="s1">enumerate(zip(weights</span><span class="s2">, </span><span class="s1">means)):</span>
            <span class="s1">X.append(</span>
                <span class="s1">rng.multivariate_normal(</span>
                    <span class="s1">m</span><span class="s2">, </span><span class="s1">precisions[</span><span class="s3">&quot;tied&quot;</span><span class="s1">]</span><span class="s2">, </span><span class="s1">int(np.round(w * n_samples))</span>
                <span class="s1">)</span>
            <span class="s1">)</span>
    <span class="s2">if </span><span class="s1">covariance_type == </span><span class="s3">&quot;full&quot;</span><span class="s1">:</span>
        <span class="s2">for </span><span class="s1">_</span><span class="s2">, </span><span class="s1">(w</span><span class="s2">, </span><span class="s1">m</span><span class="s2">, </span><span class="s1">c) </span><span class="s2">in </span><span class="s1">enumerate(zip(weights</span><span class="s2">, </span><span class="s1">means</span><span class="s2">, </span><span class="s1">precisions[</span><span class="s3">&quot;full&quot;</span><span class="s1">])):</span>
            <span class="s1">X.append(rng.multivariate_normal(m</span><span class="s2">, </span><span class="s1">c</span><span class="s2">, </span><span class="s1">int(np.round(w * n_samples))))</span>

    <span class="s1">X = np.vstack(X)</span>
    <span class="s2">return </span><span class="s1">X</span>


<span class="s2">class </span><span class="s1">RandomData:</span>
    <span class="s2">def </span><span class="s1">__init__(self</span><span class="s2">, </span><span class="s1">rng</span><span class="s2">, </span><span class="s1">n_samples=</span><span class="s4">200</span><span class="s2">, </span><span class="s1">n_components=</span><span class="s4">2</span><span class="s2">, </span><span class="s1">n_features=</span><span class="s4">2</span><span class="s2">, </span><span class="s1">scale=</span><span class="s4">50</span><span class="s1">):</span>
        <span class="s1">self.n_samples = n_samples</span>
        <span class="s1">self.n_components = n_components</span>
        <span class="s1">self.n_features = n_features</span>

        <span class="s1">self.weights = rng.rand(n_components)</span>
        <span class="s1">self.weights = self.weights / self.weights.sum()</span>
        <span class="s1">self.means = rng.rand(n_components</span><span class="s2">, </span><span class="s1">n_features) * scale</span>
        <span class="s1">self.covariances = {</span>
            <span class="s3">&quot;spherical&quot;</span><span class="s1">: </span><span class="s4">0.5 </span><span class="s1">+ rng.rand(n_components)</span><span class="s2">,</span>
            <span class="s3">&quot;diag&quot;</span><span class="s1">: (</span><span class="s4">0.5 </span><span class="s1">+ rng.rand(n_components</span><span class="s2">, </span><span class="s1">n_features)) ** </span><span class="s4">2</span><span class="s2">,</span>
            <span class="s3">&quot;tied&quot;</span><span class="s1">: make_spd_matrix(n_features</span><span class="s2">, </span><span class="s1">random_state=rng)</span><span class="s2">,</span>
            <span class="s3">&quot;full&quot;</span><span class="s1">: np.array(</span>
                <span class="s1">[</span>
                    <span class="s1">make_spd_matrix(n_features</span><span class="s2">, </span><span class="s1">random_state=rng) * </span><span class="s4">0.5</span>
                    <span class="s2">for </span><span class="s1">_ </span><span class="s2">in </span><span class="s1">range(n_components)</span>
                <span class="s1">]</span>
            <span class="s1">)</span><span class="s2">,</span>
        <span class="s1">}</span>
        <span class="s1">self.precisions = {</span>
            <span class="s3">&quot;spherical&quot;</span><span class="s1">: </span><span class="s4">1.0 </span><span class="s1">/ self.covariances[</span><span class="s3">&quot;spherical&quot;</span><span class="s1">]</span><span class="s2">,</span>
            <span class="s3">&quot;diag&quot;</span><span class="s1">: </span><span class="s4">1.0 </span><span class="s1">/ self.covariances[</span><span class="s3">&quot;diag&quot;</span><span class="s1">]</span><span class="s2">,</span>
            <span class="s3">&quot;tied&quot;</span><span class="s1">: linalg.inv(self.covariances[</span><span class="s3">&quot;tied&quot;</span><span class="s1">])</span><span class="s2">,</span>
            <span class="s3">&quot;full&quot;</span><span class="s1">: np.array(</span>
                <span class="s1">[linalg.inv(covariance) </span><span class="s2">for </span><span class="s1">covariance </span><span class="s2">in </span><span class="s1">self.covariances[</span><span class="s3">&quot;full&quot;</span><span class="s1">]]</span>
            <span class="s1">)</span><span class="s2">,</span>
        <span class="s1">}</span>

        <span class="s1">self.X = dict(</span>
            <span class="s1">zip(</span>
                <span class="s1">COVARIANCE_TYPE</span><span class="s2">,</span>
                <span class="s1">[</span>
                    <span class="s1">generate_data(</span>
                        <span class="s1">n_samples</span><span class="s2">,</span>
                        <span class="s1">n_features</span><span class="s2">,</span>
                        <span class="s1">self.weights</span><span class="s2">,</span>
                        <span class="s1">self.means</span><span class="s2">,</span>
                        <span class="s1">self.covariances</span><span class="s2">,</span>
                        <span class="s1">covar_type</span><span class="s2">,</span>
                    <span class="s1">)</span>
                    <span class="s2">for </span><span class="s1">covar_type </span><span class="s2">in </span><span class="s1">COVARIANCE_TYPE</span>
                <span class="s1">]</span><span class="s2">,</span>
            <span class="s1">)</span>
        <span class="s1">)</span>
        <span class="s1">self.Y = np.hstack(</span>
            <span class="s1">[</span>
                <span class="s1">np.full(int(np.round(w * n_samples))</span><span class="s2">, </span><span class="s1">k</span><span class="s2">, </span><span class="s1">dtype=int)</span>
                <span class="s2">for </span><span class="s1">k</span><span class="s2">, </span><span class="s1">w </span><span class="s2">in </span><span class="s1">enumerate(self.weights)</span>
            <span class="s1">]</span>
        <span class="s1">)</span>


<span class="s2">def </span><span class="s1">test_gaussian_mixture_attributes():</span>
    <span class="s0"># test bad parameters</span>
    <span class="s1">rng = np.random.RandomState(</span><span class="s4">0</span><span class="s1">)</span>
    <span class="s1">X = rng.rand(</span><span class="s4">10</span><span class="s2">, </span><span class="s4">2</span><span class="s1">)</span>

    <span class="s0"># test good parameters</span>
    <span class="s1">n_components</span><span class="s2">, </span><span class="s1">tol</span><span class="s2">, </span><span class="s1">n_init</span><span class="s2">, </span><span class="s1">max_iter</span><span class="s2">, </span><span class="s1">reg_covar = </span><span class="s4">2</span><span class="s2">, </span><span class="s4">1e-4</span><span class="s2">, </span><span class="s4">3</span><span class="s2">, </span><span class="s4">30</span><span class="s2">, </span><span class="s4">1e-1</span>
    <span class="s1">covariance_type</span><span class="s2">, </span><span class="s1">init_params = </span><span class="s3">&quot;full&quot;</span><span class="s2">, </span><span class="s3">&quot;random&quot;</span>
    <span class="s1">gmm = GaussianMixture(</span>
        <span class="s1">n_components=n_components</span><span class="s2">,</span>
        <span class="s1">tol=tol</span><span class="s2">,</span>
        <span class="s1">n_init=n_init</span><span class="s2">,</span>
        <span class="s1">max_iter=max_iter</span><span class="s2">,</span>
        <span class="s1">reg_covar=reg_covar</span><span class="s2">,</span>
        <span class="s1">covariance_type=covariance_type</span><span class="s2">,</span>
        <span class="s1">init_params=init_params</span><span class="s2">,</span>
    <span class="s1">).fit(X)</span>

    <span class="s2">assert </span><span class="s1">gmm.n_components == n_components</span>
    <span class="s2">assert </span><span class="s1">gmm.covariance_type == covariance_type</span>
    <span class="s2">assert </span><span class="s1">gmm.tol == tol</span>
    <span class="s2">assert </span><span class="s1">gmm.reg_covar == reg_covar</span>
    <span class="s2">assert </span><span class="s1">gmm.max_iter == max_iter</span>
    <span class="s2">assert </span><span class="s1">gmm.n_init == n_init</span>
    <span class="s2">assert </span><span class="s1">gmm.init_params == init_params</span>


<span class="s2">def </span><span class="s1">test_check_weights():</span>
    <span class="s1">rng = np.random.RandomState(</span><span class="s4">0</span><span class="s1">)</span>
    <span class="s1">rand_data = RandomData(rng)</span>

    <span class="s1">n_components = rand_data.n_components</span>
    <span class="s1">X = rand_data.X[</span><span class="s3">&quot;full&quot;</span><span class="s1">]</span>

    <span class="s1">g = GaussianMixture(n_components=n_components)</span>

    <span class="s0"># Check bad shape</span>
    <span class="s1">weights_bad_shape = rng.rand(n_components</span><span class="s2">, </span><span class="s4">1</span><span class="s1">)</span>
    <span class="s1">g.weights_init = weights_bad_shape</span>
    <span class="s1">msg = re.escape(</span>
        <span class="s3">&quot;The parameter 'weights' should have the shape of &quot;</span>
        <span class="s3">f&quot;(</span><span class="s2">{</span><span class="s1">n_components</span><span class="s2">}</span><span class="s3">,), but got </span><span class="s2">{</span><span class="s1">str(weights_bad_shape.shape)</span><span class="s2">}</span><span class="s3">&quot;</span>
    <span class="s1">)</span>
    <span class="s2">with </span><span class="s1">pytest.raises(ValueError</span><span class="s2">, </span><span class="s1">match=msg):</span>
        <span class="s1">g.fit(X)</span>

    <span class="s0"># Check bad range</span>
    <span class="s1">weights_bad_range = rng.rand(n_components) + </span><span class="s4">1</span>
    <span class="s1">g.weights_init = weights_bad_range</span>
    <span class="s1">msg = re.escape(</span>
        <span class="s3">&quot;The parameter 'weights' should be in the range [0, 1], but got&quot;</span>
        <span class="s3">f&quot; max value </span><span class="s2">{</span><span class="s1">np.min(weights_bad_range)</span><span class="s2">:</span><span class="s3">.5f</span><span class="s2">}</span><span class="s3">, &quot;</span>
        <span class="s3">f&quot;min value </span><span class="s2">{</span><span class="s1">np.max(weights_bad_range)</span><span class="s2">:</span><span class="s3">.5f</span><span class="s2">}</span><span class="s3">&quot;</span>
    <span class="s1">)</span>
    <span class="s2">with </span><span class="s1">pytest.raises(ValueError</span><span class="s2">, </span><span class="s1">match=msg):</span>
        <span class="s1">g.fit(X)</span>

    <span class="s0"># Check bad normalization</span>
    <span class="s1">weights_bad_norm = rng.rand(n_components)</span>
    <span class="s1">weights_bad_norm = weights_bad_norm / (weights_bad_norm.sum() + </span><span class="s4">1</span><span class="s1">)</span>
    <span class="s1">g.weights_init = weights_bad_norm</span>
    <span class="s1">msg = re.escape(</span>
        <span class="s3">&quot;The parameter 'weights' should be normalized, &quot;</span>
        <span class="s3">f&quot;but got sum(weights) = </span><span class="s2">{</span><span class="s1">np.sum(weights_bad_norm)</span><span class="s2">:</span><span class="s3">.5f</span><span class="s2">}</span><span class="s3">&quot;</span>
    <span class="s1">)</span>
    <span class="s2">with </span><span class="s1">pytest.raises(ValueError</span><span class="s2">, </span><span class="s1">match=msg):</span>
        <span class="s1">g.fit(X)</span>

    <span class="s0"># Check good weights matrix</span>
    <span class="s1">weights = rand_data.weights</span>
    <span class="s1">g = GaussianMixture(weights_init=weights</span><span class="s2">, </span><span class="s1">n_components=n_components)</span>
    <span class="s1">g.fit(X)</span>
    <span class="s1">assert_array_equal(weights</span><span class="s2">, </span><span class="s1">g.weights_init)</span>


<span class="s2">def </span><span class="s1">test_check_means():</span>
    <span class="s1">rng = np.random.RandomState(</span><span class="s4">0</span><span class="s1">)</span>
    <span class="s1">rand_data = RandomData(rng)</span>

    <span class="s1">n_components</span><span class="s2">, </span><span class="s1">n_features = rand_data.n_components</span><span class="s2">, </span><span class="s1">rand_data.n_features</span>
    <span class="s1">X = rand_data.X[</span><span class="s3">&quot;full&quot;</span><span class="s1">]</span>

    <span class="s1">g = GaussianMixture(n_components=n_components)</span>

    <span class="s0"># Check means bad shape</span>
    <span class="s1">means_bad_shape = rng.rand(n_components + </span><span class="s4">1</span><span class="s2">, </span><span class="s1">n_features)</span>
    <span class="s1">g.means_init = means_bad_shape</span>
    <span class="s1">msg = </span><span class="s3">&quot;The parameter 'means' should have the shape of &quot;</span>
    <span class="s2">with </span><span class="s1">pytest.raises(ValueError</span><span class="s2">, </span><span class="s1">match=msg):</span>
        <span class="s1">g.fit(X)</span>

    <span class="s0"># Check good means matrix</span>
    <span class="s1">means = rand_data.means</span>
    <span class="s1">g.means_init = means</span>
    <span class="s1">g.fit(X)</span>
    <span class="s1">assert_array_equal(means</span><span class="s2">, </span><span class="s1">g.means_init)</span>


<span class="s2">def </span><span class="s1">test_check_precisions():</span>
    <span class="s1">rng = np.random.RandomState(</span><span class="s4">0</span><span class="s1">)</span>
    <span class="s1">rand_data = RandomData(rng)</span>

    <span class="s1">n_components</span><span class="s2">, </span><span class="s1">n_features = rand_data.n_components</span><span class="s2">, </span><span class="s1">rand_data.n_features</span>

    <span class="s0"># Define the bad precisions for each covariance_type</span>
    <span class="s1">precisions_bad_shape = {</span>
        <span class="s3">&quot;full&quot;</span><span class="s1">: np.ones((n_components + </span><span class="s4">1</span><span class="s2">, </span><span class="s1">n_features</span><span class="s2">, </span><span class="s1">n_features))</span><span class="s2">,</span>
        <span class="s3">&quot;tied&quot;</span><span class="s1">: np.ones((n_features + </span><span class="s4">1</span><span class="s2">, </span><span class="s1">n_features + </span><span class="s4">1</span><span class="s1">))</span><span class="s2">,</span>
        <span class="s3">&quot;diag&quot;</span><span class="s1">: np.ones((n_components + </span><span class="s4">1</span><span class="s2">, </span><span class="s1">n_features))</span><span class="s2">,</span>
        <span class="s3">&quot;spherical&quot;</span><span class="s1">: np.ones((n_components + </span><span class="s4">1</span><span class="s1">))</span><span class="s2">,</span>
    <span class="s1">}</span>

    <span class="s0"># Define not positive-definite precisions</span>
    <span class="s1">precisions_not_pos = np.ones((n_components</span><span class="s2">, </span><span class="s1">n_features</span><span class="s2">, </span><span class="s1">n_features))</span>
    <span class="s1">precisions_not_pos[</span><span class="s4">0</span><span class="s1">] = np.eye(n_features)</span>
    <span class="s1">precisions_not_pos[</span><span class="s4">0</span><span class="s2">, </span><span class="s4">0</span><span class="s2">, </span><span class="s4">0</span><span class="s1">] = -</span><span class="s4">1.0</span>

    <span class="s1">precisions_not_positive = {</span>
        <span class="s3">&quot;full&quot;</span><span class="s1">: precisions_not_pos</span><span class="s2">,</span>
        <span class="s3">&quot;tied&quot;</span><span class="s1">: precisions_not_pos[</span><span class="s4">0</span><span class="s1">]</span><span class="s2">,</span>
        <span class="s3">&quot;diag&quot;</span><span class="s1">: np.full((n_components</span><span class="s2">, </span><span class="s1">n_features)</span><span class="s2">, </span><span class="s1">-</span><span class="s4">1.0</span><span class="s1">)</span><span class="s2">,</span>
        <span class="s3">&quot;spherical&quot;</span><span class="s1">: np.full(n_components</span><span class="s2">, </span><span class="s1">-</span><span class="s4">1.0</span><span class="s1">)</span><span class="s2">,</span>
    <span class="s1">}</span>

    <span class="s1">not_positive_errors = {</span>
        <span class="s3">&quot;full&quot;</span><span class="s1">: </span><span class="s3">&quot;symmetric, positive-definite&quot;</span><span class="s2">,</span>
        <span class="s3">&quot;tied&quot;</span><span class="s1">: </span><span class="s3">&quot;symmetric, positive-definite&quot;</span><span class="s2">,</span>
        <span class="s3">&quot;diag&quot;</span><span class="s1">: </span><span class="s3">&quot;positive&quot;</span><span class="s2">,</span>
        <span class="s3">&quot;spherical&quot;</span><span class="s1">: </span><span class="s3">&quot;positive&quot;</span><span class="s2">,</span>
    <span class="s1">}</span>

    <span class="s2">for </span><span class="s1">covar_type </span><span class="s2">in </span><span class="s1">COVARIANCE_TYPE:</span>
        <span class="s1">X = RandomData(rng).X[covar_type]</span>
        <span class="s1">g = GaussianMixture(</span>
            <span class="s1">n_components=n_components</span><span class="s2">, </span><span class="s1">covariance_type=covar_type</span><span class="s2">, </span><span class="s1">random_state=rng</span>
        <span class="s1">)</span>

        <span class="s0"># Check precisions with bad shapes</span>
        <span class="s1">g.precisions_init = precisions_bad_shape[covar_type]</span>
        <span class="s1">msg = </span><span class="s3">f&quot;The parameter '</span><span class="s2">{</span><span class="s1">covar_type</span><span class="s2">} </span><span class="s3">precision' should have the shape of&quot;</span>
        <span class="s2">with </span><span class="s1">pytest.raises(ValueError</span><span class="s2">, </span><span class="s1">match=msg):</span>
            <span class="s1">g.fit(X)</span>

        <span class="s0"># Check not positive precisions</span>
        <span class="s1">g.precisions_init = precisions_not_positive[covar_type]</span>
        <span class="s1">msg = </span><span class="s3">f&quot;'</span><span class="s2">{</span><span class="s1">covar_type</span><span class="s2">} </span><span class="s3">precision' should be </span><span class="s2">{</span><span class="s1">not_positive_errors[covar_type]</span><span class="s2">}</span><span class="s3">&quot;</span>
        <span class="s2">with </span><span class="s1">pytest.raises(ValueError</span><span class="s2">, </span><span class="s1">match=msg):</span>
            <span class="s1">g.fit(X)</span>

        <span class="s0"># Check the correct init of precisions_init</span>
        <span class="s1">g.precisions_init = rand_data.precisions[covar_type]</span>
        <span class="s1">g.fit(X)</span>
        <span class="s1">assert_array_equal(rand_data.precisions[covar_type]</span><span class="s2">, </span><span class="s1">g.precisions_init)</span>


<span class="s2">def </span><span class="s1">test_suffstat_sk_full():</span>
    <span class="s0"># compare the precision matrix compute from the</span>
    <span class="s0"># EmpiricalCovariance.covariance fitted on X*sqrt(resp)</span>
    <span class="s0"># with _sufficient_sk_full, n_components=1</span>
    <span class="s1">rng = np.random.RandomState(</span><span class="s4">0</span><span class="s1">)</span>
    <span class="s1">n_samples</span><span class="s2">, </span><span class="s1">n_features = </span><span class="s4">500</span><span class="s2">, </span><span class="s4">2</span>

    <span class="s0"># special case 1, assuming data is &quot;centered&quot;</span>
    <span class="s1">X = rng.rand(n_samples</span><span class="s2">, </span><span class="s1">n_features)</span>
    <span class="s1">resp = rng.rand(n_samples</span><span class="s2">, </span><span class="s4">1</span><span class="s1">)</span>
    <span class="s1">X_resp = np.sqrt(resp) * X</span>
    <span class="s1">nk = np.array([n_samples])</span>
    <span class="s1">xk = np.zeros((</span><span class="s4">1</span><span class="s2">, </span><span class="s1">n_features))</span>
    <span class="s1">covars_pred = _estimate_gaussian_covariances_full(resp</span><span class="s2">, </span><span class="s1">X</span><span class="s2">, </span><span class="s1">nk</span><span class="s2">, </span><span class="s1">xk</span><span class="s2">, </span><span class="s4">0</span><span class="s1">)</span>
    <span class="s1">ecov = EmpiricalCovariance(assume_centered=</span><span class="s2">True</span><span class="s1">)</span>
    <span class="s1">ecov.fit(X_resp)</span>
    <span class="s1">assert_almost_equal(ecov.error_norm(covars_pred[</span><span class="s4">0</span><span class="s1">]</span><span class="s2">, </span><span class="s1">norm=</span><span class="s3">&quot;frobenius&quot;</span><span class="s1">)</span><span class="s2">, </span><span class="s4">0</span><span class="s1">)</span>
    <span class="s1">assert_almost_equal(ecov.error_norm(covars_pred[</span><span class="s4">0</span><span class="s1">]</span><span class="s2">, </span><span class="s1">norm=</span><span class="s3">&quot;spectral&quot;</span><span class="s1">)</span><span class="s2">, </span><span class="s4">0</span><span class="s1">)</span>

    <span class="s0"># check the precision computation</span>
    <span class="s1">precs_chol_pred = _compute_precision_cholesky(covars_pred</span><span class="s2">, </span><span class="s3">&quot;full&quot;</span><span class="s1">)</span>
    <span class="s1">precs_pred = np.array([np.dot(prec</span><span class="s2">, </span><span class="s1">prec.T) </span><span class="s2">for </span><span class="s1">prec </span><span class="s2">in </span><span class="s1">precs_chol_pred])</span>
    <span class="s1">precs_est = np.array([linalg.inv(cov) </span><span class="s2">for </span><span class="s1">cov </span><span class="s2">in </span><span class="s1">covars_pred])</span>
    <span class="s1">assert_array_almost_equal(precs_est</span><span class="s2">, </span><span class="s1">precs_pred)</span>

    <span class="s0"># special case 2, assuming resp are all ones</span>
    <span class="s1">resp = np.ones((n_samples</span><span class="s2">, </span><span class="s4">1</span><span class="s1">))</span>
    <span class="s1">nk = np.array([n_samples])</span>
    <span class="s1">xk = X.mean(axis=</span><span class="s4">0</span><span class="s1">).reshape((</span><span class="s4">1</span><span class="s2">, </span><span class="s1">-</span><span class="s4">1</span><span class="s1">))</span>
    <span class="s1">covars_pred = _estimate_gaussian_covariances_full(resp</span><span class="s2">, </span><span class="s1">X</span><span class="s2">, </span><span class="s1">nk</span><span class="s2">, </span><span class="s1">xk</span><span class="s2">, </span><span class="s4">0</span><span class="s1">)</span>
    <span class="s1">ecov = EmpiricalCovariance(assume_centered=</span><span class="s2">False</span><span class="s1">)</span>
    <span class="s1">ecov.fit(X)</span>
    <span class="s1">assert_almost_equal(ecov.error_norm(covars_pred[</span><span class="s4">0</span><span class="s1">]</span><span class="s2">, </span><span class="s1">norm=</span><span class="s3">&quot;frobenius&quot;</span><span class="s1">)</span><span class="s2">, </span><span class="s4">0</span><span class="s1">)</span>
    <span class="s1">assert_almost_equal(ecov.error_norm(covars_pred[</span><span class="s4">0</span><span class="s1">]</span><span class="s2">, </span><span class="s1">norm=</span><span class="s3">&quot;spectral&quot;</span><span class="s1">)</span><span class="s2">, </span><span class="s4">0</span><span class="s1">)</span>

    <span class="s0"># check the precision computation</span>
    <span class="s1">precs_chol_pred = _compute_precision_cholesky(covars_pred</span><span class="s2">, </span><span class="s3">&quot;full&quot;</span><span class="s1">)</span>
    <span class="s1">precs_pred = np.array([np.dot(prec</span><span class="s2">, </span><span class="s1">prec.T) </span><span class="s2">for </span><span class="s1">prec </span><span class="s2">in </span><span class="s1">precs_chol_pred])</span>
    <span class="s1">precs_est = np.array([linalg.inv(cov) </span><span class="s2">for </span><span class="s1">cov </span><span class="s2">in </span><span class="s1">covars_pred])</span>
    <span class="s1">assert_array_almost_equal(precs_est</span><span class="s2">, </span><span class="s1">precs_pred)</span>


<span class="s2">def </span><span class="s1">test_suffstat_sk_tied():</span>
    <span class="s0"># use equation Nk * Sk / N = S_tied</span>
    <span class="s1">rng = np.random.RandomState(</span><span class="s4">0</span><span class="s1">)</span>
    <span class="s1">n_samples</span><span class="s2">, </span><span class="s1">n_features</span><span class="s2">, </span><span class="s1">n_components = </span><span class="s4">500</span><span class="s2">, </span><span class="s4">2</span><span class="s2">, </span><span class="s4">2</span>

    <span class="s1">resp = rng.rand(n_samples</span><span class="s2">, </span><span class="s1">n_components)</span>
    <span class="s1">resp = resp / resp.sum(axis=</span><span class="s4">1</span><span class="s1">)[:</span><span class="s2">, </span><span class="s1">np.newaxis]</span>
    <span class="s1">X = rng.rand(n_samples</span><span class="s2">, </span><span class="s1">n_features)</span>
    <span class="s1">nk = resp.sum(axis=</span><span class="s4">0</span><span class="s1">)</span>
    <span class="s1">xk = np.dot(resp.T</span><span class="s2">, </span><span class="s1">X) / nk[:</span><span class="s2">, </span><span class="s1">np.newaxis]</span>

    <span class="s1">covars_pred_full = _estimate_gaussian_covariances_full(resp</span><span class="s2">, </span><span class="s1">X</span><span class="s2">, </span><span class="s1">nk</span><span class="s2">, </span><span class="s1">xk</span><span class="s2">, </span><span class="s4">0</span><span class="s1">)</span>
    <span class="s1">covars_pred_full = (</span>
        <span class="s1">np.sum(nk[:</span><span class="s2">, </span><span class="s1">np.newaxis</span><span class="s2">, </span><span class="s1">np.newaxis] * covars_pred_full</span><span class="s2">, </span><span class="s4">0</span><span class="s1">) / n_samples</span>
    <span class="s1">)</span>

    <span class="s1">covars_pred_tied = _estimate_gaussian_covariances_tied(resp</span><span class="s2">, </span><span class="s1">X</span><span class="s2">, </span><span class="s1">nk</span><span class="s2">, </span><span class="s1">xk</span><span class="s2">, </span><span class="s4">0</span><span class="s1">)</span>

    <span class="s1">ecov = EmpiricalCovariance()</span>
    <span class="s1">ecov.covariance_ = covars_pred_full</span>
    <span class="s1">assert_almost_equal(ecov.error_norm(covars_pred_tied</span><span class="s2">, </span><span class="s1">norm=</span><span class="s3">&quot;frobenius&quot;</span><span class="s1">)</span><span class="s2">, </span><span class="s4">0</span><span class="s1">)</span>
    <span class="s1">assert_almost_equal(ecov.error_norm(covars_pred_tied</span><span class="s2">, </span><span class="s1">norm=</span><span class="s3">&quot;spectral&quot;</span><span class="s1">)</span><span class="s2">, </span><span class="s4">0</span><span class="s1">)</span>

    <span class="s0"># check the precision computation</span>
    <span class="s1">precs_chol_pred = _compute_precision_cholesky(covars_pred_tied</span><span class="s2">, </span><span class="s3">&quot;tied&quot;</span><span class="s1">)</span>
    <span class="s1">precs_pred = np.dot(precs_chol_pred</span><span class="s2">, </span><span class="s1">precs_chol_pred.T)</span>
    <span class="s1">precs_est = linalg.inv(covars_pred_tied)</span>
    <span class="s1">assert_array_almost_equal(precs_est</span><span class="s2">, </span><span class="s1">precs_pred)</span>


<span class="s2">def </span><span class="s1">test_suffstat_sk_diag():</span>
    <span class="s0"># test against 'full' case</span>
    <span class="s1">rng = np.random.RandomState(</span><span class="s4">0</span><span class="s1">)</span>
    <span class="s1">n_samples</span><span class="s2">, </span><span class="s1">n_features</span><span class="s2">, </span><span class="s1">n_components = </span><span class="s4">500</span><span class="s2">, </span><span class="s4">2</span><span class="s2">, </span><span class="s4">2</span>

    <span class="s1">resp = rng.rand(n_samples</span><span class="s2">, </span><span class="s1">n_components)</span>
    <span class="s1">resp = resp / resp.sum(axis=</span><span class="s4">1</span><span class="s1">)[:</span><span class="s2">, </span><span class="s1">np.newaxis]</span>
    <span class="s1">X = rng.rand(n_samples</span><span class="s2">, </span><span class="s1">n_features)</span>
    <span class="s1">nk = resp.sum(axis=</span><span class="s4">0</span><span class="s1">)</span>
    <span class="s1">xk = np.dot(resp.T</span><span class="s2">, </span><span class="s1">X) / nk[:</span><span class="s2">, </span><span class="s1">np.newaxis]</span>
    <span class="s1">covars_pred_full = _estimate_gaussian_covariances_full(resp</span><span class="s2">, </span><span class="s1">X</span><span class="s2">, </span><span class="s1">nk</span><span class="s2">, </span><span class="s1">xk</span><span class="s2">, </span><span class="s4">0</span><span class="s1">)</span>
    <span class="s1">covars_pred_diag = _estimate_gaussian_covariances_diag(resp</span><span class="s2">, </span><span class="s1">X</span><span class="s2">, </span><span class="s1">nk</span><span class="s2">, </span><span class="s1">xk</span><span class="s2">, </span><span class="s4">0</span><span class="s1">)</span>

    <span class="s1">ecov = EmpiricalCovariance()</span>
    <span class="s2">for </span><span class="s1">cov_full</span><span class="s2">, </span><span class="s1">cov_diag </span><span class="s2">in </span><span class="s1">zip(covars_pred_full</span><span class="s2">, </span><span class="s1">covars_pred_diag):</span>
        <span class="s1">ecov.covariance_ = np.diag(np.diag(cov_full))</span>
        <span class="s1">cov_diag = np.diag(cov_diag)</span>
        <span class="s1">assert_almost_equal(ecov.error_norm(cov_diag</span><span class="s2">, </span><span class="s1">norm=</span><span class="s3">&quot;frobenius&quot;</span><span class="s1">)</span><span class="s2">, </span><span class="s4">0</span><span class="s1">)</span>
        <span class="s1">assert_almost_equal(ecov.error_norm(cov_diag</span><span class="s2">, </span><span class="s1">norm=</span><span class="s3">&quot;spectral&quot;</span><span class="s1">)</span><span class="s2">, </span><span class="s4">0</span><span class="s1">)</span>

    <span class="s0"># check the precision computation</span>
    <span class="s1">precs_chol_pred = _compute_precision_cholesky(covars_pred_diag</span><span class="s2">, </span><span class="s3">&quot;diag&quot;</span><span class="s1">)</span>
    <span class="s1">assert_almost_equal(covars_pred_diag</span><span class="s2">, </span><span class="s4">1.0 </span><span class="s1">/ precs_chol_pred**</span><span class="s4">2</span><span class="s1">)</span>


<span class="s2">def </span><span class="s1">test_gaussian_suffstat_sk_spherical():</span>
    <span class="s0"># computing spherical covariance equals to the variance of one-dimension</span>
    <span class="s0"># data after flattening, n_components=1</span>
    <span class="s1">rng = np.random.RandomState(</span><span class="s4">0</span><span class="s1">)</span>
    <span class="s1">n_samples</span><span class="s2">, </span><span class="s1">n_features = </span><span class="s4">500</span><span class="s2">, </span><span class="s4">2</span>

    <span class="s1">X = rng.rand(n_samples</span><span class="s2">, </span><span class="s1">n_features)</span>
    <span class="s1">X = X - X.mean()</span>
    <span class="s1">resp = np.ones((n_samples</span><span class="s2">, </span><span class="s4">1</span><span class="s1">))</span>
    <span class="s1">nk = np.array([n_samples])</span>
    <span class="s1">xk = X.mean()</span>
    <span class="s1">covars_pred_spherical = _estimate_gaussian_covariances_spherical(resp</span><span class="s2">, </span><span class="s1">X</span><span class="s2">, </span><span class="s1">nk</span><span class="s2">, </span><span class="s1">xk</span><span class="s2">, </span><span class="s4">0</span><span class="s1">)</span>
    <span class="s1">covars_pred_spherical2 = np.dot(X.flatten().T</span><span class="s2">, </span><span class="s1">X.flatten()) / (</span>
        <span class="s1">n_features * n_samples</span>
    <span class="s1">)</span>
    <span class="s1">assert_almost_equal(covars_pred_spherical</span><span class="s2">, </span><span class="s1">covars_pred_spherical2)</span>

    <span class="s0"># check the precision computation</span>
    <span class="s1">precs_chol_pred = _compute_precision_cholesky(covars_pred_spherical</span><span class="s2">, </span><span class="s3">&quot;spherical&quot;</span><span class="s1">)</span>
    <span class="s1">assert_almost_equal(covars_pred_spherical</span><span class="s2">, </span><span class="s4">1.0 </span><span class="s1">/ precs_chol_pred**</span><span class="s4">2</span><span class="s1">)</span>


<span class="s2">def </span><span class="s1">test_compute_log_det_cholesky():</span>
    <span class="s1">n_features = </span><span class="s4">2</span>
    <span class="s1">rand_data = RandomData(np.random.RandomState(</span><span class="s4">0</span><span class="s1">))</span>

    <span class="s2">for </span><span class="s1">covar_type </span><span class="s2">in </span><span class="s1">COVARIANCE_TYPE:</span>
        <span class="s1">covariance = rand_data.covariances[covar_type]</span>

        <span class="s2">if </span><span class="s1">covar_type == </span><span class="s3">&quot;full&quot;</span><span class="s1">:</span>
            <span class="s1">predected_det = np.array([linalg.det(cov) </span><span class="s2">for </span><span class="s1">cov </span><span class="s2">in </span><span class="s1">covariance])</span>
        <span class="s2">elif </span><span class="s1">covar_type == </span><span class="s3">&quot;tied&quot;</span><span class="s1">:</span>
            <span class="s1">predected_det = linalg.det(covariance)</span>
        <span class="s2">elif </span><span class="s1">covar_type == </span><span class="s3">&quot;diag&quot;</span><span class="s1">:</span>
            <span class="s1">predected_det = np.array([np.prod(cov) </span><span class="s2">for </span><span class="s1">cov </span><span class="s2">in </span><span class="s1">covariance])</span>
        <span class="s2">elif </span><span class="s1">covar_type == </span><span class="s3">&quot;spherical&quot;</span><span class="s1">:</span>
            <span class="s1">predected_det = covariance**n_features</span>

        <span class="s0"># We compute the cholesky decomposition of the covariance matrix</span>
        <span class="s1">expected_det = _compute_log_det_cholesky(</span>
            <span class="s1">_compute_precision_cholesky(covariance</span><span class="s2">, </span><span class="s1">covar_type)</span><span class="s2">,</span>
            <span class="s1">covar_type</span><span class="s2">,</span>
            <span class="s1">n_features=n_features</span><span class="s2">,</span>
        <span class="s1">)</span>
        <span class="s1">assert_array_almost_equal(expected_det</span><span class="s2">, </span><span class="s1">-</span><span class="s4">0.5 </span><span class="s1">* np.log(predected_det))</span>


<span class="s2">def </span><span class="s1">_naive_lmvnpdf_diag(X</span><span class="s2">, </span><span class="s1">means</span><span class="s2">, </span><span class="s1">covars):</span>
    <span class="s1">resp = np.empty((len(X)</span><span class="s2">, </span><span class="s1">len(means)))</span>
    <span class="s1">stds = np.sqrt(covars)</span>
    <span class="s2">for </span><span class="s1">i</span><span class="s2">, </span><span class="s1">(mean</span><span class="s2">, </span><span class="s1">std) </span><span class="s2">in </span><span class="s1">enumerate(zip(means</span><span class="s2">, </span><span class="s1">stds)):</span>
        <span class="s1">resp[:</span><span class="s2">, </span><span class="s1">i] = stats.norm.logpdf(X</span><span class="s2">, </span><span class="s1">mean</span><span class="s2">, </span><span class="s1">std).sum(axis=</span><span class="s4">1</span><span class="s1">)</span>
    <span class="s2">return </span><span class="s1">resp</span>


<span class="s2">def </span><span class="s1">test_gaussian_mixture_log_probabilities():</span>
    <span class="s2">from </span><span class="s1">sklearn.mixture._gaussian_mixture </span><span class="s2">import </span><span class="s1">_estimate_log_gaussian_prob</span>

    <span class="s0"># test against with _naive_lmvnpdf_diag</span>
    <span class="s1">rng = np.random.RandomState(</span><span class="s4">0</span><span class="s1">)</span>
    <span class="s1">rand_data = RandomData(rng)</span>
    <span class="s1">n_samples = </span><span class="s4">500</span>
    <span class="s1">n_features = rand_data.n_features</span>
    <span class="s1">n_components = rand_data.n_components</span>

    <span class="s1">means = rand_data.means</span>
    <span class="s1">covars_diag = rng.rand(n_components</span><span class="s2">, </span><span class="s1">n_features)</span>
    <span class="s1">X = rng.rand(n_samples</span><span class="s2">, </span><span class="s1">n_features)</span>
    <span class="s1">log_prob_naive = _naive_lmvnpdf_diag(X</span><span class="s2">, </span><span class="s1">means</span><span class="s2">, </span><span class="s1">covars_diag)</span>

    <span class="s0"># full covariances</span>
    <span class="s1">precs_full = np.array([np.diag(</span><span class="s4">1.0 </span><span class="s1">/ np.sqrt(x)) </span><span class="s2">for </span><span class="s1">x </span><span class="s2">in </span><span class="s1">covars_diag])</span>

    <span class="s1">log_prob = _estimate_log_gaussian_prob(X</span><span class="s2">, </span><span class="s1">means</span><span class="s2">, </span><span class="s1">precs_full</span><span class="s2">, </span><span class="s3">&quot;full&quot;</span><span class="s1">)</span>
    <span class="s1">assert_array_almost_equal(log_prob</span><span class="s2">, </span><span class="s1">log_prob_naive)</span>

    <span class="s0"># diag covariances</span>
    <span class="s1">precs_chol_diag = </span><span class="s4">1.0 </span><span class="s1">/ np.sqrt(covars_diag)</span>
    <span class="s1">log_prob = _estimate_log_gaussian_prob(X</span><span class="s2">, </span><span class="s1">means</span><span class="s2">, </span><span class="s1">precs_chol_diag</span><span class="s2">, </span><span class="s3">&quot;diag&quot;</span><span class="s1">)</span>
    <span class="s1">assert_array_almost_equal(log_prob</span><span class="s2">, </span><span class="s1">log_prob_naive)</span>

    <span class="s0"># tied</span>
    <span class="s1">covars_tied = np.array([x </span><span class="s2">for </span><span class="s1">x </span><span class="s2">in </span><span class="s1">covars_diag]).mean(axis=</span><span class="s4">0</span><span class="s1">)</span>
    <span class="s1">precs_tied = np.diag(np.sqrt(</span><span class="s4">1.0 </span><span class="s1">/ covars_tied))</span>

    <span class="s1">log_prob_naive = _naive_lmvnpdf_diag(X</span><span class="s2">, </span><span class="s1">means</span><span class="s2">, </span><span class="s1">[covars_tied] * n_components)</span>
    <span class="s1">log_prob = _estimate_log_gaussian_prob(X</span><span class="s2">, </span><span class="s1">means</span><span class="s2">, </span><span class="s1">precs_tied</span><span class="s2">, </span><span class="s3">&quot;tied&quot;</span><span class="s1">)</span>

    <span class="s1">assert_array_almost_equal(log_prob</span><span class="s2">, </span><span class="s1">log_prob_naive)</span>

    <span class="s0"># spherical</span>
    <span class="s1">covars_spherical = covars_diag.mean(axis=</span><span class="s4">1</span><span class="s1">)</span>
    <span class="s1">precs_spherical = </span><span class="s4">1.0 </span><span class="s1">/ np.sqrt(covars_diag.mean(axis=</span><span class="s4">1</span><span class="s1">))</span>
    <span class="s1">log_prob_naive = _naive_lmvnpdf_diag(</span>
        <span class="s1">X</span><span class="s2">, </span><span class="s1">means</span><span class="s2">, </span><span class="s1">[[k] * n_features </span><span class="s2">for </span><span class="s1">k </span><span class="s2">in </span><span class="s1">covars_spherical]</span>
    <span class="s1">)</span>
    <span class="s1">log_prob = _estimate_log_gaussian_prob(X</span><span class="s2">, </span><span class="s1">means</span><span class="s2">, </span><span class="s1">precs_spherical</span><span class="s2">, </span><span class="s3">&quot;spherical&quot;</span><span class="s1">)</span>
    <span class="s1">assert_array_almost_equal(log_prob</span><span class="s2">, </span><span class="s1">log_prob_naive)</span>


<span class="s0"># skip tests on weighted_log_probabilities, log_weights</span>


<span class="s2">def </span><span class="s1">test_gaussian_mixture_estimate_log_prob_resp():</span>
    <span class="s0"># test whether responsibilities are normalized</span>
    <span class="s1">rng = np.random.RandomState(</span><span class="s4">0</span><span class="s1">)</span>
    <span class="s1">rand_data = RandomData(rng</span><span class="s2">, </span><span class="s1">scale=</span><span class="s4">5</span><span class="s1">)</span>
    <span class="s1">n_samples = rand_data.n_samples</span>
    <span class="s1">n_features = rand_data.n_features</span>
    <span class="s1">n_components = rand_data.n_components</span>

    <span class="s1">X = rng.rand(n_samples</span><span class="s2">, </span><span class="s1">n_features)</span>
    <span class="s2">for </span><span class="s1">covar_type </span><span class="s2">in </span><span class="s1">COVARIANCE_TYPE:</span>
        <span class="s1">weights = rand_data.weights</span>
        <span class="s1">means = rand_data.means</span>
        <span class="s1">precisions = rand_data.precisions[covar_type]</span>
        <span class="s1">g = GaussianMixture(</span>
            <span class="s1">n_components=n_components</span><span class="s2">,</span>
            <span class="s1">random_state=rng</span><span class="s2">,</span>
            <span class="s1">weights_init=weights</span><span class="s2">,</span>
            <span class="s1">means_init=means</span><span class="s2">,</span>
            <span class="s1">precisions_init=precisions</span><span class="s2">,</span>
            <span class="s1">covariance_type=covar_type</span><span class="s2">,</span>
        <span class="s1">)</span>
        <span class="s1">g.fit(X)</span>
        <span class="s1">resp = g.predict_proba(X)</span>
        <span class="s1">assert_array_almost_equal(resp.sum(axis=</span><span class="s4">1</span><span class="s1">)</span><span class="s2">, </span><span class="s1">np.ones(n_samples))</span>
        <span class="s1">assert_array_equal(g.weights_init</span><span class="s2">, </span><span class="s1">weights)</span>
        <span class="s1">assert_array_equal(g.means_init</span><span class="s2">, </span><span class="s1">means)</span>
        <span class="s1">assert_array_equal(g.precisions_init</span><span class="s2">, </span><span class="s1">precisions)</span>


<span class="s2">def </span><span class="s1">test_gaussian_mixture_predict_predict_proba():</span>
    <span class="s1">rng = np.random.RandomState(</span><span class="s4">0</span><span class="s1">)</span>
    <span class="s1">rand_data = RandomData(rng)</span>
    <span class="s2">for </span><span class="s1">covar_type </span><span class="s2">in </span><span class="s1">COVARIANCE_TYPE:</span>
        <span class="s1">X = rand_data.X[covar_type]</span>
        <span class="s1">Y = rand_data.Y</span>
        <span class="s1">g = GaussianMixture(</span>
            <span class="s1">n_components=rand_data.n_components</span><span class="s2">,</span>
            <span class="s1">random_state=rng</span><span class="s2">,</span>
            <span class="s1">weights_init=rand_data.weights</span><span class="s2">,</span>
            <span class="s1">means_init=rand_data.means</span><span class="s2">,</span>
            <span class="s1">precisions_init=rand_data.precisions[covar_type]</span><span class="s2">,</span>
            <span class="s1">covariance_type=covar_type</span><span class="s2">,</span>
        <span class="s1">)</span>

        <span class="s0"># Check a warning message arrive if we don't do fit</span>
        <span class="s1">msg = (</span>
            <span class="s3">&quot;This GaussianMixture instance is not fitted yet. Call 'fit' &quot;</span>
            <span class="s3">&quot;with appropriate arguments before using this estimator.&quot;</span>
        <span class="s1">)</span>
        <span class="s2">with </span><span class="s1">pytest.raises(NotFittedError</span><span class="s2">, </span><span class="s1">match=msg):</span>
            <span class="s1">g.predict(X)</span>

        <span class="s1">g.fit(X)</span>
        <span class="s1">Y_pred = g.predict(X)</span>
        <span class="s1">Y_pred_proba = g.predict_proba(X).argmax(axis=</span><span class="s4">1</span><span class="s1">)</span>
        <span class="s1">assert_array_equal(Y_pred</span><span class="s2">, </span><span class="s1">Y_pred_proba)</span>
        <span class="s2">assert </span><span class="s1">adjusted_rand_score(Y</span><span class="s2">, </span><span class="s1">Y_pred) &gt; </span><span class="s4">0.95</span>


<span class="s1">@pytest.mark.filterwarnings(</span><span class="s3">&quot;ignore:.*did not converge.*&quot;</span><span class="s1">)</span>
<span class="s1">@pytest.mark.parametrize(</span>
    <span class="s3">&quot;seed, max_iter, tol&quot;</span><span class="s2">,</span>
    <span class="s1">[</span>
        <span class="s1">(</span><span class="s4">0</span><span class="s2">, </span><span class="s4">2</span><span class="s2">, </span><span class="s4">1e-7</span><span class="s1">)</span><span class="s2">,  </span><span class="s0"># strict non-convergence</span>
        <span class="s1">(</span><span class="s4">1</span><span class="s2">, </span><span class="s4">2</span><span class="s2">, </span><span class="s4">1e-1</span><span class="s1">)</span><span class="s2">,  </span><span class="s0"># loose non-convergence</span>
        <span class="s1">(</span><span class="s4">3</span><span class="s2">, </span><span class="s4">300</span><span class="s2">, </span><span class="s4">1e-7</span><span class="s1">)</span><span class="s2">,  </span><span class="s0"># strict convergence</span>
        <span class="s1">(</span><span class="s4">4</span><span class="s2">, </span><span class="s4">300</span><span class="s2">, </span><span class="s4">1e-1</span><span class="s1">)</span><span class="s2">,  </span><span class="s0"># loose convergence</span>
    <span class="s1">]</span><span class="s2">,</span>
<span class="s1">)</span>
<span class="s2">def </span><span class="s1">test_gaussian_mixture_fit_predict(seed</span><span class="s2">, </span><span class="s1">max_iter</span><span class="s2">, </span><span class="s1">tol):</span>
    <span class="s1">rng = np.random.RandomState(seed)</span>
    <span class="s1">rand_data = RandomData(rng)</span>
    <span class="s2">for </span><span class="s1">covar_type </span><span class="s2">in </span><span class="s1">COVARIANCE_TYPE:</span>
        <span class="s1">X = rand_data.X[covar_type]</span>
        <span class="s1">Y = rand_data.Y</span>
        <span class="s1">g = GaussianMixture(</span>
            <span class="s1">n_components=rand_data.n_components</span><span class="s2">,</span>
            <span class="s1">random_state=rng</span><span class="s2">,</span>
            <span class="s1">weights_init=rand_data.weights</span><span class="s2">,</span>
            <span class="s1">means_init=rand_data.means</span><span class="s2">,</span>
            <span class="s1">precisions_init=rand_data.precisions[covar_type]</span><span class="s2">,</span>
            <span class="s1">covariance_type=covar_type</span><span class="s2">,</span>
            <span class="s1">max_iter=max_iter</span><span class="s2">,</span>
            <span class="s1">tol=tol</span><span class="s2">,</span>
        <span class="s1">)</span>

        <span class="s0"># check if fit_predict(X) is equivalent to fit(X).predict(X)</span>
        <span class="s1">f = copy.deepcopy(g)</span>
        <span class="s1">Y_pred1 = f.fit(X).predict(X)</span>
        <span class="s1">Y_pred2 = g.fit_predict(X)</span>
        <span class="s1">assert_array_equal(Y_pred1</span><span class="s2">, </span><span class="s1">Y_pred2)</span>
        <span class="s2">assert </span><span class="s1">adjusted_rand_score(Y</span><span class="s2">, </span><span class="s1">Y_pred2) &gt; </span><span class="s4">0.95</span>


<span class="s2">def </span><span class="s1">test_gaussian_mixture_fit_predict_n_init():</span>
    <span class="s0"># Check that fit_predict is equivalent to fit.predict, when n_init &gt; 1</span>
    <span class="s1">X = np.random.RandomState(</span><span class="s4">0</span><span class="s1">).randn(</span><span class="s4">1000</span><span class="s2">, </span><span class="s4">5</span><span class="s1">)</span>
    <span class="s1">gm = GaussianMixture(n_components=</span><span class="s4">5</span><span class="s2">, </span><span class="s1">n_init=</span><span class="s4">5</span><span class="s2">, </span><span class="s1">random_state=</span><span class="s4">0</span><span class="s1">)</span>
    <span class="s1">y_pred1 = gm.fit_predict(X)</span>
    <span class="s1">y_pred2 = gm.predict(X)</span>
    <span class="s1">assert_array_equal(y_pred1</span><span class="s2">, </span><span class="s1">y_pred2)</span>


<span class="s2">def </span><span class="s1">test_gaussian_mixture_fit():</span>
    <span class="s0"># recover the ground truth</span>
    <span class="s1">rng = np.random.RandomState(</span><span class="s4">0</span><span class="s1">)</span>
    <span class="s1">rand_data = RandomData(rng)</span>
    <span class="s1">n_features = rand_data.n_features</span>
    <span class="s1">n_components = rand_data.n_components</span>

    <span class="s2">for </span><span class="s1">covar_type </span><span class="s2">in </span><span class="s1">COVARIANCE_TYPE:</span>
        <span class="s1">X = rand_data.X[covar_type]</span>
        <span class="s1">g = GaussianMixture(</span>
            <span class="s1">n_components=n_components</span><span class="s2">,</span>
            <span class="s1">n_init=</span><span class="s4">20</span><span class="s2">,</span>
            <span class="s1">reg_covar=</span><span class="s4">0</span><span class="s2">,</span>
            <span class="s1">random_state=rng</span><span class="s2">,</span>
            <span class="s1">covariance_type=covar_type</span><span class="s2">,</span>
        <span class="s1">)</span>
        <span class="s1">g.fit(X)</span>

        <span class="s0"># needs more data to pass the test with rtol=1e-7</span>
        <span class="s1">assert_allclose(</span>
            <span class="s1">np.sort(g.weights_)</span><span class="s2">, </span><span class="s1">np.sort(rand_data.weights)</span><span class="s2">, </span><span class="s1">rtol=</span><span class="s4">0.1</span><span class="s2">, </span><span class="s1">atol=</span><span class="s4">1e-2</span>
        <span class="s1">)</span>

        <span class="s1">arg_idx1 = g.means_[:</span><span class="s2">, </span><span class="s4">0</span><span class="s1">].argsort()</span>
        <span class="s1">arg_idx2 = rand_data.means[:</span><span class="s2">, </span><span class="s4">0</span><span class="s1">].argsort()</span>
        <span class="s1">assert_allclose(</span>
            <span class="s1">g.means_[arg_idx1]</span><span class="s2">, </span><span class="s1">rand_data.means[arg_idx2]</span><span class="s2">, </span><span class="s1">rtol=</span><span class="s4">0.1</span><span class="s2">, </span><span class="s1">atol=</span><span class="s4">1e-2</span>
        <span class="s1">)</span>

        <span class="s2">if </span><span class="s1">covar_type == </span><span class="s3">&quot;full&quot;</span><span class="s1">:</span>
            <span class="s1">prec_pred = g.precisions_</span>
            <span class="s1">prec_test = rand_data.precisions[</span><span class="s3">&quot;full&quot;</span><span class="s1">]</span>
        <span class="s2">elif </span><span class="s1">covar_type == </span><span class="s3">&quot;tied&quot;</span><span class="s1">:</span>
            <span class="s1">prec_pred = np.array([g.precisions_] * n_components)</span>
            <span class="s1">prec_test = np.array([rand_data.precisions[</span><span class="s3">&quot;tied&quot;</span><span class="s1">]] * n_components)</span>
        <span class="s2">elif </span><span class="s1">covar_type == </span><span class="s3">&quot;spherical&quot;</span><span class="s1">:</span>
            <span class="s1">prec_pred = np.array([np.eye(n_features) * c </span><span class="s2">for </span><span class="s1">c </span><span class="s2">in </span><span class="s1">g.precisions_])</span>
            <span class="s1">prec_test = np.array(</span>
                <span class="s1">[np.eye(n_features) * c </span><span class="s2">for </span><span class="s1">c </span><span class="s2">in </span><span class="s1">rand_data.precisions[</span><span class="s3">&quot;spherical&quot;</span><span class="s1">]]</span>
            <span class="s1">)</span>
        <span class="s2">elif </span><span class="s1">covar_type == </span><span class="s3">&quot;diag&quot;</span><span class="s1">:</span>
            <span class="s1">prec_pred = np.array([np.diag(d) </span><span class="s2">for </span><span class="s1">d </span><span class="s2">in </span><span class="s1">g.precisions_])</span>
            <span class="s1">prec_test = np.array([np.diag(d) </span><span class="s2">for </span><span class="s1">d </span><span class="s2">in </span><span class="s1">rand_data.precisions[</span><span class="s3">&quot;diag&quot;</span><span class="s1">]])</span>

        <span class="s1">arg_idx1 = np.trace(prec_pred</span><span class="s2">, </span><span class="s1">axis1=</span><span class="s4">1</span><span class="s2">, </span><span class="s1">axis2=</span><span class="s4">2</span><span class="s1">).argsort()</span>
        <span class="s1">arg_idx2 = np.trace(prec_test</span><span class="s2">, </span><span class="s1">axis1=</span><span class="s4">1</span><span class="s2">, </span><span class="s1">axis2=</span><span class="s4">2</span><span class="s1">).argsort()</span>
        <span class="s2">for </span><span class="s1">k</span><span class="s2">, </span><span class="s1">h </span><span class="s2">in </span><span class="s1">zip(arg_idx1</span><span class="s2">, </span><span class="s1">arg_idx2):</span>
            <span class="s1">ecov = EmpiricalCovariance()</span>
            <span class="s1">ecov.covariance_ = prec_test[h]</span>
            <span class="s0"># the accuracy depends on the number of data and randomness, rng</span>
            <span class="s1">assert_allclose(ecov.error_norm(prec_pred[k])</span><span class="s2">, </span><span class="s4">0</span><span class="s2">, </span><span class="s1">atol=</span><span class="s4">0.15</span><span class="s1">)</span>


<span class="s2">def </span><span class="s1">test_gaussian_mixture_fit_best_params():</span>
    <span class="s1">rng = np.random.RandomState(</span><span class="s4">0</span><span class="s1">)</span>
    <span class="s1">rand_data = RandomData(rng)</span>
    <span class="s1">n_components = rand_data.n_components</span>
    <span class="s1">n_init = </span><span class="s4">10</span>
    <span class="s2">for </span><span class="s1">covar_type </span><span class="s2">in </span><span class="s1">COVARIANCE_TYPE:</span>
        <span class="s1">X = rand_data.X[covar_type]</span>
        <span class="s1">g = GaussianMixture(</span>
            <span class="s1">n_components=n_components</span><span class="s2">,</span>
            <span class="s1">n_init=</span><span class="s4">1</span><span class="s2">,</span>
            <span class="s1">reg_covar=</span><span class="s4">0</span><span class="s2">,</span>
            <span class="s1">random_state=rng</span><span class="s2">,</span>
            <span class="s1">covariance_type=covar_type</span><span class="s2">,</span>
        <span class="s1">)</span>
        <span class="s1">ll = []</span>
        <span class="s2">for </span><span class="s1">_ </span><span class="s2">in </span><span class="s1">range(n_init):</span>
            <span class="s1">g.fit(X)</span>
            <span class="s1">ll.append(g.score(X))</span>
        <span class="s1">ll = np.array(ll)</span>
        <span class="s1">g_best = GaussianMixture(</span>
            <span class="s1">n_components=n_components</span><span class="s2">,</span>
            <span class="s1">n_init=n_init</span><span class="s2">,</span>
            <span class="s1">reg_covar=</span><span class="s4">0</span><span class="s2">,</span>
            <span class="s1">random_state=rng</span><span class="s2">,</span>
            <span class="s1">covariance_type=covar_type</span><span class="s2">,</span>
        <span class="s1">)</span>
        <span class="s1">g_best.fit(X)</span>
        <span class="s1">assert_almost_equal(ll.min()</span><span class="s2">, </span><span class="s1">g_best.score(X))</span>


<span class="s2">def </span><span class="s1">test_gaussian_mixture_fit_convergence_warning():</span>
    <span class="s1">rng = np.random.RandomState(</span><span class="s4">0</span><span class="s1">)</span>
    <span class="s1">rand_data = RandomData(rng</span><span class="s2">, </span><span class="s1">scale=</span><span class="s4">1</span><span class="s1">)</span>
    <span class="s1">n_components = rand_data.n_components</span>
    <span class="s1">max_iter = </span><span class="s4">1</span>
    <span class="s2">for </span><span class="s1">covar_type </span><span class="s2">in </span><span class="s1">COVARIANCE_TYPE:</span>
        <span class="s1">X = rand_data.X[covar_type]</span>
        <span class="s1">g = GaussianMixture(</span>
            <span class="s1">n_components=n_components</span><span class="s2">,</span>
            <span class="s1">n_init=</span><span class="s4">1</span><span class="s2">,</span>
            <span class="s1">max_iter=max_iter</span><span class="s2">,</span>
            <span class="s1">reg_covar=</span><span class="s4">0</span><span class="s2">,</span>
            <span class="s1">random_state=rng</span><span class="s2">,</span>
            <span class="s1">covariance_type=covar_type</span><span class="s2">,</span>
        <span class="s1">)</span>
        <span class="s1">msg = (</span>
            <span class="s3">f&quot;Initialization </span><span class="s2">{</span><span class="s1">max_iter</span><span class="s2">} </span><span class="s3">did not converge. Try different init &quot;</span>
            <span class="s3">&quot;parameters, or increase max_iter, tol or check for degenerate&quot;</span>
            <span class="s3">&quot; data.&quot;</span>
        <span class="s1">)</span>
        <span class="s2">with </span><span class="s1">pytest.warns(ConvergenceWarning</span><span class="s2">, </span><span class="s1">match=msg):</span>
            <span class="s1">g.fit(X)</span>


<span class="s2">def </span><span class="s1">test_multiple_init():</span>
    <span class="s0"># Test that multiple inits does not much worse than a single one</span>
    <span class="s1">rng = np.random.RandomState(</span><span class="s4">0</span><span class="s1">)</span>
    <span class="s1">n_samples</span><span class="s2">, </span><span class="s1">n_features</span><span class="s2">, </span><span class="s1">n_components = </span><span class="s4">50</span><span class="s2">, </span><span class="s4">5</span><span class="s2">, </span><span class="s4">2</span>
    <span class="s1">X = rng.randn(n_samples</span><span class="s2">, </span><span class="s1">n_features)</span>
    <span class="s2">for </span><span class="s1">cv_type </span><span class="s2">in </span><span class="s1">COVARIANCE_TYPE:</span>
        <span class="s1">train1 = (</span>
            <span class="s1">GaussianMixture(</span>
                <span class="s1">n_components=n_components</span><span class="s2">, </span><span class="s1">covariance_type=cv_type</span><span class="s2">, </span><span class="s1">random_state=</span><span class="s4">0</span>
            <span class="s1">)</span>
            <span class="s1">.fit(X)</span>
            <span class="s1">.score(X)</span>
        <span class="s1">)</span>
        <span class="s1">train2 = (</span>
            <span class="s1">GaussianMixture(</span>
                <span class="s1">n_components=n_components</span><span class="s2">,</span>
                <span class="s1">covariance_type=cv_type</span><span class="s2">,</span>
                <span class="s1">random_state=</span><span class="s4">0</span><span class="s2">,</span>
                <span class="s1">n_init=</span><span class="s4">5</span><span class="s2">,</span>
            <span class="s1">)</span>
            <span class="s1">.fit(X)</span>
            <span class="s1">.score(X)</span>
        <span class="s1">)</span>
        <span class="s2">assert </span><span class="s1">train2 &gt;= train1</span>


<span class="s2">def </span><span class="s1">test_gaussian_mixture_n_parameters():</span>
    <span class="s0"># Test that the right number of parameters is estimated</span>
    <span class="s1">rng = np.random.RandomState(</span><span class="s4">0</span><span class="s1">)</span>
    <span class="s1">n_samples</span><span class="s2">, </span><span class="s1">n_features</span><span class="s2">, </span><span class="s1">n_components = </span><span class="s4">50</span><span class="s2">, </span><span class="s4">5</span><span class="s2">, </span><span class="s4">2</span>
    <span class="s1">X = rng.randn(n_samples</span><span class="s2">, </span><span class="s1">n_features)</span>
    <span class="s1">n_params = {</span><span class="s3">&quot;spherical&quot;</span><span class="s1">: </span><span class="s4">13</span><span class="s2">, </span><span class="s3">&quot;diag&quot;</span><span class="s1">: </span><span class="s4">21</span><span class="s2">, </span><span class="s3">&quot;tied&quot;</span><span class="s1">: </span><span class="s4">26</span><span class="s2">, </span><span class="s3">&quot;full&quot;</span><span class="s1">: </span><span class="s4">41</span><span class="s1">}</span>
    <span class="s2">for </span><span class="s1">cv_type </span><span class="s2">in </span><span class="s1">COVARIANCE_TYPE:</span>
        <span class="s1">g = GaussianMixture(</span>
            <span class="s1">n_components=n_components</span><span class="s2">, </span><span class="s1">covariance_type=cv_type</span><span class="s2">, </span><span class="s1">random_state=rng</span>
        <span class="s1">).fit(X)</span>
        <span class="s2">assert </span><span class="s1">g._n_parameters() == n_params[cv_type]</span>


<span class="s2">def </span><span class="s1">test_bic_1d_1component():</span>
    <span class="s0"># Test all of the covariance_types return the same BIC score for</span>
    <span class="s0"># 1-dimensional, 1 component fits.</span>
    <span class="s1">rng = np.random.RandomState(</span><span class="s4">0</span><span class="s1">)</span>
    <span class="s1">n_samples</span><span class="s2">, </span><span class="s1">n_dim</span><span class="s2">, </span><span class="s1">n_components = </span><span class="s4">100</span><span class="s2">, </span><span class="s4">1</span><span class="s2">, </span><span class="s4">1</span>
    <span class="s1">X = rng.randn(n_samples</span><span class="s2">, </span><span class="s1">n_dim)</span>
    <span class="s1">bic_full = (</span>
        <span class="s1">GaussianMixture(</span>
            <span class="s1">n_components=n_components</span><span class="s2">, </span><span class="s1">covariance_type=</span><span class="s3">&quot;full&quot;</span><span class="s2">, </span><span class="s1">random_state=rng</span>
        <span class="s1">)</span>
        <span class="s1">.fit(X)</span>
        <span class="s1">.bic(X)</span>
    <span class="s1">)</span>
    <span class="s2">for </span><span class="s1">covariance_type </span><span class="s2">in </span><span class="s1">[</span><span class="s3">&quot;tied&quot;</span><span class="s2">, </span><span class="s3">&quot;diag&quot;</span><span class="s2">, </span><span class="s3">&quot;spherical&quot;</span><span class="s1">]:</span>
        <span class="s1">bic = (</span>
            <span class="s1">GaussianMixture(</span>
                <span class="s1">n_components=n_components</span><span class="s2">,</span>
                <span class="s1">covariance_type=covariance_type</span><span class="s2">,</span>
                <span class="s1">random_state=rng</span><span class="s2">,</span>
            <span class="s1">)</span>
            <span class="s1">.fit(X)</span>
            <span class="s1">.bic(X)</span>
        <span class="s1">)</span>
        <span class="s1">assert_almost_equal(bic_full</span><span class="s2">, </span><span class="s1">bic)</span>


<span class="s2">def </span><span class="s1">test_gaussian_mixture_aic_bic():</span>
    <span class="s0"># Test the aic and bic criteria</span>
    <span class="s1">rng = np.random.RandomState(</span><span class="s4">0</span><span class="s1">)</span>
    <span class="s1">n_samples</span><span class="s2">, </span><span class="s1">n_features</span><span class="s2">, </span><span class="s1">n_components = </span><span class="s4">50</span><span class="s2">, </span><span class="s4">3</span><span class="s2">, </span><span class="s4">2</span>
    <span class="s1">X = rng.randn(n_samples</span><span class="s2">, </span><span class="s1">n_features)</span>
    <span class="s0"># standard gaussian entropy</span>
    <span class="s1">sgh = </span><span class="s4">0.5 </span><span class="s1">* (</span>
        <span class="s1">fast_logdet(np.cov(X.T</span><span class="s2">, </span><span class="s1">bias=</span><span class="s4">1</span><span class="s1">)) + n_features * (</span><span class="s4">1 </span><span class="s1">+ np.log(</span><span class="s4">2 </span><span class="s1">* np.pi))</span>
    <span class="s1">)</span>
    <span class="s2">for </span><span class="s1">cv_type </span><span class="s2">in </span><span class="s1">COVARIANCE_TYPE:</span>
        <span class="s1">g = GaussianMixture(</span>
            <span class="s1">n_components=n_components</span><span class="s2">,</span>
            <span class="s1">covariance_type=cv_type</span><span class="s2">,</span>
            <span class="s1">random_state=rng</span><span class="s2">,</span>
            <span class="s1">max_iter=</span><span class="s4">200</span><span class="s2">,</span>
        <span class="s1">)</span>
        <span class="s1">g.fit(X)</span>
        <span class="s1">aic = </span><span class="s4">2 </span><span class="s1">* n_samples * sgh + </span><span class="s4">2 </span><span class="s1">* g._n_parameters()</span>
        <span class="s1">bic = </span><span class="s4">2 </span><span class="s1">* n_samples * sgh + np.log(n_samples) * g._n_parameters()</span>
        <span class="s1">bound = n_features / np.sqrt(n_samples)</span>
        <span class="s2">assert </span><span class="s1">(g.aic(X) - aic) / n_samples &lt; bound</span>
        <span class="s2">assert </span><span class="s1">(g.bic(X) - bic) / n_samples &lt; bound</span>


<span class="s2">def </span><span class="s1">test_gaussian_mixture_verbose():</span>
    <span class="s1">rng = np.random.RandomState(</span><span class="s4">0</span><span class="s1">)</span>
    <span class="s1">rand_data = RandomData(rng)</span>
    <span class="s1">n_components = rand_data.n_components</span>
    <span class="s2">for </span><span class="s1">covar_type </span><span class="s2">in </span><span class="s1">COVARIANCE_TYPE:</span>
        <span class="s1">X = rand_data.X[covar_type]</span>
        <span class="s1">g = GaussianMixture(</span>
            <span class="s1">n_components=n_components</span><span class="s2">,</span>
            <span class="s1">n_init=</span><span class="s4">1</span><span class="s2">,</span>
            <span class="s1">reg_covar=</span><span class="s4">0</span><span class="s2">,</span>
            <span class="s1">random_state=rng</span><span class="s2">,</span>
            <span class="s1">covariance_type=covar_type</span><span class="s2">,</span>
            <span class="s1">verbose=</span><span class="s4">1</span><span class="s2">,</span>
        <span class="s1">)</span>
        <span class="s1">h = GaussianMixture(</span>
            <span class="s1">n_components=n_components</span><span class="s2">,</span>
            <span class="s1">n_init=</span><span class="s4">1</span><span class="s2">,</span>
            <span class="s1">reg_covar=</span><span class="s4">0</span><span class="s2">,</span>
            <span class="s1">random_state=rng</span><span class="s2">,</span>
            <span class="s1">covariance_type=covar_type</span><span class="s2">,</span>
            <span class="s1">verbose=</span><span class="s4">2</span><span class="s2">,</span>
        <span class="s1">)</span>
        <span class="s1">old_stdout = sys.stdout</span>
        <span class="s1">sys.stdout = StringIO()</span>
        <span class="s2">try</span><span class="s1">:</span>
            <span class="s1">g.fit(X)</span>
            <span class="s1">h.fit(X)</span>
        <span class="s2">finally</span><span class="s1">:</span>
            <span class="s1">sys.stdout = old_stdout</span>


<span class="s1">@pytest.mark.filterwarnings(</span><span class="s3">&quot;ignore:.*did not converge.*&quot;</span><span class="s1">)</span>
<span class="s1">@pytest.mark.parametrize(</span><span class="s3">&quot;seed&quot;</span><span class="s2">, </span><span class="s1">(</span><span class="s4">0</span><span class="s2">, </span><span class="s4">1</span><span class="s2">, </span><span class="s4">2</span><span class="s1">))</span>
<span class="s2">def </span><span class="s1">test_warm_start(seed):</span>
    <span class="s1">random_state = seed</span>
    <span class="s1">rng = np.random.RandomState(random_state)</span>
    <span class="s1">n_samples</span><span class="s2">, </span><span class="s1">n_features</span><span class="s2">, </span><span class="s1">n_components = </span><span class="s4">500</span><span class="s2">, </span><span class="s4">2</span><span class="s2">, </span><span class="s4">2</span>
    <span class="s1">X = rng.rand(n_samples</span><span class="s2">, </span><span class="s1">n_features)</span>

    <span class="s0"># Assert the warm_start give the same result for the same number of iter</span>
    <span class="s1">g = GaussianMixture(</span>
        <span class="s1">n_components=n_components</span><span class="s2">,</span>
        <span class="s1">n_init=</span><span class="s4">1</span><span class="s2">,</span>
        <span class="s1">max_iter=</span><span class="s4">2</span><span class="s2">,</span>
        <span class="s1">reg_covar=</span><span class="s4">0</span><span class="s2">,</span>
        <span class="s1">random_state=random_state</span><span class="s2">,</span>
        <span class="s1">warm_start=</span><span class="s2">False,</span>
    <span class="s1">)</span>
    <span class="s1">h = GaussianMixture(</span>
        <span class="s1">n_components=n_components</span><span class="s2">,</span>
        <span class="s1">n_init=</span><span class="s4">1</span><span class="s2">,</span>
        <span class="s1">max_iter=</span><span class="s4">1</span><span class="s2">,</span>
        <span class="s1">reg_covar=</span><span class="s4">0</span><span class="s2">,</span>
        <span class="s1">random_state=random_state</span><span class="s2">,</span>
        <span class="s1">warm_start=</span><span class="s2">True,</span>
    <span class="s1">)</span>

    <span class="s1">g.fit(X)</span>
    <span class="s1">score1 = h.fit(X).score(X)</span>
    <span class="s1">score2 = h.fit(X).score(X)</span>

    <span class="s1">assert_almost_equal(g.weights_</span><span class="s2">, </span><span class="s1">h.weights_)</span>
    <span class="s1">assert_almost_equal(g.means_</span><span class="s2">, </span><span class="s1">h.means_)</span>
    <span class="s1">assert_almost_equal(g.precisions_</span><span class="s2">, </span><span class="s1">h.precisions_)</span>
    <span class="s2">assert </span><span class="s1">score2 &gt; score1</span>

    <span class="s0"># Assert that by using warm_start we can converge to a good solution</span>
    <span class="s1">g = GaussianMixture(</span>
        <span class="s1">n_components=n_components</span><span class="s2">,</span>
        <span class="s1">n_init=</span><span class="s4">1</span><span class="s2">,</span>
        <span class="s1">max_iter=</span><span class="s4">5</span><span class="s2">,</span>
        <span class="s1">reg_covar=</span><span class="s4">0</span><span class="s2">,</span>
        <span class="s1">random_state=random_state</span><span class="s2">,</span>
        <span class="s1">warm_start=</span><span class="s2">False,</span>
        <span class="s1">tol=</span><span class="s4">1e-6</span><span class="s2">,</span>
    <span class="s1">)</span>
    <span class="s1">h = GaussianMixture(</span>
        <span class="s1">n_components=n_components</span><span class="s2">,</span>
        <span class="s1">n_init=</span><span class="s4">1</span><span class="s2">,</span>
        <span class="s1">max_iter=</span><span class="s4">5</span><span class="s2">,</span>
        <span class="s1">reg_covar=</span><span class="s4">0</span><span class="s2">,</span>
        <span class="s1">random_state=random_state</span><span class="s2">,</span>
        <span class="s1">warm_start=</span><span class="s2">True,</span>
        <span class="s1">tol=</span><span class="s4">1e-6</span><span class="s2">,</span>
    <span class="s1">)</span>

    <span class="s1">g.fit(X)</span>
    <span class="s2">assert not </span><span class="s1">g.converged_</span>

    <span class="s1">h.fit(X)</span>
    <span class="s0"># depending on the data there is large variability in the number of</span>
    <span class="s0"># refit necessary to converge due to the complete randomness of the</span>
    <span class="s0"># data</span>
    <span class="s2">for </span><span class="s1">_ </span><span class="s2">in </span><span class="s1">range(</span><span class="s4">1000</span><span class="s1">):</span>
        <span class="s1">h.fit(X)</span>
        <span class="s2">if </span><span class="s1">h.converged_:</span>
            <span class="s2">break</span>
    <span class="s2">assert </span><span class="s1">h.converged_</span>


<span class="s1">@ignore_warnings(category=ConvergenceWarning)</span>
<span class="s2">def </span><span class="s1">test_convergence_detected_with_warm_start():</span>
    <span class="s0"># We check that convergence is detected when warm_start=True</span>
    <span class="s1">rng = np.random.RandomState(</span><span class="s4">0</span><span class="s1">)</span>
    <span class="s1">rand_data = RandomData(rng)</span>
    <span class="s1">n_components = rand_data.n_components</span>
    <span class="s1">X = rand_data.X[</span><span class="s3">&quot;full&quot;</span><span class="s1">]</span>

    <span class="s2">for </span><span class="s1">max_iter </span><span class="s2">in </span><span class="s1">(</span><span class="s4">1</span><span class="s2">, </span><span class="s4">2</span><span class="s2">, </span><span class="s4">50</span><span class="s1">):</span>
        <span class="s1">gmm = GaussianMixture(</span>
            <span class="s1">n_components=n_components</span><span class="s2">,</span>
            <span class="s1">warm_start=</span><span class="s2">True,</span>
            <span class="s1">max_iter=max_iter</span><span class="s2">,</span>
            <span class="s1">random_state=rng</span><span class="s2">,</span>
        <span class="s1">)</span>
        <span class="s2">for </span><span class="s1">_ </span><span class="s2">in </span><span class="s1">range(</span><span class="s4">100</span><span class="s1">):</span>
            <span class="s1">gmm.fit(X)</span>
            <span class="s2">if </span><span class="s1">gmm.converged_:</span>
                <span class="s2">break</span>
        <span class="s2">assert </span><span class="s1">gmm.converged_</span>
        <span class="s2">assert </span><span class="s1">max_iter &gt;= gmm.n_iter_</span>


<span class="s2">def </span><span class="s1">test_score():</span>
    <span class="s1">covar_type = </span><span class="s3">&quot;full&quot;</span>
    <span class="s1">rng = np.random.RandomState(</span><span class="s4">0</span><span class="s1">)</span>
    <span class="s1">rand_data = RandomData(rng</span><span class="s2">, </span><span class="s1">scale=</span><span class="s4">7</span><span class="s1">)</span>
    <span class="s1">n_components = rand_data.n_components</span>
    <span class="s1">X = rand_data.X[covar_type]</span>

    <span class="s0"># Check the error message if we don't call fit</span>
    <span class="s1">gmm1 = GaussianMixture(</span>
        <span class="s1">n_components=n_components</span><span class="s2">,</span>
        <span class="s1">n_init=</span><span class="s4">1</span><span class="s2">,</span>
        <span class="s1">max_iter=</span><span class="s4">1</span><span class="s2">,</span>
        <span class="s1">reg_covar=</span><span class="s4">0</span><span class="s2">,</span>
        <span class="s1">random_state=rng</span><span class="s2">,</span>
        <span class="s1">covariance_type=covar_type</span><span class="s2">,</span>
    <span class="s1">)</span>
    <span class="s1">msg = (</span>
        <span class="s3">&quot;This GaussianMixture instance is not fitted yet. Call 'fit' with &quot;</span>
        <span class="s3">&quot;appropriate arguments before using this estimator.&quot;</span>
    <span class="s1">)</span>
    <span class="s2">with </span><span class="s1">pytest.raises(NotFittedError</span><span class="s2">, </span><span class="s1">match=msg):</span>
        <span class="s1">gmm1.score(X)</span>

    <span class="s0"># Check score value</span>
    <span class="s2">with </span><span class="s1">warnings.catch_warnings():</span>
        <span class="s1">warnings.simplefilter(</span><span class="s3">&quot;ignore&quot;</span><span class="s2">, </span><span class="s1">ConvergenceWarning)</span>
        <span class="s1">gmm1.fit(X)</span>
    <span class="s1">gmm_score = gmm1.score(X)</span>
    <span class="s1">gmm_score_proba = gmm1.score_samples(X).mean()</span>
    <span class="s1">assert_almost_equal(gmm_score</span><span class="s2">, </span><span class="s1">gmm_score_proba)</span>

    <span class="s0"># Check if the score increase</span>
    <span class="s1">gmm2 = GaussianMixture(</span>
        <span class="s1">n_components=n_components</span><span class="s2">,</span>
        <span class="s1">n_init=</span><span class="s4">1</span><span class="s2">,</span>
        <span class="s1">reg_covar=</span><span class="s4">0</span><span class="s2">,</span>
        <span class="s1">random_state=rng</span><span class="s2">,</span>
        <span class="s1">covariance_type=covar_type</span><span class="s2">,</span>
    <span class="s1">).fit(X)</span>
    <span class="s2">assert </span><span class="s1">gmm2.score(X) &gt; gmm1.score(X)</span>


<span class="s2">def </span><span class="s1">test_score_samples():</span>
    <span class="s1">covar_type = </span><span class="s3">&quot;full&quot;</span>
    <span class="s1">rng = np.random.RandomState(</span><span class="s4">0</span><span class="s1">)</span>
    <span class="s1">rand_data = RandomData(rng</span><span class="s2">, </span><span class="s1">scale=</span><span class="s4">7</span><span class="s1">)</span>
    <span class="s1">n_components = rand_data.n_components</span>
    <span class="s1">X = rand_data.X[covar_type]</span>

    <span class="s0"># Check the error message if we don't call fit</span>
    <span class="s1">gmm = GaussianMixture(</span>
        <span class="s1">n_components=n_components</span><span class="s2">,</span>
        <span class="s1">n_init=</span><span class="s4">1</span><span class="s2">,</span>
        <span class="s1">reg_covar=</span><span class="s4">0</span><span class="s2">,</span>
        <span class="s1">random_state=rng</span><span class="s2">,</span>
        <span class="s1">covariance_type=covar_type</span><span class="s2">,</span>
    <span class="s1">)</span>
    <span class="s1">msg = (</span>
        <span class="s3">&quot;This GaussianMixture instance is not fitted yet. Call 'fit' with &quot;</span>
        <span class="s3">&quot;appropriate arguments before using this estimator.&quot;</span>
    <span class="s1">)</span>
    <span class="s2">with </span><span class="s1">pytest.raises(NotFittedError</span><span class="s2">, </span><span class="s1">match=msg):</span>
        <span class="s1">gmm.score_samples(X)</span>

    <span class="s1">gmm_score_samples = gmm.fit(X).score_samples(X)</span>
    <span class="s2">assert </span><span class="s1">gmm_score_samples.shape[</span><span class="s4">0</span><span class="s1">] == rand_data.n_samples</span>


<span class="s2">def </span><span class="s1">test_monotonic_likelihood():</span>
    <span class="s0"># We check that each step of the EM without regularization improve</span>
    <span class="s0"># monotonically the training set likelihood</span>
    <span class="s1">rng = np.random.RandomState(</span><span class="s4">0</span><span class="s1">)</span>
    <span class="s1">rand_data = RandomData(rng</span><span class="s2">, </span><span class="s1">scale=</span><span class="s4">7</span><span class="s1">)</span>
    <span class="s1">n_components = rand_data.n_components</span>

    <span class="s2">for </span><span class="s1">covar_type </span><span class="s2">in </span><span class="s1">COVARIANCE_TYPE:</span>
        <span class="s1">X = rand_data.X[covar_type]</span>
        <span class="s1">gmm = GaussianMixture(</span>
            <span class="s1">n_components=n_components</span><span class="s2">,</span>
            <span class="s1">covariance_type=covar_type</span><span class="s2">,</span>
            <span class="s1">reg_covar=</span><span class="s4">0</span><span class="s2">,</span>
            <span class="s1">warm_start=</span><span class="s2">True,</span>
            <span class="s1">max_iter=</span><span class="s4">1</span><span class="s2">,</span>
            <span class="s1">random_state=rng</span><span class="s2">,</span>
            <span class="s1">tol=</span><span class="s4">1e-7</span><span class="s2">,</span>
        <span class="s1">)</span>
        <span class="s1">current_log_likelihood = -np.inf</span>
        <span class="s2">with </span><span class="s1">warnings.catch_warnings():</span>
            <span class="s1">warnings.simplefilter(</span><span class="s3">&quot;ignore&quot;</span><span class="s2">, </span><span class="s1">ConvergenceWarning)</span>
            <span class="s0"># Do one training iteration at a time so we can make sure that the</span>
            <span class="s0"># training log likelihood increases after each iteration.</span>
            <span class="s2">for </span><span class="s1">_ </span><span class="s2">in </span><span class="s1">range(</span><span class="s4">600</span><span class="s1">):</span>
                <span class="s1">prev_log_likelihood = current_log_likelihood</span>
                <span class="s1">current_log_likelihood = gmm.fit(X).score(X)</span>
                <span class="s2">assert </span><span class="s1">current_log_likelihood &gt;= prev_log_likelihood</span>

                <span class="s2">if </span><span class="s1">gmm.converged_:</span>
                    <span class="s2">break</span>

            <span class="s2">assert </span><span class="s1">gmm.converged_</span>


<span class="s2">def </span><span class="s1">test_regularisation():</span>
    <span class="s0"># We train the GaussianMixture on degenerate data by defining two clusters</span>
    <span class="s0"># of a 0 covariance.</span>
    <span class="s1">rng = np.random.RandomState(</span><span class="s4">0</span><span class="s1">)</span>
    <span class="s1">n_samples</span><span class="s2">, </span><span class="s1">n_features = </span><span class="s4">10</span><span class="s2">, </span><span class="s4">5</span>

    <span class="s1">X = np.vstack(</span>
        <span class="s1">(np.ones((n_samples // </span><span class="s4">2</span><span class="s2">, </span><span class="s1">n_features))</span><span class="s2">, </span><span class="s1">np.zeros((n_samples // </span><span class="s4">2</span><span class="s2">, </span><span class="s1">n_features)))</span>
    <span class="s1">)</span>

    <span class="s2">for </span><span class="s1">covar_type </span><span class="s2">in </span><span class="s1">COVARIANCE_TYPE:</span>
        <span class="s1">gmm = GaussianMixture(</span>
            <span class="s1">n_components=n_samples</span><span class="s2">,</span>
            <span class="s1">reg_covar=</span><span class="s4">0</span><span class="s2">,</span>
            <span class="s1">covariance_type=covar_type</span><span class="s2">,</span>
            <span class="s1">random_state=rng</span><span class="s2">,</span>
        <span class="s1">)</span>

        <span class="s2">with </span><span class="s1">warnings.catch_warnings():</span>
            <span class="s1">warnings.simplefilter(</span><span class="s3">&quot;ignore&quot;</span><span class="s2">, </span><span class="s1">RuntimeWarning)</span>
            <span class="s1">msg = re.escape(</span>
                <span class="s3">&quot;Fitting the mixture model failed because some components have&quot;</span>
                <span class="s3">&quot; ill-defined empirical covariance (for instance caused by &quot;</span>
                <span class="s3">&quot;singleton or collapsed samples). Try to decrease the number &quot;</span>
                <span class="s3">&quot;of components, or increase reg_covar.&quot;</span>
            <span class="s1">)</span>
            <span class="s2">with </span><span class="s1">pytest.raises(ValueError</span><span class="s2">, </span><span class="s1">match=msg):</span>
                <span class="s1">gmm.fit(X)</span>

            <span class="s1">gmm.set_params(reg_covar=</span><span class="s4">1e-6</span><span class="s1">).fit(X)</span>


<span class="s2">def </span><span class="s1">test_property():</span>
    <span class="s1">rng = np.random.RandomState(</span><span class="s4">0</span><span class="s1">)</span>
    <span class="s1">rand_data = RandomData(rng</span><span class="s2">, </span><span class="s1">scale=</span><span class="s4">7</span><span class="s1">)</span>
    <span class="s1">n_components = rand_data.n_components</span>

    <span class="s2">for </span><span class="s1">covar_type </span><span class="s2">in </span><span class="s1">COVARIANCE_TYPE:</span>
        <span class="s1">X = rand_data.X[covar_type]</span>
        <span class="s1">gmm = GaussianMixture(</span>
            <span class="s1">n_components=n_components</span><span class="s2">,</span>
            <span class="s1">covariance_type=covar_type</span><span class="s2">,</span>
            <span class="s1">random_state=rng</span><span class="s2">,</span>
            <span class="s1">n_init=</span><span class="s4">5</span><span class="s2">,</span>
        <span class="s1">)</span>
        <span class="s1">gmm.fit(X)</span>
        <span class="s2">if </span><span class="s1">covar_type == </span><span class="s3">&quot;full&quot;</span><span class="s1">:</span>
            <span class="s2">for </span><span class="s1">prec</span><span class="s2">, </span><span class="s1">covar </span><span class="s2">in </span><span class="s1">zip(gmm.precisions_</span><span class="s2">, </span><span class="s1">gmm.covariances_):</span>
                <span class="s1">assert_array_almost_equal(linalg.inv(prec)</span><span class="s2">, </span><span class="s1">covar)</span>
        <span class="s2">elif </span><span class="s1">covar_type == </span><span class="s3">&quot;tied&quot;</span><span class="s1">:</span>
            <span class="s1">assert_array_almost_equal(linalg.inv(gmm.precisions_)</span><span class="s2">, </span><span class="s1">gmm.covariances_)</span>
        <span class="s2">else</span><span class="s1">:</span>
            <span class="s1">assert_array_almost_equal(gmm.precisions_</span><span class="s2">, </span><span class="s4">1.0 </span><span class="s1">/ gmm.covariances_)</span>


<span class="s2">def </span><span class="s1">test_sample():</span>
    <span class="s1">rng = np.random.RandomState(</span><span class="s4">0</span><span class="s1">)</span>
    <span class="s1">rand_data = RandomData(rng</span><span class="s2">, </span><span class="s1">scale=</span><span class="s4">7</span><span class="s2">, </span><span class="s1">n_components=</span><span class="s4">3</span><span class="s1">)</span>
    <span class="s1">n_features</span><span class="s2">, </span><span class="s1">n_components = rand_data.n_features</span><span class="s2">, </span><span class="s1">rand_data.n_components</span>

    <span class="s2">for </span><span class="s1">covar_type </span><span class="s2">in </span><span class="s1">COVARIANCE_TYPE:</span>
        <span class="s1">X = rand_data.X[covar_type]</span>

        <span class="s1">gmm = GaussianMixture(</span>
            <span class="s1">n_components=n_components</span><span class="s2">, </span><span class="s1">covariance_type=covar_type</span><span class="s2">, </span><span class="s1">random_state=rng</span>
        <span class="s1">)</span>
        <span class="s0"># To sample we need that GaussianMixture is fitted</span>
        <span class="s1">msg = </span><span class="s3">&quot;This GaussianMixture instance is not fitted&quot;</span>
        <span class="s2">with </span><span class="s1">pytest.raises(NotFittedError</span><span class="s2">, </span><span class="s1">match=msg):</span>
            <span class="s1">gmm.sample(</span><span class="s4">0</span><span class="s1">)</span>
        <span class="s1">gmm.fit(X)</span>

        <span class="s1">msg = </span><span class="s3">&quot;Invalid value for 'n_samples'&quot;</span>
        <span class="s2">with </span><span class="s1">pytest.raises(ValueError</span><span class="s2">, </span><span class="s1">match=msg):</span>
            <span class="s1">gmm.sample(</span><span class="s4">0</span><span class="s1">)</span>

        <span class="s0"># Just to make sure the class samples correctly</span>
        <span class="s1">n_samples = </span><span class="s4">20000</span>
        <span class="s1">X_s</span><span class="s2">, </span><span class="s1">y_s = gmm.sample(n_samples)</span>

        <span class="s2">for </span><span class="s1">k </span><span class="s2">in </span><span class="s1">range(n_components):</span>
            <span class="s2">if </span><span class="s1">covar_type == </span><span class="s3">&quot;full&quot;</span><span class="s1">:</span>
                <span class="s1">assert_array_almost_equal(</span>
                    <span class="s1">gmm.covariances_[k]</span><span class="s2">, </span><span class="s1">np.cov(X_s[y_s == k].T)</span><span class="s2">, </span><span class="s1">decimal=</span><span class="s4">1</span>
                <span class="s1">)</span>
            <span class="s2">elif </span><span class="s1">covar_type == </span><span class="s3">&quot;tied&quot;</span><span class="s1">:</span>
                <span class="s1">assert_array_almost_equal(</span>
                    <span class="s1">gmm.covariances_</span><span class="s2">, </span><span class="s1">np.cov(X_s[y_s == k].T)</span><span class="s2">, </span><span class="s1">decimal=</span><span class="s4">1</span>
                <span class="s1">)</span>
            <span class="s2">elif </span><span class="s1">covar_type == </span><span class="s3">&quot;diag&quot;</span><span class="s1">:</span>
                <span class="s1">assert_array_almost_equal(</span>
                    <span class="s1">gmm.covariances_[k]</span><span class="s2">, </span><span class="s1">np.diag(np.cov(X_s[y_s == k].T))</span><span class="s2">, </span><span class="s1">decimal=</span><span class="s4">1</span>
                <span class="s1">)</span>
            <span class="s2">else</span><span class="s1">:</span>
                <span class="s1">assert_array_almost_equal(</span>
                    <span class="s1">gmm.covariances_[k]</span><span class="s2">,</span>
                    <span class="s1">np.var(X_s[y_s == k] - gmm.means_[k])</span><span class="s2">,</span>
                    <span class="s1">decimal=</span><span class="s4">1</span><span class="s2">,</span>
                <span class="s1">)</span>

        <span class="s1">means_s = np.array([np.mean(X_s[y_s == k]</span><span class="s2">, </span><span class="s4">0</span><span class="s1">) </span><span class="s2">for </span><span class="s1">k </span><span class="s2">in </span><span class="s1">range(n_components)])</span>
        <span class="s1">assert_array_almost_equal(gmm.means_</span><span class="s2">, </span><span class="s1">means_s</span><span class="s2">, </span><span class="s1">decimal=</span><span class="s4">1</span><span class="s1">)</span>

        <span class="s0"># Check shapes of sampled data, see</span>
        <span class="s0"># https://github.com/scikit-learn/scikit-learn/issues/7701</span>
        <span class="s2">assert </span><span class="s1">X_s.shape == (n_samples</span><span class="s2">, </span><span class="s1">n_features)</span>

        <span class="s2">for </span><span class="s1">sample_size </span><span class="s2">in </span><span class="s1">range(</span><span class="s4">1</span><span class="s2">, </span><span class="s4">100</span><span class="s1">):</span>
            <span class="s1">X_s</span><span class="s2">, </span><span class="s1">_ = gmm.sample(sample_size)</span>
            <span class="s2">assert </span><span class="s1">X_s.shape == (sample_size</span><span class="s2">, </span><span class="s1">n_features)</span>


<span class="s1">@ignore_warnings(category=ConvergenceWarning)</span>
<span class="s2">def </span><span class="s1">test_init():</span>
    <span class="s0"># We check that by increasing the n_init number we have a better solution</span>
    <span class="s2">for </span><span class="s1">random_state </span><span class="s2">in </span><span class="s1">range(</span><span class="s4">15</span><span class="s1">):</span>
        <span class="s1">rand_data = RandomData(</span>
            <span class="s1">np.random.RandomState(random_state)</span><span class="s2">, </span><span class="s1">n_samples=</span><span class="s4">50</span><span class="s2">, </span><span class="s1">scale=</span><span class="s4">1</span>
        <span class="s1">)</span>
        <span class="s1">n_components = rand_data.n_components</span>
        <span class="s1">X = rand_data.X[</span><span class="s3">&quot;full&quot;</span><span class="s1">]</span>

        <span class="s1">gmm1 = GaussianMixture(</span>
            <span class="s1">n_components=n_components</span><span class="s2">, </span><span class="s1">n_init=</span><span class="s4">1</span><span class="s2">, </span><span class="s1">max_iter=</span><span class="s4">1</span><span class="s2">, </span><span class="s1">random_state=random_state</span>
        <span class="s1">).fit(X)</span>
        <span class="s1">gmm2 = GaussianMixture(</span>
            <span class="s1">n_components=n_components</span><span class="s2">, </span><span class="s1">n_init=</span><span class="s4">10</span><span class="s2">, </span><span class="s1">max_iter=</span><span class="s4">1</span><span class="s2">, </span><span class="s1">random_state=random_state</span>
        <span class="s1">).fit(X)</span>

        <span class="s2">assert </span><span class="s1">gmm2.lower_bound_ &gt;= gmm1.lower_bound_</span>


<span class="s2">def </span><span class="s1">test_gaussian_mixture_setting_best_params():</span>
    <span class="s5">&quot;&quot;&quot;`GaussianMixture`'s best_parameters, `n_iter_` and `lower_bound_` 
    must be set appropriately in the case of divergence. 
 
    Non-regression test for: 
    https://github.com/scikit-learn/scikit-learn/issues/18216 
    &quot;&quot;&quot;</span>
    <span class="s1">rnd = np.random.RandomState(</span><span class="s4">0</span><span class="s1">)</span>
    <span class="s1">n_samples = </span><span class="s4">30</span>
    <span class="s1">X = rnd.uniform(size=(n_samples</span><span class="s2">, </span><span class="s4">3</span><span class="s1">))</span>

    <span class="s0"># following initialization parameters were found to lead to divergence</span>
    <span class="s1">means_init = np.array(</span>
        <span class="s1">[</span>
            <span class="s1">[</span><span class="s4">0.670637869618158</span><span class="s2">, </span><span class="s4">0.21038256107384043</span><span class="s2">, </span><span class="s4">0.12892629765485303</span><span class="s1">]</span><span class="s2">,</span>
            <span class="s1">[</span><span class="s4">0.09394051075844147</span><span class="s2">, </span><span class="s4">0.5759464955561779</span><span class="s2">, </span><span class="s4">0.929296197576212</span><span class="s1">]</span><span class="s2">,</span>
            <span class="s1">[</span><span class="s4">0.5033230372781258</span><span class="s2">, </span><span class="s4">0.9569852381759425</span><span class="s2">, </span><span class="s4">0.08654043447295741</span><span class="s1">]</span><span class="s2">,</span>
            <span class="s1">[</span><span class="s4">0.18578301420435747</span><span class="s2">, </span><span class="s4">0.5531158970919143</span><span class="s2">, </span><span class="s4">0.19388943970532435</span><span class="s1">]</span><span class="s2">,</span>
            <span class="s1">[</span><span class="s4">0.4548589928173794</span><span class="s2">, </span><span class="s4">0.35182513658825276</span><span class="s2">, </span><span class="s4">0.568146063202464</span><span class="s1">]</span><span class="s2">,</span>
            <span class="s1">[</span><span class="s4">0.609279894978321</span><span class="s2">, </span><span class="s4">0.7929063819678847</span><span class="s2">, </span><span class="s4">0.9620097270828052</span><span class="s1">]</span><span class="s2">,</span>
        <span class="s1">]</span>
    <span class="s1">)</span>
    <span class="s1">precisions_init = np.array(</span>
        <span class="s1">[</span>
            <span class="s4">999999.999604483</span><span class="s2">,</span>
            <span class="s4">999999.9990869573</span><span class="s2">,</span>
            <span class="s4">553.7603944542167</span><span class="s2">,</span>
            <span class="s4">204.78596008931834</span><span class="s2">,</span>
            <span class="s4">15.867423501783637</span><span class="s2">,</span>
            <span class="s4">85.4595728389735</span><span class="s2">,</span>
        <span class="s1">]</span>
    <span class="s1">)</span>
    <span class="s1">weights_init = [</span>
        <span class="s4">0.03333333333333341</span><span class="s2">,</span>
        <span class="s4">0.03333333333333341</span><span class="s2">,</span>
        <span class="s4">0.06666666666666674</span><span class="s2">,</span>
        <span class="s4">0.06666666666666674</span><span class="s2">,</span>
        <span class="s4">0.7000000000000001</span><span class="s2">,</span>
        <span class="s4">0.10000000000000007</span><span class="s2">,</span>
    <span class="s1">]</span>

    <span class="s1">gmm = GaussianMixture(</span>
        <span class="s1">covariance_type=</span><span class="s3">&quot;spherical&quot;</span><span class="s2">,</span>
        <span class="s1">reg_covar=</span><span class="s4">0</span><span class="s2">,</span>
        <span class="s1">means_init=means_init</span><span class="s2">,</span>
        <span class="s1">weights_init=weights_init</span><span class="s2">,</span>
        <span class="s1">random_state=rnd</span><span class="s2">,</span>
        <span class="s1">n_components=len(weights_init)</span><span class="s2">,</span>
        <span class="s1">precisions_init=precisions_init</span><span class="s2">,</span>
        <span class="s1">max_iter=</span><span class="s4">1</span><span class="s2">,</span>
    <span class="s1">)</span>
    <span class="s0"># ensure that no error is thrown during fit</span>
    <span class="s1">gmm.fit(X)</span>

    <span class="s0"># check that the fit did not converge</span>
    <span class="s2">assert not </span><span class="s1">gmm.converged_</span>

    <span class="s0"># check that parameters are set for gmm</span>
    <span class="s2">for </span><span class="s1">attr </span><span class="s2">in </span><span class="s1">[</span>
        <span class="s3">&quot;weights_&quot;</span><span class="s2">,</span>
        <span class="s3">&quot;means_&quot;</span><span class="s2">,</span>
        <span class="s3">&quot;covariances_&quot;</span><span class="s2">,</span>
        <span class="s3">&quot;precisions_cholesky_&quot;</span><span class="s2">,</span>
        <span class="s3">&quot;n_iter_&quot;</span><span class="s2">,</span>
        <span class="s3">&quot;lower_bound_&quot;</span><span class="s2">,</span>
    <span class="s1">]:</span>
        <span class="s2">assert </span><span class="s1">hasattr(gmm</span><span class="s2">, </span><span class="s1">attr)</span>


<span class="s1">@pytest.mark.parametrize(</span>
    <span class="s3">&quot;init_params&quot;</span><span class="s2">, </span><span class="s1">[</span><span class="s3">&quot;random&quot;</span><span class="s2">, </span><span class="s3">&quot;random_from_data&quot;</span><span class="s2">, </span><span class="s3">&quot;k-means++&quot;</span><span class="s2">, </span><span class="s3">&quot;kmeans&quot;</span><span class="s1">]</span>
<span class="s1">)</span>
<span class="s2">def </span><span class="s1">test_init_means_not_duplicated(init_params</span><span class="s2">, </span><span class="s1">global_random_seed):</span>
    <span class="s0"># Check that all initialisations provide not duplicated starting means</span>
    <span class="s1">rng = np.random.RandomState(global_random_seed)</span>
    <span class="s1">rand_data = RandomData(rng</span><span class="s2">, </span><span class="s1">scale=</span><span class="s4">5</span><span class="s1">)</span>
    <span class="s1">n_components = rand_data.n_components</span>
    <span class="s1">X = rand_data.X[</span><span class="s3">&quot;full&quot;</span><span class="s1">]</span>

    <span class="s1">gmm = GaussianMixture(</span>
        <span class="s1">n_components=n_components</span><span class="s2">, </span><span class="s1">init_params=init_params</span><span class="s2">, </span><span class="s1">random_state=rng</span><span class="s2">, </span><span class="s1">max_iter=</span><span class="s4">0</span>
    <span class="s1">)</span>
    <span class="s1">gmm.fit(X)</span>

    <span class="s1">means = gmm.means_</span>
    <span class="s2">for </span><span class="s1">i_mean</span><span class="s2">, </span><span class="s1">j_mean </span><span class="s2">in </span><span class="s1">itertools.combinations(means</span><span class="s2">, </span><span class="s1">r=</span><span class="s4">2</span><span class="s1">):</span>
        <span class="s2">assert not </span><span class="s1">np.allclose(i_mean</span><span class="s2">, </span><span class="s1">j_mean)</span>


<span class="s1">@pytest.mark.parametrize(</span>
    <span class="s3">&quot;init_params&quot;</span><span class="s2">, </span><span class="s1">[</span><span class="s3">&quot;random&quot;</span><span class="s2">, </span><span class="s3">&quot;random_from_data&quot;</span><span class="s2">, </span><span class="s3">&quot;k-means++&quot;</span><span class="s2">, </span><span class="s3">&quot;kmeans&quot;</span><span class="s1">]</span>
<span class="s1">)</span>
<span class="s2">def </span><span class="s1">test_means_for_all_inits(init_params</span><span class="s2">, </span><span class="s1">global_random_seed):</span>
    <span class="s0"># Check fitted means properties for all initializations</span>
    <span class="s1">rng = np.random.RandomState(global_random_seed)</span>
    <span class="s1">rand_data = RandomData(rng</span><span class="s2">, </span><span class="s1">scale=</span><span class="s4">5</span><span class="s1">)</span>
    <span class="s1">n_components = rand_data.n_components</span>
    <span class="s1">X = rand_data.X[</span><span class="s3">&quot;full&quot;</span><span class="s1">]</span>

    <span class="s1">gmm = GaussianMixture(</span>
        <span class="s1">n_components=n_components</span><span class="s2">, </span><span class="s1">init_params=init_params</span><span class="s2">, </span><span class="s1">random_state=rng</span>
    <span class="s1">)</span>
    <span class="s1">gmm.fit(X)</span>

    <span class="s2">assert </span><span class="s1">gmm.means_.shape == (n_components</span><span class="s2">, </span><span class="s1">X.shape[</span><span class="s4">1</span><span class="s1">])</span>
    <span class="s2">assert </span><span class="s1">np.all(X.min(axis=</span><span class="s4">0</span><span class="s1">) &lt;= gmm.means_)</span>
    <span class="s2">assert </span><span class="s1">np.all(gmm.means_ &lt;= X.max(axis=</span><span class="s4">0</span><span class="s1">))</span>
    <span class="s2">assert </span><span class="s1">gmm.converged_</span>


<span class="s2">def </span><span class="s1">test_max_iter_zero():</span>
    <span class="s0"># Check that max_iter=0 returns initialisation as expected</span>
    <span class="s0"># Pick arbitrary initial means and check equal to max_iter=0</span>
    <span class="s1">rng = np.random.RandomState(</span><span class="s4">0</span><span class="s1">)</span>
    <span class="s1">rand_data = RandomData(rng</span><span class="s2">, </span><span class="s1">scale=</span><span class="s4">5</span><span class="s1">)</span>
    <span class="s1">n_components = rand_data.n_components</span>
    <span class="s1">X = rand_data.X[</span><span class="s3">&quot;full&quot;</span><span class="s1">]</span>
    <span class="s1">means_init = [[</span><span class="s4">20</span><span class="s2">, </span><span class="s4">30</span><span class="s1">]</span><span class="s2">, </span><span class="s1">[</span><span class="s4">30</span><span class="s2">, </span><span class="s4">25</span><span class="s1">]]</span>
    <span class="s1">gmm = GaussianMixture(</span>
        <span class="s1">n_components=n_components</span><span class="s2">,</span>
        <span class="s1">random_state=rng</span><span class="s2">,</span>
        <span class="s1">means_init=means_init</span><span class="s2">,</span>
        <span class="s1">tol=</span><span class="s4">1e-06</span><span class="s2">,</span>
        <span class="s1">max_iter=</span><span class="s4">0</span><span class="s2">,</span>
    <span class="s1">)</span>
    <span class="s1">gmm.fit(X)</span>

    <span class="s1">assert_allclose(gmm.means_</span><span class="s2">, </span><span class="s1">means_init)</span>


<span class="s2">def </span><span class="s1">test_gaussian_mixture_precisions_init_diag():</span>
    <span class="s5">&quot;&quot;&quot;Check that we properly initialize `precision_cholesky_` when we manually 
    provide the precision matrix. 
 
    In this regard, we check the consistency between estimating the precision 
    matrix and providing the same precision matrix as initialization. It should 
    lead to the same results with the same number of iterations. 
 
    If the initialization is wrong then the number of iterations will increase. 
 
    Non-regression test for: 
    https://github.com/scikit-learn/scikit-learn/issues/16944 
    &quot;&quot;&quot;</span>
    <span class="s0"># generate a toy dataset</span>
    <span class="s1">n_samples = </span><span class="s4">300</span>
    <span class="s1">rng = np.random.RandomState(</span><span class="s4">0</span><span class="s1">)</span>
    <span class="s1">shifted_gaussian = rng.randn(n_samples</span><span class="s2">, </span><span class="s4">2</span><span class="s1">) + np.array([</span><span class="s4">20</span><span class="s2">, </span><span class="s4">20</span><span class="s1">])</span>
    <span class="s1">C = np.array([[</span><span class="s4">0.0</span><span class="s2">, </span><span class="s1">-</span><span class="s4">0.7</span><span class="s1">]</span><span class="s2">, </span><span class="s1">[</span><span class="s4">3.5</span><span class="s2">, </span><span class="s4">0.7</span><span class="s1">]])</span>
    <span class="s1">stretched_gaussian = np.dot(rng.randn(n_samples</span><span class="s2">, </span><span class="s4">2</span><span class="s1">)</span><span class="s2">, </span><span class="s1">C)</span>
    <span class="s1">X = np.vstack([shifted_gaussian</span><span class="s2">, </span><span class="s1">stretched_gaussian])</span>

    <span class="s0"># common parameters to check the consistency of precision initialization</span>
    <span class="s1">n_components</span><span class="s2">, </span><span class="s1">covariance_type</span><span class="s2">, </span><span class="s1">reg_covar</span><span class="s2">, </span><span class="s1">random_state = </span><span class="s4">2</span><span class="s2">, </span><span class="s3">&quot;diag&quot;</span><span class="s2">, </span><span class="s4">1e-6</span><span class="s2">, </span><span class="s4">0</span>

    <span class="s0"># execute the manual initialization to compute the precision matrix:</span>
    <span class="s0"># - run KMeans to have an initial guess</span>
    <span class="s0"># - estimate the covariance</span>
    <span class="s0"># - compute the precision matrix from the estimated covariance</span>
    <span class="s1">resp = np.zeros((X.shape[</span><span class="s4">0</span><span class="s1">]</span><span class="s2">, </span><span class="s1">n_components))</span>
    <span class="s1">label = (</span>
        <span class="s1">KMeans(n_clusters=n_components</span><span class="s2">, </span><span class="s1">n_init=</span><span class="s4">1</span><span class="s2">, </span><span class="s1">random_state=random_state)</span>
        <span class="s1">.fit(X)</span>
        <span class="s1">.labels_</span>
    <span class="s1">)</span>
    <span class="s1">resp[np.arange(X.shape[</span><span class="s4">0</span><span class="s1">])</span><span class="s2">, </span><span class="s1">label] = </span><span class="s4">1</span>
    <span class="s1">_</span><span class="s2">, </span><span class="s1">_</span><span class="s2">, </span><span class="s1">covariance = _estimate_gaussian_parameters(</span>
        <span class="s1">X</span><span class="s2">, </span><span class="s1">resp</span><span class="s2">, </span><span class="s1">reg_covar=reg_covar</span><span class="s2">, </span><span class="s1">covariance_type=covariance_type</span>
    <span class="s1">)</span>
    <span class="s1">precisions_init = </span><span class="s4">1 </span><span class="s1">/ covariance</span>

    <span class="s1">gm_with_init = GaussianMixture(</span>
        <span class="s1">n_components=n_components</span><span class="s2">,</span>
        <span class="s1">covariance_type=covariance_type</span><span class="s2">,</span>
        <span class="s1">reg_covar=reg_covar</span><span class="s2">,</span>
        <span class="s1">precisions_init=precisions_init</span><span class="s2">,</span>
        <span class="s1">random_state=random_state</span><span class="s2">,</span>
    <span class="s1">).fit(X)</span>

    <span class="s1">gm_without_init = GaussianMixture(</span>
        <span class="s1">n_components=n_components</span><span class="s2">,</span>
        <span class="s1">covariance_type=covariance_type</span><span class="s2">,</span>
        <span class="s1">reg_covar=reg_covar</span><span class="s2">,</span>
        <span class="s1">random_state=random_state</span><span class="s2">,</span>
    <span class="s1">).fit(X)</span>

    <span class="s2">assert </span><span class="s1">gm_without_init.n_iter_ == gm_with_init.n_iter_</span>
    <span class="s1">assert_allclose(</span>
        <span class="s1">gm_with_init.precisions_cholesky_</span><span class="s2">, </span><span class="s1">gm_without_init.precisions_cholesky_</span>
    <span class="s1">)</span>


<span class="s2">def </span><span class="s1">_generate_data(seed</span><span class="s2">, </span><span class="s1">n_samples</span><span class="s2">, </span><span class="s1">n_features</span><span class="s2">, </span><span class="s1">n_components):</span>
    <span class="s5">&quot;&quot;&quot;Randomly generate samples and responsibilities.&quot;&quot;&quot;</span>
    <span class="s1">rs = np.random.RandomState(seed)</span>
    <span class="s1">X = rs.random_sample((n_samples</span><span class="s2">, </span><span class="s1">n_features))</span>
    <span class="s1">resp = rs.random_sample((n_samples</span><span class="s2">, </span><span class="s1">n_components))</span>
    <span class="s1">resp /= resp.sum(axis=</span><span class="s4">1</span><span class="s1">)[:</span><span class="s2">, </span><span class="s1">np.newaxis]</span>
    <span class="s2">return </span><span class="s1">X</span><span class="s2">, </span><span class="s1">resp</span>


<span class="s2">def </span><span class="s1">_calculate_precisions(X</span><span class="s2">, </span><span class="s1">resp</span><span class="s2">, </span><span class="s1">covariance_type):</span>
    <span class="s5">&quot;&quot;&quot;Calculate precision matrix of X and its Cholesky decomposition 
    for the given covariance type. 
    &quot;&quot;&quot;</span>
    <span class="s1">reg_covar = </span><span class="s4">1e-6</span>
    <span class="s1">weights</span><span class="s2">, </span><span class="s1">means</span><span class="s2">, </span><span class="s1">covariances = _estimate_gaussian_parameters(</span>
        <span class="s1">X</span><span class="s2">, </span><span class="s1">resp</span><span class="s2">, </span><span class="s1">reg_covar</span><span class="s2">, </span><span class="s1">covariance_type</span>
    <span class="s1">)</span>
    <span class="s1">precisions_cholesky = _compute_precision_cholesky(covariances</span><span class="s2">, </span><span class="s1">covariance_type)</span>

    <span class="s1">_</span><span class="s2">, </span><span class="s1">n_components = resp.shape</span>
    <span class="s0"># Instantiate a `GaussianMixture` model in order to use its</span>
    <span class="s0"># `_set_parameters` method to return the `precisions_` and</span>
    <span class="s0">#  `precisions_cholesky_` from matching the `covariance_type`</span>
    <span class="s0"># provided.</span>
    <span class="s1">gmm = GaussianMixture(n_components=n_components</span><span class="s2">, </span><span class="s1">covariance_type=covariance_type)</span>
    <span class="s1">params = (weights</span><span class="s2">, </span><span class="s1">means</span><span class="s2">, </span><span class="s1">covariances</span><span class="s2">, </span><span class="s1">precisions_cholesky)</span>
    <span class="s1">gmm._set_parameters(params)</span>
    <span class="s2">return </span><span class="s1">gmm.precisions_</span><span class="s2">, </span><span class="s1">gmm.precisions_cholesky_</span>


<span class="s1">@pytest.mark.parametrize(</span><span class="s3">&quot;covariance_type&quot;</span><span class="s2">, </span><span class="s1">COVARIANCE_TYPE)</span>
<span class="s2">def </span><span class="s1">test_gaussian_mixture_precisions_init(covariance_type</span><span class="s2">, </span><span class="s1">global_random_seed):</span>
    <span class="s5">&quot;&quot;&quot;Non-regression test for #26415.&quot;&quot;&quot;</span>

    <span class="s1">X</span><span class="s2">, </span><span class="s1">resp = _generate_data(</span>
        <span class="s1">seed=global_random_seed</span><span class="s2">,</span>
        <span class="s1">n_samples=</span><span class="s4">100</span><span class="s2">,</span>
        <span class="s1">n_features=</span><span class="s4">3</span><span class="s2">,</span>
        <span class="s1">n_components=</span><span class="s4">4</span><span class="s2">,</span>
    <span class="s1">)</span>

    <span class="s1">precisions_init</span><span class="s2">, </span><span class="s1">desired_precisions_cholesky = _calculate_precisions(</span>
        <span class="s1">X</span><span class="s2">, </span><span class="s1">resp</span><span class="s2">, </span><span class="s1">covariance_type</span>
    <span class="s1">)</span>
    <span class="s1">gmm = GaussianMixture(</span>
        <span class="s1">covariance_type=covariance_type</span><span class="s2">, </span><span class="s1">precisions_init=precisions_init</span>
    <span class="s1">)</span>
    <span class="s1">gmm._initialize(X</span><span class="s2">, </span><span class="s1">resp)</span>
    <span class="s1">actual_precisions_cholesky = gmm.precisions_cholesky_</span>
    <span class="s1">assert_allclose(actual_precisions_cholesky</span><span class="s2">, </span><span class="s1">desired_precisions_cholesky)</span>


<span class="s2">def </span><span class="s1">test_gaussian_mixture_single_component_stable():</span>
    <span class="s5">&quot;&quot;&quot; 
    Non-regression test for #23032 ensuring 1-component GM works on only a 
    few samples. 
    &quot;&quot;&quot;</span>
    <span class="s1">rng = np.random.RandomState(</span><span class="s4">0</span><span class="s1">)</span>
    <span class="s1">X = rng.multivariate_normal(np.zeros(</span><span class="s4">2</span><span class="s1">)</span><span class="s2">, </span><span class="s1">np.identity(</span><span class="s4">2</span><span class="s1">)</span><span class="s2">, </span><span class="s1">size=</span><span class="s4">3</span><span class="s1">)</span>
    <span class="s1">gm = GaussianMixture(n_components=</span><span class="s4">1</span><span class="s1">)</span>
    <span class="s1">gm.fit(X).sample()</span>
</pre>
</body>
</html>