<html>
<head>
<title>test_from_model.py</title>
<meta http-equiv="Content-Type" content="text/html; charset=utf-8">
<style type="text/css">
.s0 { color: #cc7832;}
.s1 { color: #a9b7c6;}
.s2 { color: #6a8759;}
.s3 { color: #6897bb;}
.s4 { color: #808080;}
.s5 { color: #629755; font-style: italic;}
</style>
</head>
<body bgcolor="#2b2b2b">
<table CELLSPACING=0 CELLPADDING=5 COLS=1 WIDTH="100%" BGCOLOR="#606060" >
<tr><td><center>
<font face="Arial, Helvetica" color="#000000">
test_from_model.py</font>
</center></td></tr></table>
<pre><span class="s0">import </span><span class="s1">re</span>
<span class="s0">import </span><span class="s1">warnings</span>
<span class="s0">from </span><span class="s1">unittest.mock </span><span class="s0">import </span><span class="s1">Mock</span>

<span class="s0">import </span><span class="s1">numpy </span><span class="s0">as </span><span class="s1">np</span>
<span class="s0">import </span><span class="s1">pytest</span>

<span class="s0">from </span><span class="s1">sklearn </span><span class="s0">import </span><span class="s1">datasets</span>
<span class="s0">from </span><span class="s1">sklearn.base </span><span class="s0">import </span><span class="s1">BaseEstimator</span>
<span class="s0">from </span><span class="s1">sklearn.cross_decomposition </span><span class="s0">import </span><span class="s1">CCA</span><span class="s0">, </span><span class="s1">PLSCanonical</span><span class="s0">, </span><span class="s1">PLSRegression</span>
<span class="s0">from </span><span class="s1">sklearn.datasets </span><span class="s0">import </span><span class="s1">make_friedman1</span>
<span class="s0">from </span><span class="s1">sklearn.decomposition </span><span class="s0">import </span><span class="s1">PCA</span>
<span class="s0">from </span><span class="s1">sklearn.ensemble </span><span class="s0">import </span><span class="s1">HistGradientBoostingClassifier</span><span class="s0">, </span><span class="s1">RandomForestClassifier</span>
<span class="s0">from </span><span class="s1">sklearn.exceptions </span><span class="s0">import </span><span class="s1">NotFittedError</span>
<span class="s0">from </span><span class="s1">sklearn.feature_selection </span><span class="s0">import </span><span class="s1">SelectFromModel</span>
<span class="s0">from </span><span class="s1">sklearn.linear_model </span><span class="s0">import </span><span class="s1">(</span>
    <span class="s1">ElasticNet</span><span class="s0">,</span>
    <span class="s1">ElasticNetCV</span><span class="s0">,</span>
    <span class="s1">Lasso</span><span class="s0">,</span>
    <span class="s1">LassoCV</span><span class="s0">,</span>
    <span class="s1">LogisticRegression</span><span class="s0">,</span>
    <span class="s1">PassiveAggressiveClassifier</span><span class="s0">,</span>
    <span class="s1">SGDClassifier</span><span class="s0">,</span>
<span class="s1">)</span>
<span class="s0">from </span><span class="s1">sklearn.pipeline </span><span class="s0">import </span><span class="s1">make_pipeline</span>
<span class="s0">from </span><span class="s1">sklearn.svm </span><span class="s0">import </span><span class="s1">LinearSVC</span>
<span class="s0">from </span><span class="s1">sklearn.utils._testing </span><span class="s0">import </span><span class="s1">(</span>
    <span class="s1">MinimalClassifier</span><span class="s0">,</span>
    <span class="s1">assert_allclose</span><span class="s0">,</span>
    <span class="s1">assert_array_almost_equal</span><span class="s0">,</span>
    <span class="s1">assert_array_equal</span><span class="s0">,</span>
    <span class="s1">skip_if_32bit</span><span class="s0">,</span>
<span class="s1">)</span>


<span class="s0">class </span><span class="s1">NaNTag(BaseEstimator):</span>
    <span class="s0">def </span><span class="s1">_more_tags(self):</span>
        <span class="s0">return </span><span class="s1">{</span><span class="s2">&quot;allow_nan&quot;</span><span class="s1">: </span><span class="s0">True</span><span class="s1">}</span>


<span class="s0">class </span><span class="s1">NoNaNTag(BaseEstimator):</span>
    <span class="s0">def </span><span class="s1">_more_tags(self):</span>
        <span class="s0">return </span><span class="s1">{</span><span class="s2">&quot;allow_nan&quot;</span><span class="s1">: </span><span class="s0">False</span><span class="s1">}</span>


<span class="s0">class </span><span class="s1">NaNTagRandomForest(RandomForestClassifier):</span>
    <span class="s0">def </span><span class="s1">_more_tags(self):</span>
        <span class="s0">return </span><span class="s1">{</span><span class="s2">&quot;allow_nan&quot;</span><span class="s1">: </span><span class="s0">True</span><span class="s1">}</span>


<span class="s1">iris = datasets.load_iris()</span>
<span class="s1">data</span><span class="s0">, </span><span class="s1">y = iris.data</span><span class="s0">, </span><span class="s1">iris.target</span>
<span class="s1">rng = np.random.RandomState(</span><span class="s3">0</span><span class="s1">)</span>


<span class="s0">def </span><span class="s1">test_invalid_input():</span>
    <span class="s1">clf = SGDClassifier(</span>
        <span class="s1">alpha=</span><span class="s3">0.1</span><span class="s0">, </span><span class="s1">max_iter=</span><span class="s3">10</span><span class="s0">, </span><span class="s1">shuffle=</span><span class="s0">True, </span><span class="s1">random_state=</span><span class="s0">None, </span><span class="s1">tol=</span><span class="s0">None</span>
    <span class="s1">)</span>
    <span class="s0">for </span><span class="s1">threshold </span><span class="s0">in </span><span class="s1">[</span><span class="s2">&quot;gobbledigook&quot;</span><span class="s0">, </span><span class="s2">&quot;.5 * gobbledigook&quot;</span><span class="s1">]:</span>
        <span class="s1">model = SelectFromModel(clf</span><span class="s0">, </span><span class="s1">threshold=threshold)</span>
        <span class="s1">model.fit(data</span><span class="s0">, </span><span class="s1">y)</span>
        <span class="s0">with </span><span class="s1">pytest.raises(ValueError):</span>
            <span class="s1">model.transform(data)</span>


<span class="s0">def </span><span class="s1">test_input_estimator_unchanged():</span>
    <span class="s4"># Test that SelectFromModel fits on a clone of the estimator.</span>
    <span class="s1">est = RandomForestClassifier()</span>
    <span class="s1">transformer = SelectFromModel(estimator=est)</span>
    <span class="s1">transformer.fit(data</span><span class="s0">, </span><span class="s1">y)</span>
    <span class="s0">assert </span><span class="s1">transformer.estimator </span><span class="s0">is </span><span class="s1">est</span>


<span class="s1">@pytest.mark.parametrize(</span>
    <span class="s2">&quot;max_features, err_type, err_msg&quot;</span><span class="s0">,</span>
    <span class="s1">[</span>
        <span class="s1">(</span>
            <span class="s1">data.shape[</span><span class="s3">1</span><span class="s1">] + </span><span class="s3">1</span><span class="s0">,</span>
            <span class="s1">ValueError</span><span class="s0">,</span>
            <span class="s2">&quot;max_features ==&quot;</span><span class="s0">,</span>
        <span class="s1">)</span><span class="s0">,</span>
        <span class="s1">(</span>
            <span class="s0">lambda </span><span class="s1">X: </span><span class="s3">1.5</span><span class="s0">,</span>
            <span class="s1">TypeError</span><span class="s0">,</span>
            <span class="s2">&quot;max_features must be an instance of int, not float.&quot;</span><span class="s0">,</span>
        <span class="s1">)</span><span class="s0">,</span>
        <span class="s1">(</span>
            <span class="s0">lambda </span><span class="s1">X: data.shape[</span><span class="s3">1</span><span class="s1">] + </span><span class="s3">1</span><span class="s0">,</span>
            <span class="s1">ValueError</span><span class="s0">,</span>
            <span class="s2">&quot;max_features ==&quot;</span><span class="s0">,</span>
        <span class="s1">)</span><span class="s0">,</span>
        <span class="s1">(</span>
            <span class="s0">lambda </span><span class="s1">X: -</span><span class="s3">1</span><span class="s0">,</span>
            <span class="s1">ValueError</span><span class="s0">,</span>
            <span class="s2">&quot;max_features ==&quot;</span><span class="s0">,</span>
        <span class="s1">)</span><span class="s0">,</span>
    <span class="s1">]</span><span class="s0">,</span>
<span class="s1">)</span>
<span class="s0">def </span><span class="s1">test_max_features_error(max_features</span><span class="s0">, </span><span class="s1">err_type</span><span class="s0">, </span><span class="s1">err_msg):</span>
    <span class="s1">err_msg = re.escape(err_msg)</span>
    <span class="s1">clf = RandomForestClassifier(n_estimators=</span><span class="s3">5</span><span class="s0">, </span><span class="s1">random_state=</span><span class="s3">0</span><span class="s1">)</span>

    <span class="s1">transformer = SelectFromModel(</span>
        <span class="s1">estimator=clf</span><span class="s0">, </span><span class="s1">max_features=max_features</span><span class="s0">, </span><span class="s1">threshold=-np.inf</span>
    <span class="s1">)</span>
    <span class="s0">with </span><span class="s1">pytest.raises(err_type</span><span class="s0">, </span><span class="s1">match=err_msg):</span>
        <span class="s1">transformer.fit(data</span><span class="s0">, </span><span class="s1">y)</span>


<span class="s1">@pytest.mark.parametrize(</span><span class="s2">&quot;max_features&quot;</span><span class="s0">, </span><span class="s1">[</span><span class="s3">0</span><span class="s0">, </span><span class="s3">2</span><span class="s0">, </span><span class="s1">data.shape[</span><span class="s3">1</span><span class="s1">]</span><span class="s0">, None</span><span class="s1">])</span>
<span class="s0">def </span><span class="s1">test_inferred_max_features_integer(max_features):</span>
    <span class="s5">&quot;&quot;&quot;Check max_features_ and output shape for integer max_features.&quot;&quot;&quot;</span>
    <span class="s1">clf = RandomForestClassifier(n_estimators=</span><span class="s3">5</span><span class="s0">, </span><span class="s1">random_state=</span><span class="s3">0</span><span class="s1">)</span>
    <span class="s1">transformer = SelectFromModel(</span>
        <span class="s1">estimator=clf</span><span class="s0">, </span><span class="s1">max_features=max_features</span><span class="s0">, </span><span class="s1">threshold=-np.inf</span>
    <span class="s1">)</span>
    <span class="s1">X_trans = transformer.fit_transform(data</span><span class="s0">, </span><span class="s1">y)</span>
    <span class="s0">if </span><span class="s1">max_features </span><span class="s0">is not None</span><span class="s1">:</span>
        <span class="s0">assert </span><span class="s1">transformer.max_features_ == max_features</span>
        <span class="s0">assert </span><span class="s1">X_trans.shape[</span><span class="s3">1</span><span class="s1">] == transformer.max_features_</span>
    <span class="s0">else</span><span class="s1">:</span>
        <span class="s0">assert not </span><span class="s1">hasattr(transformer</span><span class="s0">, </span><span class="s2">&quot;max_features_&quot;</span><span class="s1">)</span>
        <span class="s0">assert </span><span class="s1">X_trans.shape[</span><span class="s3">1</span><span class="s1">] == data.shape[</span><span class="s3">1</span><span class="s1">]</span>


<span class="s1">@pytest.mark.parametrize(</span>
    <span class="s2">&quot;max_features&quot;</span><span class="s0">,</span>
    <span class="s1">[</span><span class="s0">lambda </span><span class="s1">X: </span><span class="s3">1</span><span class="s0">, lambda </span><span class="s1">X: X.shape[</span><span class="s3">1</span><span class="s1">]</span><span class="s0">, lambda </span><span class="s1">X: min(X.shape[</span><span class="s3">1</span><span class="s1">]</span><span class="s0">, </span><span class="s3">10000</span><span class="s1">)]</span><span class="s0">,</span>
<span class="s1">)</span>
<span class="s0">def </span><span class="s1">test_inferred_max_features_callable(max_features):</span>
    <span class="s5">&quot;&quot;&quot;Check max_features_ and output shape for callable max_features.&quot;&quot;&quot;</span>
    <span class="s1">clf = RandomForestClassifier(n_estimators=</span><span class="s3">5</span><span class="s0">, </span><span class="s1">random_state=</span><span class="s3">0</span><span class="s1">)</span>
    <span class="s1">transformer = SelectFromModel(</span>
        <span class="s1">estimator=clf</span><span class="s0">, </span><span class="s1">max_features=max_features</span><span class="s0">, </span><span class="s1">threshold=-np.inf</span>
    <span class="s1">)</span>
    <span class="s1">X_trans = transformer.fit_transform(data</span><span class="s0">, </span><span class="s1">y)</span>
    <span class="s0">assert </span><span class="s1">transformer.max_features_ == max_features(data)</span>
    <span class="s0">assert </span><span class="s1">X_trans.shape[</span><span class="s3">1</span><span class="s1">] == transformer.max_features_</span>


<span class="s1">@pytest.mark.parametrize(</span><span class="s2">&quot;max_features&quot;</span><span class="s0">, </span><span class="s1">[</span><span class="s0">lambda </span><span class="s1">X: round(len(X[</span><span class="s3">0</span><span class="s1">]) / </span><span class="s3">2</span><span class="s1">)</span><span class="s0">, </span><span class="s3">2</span><span class="s1">])</span>
<span class="s0">def </span><span class="s1">test_max_features_array_like(max_features):</span>
    <span class="s1">X = [</span>
        <span class="s1">[</span><span class="s3">0.87</span><span class="s0">, </span><span class="s1">-</span><span class="s3">1.34</span><span class="s0">, </span><span class="s3">0.31</span><span class="s1">]</span><span class="s0">,</span>
        <span class="s1">[-</span><span class="s3">2.79</span><span class="s0">, </span><span class="s1">-</span><span class="s3">0.02</span><span class="s0">, </span><span class="s1">-</span><span class="s3">0.85</span><span class="s1">]</span><span class="s0">,</span>
        <span class="s1">[-</span><span class="s3">1.34</span><span class="s0">, </span><span class="s1">-</span><span class="s3">0.48</span><span class="s0">, </span><span class="s1">-</span><span class="s3">2.55</span><span class="s1">]</span><span class="s0">,</span>
        <span class="s1">[</span><span class="s3">1.92</span><span class="s0">, </span><span class="s3">1.48</span><span class="s0">, </span><span class="s3">0.65</span><span class="s1">]</span><span class="s0">,</span>
    <span class="s1">]</span>
    <span class="s1">y = [</span><span class="s3">0</span><span class="s0">, </span><span class="s3">1</span><span class="s0">, </span><span class="s3">0</span><span class="s0">, </span><span class="s3">1</span><span class="s1">]</span>

    <span class="s1">clf = RandomForestClassifier(n_estimators=</span><span class="s3">5</span><span class="s0">, </span><span class="s1">random_state=</span><span class="s3">0</span><span class="s1">)</span>
    <span class="s1">transformer = SelectFromModel(</span>
        <span class="s1">estimator=clf</span><span class="s0">, </span><span class="s1">max_features=max_features</span><span class="s0">, </span><span class="s1">threshold=-np.inf</span>
    <span class="s1">)</span>
    <span class="s1">X_trans = transformer.fit_transform(X</span><span class="s0">, </span><span class="s1">y)</span>
    <span class="s0">assert </span><span class="s1">X_trans.shape[</span><span class="s3">1</span><span class="s1">] == transformer.max_features_</span>


<span class="s1">@pytest.mark.parametrize(</span>
    <span class="s2">&quot;max_features&quot;</span><span class="s0">,</span>
    <span class="s1">[</span><span class="s0">lambda </span><span class="s1">X: min(X.shape[</span><span class="s3">1</span><span class="s1">]</span><span class="s0">, </span><span class="s3">10000</span><span class="s1">)</span><span class="s0">, lambda </span><span class="s1">X: X.shape[</span><span class="s3">1</span><span class="s1">]</span><span class="s0">, lambda </span><span class="s1">X: </span><span class="s3">1</span><span class="s1">]</span><span class="s0">,</span>
<span class="s1">)</span>
<span class="s0">def </span><span class="s1">test_max_features_callable_data(max_features):</span>
    <span class="s5">&quot;&quot;&quot;Tests that the callable passed to `fit` is called on X.&quot;&quot;&quot;</span>
    <span class="s1">clf = RandomForestClassifier(n_estimators=</span><span class="s3">50</span><span class="s0">, </span><span class="s1">random_state=</span><span class="s3">0</span><span class="s1">)</span>
    <span class="s1">m = Mock(side_effect=max_features)</span>
    <span class="s1">transformer = SelectFromModel(estimator=clf</span><span class="s0">, </span><span class="s1">max_features=m</span><span class="s0">, </span><span class="s1">threshold=-np.inf)</span>
    <span class="s1">transformer.fit_transform(data</span><span class="s0">, </span><span class="s1">y)</span>
    <span class="s1">m.assert_called_with(data)</span>


<span class="s0">class </span><span class="s1">FixedImportanceEstimator(BaseEstimator):</span>
    <span class="s0">def </span><span class="s1">__init__(self</span><span class="s0">, </span><span class="s1">importances):</span>
        <span class="s1">self.importances = importances</span>

    <span class="s0">def </span><span class="s1">fit(self</span><span class="s0">, </span><span class="s1">X</span><span class="s0">, </span><span class="s1">y=</span><span class="s0">None</span><span class="s1">):</span>
        <span class="s1">self.feature_importances_ = np.array(self.importances)</span>


<span class="s0">def </span><span class="s1">test_max_features():</span>
    <span class="s4"># Test max_features parameter using various values</span>
    <span class="s1">X</span><span class="s0">, </span><span class="s1">y = datasets.make_classification(</span>
        <span class="s1">n_samples=</span><span class="s3">1000</span><span class="s0">,</span>
        <span class="s1">n_features=</span><span class="s3">10</span><span class="s0">,</span>
        <span class="s1">n_informative=</span><span class="s3">3</span><span class="s0">,</span>
        <span class="s1">n_redundant=</span><span class="s3">0</span><span class="s0">,</span>
        <span class="s1">n_repeated=</span><span class="s3">0</span><span class="s0">,</span>
        <span class="s1">shuffle=</span><span class="s0">False,</span>
        <span class="s1">random_state=</span><span class="s3">0</span><span class="s0">,</span>
    <span class="s1">)</span>
    <span class="s1">max_features = X.shape[</span><span class="s3">1</span><span class="s1">]</span>
    <span class="s1">est = RandomForestClassifier(n_estimators=</span><span class="s3">50</span><span class="s0">, </span><span class="s1">random_state=</span><span class="s3">0</span><span class="s1">)</span>

    <span class="s1">transformer1 = SelectFromModel(estimator=est</span><span class="s0">, </span><span class="s1">threshold=-np.inf)</span>
    <span class="s1">transformer2 = SelectFromModel(</span>
        <span class="s1">estimator=est</span><span class="s0">, </span><span class="s1">max_features=max_features</span><span class="s0">, </span><span class="s1">threshold=-np.inf</span>
    <span class="s1">)</span>
    <span class="s1">X_new1 = transformer1.fit_transform(X</span><span class="s0">, </span><span class="s1">y)</span>
    <span class="s1">X_new2 = transformer2.fit_transform(X</span><span class="s0">, </span><span class="s1">y)</span>
    <span class="s1">assert_allclose(X_new1</span><span class="s0">, </span><span class="s1">X_new2)</span>

    <span class="s4"># Test max_features against actual model.</span>
    <span class="s1">transformer1 = SelectFromModel(estimator=Lasso(alpha=</span><span class="s3">0.025</span><span class="s0">, </span><span class="s1">random_state=</span><span class="s3">42</span><span class="s1">))</span>
    <span class="s1">X_new1 = transformer1.fit_transform(X</span><span class="s0">, </span><span class="s1">y)</span>
    <span class="s1">scores1 = np.abs(transformer1.estimator_.coef_)</span>
    <span class="s1">candidate_indices1 = np.argsort(-scores1</span><span class="s0">, </span><span class="s1">kind=</span><span class="s2">&quot;mergesort&quot;</span><span class="s1">)</span>

    <span class="s0">for </span><span class="s1">n_features </span><span class="s0">in </span><span class="s1">range(</span><span class="s3">1</span><span class="s0">, </span><span class="s1">X_new1.shape[</span><span class="s3">1</span><span class="s1">] + </span><span class="s3">1</span><span class="s1">):</span>
        <span class="s1">transformer2 = SelectFromModel(</span>
            <span class="s1">estimator=Lasso(alpha=</span><span class="s3">0.025</span><span class="s0">, </span><span class="s1">random_state=</span><span class="s3">42</span><span class="s1">)</span><span class="s0">,</span>
            <span class="s1">max_features=n_features</span><span class="s0">,</span>
            <span class="s1">threshold=-np.inf</span><span class="s0">,</span>
        <span class="s1">)</span>
        <span class="s1">X_new2 = transformer2.fit_transform(X</span><span class="s0">, </span><span class="s1">y)</span>
        <span class="s1">scores2 = np.abs(transformer2.estimator_.coef_)</span>
        <span class="s1">candidate_indices2 = np.argsort(-scores2</span><span class="s0">, </span><span class="s1">kind=</span><span class="s2">&quot;mergesort&quot;</span><span class="s1">)</span>
        <span class="s1">assert_allclose(</span>
            <span class="s1">X[:</span><span class="s0">, </span><span class="s1">candidate_indices1[:n_features]]</span><span class="s0">, </span><span class="s1">X[:</span><span class="s0">, </span><span class="s1">candidate_indices2[:n_features]]</span>
        <span class="s1">)</span>
    <span class="s1">assert_allclose(transformer1.estimator_.coef_</span><span class="s0">, </span><span class="s1">transformer2.estimator_.coef_)</span>


<span class="s0">def </span><span class="s1">test_max_features_tiebreak():</span>
    <span class="s4"># Test if max_features can break tie among feature importance</span>
    <span class="s1">X</span><span class="s0">, </span><span class="s1">y = datasets.make_classification(</span>
        <span class="s1">n_samples=</span><span class="s3">1000</span><span class="s0">,</span>
        <span class="s1">n_features=</span><span class="s3">10</span><span class="s0">,</span>
        <span class="s1">n_informative=</span><span class="s3">3</span><span class="s0">,</span>
        <span class="s1">n_redundant=</span><span class="s3">0</span><span class="s0">,</span>
        <span class="s1">n_repeated=</span><span class="s3">0</span><span class="s0">,</span>
        <span class="s1">shuffle=</span><span class="s0">False,</span>
        <span class="s1">random_state=</span><span class="s3">0</span><span class="s0">,</span>
    <span class="s1">)</span>
    <span class="s1">max_features = X.shape[</span><span class="s3">1</span><span class="s1">]</span>

    <span class="s1">feature_importances = np.array([</span><span class="s3">4</span><span class="s0">, </span><span class="s3">4</span><span class="s0">, </span><span class="s3">4</span><span class="s0">, </span><span class="s3">4</span><span class="s0">, </span><span class="s3">3</span><span class="s0">, </span><span class="s3">3</span><span class="s0">, </span><span class="s3">3</span><span class="s0">, </span><span class="s3">2</span><span class="s0">, </span><span class="s3">2</span><span class="s0">, </span><span class="s3">1</span><span class="s1">])</span>
    <span class="s0">for </span><span class="s1">n_features </span><span class="s0">in </span><span class="s1">range(</span><span class="s3">1</span><span class="s0">, </span><span class="s1">max_features + </span><span class="s3">1</span><span class="s1">):</span>
        <span class="s1">transformer = SelectFromModel(</span>
            <span class="s1">FixedImportanceEstimator(feature_importances)</span><span class="s0">,</span>
            <span class="s1">max_features=n_features</span><span class="s0">,</span>
            <span class="s1">threshold=-np.inf</span><span class="s0">,</span>
        <span class="s1">)</span>
        <span class="s1">X_new = transformer.fit_transform(X</span><span class="s0">, </span><span class="s1">y)</span>
        <span class="s1">selected_feature_indices = np.where(transformer._get_support_mask())[</span><span class="s3">0</span><span class="s1">]</span>
        <span class="s1">assert_array_equal(selected_feature_indices</span><span class="s0">, </span><span class="s1">np.arange(n_features))</span>
        <span class="s0">assert </span><span class="s1">X_new.shape[</span><span class="s3">1</span><span class="s1">] == n_features</span>


<span class="s0">def </span><span class="s1">test_threshold_and_max_features():</span>
    <span class="s1">X</span><span class="s0">, </span><span class="s1">y = datasets.make_classification(</span>
        <span class="s1">n_samples=</span><span class="s3">1000</span><span class="s0">,</span>
        <span class="s1">n_features=</span><span class="s3">10</span><span class="s0">,</span>
        <span class="s1">n_informative=</span><span class="s3">3</span><span class="s0">,</span>
        <span class="s1">n_redundant=</span><span class="s3">0</span><span class="s0">,</span>
        <span class="s1">n_repeated=</span><span class="s3">0</span><span class="s0">,</span>
        <span class="s1">shuffle=</span><span class="s0">False,</span>
        <span class="s1">random_state=</span><span class="s3">0</span><span class="s0">,</span>
    <span class="s1">)</span>
    <span class="s1">est = RandomForestClassifier(n_estimators=</span><span class="s3">50</span><span class="s0">, </span><span class="s1">random_state=</span><span class="s3">0</span><span class="s1">)</span>

    <span class="s1">transformer1 = SelectFromModel(estimator=est</span><span class="s0">, </span><span class="s1">max_features=</span><span class="s3">3</span><span class="s0">, </span><span class="s1">threshold=-np.inf)</span>
    <span class="s1">X_new1 = transformer1.fit_transform(X</span><span class="s0">, </span><span class="s1">y)</span>

    <span class="s1">transformer2 = SelectFromModel(estimator=est</span><span class="s0">, </span><span class="s1">threshold=</span><span class="s3">0.04</span><span class="s1">)</span>
    <span class="s1">X_new2 = transformer2.fit_transform(X</span><span class="s0">, </span><span class="s1">y)</span>

    <span class="s1">transformer3 = SelectFromModel(estimator=est</span><span class="s0">, </span><span class="s1">max_features=</span><span class="s3">3</span><span class="s0">, </span><span class="s1">threshold=</span><span class="s3">0.04</span><span class="s1">)</span>
    <span class="s1">X_new3 = transformer3.fit_transform(X</span><span class="s0">, </span><span class="s1">y)</span>
    <span class="s0">assert </span><span class="s1">X_new3.shape[</span><span class="s3">1</span><span class="s1">] == min(X_new1.shape[</span><span class="s3">1</span><span class="s1">]</span><span class="s0">, </span><span class="s1">X_new2.shape[</span><span class="s3">1</span><span class="s1">])</span>
    <span class="s1">selected_indices = transformer3.transform(np.arange(X.shape[</span><span class="s3">1</span><span class="s1">])[np.newaxis</span><span class="s0">, </span><span class="s1">:])</span>
    <span class="s1">assert_allclose(X_new3</span><span class="s0">, </span><span class="s1">X[:</span><span class="s0">, </span><span class="s1">selected_indices[</span><span class="s3">0</span><span class="s1">]])</span>


<span class="s1">@skip_if_32bit</span>
<span class="s0">def </span><span class="s1">test_feature_importances():</span>
    <span class="s1">X</span><span class="s0">, </span><span class="s1">y = datasets.make_classification(</span>
        <span class="s1">n_samples=</span><span class="s3">1000</span><span class="s0">,</span>
        <span class="s1">n_features=</span><span class="s3">10</span><span class="s0">,</span>
        <span class="s1">n_informative=</span><span class="s3">3</span><span class="s0">,</span>
        <span class="s1">n_redundant=</span><span class="s3">0</span><span class="s0">,</span>
        <span class="s1">n_repeated=</span><span class="s3">0</span><span class="s0">,</span>
        <span class="s1">shuffle=</span><span class="s0">False,</span>
        <span class="s1">random_state=</span><span class="s3">0</span><span class="s0">,</span>
    <span class="s1">)</span>

    <span class="s1">est = RandomForestClassifier(n_estimators=</span><span class="s3">50</span><span class="s0">, </span><span class="s1">random_state=</span><span class="s3">0</span><span class="s1">)</span>
    <span class="s0">for </span><span class="s1">threshold</span><span class="s0">, </span><span class="s1">func </span><span class="s0">in </span><span class="s1">zip([</span><span class="s2">&quot;mean&quot;</span><span class="s0">, </span><span class="s2">&quot;median&quot;</span><span class="s1">]</span><span class="s0">, </span><span class="s1">[np.mean</span><span class="s0">, </span><span class="s1">np.median]):</span>
        <span class="s1">transformer = SelectFromModel(estimator=est</span><span class="s0">, </span><span class="s1">threshold=threshold)</span>
        <span class="s1">transformer.fit(X</span><span class="s0">, </span><span class="s1">y)</span>
        <span class="s0">assert </span><span class="s1">hasattr(transformer.estimator_</span><span class="s0">, </span><span class="s2">&quot;feature_importances_&quot;</span><span class="s1">)</span>

        <span class="s1">X_new = transformer.transform(X)</span>
        <span class="s0">assert </span><span class="s1">X_new.shape[</span><span class="s3">1</span><span class="s1">] &lt; X.shape[</span><span class="s3">1</span><span class="s1">]</span>
        <span class="s1">importances = transformer.estimator_.feature_importances_</span>

        <span class="s1">feature_mask = np.abs(importances) &gt; func(importances)</span>
        <span class="s1">assert_array_almost_equal(X_new</span><span class="s0">, </span><span class="s1">X[:</span><span class="s0">, </span><span class="s1">feature_mask])</span>


<span class="s0">def </span><span class="s1">test_sample_weight():</span>
    <span class="s4"># Ensure sample weights are passed to underlying estimator</span>
    <span class="s1">X</span><span class="s0">, </span><span class="s1">y = datasets.make_classification(</span>
        <span class="s1">n_samples=</span><span class="s3">100</span><span class="s0">,</span>
        <span class="s1">n_features=</span><span class="s3">10</span><span class="s0">,</span>
        <span class="s1">n_informative=</span><span class="s3">3</span><span class="s0">,</span>
        <span class="s1">n_redundant=</span><span class="s3">0</span><span class="s0">,</span>
        <span class="s1">n_repeated=</span><span class="s3">0</span><span class="s0">,</span>
        <span class="s1">shuffle=</span><span class="s0">False,</span>
        <span class="s1">random_state=</span><span class="s3">0</span><span class="s0">,</span>
    <span class="s1">)</span>

    <span class="s4"># Check with sample weights</span>
    <span class="s1">sample_weight = np.ones(y.shape)</span>
    <span class="s1">sample_weight[y == </span><span class="s3">1</span><span class="s1">] *= </span><span class="s3">100</span>

    <span class="s1">est = LogisticRegression(random_state=</span><span class="s3">0</span><span class="s0">, </span><span class="s1">fit_intercept=</span><span class="s0">False</span><span class="s1">)</span>
    <span class="s1">transformer = SelectFromModel(estimator=est)</span>
    <span class="s1">transformer.fit(X</span><span class="s0">, </span><span class="s1">y</span><span class="s0">, </span><span class="s1">sample_weight=</span><span class="s0">None</span><span class="s1">)</span>
    <span class="s1">mask = transformer._get_support_mask()</span>
    <span class="s1">transformer.fit(X</span><span class="s0">, </span><span class="s1">y</span><span class="s0">, </span><span class="s1">sample_weight=sample_weight)</span>
    <span class="s1">weighted_mask = transformer._get_support_mask()</span>
    <span class="s0">assert not </span><span class="s1">np.all(weighted_mask == mask)</span>
    <span class="s1">transformer.fit(X</span><span class="s0">, </span><span class="s1">y</span><span class="s0">, </span><span class="s1">sample_weight=</span><span class="s3">3 </span><span class="s1">* sample_weight)</span>
    <span class="s1">reweighted_mask = transformer._get_support_mask()</span>
    <span class="s0">assert </span><span class="s1">np.all(weighted_mask == reweighted_mask)</span>


<span class="s1">@pytest.mark.parametrize(</span>
    <span class="s2">&quot;estimator&quot;</span><span class="s0">,</span>
    <span class="s1">[</span>
        <span class="s1">Lasso(alpha=</span><span class="s3">0.1</span><span class="s0">, </span><span class="s1">random_state=</span><span class="s3">42</span><span class="s1">)</span><span class="s0">,</span>
        <span class="s1">LassoCV(random_state=</span><span class="s3">42</span><span class="s1">)</span><span class="s0">,</span>
        <span class="s1">ElasticNet(l1_ratio=</span><span class="s3">1</span><span class="s0">, </span><span class="s1">random_state=</span><span class="s3">42</span><span class="s1">)</span><span class="s0">,</span>
        <span class="s1">ElasticNetCV(l1_ratio=[</span><span class="s3">1</span><span class="s1">]</span><span class="s0">, </span><span class="s1">random_state=</span><span class="s3">42</span><span class="s1">)</span><span class="s0">,</span>
    <span class="s1">]</span><span class="s0">,</span>
<span class="s1">)</span>
<span class="s0">def </span><span class="s1">test_coef_default_threshold(estimator):</span>
    <span class="s1">X</span><span class="s0">, </span><span class="s1">y = datasets.make_classification(</span>
        <span class="s1">n_samples=</span><span class="s3">100</span><span class="s0">,</span>
        <span class="s1">n_features=</span><span class="s3">10</span><span class="s0">,</span>
        <span class="s1">n_informative=</span><span class="s3">3</span><span class="s0">,</span>
        <span class="s1">n_redundant=</span><span class="s3">0</span><span class="s0">,</span>
        <span class="s1">n_repeated=</span><span class="s3">0</span><span class="s0">,</span>
        <span class="s1">shuffle=</span><span class="s0">False,</span>
        <span class="s1">random_state=</span><span class="s3">0</span><span class="s0">,</span>
    <span class="s1">)</span>

    <span class="s4"># For the Lasso and related models, the threshold defaults to 1e-5</span>
    <span class="s1">transformer = SelectFromModel(estimator=estimator)</span>
    <span class="s1">transformer.fit(X</span><span class="s0">, </span><span class="s1">y)</span>
    <span class="s1">X_new = transformer.transform(X)</span>
    <span class="s1">mask = np.abs(transformer.estimator_.coef_) &gt; </span><span class="s3">1e-5</span>
    <span class="s1">assert_array_almost_equal(X_new</span><span class="s0">, </span><span class="s1">X[:</span><span class="s0">, </span><span class="s1">mask])</span>


<span class="s1">@skip_if_32bit</span>
<span class="s0">def </span><span class="s1">test_2d_coef():</span>
    <span class="s1">X</span><span class="s0">, </span><span class="s1">y = datasets.make_classification(</span>
        <span class="s1">n_samples=</span><span class="s3">1000</span><span class="s0">,</span>
        <span class="s1">n_features=</span><span class="s3">10</span><span class="s0">,</span>
        <span class="s1">n_informative=</span><span class="s3">3</span><span class="s0">,</span>
        <span class="s1">n_redundant=</span><span class="s3">0</span><span class="s0">,</span>
        <span class="s1">n_repeated=</span><span class="s3">0</span><span class="s0">,</span>
        <span class="s1">shuffle=</span><span class="s0">False,</span>
        <span class="s1">random_state=</span><span class="s3">0</span><span class="s0">,</span>
        <span class="s1">n_classes=</span><span class="s3">4</span><span class="s0">,</span>
    <span class="s1">)</span>

    <span class="s1">est = LogisticRegression()</span>
    <span class="s0">for </span><span class="s1">threshold</span><span class="s0">, </span><span class="s1">func </span><span class="s0">in </span><span class="s1">zip([</span><span class="s2">&quot;mean&quot;</span><span class="s0">, </span><span class="s2">&quot;median&quot;</span><span class="s1">]</span><span class="s0">, </span><span class="s1">[np.mean</span><span class="s0">, </span><span class="s1">np.median]):</span>
        <span class="s0">for </span><span class="s1">order </span><span class="s0">in </span><span class="s1">[</span><span class="s3">1</span><span class="s0">, </span><span class="s3">2</span><span class="s0">, </span><span class="s1">np.inf]:</span>
            <span class="s4"># Fit SelectFromModel a multi-class problem</span>
            <span class="s1">transformer = SelectFromModel(</span>
                <span class="s1">estimator=LogisticRegression()</span><span class="s0">, </span><span class="s1">threshold=threshold</span><span class="s0">, </span><span class="s1">norm_order=order</span>
            <span class="s1">)</span>
            <span class="s1">transformer.fit(X</span><span class="s0">, </span><span class="s1">y)</span>
            <span class="s0">assert </span><span class="s1">hasattr(transformer.estimator_</span><span class="s0">, </span><span class="s2">&quot;coef_&quot;</span><span class="s1">)</span>
            <span class="s1">X_new = transformer.transform(X)</span>
            <span class="s0">assert </span><span class="s1">X_new.shape[</span><span class="s3">1</span><span class="s1">] &lt; X.shape[</span><span class="s3">1</span><span class="s1">]</span>

            <span class="s4"># Manually check that the norm is correctly performed</span>
            <span class="s1">est.fit(X</span><span class="s0">, </span><span class="s1">y)</span>
            <span class="s1">importances = np.linalg.norm(est.coef_</span><span class="s0">, </span><span class="s1">axis=</span><span class="s3">0</span><span class="s0">, </span><span class="s1">ord=order)</span>
            <span class="s1">feature_mask = importances &gt; func(importances)</span>
            <span class="s1">assert_array_almost_equal(X_new</span><span class="s0">, </span><span class="s1">X[:</span><span class="s0">, </span><span class="s1">feature_mask])</span>


<span class="s0">def </span><span class="s1">test_partial_fit():</span>
    <span class="s1">est = PassiveAggressiveClassifier(</span>
        <span class="s1">random_state=</span><span class="s3">0</span><span class="s0">, </span><span class="s1">shuffle=</span><span class="s0">False, </span><span class="s1">max_iter=</span><span class="s3">5</span><span class="s0">, </span><span class="s1">tol=</span><span class="s0">None</span>
    <span class="s1">)</span>
    <span class="s1">transformer = SelectFromModel(estimator=est)</span>
    <span class="s1">transformer.partial_fit(data</span><span class="s0">, </span><span class="s1">y</span><span class="s0">, </span><span class="s1">classes=np.unique(y))</span>
    <span class="s1">old_model = transformer.estimator_</span>
    <span class="s1">transformer.partial_fit(data</span><span class="s0">, </span><span class="s1">y</span><span class="s0">, </span><span class="s1">classes=np.unique(y))</span>
    <span class="s1">new_model = transformer.estimator_</span>
    <span class="s0">assert </span><span class="s1">old_model </span><span class="s0">is </span><span class="s1">new_model</span>

    <span class="s1">X_transform = transformer.transform(data)</span>
    <span class="s1">transformer.fit(np.vstack((data</span><span class="s0">, </span><span class="s1">data))</span><span class="s0">, </span><span class="s1">np.concatenate((y</span><span class="s0">, </span><span class="s1">y)))</span>
    <span class="s1">assert_array_almost_equal(X_transform</span><span class="s0">, </span><span class="s1">transformer.transform(data))</span>

    <span class="s4"># check that if est doesn't have partial_fit, neither does SelectFromModel</span>
    <span class="s1">transformer = SelectFromModel(estimator=RandomForestClassifier())</span>
    <span class="s0">assert not </span><span class="s1">hasattr(transformer</span><span class="s0">, </span><span class="s2">&quot;partial_fit&quot;</span><span class="s1">)</span>


<span class="s0">def </span><span class="s1">test_calling_fit_reinitializes():</span>
    <span class="s1">est = LinearSVC(dual=</span><span class="s2">&quot;auto&quot;</span><span class="s0">, </span><span class="s1">random_state=</span><span class="s3">0</span><span class="s1">)</span>
    <span class="s1">transformer = SelectFromModel(estimator=est)</span>
    <span class="s1">transformer.fit(data</span><span class="s0">, </span><span class="s1">y)</span>
    <span class="s1">transformer.set_params(estimator__C=</span><span class="s3">100</span><span class="s1">)</span>
    <span class="s1">transformer.fit(data</span><span class="s0">, </span><span class="s1">y)</span>
    <span class="s0">assert </span><span class="s1">transformer.estimator_.C == </span><span class="s3">100</span>


<span class="s0">def </span><span class="s1">test_prefit():</span>
    <span class="s4"># Test all possible combinations of the prefit parameter.</span>

    <span class="s4"># Passing a prefit parameter with the selected model</span>
    <span class="s4"># and fitting a unfit model with prefit=False should give same results.</span>
    <span class="s1">clf = SGDClassifier(alpha=</span><span class="s3">0.1</span><span class="s0">, </span><span class="s1">max_iter=</span><span class="s3">10</span><span class="s0">, </span><span class="s1">shuffle=</span><span class="s0">True, </span><span class="s1">random_state=</span><span class="s3">0</span><span class="s0">, </span><span class="s1">tol=</span><span class="s0">None</span><span class="s1">)</span>
    <span class="s1">model = SelectFromModel(clf)</span>
    <span class="s1">model.fit(data</span><span class="s0">, </span><span class="s1">y)</span>
    <span class="s1">X_transform = model.transform(data)</span>
    <span class="s1">clf.fit(data</span><span class="s0">, </span><span class="s1">y)</span>
    <span class="s1">model = SelectFromModel(clf</span><span class="s0">, </span><span class="s1">prefit=</span><span class="s0">True</span><span class="s1">)</span>
    <span class="s1">assert_array_almost_equal(model.transform(data)</span><span class="s0">, </span><span class="s1">X_transform)</span>
    <span class="s1">model.fit(data</span><span class="s0">, </span><span class="s1">y)</span>
    <span class="s0">assert </span><span class="s1">model.estimator_ </span><span class="s0">is not </span><span class="s1">clf</span>

    <span class="s4"># Check that the model is rewritten if prefit=False and a fitted model is</span>
    <span class="s4"># passed</span>
    <span class="s1">model = SelectFromModel(clf</span><span class="s0">, </span><span class="s1">prefit=</span><span class="s0">False</span><span class="s1">)</span>
    <span class="s1">model.fit(data</span><span class="s0">, </span><span class="s1">y)</span>
    <span class="s1">assert_array_almost_equal(model.transform(data)</span><span class="s0">, </span><span class="s1">X_transform)</span>

    <span class="s4"># Check that passing an unfitted estimator with `prefit=True` raises a</span>
    <span class="s4"># `ValueError`</span>
    <span class="s1">clf = SGDClassifier(alpha=</span><span class="s3">0.1</span><span class="s0">, </span><span class="s1">max_iter=</span><span class="s3">10</span><span class="s0">, </span><span class="s1">shuffle=</span><span class="s0">True, </span><span class="s1">random_state=</span><span class="s3">0</span><span class="s0">, </span><span class="s1">tol=</span><span class="s0">None</span><span class="s1">)</span>
    <span class="s1">model = SelectFromModel(clf</span><span class="s0">, </span><span class="s1">prefit=</span><span class="s0">True</span><span class="s1">)</span>
    <span class="s1">err_msg = </span><span class="s2">&quot;When `prefit=True`, `estimator` is expected to be a fitted estimator.&quot;</span>
    <span class="s0">with </span><span class="s1">pytest.raises(NotFittedError</span><span class="s0">, </span><span class="s1">match=err_msg):</span>
        <span class="s1">model.fit(data</span><span class="s0">, </span><span class="s1">y)</span>
    <span class="s0">with </span><span class="s1">pytest.raises(NotFittedError</span><span class="s0">, </span><span class="s1">match=err_msg):</span>
        <span class="s1">model.partial_fit(data</span><span class="s0">, </span><span class="s1">y)</span>
    <span class="s0">with </span><span class="s1">pytest.raises(NotFittedError</span><span class="s0">, </span><span class="s1">match=err_msg):</span>
        <span class="s1">model.transform(data)</span>

    <span class="s4"># Check that the internal parameters of prefitted model are not changed</span>
    <span class="s4"># when calling `fit` or `partial_fit` with `prefit=True`</span>
    <span class="s1">clf = SGDClassifier(alpha=</span><span class="s3">0.1</span><span class="s0">, </span><span class="s1">max_iter=</span><span class="s3">10</span><span class="s0">, </span><span class="s1">shuffle=</span><span class="s0">True, </span><span class="s1">tol=</span><span class="s0">None</span><span class="s1">).fit(data</span><span class="s0">, </span><span class="s1">y)</span>
    <span class="s1">model = SelectFromModel(clf</span><span class="s0">, </span><span class="s1">prefit=</span><span class="s0">True</span><span class="s1">)</span>
    <span class="s1">model.fit(data</span><span class="s0">, </span><span class="s1">y)</span>
    <span class="s1">assert_allclose(model.estimator_.coef_</span><span class="s0">, </span><span class="s1">clf.coef_)</span>
    <span class="s1">model.partial_fit(data</span><span class="s0">, </span><span class="s1">y)</span>
    <span class="s1">assert_allclose(model.estimator_.coef_</span><span class="s0">, </span><span class="s1">clf.coef_)</span>


<span class="s0">def </span><span class="s1">test_prefit_max_features():</span>
    <span class="s5">&quot;&quot;&quot;Check the interaction between `prefit` and `max_features`.&quot;&quot;&quot;</span>
    <span class="s4"># case 1: an error should be raised at `transform` if `fit` was not called to</span>
    <span class="s4"># validate the attributes</span>
    <span class="s1">estimator = RandomForestClassifier(n_estimators=</span><span class="s3">5</span><span class="s0">, </span><span class="s1">random_state=</span><span class="s3">0</span><span class="s1">)</span>
    <span class="s1">estimator.fit(data</span><span class="s0">, </span><span class="s1">y)</span>
    <span class="s1">model = SelectFromModel(estimator</span><span class="s0">, </span><span class="s1">prefit=</span><span class="s0">True, </span><span class="s1">max_features=</span><span class="s0">lambda </span><span class="s1">X: X.shape[</span><span class="s3">1</span><span class="s1">])</span>

    <span class="s1">err_msg = (</span>
        <span class="s2">&quot;When `prefit=True` and `max_features` is a callable, call `fit` &quot;</span>
        <span class="s2">&quot;before calling `transform`.&quot;</span>
    <span class="s1">)</span>
    <span class="s0">with </span><span class="s1">pytest.raises(NotFittedError</span><span class="s0">, </span><span class="s1">match=err_msg):</span>
        <span class="s1">model.transform(data)</span>

    <span class="s4"># case 2: `max_features` is not validated and different from an integer</span>
    <span class="s4"># FIXME: we cannot validate the upper bound of the attribute at transform</span>
    <span class="s4"># and we should force calling `fit` if we intend to force the attribute</span>
    <span class="s4"># to have such an upper bound.</span>
    <span class="s1">max_features = </span><span class="s3">2.5</span>
    <span class="s1">model.set_params(max_features=max_features)</span>
    <span class="s0">with </span><span class="s1">pytest.raises(ValueError</span><span class="s0">, </span><span class="s1">match=</span><span class="s2">&quot;`max_features` must be an integer&quot;</span><span class="s1">):</span>
        <span class="s1">model.transform(data)</span>


<span class="s0">def </span><span class="s1">test_prefit_get_feature_names_out():</span>
    <span class="s5">&quot;&quot;&quot;Check the interaction between prefit and the feature names.&quot;&quot;&quot;</span>
    <span class="s1">clf = RandomForestClassifier(n_estimators=</span><span class="s3">2</span><span class="s0">, </span><span class="s1">random_state=</span><span class="s3">0</span><span class="s1">)</span>
    <span class="s1">clf.fit(data</span><span class="s0">, </span><span class="s1">y)</span>
    <span class="s1">model = SelectFromModel(clf</span><span class="s0">, </span><span class="s1">prefit=</span><span class="s0">True, </span><span class="s1">max_features=</span><span class="s3">1</span><span class="s1">)</span>

    <span class="s1">name = type(model).__name__</span>
    <span class="s1">err_msg = (</span>
        <span class="s2">f&quot;This </span><span class="s0">{</span><span class="s1">name</span><span class="s0">} </span><span class="s2">instance is not fitted yet. Call 'fit' with &quot;</span>
        <span class="s2">&quot;appropriate arguments before using this estimator.&quot;</span>
    <span class="s1">)</span>
    <span class="s0">with </span><span class="s1">pytest.raises(NotFittedError</span><span class="s0">, </span><span class="s1">match=err_msg):</span>
        <span class="s1">model.get_feature_names_out()</span>

    <span class="s1">model.fit(data</span><span class="s0">, </span><span class="s1">y)</span>
    <span class="s1">feature_names = model.get_feature_names_out()</span>
    <span class="s0">assert </span><span class="s1">feature_names == [</span><span class="s2">&quot;x3&quot;</span><span class="s1">]</span>


<span class="s0">def </span><span class="s1">test_threshold_string():</span>
    <span class="s1">est = RandomForestClassifier(n_estimators=</span><span class="s3">50</span><span class="s0">, </span><span class="s1">random_state=</span><span class="s3">0</span><span class="s1">)</span>
    <span class="s1">model = SelectFromModel(est</span><span class="s0">, </span><span class="s1">threshold=</span><span class="s2">&quot;0.5*mean&quot;</span><span class="s1">)</span>
    <span class="s1">model.fit(data</span><span class="s0">, </span><span class="s1">y)</span>
    <span class="s1">X_transform = model.transform(data)</span>

    <span class="s4"># Calculate the threshold from the estimator directly.</span>
    <span class="s1">est.fit(data</span><span class="s0">, </span><span class="s1">y)</span>
    <span class="s1">threshold = </span><span class="s3">0.5 </span><span class="s1">* np.mean(est.feature_importances_)</span>
    <span class="s1">mask = est.feature_importances_ &gt; threshold</span>
    <span class="s1">assert_array_almost_equal(X_transform</span><span class="s0">, </span><span class="s1">data[:</span><span class="s0">, </span><span class="s1">mask])</span>


<span class="s0">def </span><span class="s1">test_threshold_without_refitting():</span>
    <span class="s4"># Test that the threshold can be set without refitting the model.</span>
    <span class="s1">clf = SGDClassifier(alpha=</span><span class="s3">0.1</span><span class="s0">, </span><span class="s1">max_iter=</span><span class="s3">10</span><span class="s0">, </span><span class="s1">shuffle=</span><span class="s0">True, </span><span class="s1">random_state=</span><span class="s3">0</span><span class="s0">, </span><span class="s1">tol=</span><span class="s0">None</span><span class="s1">)</span>
    <span class="s1">model = SelectFromModel(clf</span><span class="s0">, </span><span class="s1">threshold=</span><span class="s2">&quot;0.1 * mean&quot;</span><span class="s1">)</span>
    <span class="s1">model.fit(data</span><span class="s0">, </span><span class="s1">y)</span>
    <span class="s1">X_transform = model.transform(data)</span>

    <span class="s4"># Set a higher threshold to filter out more features.</span>
    <span class="s1">model.threshold = </span><span class="s2">&quot;1.0 * mean&quot;</span>
    <span class="s0">assert </span><span class="s1">X_transform.shape[</span><span class="s3">1</span><span class="s1">] &gt; model.transform(data).shape[</span><span class="s3">1</span><span class="s1">]</span>


<span class="s0">def </span><span class="s1">test_fit_accepts_nan_inf():</span>
    <span class="s4"># Test that fit doesn't check for np.inf and np.nan values.</span>
    <span class="s1">clf = HistGradientBoostingClassifier(random_state=</span><span class="s3">0</span><span class="s1">)</span>

    <span class="s1">model = SelectFromModel(estimator=clf)</span>

    <span class="s1">nan_data = data.copy()</span>
    <span class="s1">nan_data[</span><span class="s3">0</span><span class="s1">] = np.nan</span>
    <span class="s1">nan_data[</span><span class="s3">1</span><span class="s1">] = np.inf</span>

    <span class="s1">model.fit(data</span><span class="s0">, </span><span class="s1">y)</span>


<span class="s0">def </span><span class="s1">test_transform_accepts_nan_inf():</span>
    <span class="s4"># Test that transform doesn't check for np.inf and np.nan values.</span>
    <span class="s1">clf = NaNTagRandomForest(n_estimators=</span><span class="s3">100</span><span class="s0">, </span><span class="s1">random_state=</span><span class="s3">0</span><span class="s1">)</span>
    <span class="s1">nan_data = data.copy()</span>

    <span class="s1">model = SelectFromModel(estimator=clf)</span>
    <span class="s1">model.fit(nan_data</span><span class="s0">, </span><span class="s1">y)</span>

    <span class="s1">nan_data[</span><span class="s3">0</span><span class="s1">] = np.nan</span>
    <span class="s1">nan_data[</span><span class="s3">1</span><span class="s1">] = np.inf</span>

    <span class="s1">model.transform(nan_data)</span>


<span class="s0">def </span><span class="s1">test_allow_nan_tag_comes_from_estimator():</span>
    <span class="s1">allow_nan_est = NaNTag()</span>
    <span class="s1">model = SelectFromModel(estimator=allow_nan_est)</span>
    <span class="s0">assert </span><span class="s1">model._get_tags()[</span><span class="s2">&quot;allow_nan&quot;</span><span class="s1">] </span><span class="s0">is True</span>

    <span class="s1">no_nan_est = NoNaNTag()</span>
    <span class="s1">model = SelectFromModel(estimator=no_nan_est)</span>
    <span class="s0">assert </span><span class="s1">model._get_tags()[</span><span class="s2">&quot;allow_nan&quot;</span><span class="s1">] </span><span class="s0">is False</span>


<span class="s0">def </span><span class="s1">_pca_importances(pca_estimator):</span>
    <span class="s0">return </span><span class="s1">np.abs(pca_estimator.explained_variance_)</span>


<span class="s1">@pytest.mark.parametrize(</span>
    <span class="s2">&quot;estimator, importance_getter&quot;</span><span class="s0">,</span>
    <span class="s1">[</span>
        <span class="s1">(</span>
            <span class="s1">make_pipeline(PCA(random_state=</span><span class="s3">0</span><span class="s1">)</span><span class="s0">, </span><span class="s1">LogisticRegression())</span><span class="s0">,</span>
            <span class="s2">&quot;named_steps.logisticregression.coef_&quot;</span><span class="s0">,</span>
        <span class="s1">)</span><span class="s0">,</span>
        <span class="s1">(PCA(random_state=</span><span class="s3">0</span><span class="s1">)</span><span class="s0">, </span><span class="s1">_pca_importances)</span><span class="s0">,</span>
    <span class="s1">]</span><span class="s0">,</span>
<span class="s1">)</span>
<span class="s0">def </span><span class="s1">test_importance_getter(estimator</span><span class="s0">, </span><span class="s1">importance_getter):</span>
    <span class="s1">selector = SelectFromModel(</span>
        <span class="s1">estimator</span><span class="s0">, </span><span class="s1">threshold=</span><span class="s2">&quot;mean&quot;</span><span class="s0">, </span><span class="s1">importance_getter=importance_getter</span>
    <span class="s1">)</span>
    <span class="s1">selector.fit(data</span><span class="s0">, </span><span class="s1">y)</span>
    <span class="s0">assert </span><span class="s1">selector.transform(data).shape[</span><span class="s3">1</span><span class="s1">] == </span><span class="s3">1</span>


<span class="s1">@pytest.mark.parametrize(</span><span class="s2">&quot;PLSEstimator&quot;</span><span class="s0">, </span><span class="s1">[CCA</span><span class="s0">, </span><span class="s1">PLSCanonical</span><span class="s0">, </span><span class="s1">PLSRegression])</span>
<span class="s0">def </span><span class="s1">test_select_from_model_pls(PLSEstimator):</span>
    <span class="s5">&quot;&quot;&quot;Check the behaviour of SelectFromModel with PLS estimators. 
 
    Non-regression test for: 
    https://github.com/scikit-learn/scikit-learn/issues/12410 
    &quot;&quot;&quot;</span>
    <span class="s1">X</span><span class="s0">, </span><span class="s1">y = make_friedman1(n_samples=</span><span class="s3">50</span><span class="s0">, </span><span class="s1">n_features=</span><span class="s3">10</span><span class="s0">, </span><span class="s1">random_state=</span><span class="s3">0</span><span class="s1">)</span>
    <span class="s1">estimator = PLSEstimator(n_components=</span><span class="s3">1</span><span class="s1">)</span>
    <span class="s1">model = make_pipeline(SelectFromModel(estimator)</span><span class="s0">, </span><span class="s1">estimator).fit(X</span><span class="s0">, </span><span class="s1">y)</span>
    <span class="s0">assert </span><span class="s1">model.score(X</span><span class="s0">, </span><span class="s1">y) &gt; </span><span class="s3">0.5</span>


<span class="s0">def </span><span class="s1">test_estimator_does_not_support_feature_names():</span>
    <span class="s5">&quot;&quot;&quot;SelectFromModel works with estimators that do not support feature_names_in_. 
 
    Non-regression test for #21949. 
    &quot;&quot;&quot;</span>
    <span class="s1">pytest.importorskip(</span><span class="s2">&quot;pandas&quot;</span><span class="s1">)</span>
    <span class="s1">X</span><span class="s0">, </span><span class="s1">y = datasets.load_iris(as_frame=</span><span class="s0">True, </span><span class="s1">return_X_y=</span><span class="s0">True</span><span class="s1">)</span>
    <span class="s1">all_feature_names = set(X.columns)</span>

    <span class="s0">def </span><span class="s1">importance_getter(estimator):</span>
        <span class="s0">return </span><span class="s1">np.arange(X.shape[</span><span class="s3">1</span><span class="s1">])</span>

    <span class="s1">selector = SelectFromModel(</span>
        <span class="s1">MinimalClassifier()</span><span class="s0">, </span><span class="s1">importance_getter=importance_getter</span>
    <span class="s1">).fit(X</span><span class="s0">, </span><span class="s1">y)</span>

    <span class="s4"># selector learns the feature names itself</span>
    <span class="s1">assert_array_equal(selector.feature_names_in_</span><span class="s0">, </span><span class="s1">X.columns)</span>

    <span class="s1">feature_names_out = set(selector.get_feature_names_out())</span>
    <span class="s0">assert </span><span class="s1">feature_names_out &lt; all_feature_names</span>

    <span class="s0">with </span><span class="s1">warnings.catch_warnings():</span>
        <span class="s1">warnings.simplefilter(</span><span class="s2">&quot;error&quot;</span><span class="s0">, </span><span class="s1">UserWarning)</span>

        <span class="s1">selector.transform(X.iloc[</span><span class="s3">1</span><span class="s1">:</span><span class="s3">3</span><span class="s1">])</span>


<span class="s1">@pytest.mark.parametrize(</span>
    <span class="s2">&quot;error, err_msg, max_features&quot;</span><span class="s0">,</span>
    <span class="s1">(</span>
        <span class="s1">[ValueError</span><span class="s0">, </span><span class="s2">&quot;max_features == 10, must be &lt;= 4&quot;</span><span class="s0">, </span><span class="s3">10</span><span class="s1">]</span><span class="s0">,</span>
        <span class="s1">[ValueError</span><span class="s0">, </span><span class="s2">&quot;max_features == 5, must be &lt;= 4&quot;</span><span class="s0">, lambda </span><span class="s1">x: x.shape[</span><span class="s3">1</span><span class="s1">] + </span><span class="s3">1</span><span class="s1">]</span><span class="s0">,</span>
    <span class="s1">)</span><span class="s0">,</span>
<span class="s1">)</span>
<span class="s0">def </span><span class="s1">test_partial_fit_validate_max_features(error</span><span class="s0">, </span><span class="s1">err_msg</span><span class="s0">, </span><span class="s1">max_features):</span>
    <span class="s5">&quot;&quot;&quot;Test that partial_fit from SelectFromModel validates `max_features`.&quot;&quot;&quot;</span>
    <span class="s1">X</span><span class="s0">, </span><span class="s1">y = datasets.make_classification(</span>
        <span class="s1">n_samples=</span><span class="s3">100</span><span class="s0">,</span>
        <span class="s1">n_features=</span><span class="s3">4</span><span class="s0">,</span>
        <span class="s1">random_state=</span><span class="s3">0</span><span class="s0">,</span>
    <span class="s1">)</span>

    <span class="s0">with </span><span class="s1">pytest.raises(error</span><span class="s0">, </span><span class="s1">match=err_msg):</span>
        <span class="s1">SelectFromModel(</span>
            <span class="s1">estimator=SGDClassifier()</span><span class="s0">, </span><span class="s1">max_features=max_features</span>
        <span class="s1">).partial_fit(X</span><span class="s0">, </span><span class="s1">y</span><span class="s0">, </span><span class="s1">classes=[</span><span class="s3">0</span><span class="s0">, </span><span class="s3">1</span><span class="s1">])</span>


<span class="s1">@pytest.mark.parametrize(</span><span class="s2">&quot;as_frame&quot;</span><span class="s0">, </span><span class="s1">[</span><span class="s0">True, False</span><span class="s1">])</span>
<span class="s0">def </span><span class="s1">test_partial_fit_validate_feature_names(as_frame):</span>
    <span class="s5">&quot;&quot;&quot;Test that partial_fit from SelectFromModel validates `feature_names_in_`.&quot;&quot;&quot;</span>
    <span class="s1">pytest.importorskip(</span><span class="s2">&quot;pandas&quot;</span><span class="s1">)</span>
    <span class="s1">X</span><span class="s0">, </span><span class="s1">y = datasets.load_iris(as_frame=as_frame</span><span class="s0">, </span><span class="s1">return_X_y=</span><span class="s0">True</span><span class="s1">)</span>

    <span class="s1">selector = SelectFromModel(estimator=SGDClassifier()</span><span class="s0">, </span><span class="s1">max_features=</span><span class="s3">4</span><span class="s1">).partial_fit(</span>
        <span class="s1">X</span><span class="s0">, </span><span class="s1">y</span><span class="s0">, </span><span class="s1">classes=[</span><span class="s3">0</span><span class="s0">, </span><span class="s3">1</span><span class="s0">, </span><span class="s3">2</span><span class="s1">]</span>
    <span class="s1">)</span>
    <span class="s0">if </span><span class="s1">as_frame:</span>
        <span class="s1">assert_array_equal(selector.feature_names_in_</span><span class="s0">, </span><span class="s1">X.columns)</span>
    <span class="s0">else</span><span class="s1">:</span>
        <span class="s0">assert not </span><span class="s1">hasattr(selector</span><span class="s0">, </span><span class="s2">&quot;feature_names_in_&quot;</span><span class="s1">)</span>
</pre>
</body>
</html>