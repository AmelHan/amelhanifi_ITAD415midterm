<html>
<head>
<title>_hessian_update_strategy.py</title>
<meta http-equiv="Content-Type" content="text/html; charset=utf-8">
<style type="text/css">
.s0 { color: #629755; font-style: italic;}
.s1 { color: #a9b7c6;}
.s2 { color: #cc7832;}
.s3 { color: #6a8759;}
.s4 { color: #808080;}
.s5 { color: #6897bb;}
</style>
</head>
<body bgcolor="#2b2b2b">
<table CELLSPACING=0 CELLPADDING=5 COLS=1 WIDTH="100%" BGCOLOR="#606060" >
<tr><td><center>
<font face="Arial, Helvetica" color="#000000">
_hessian_update_strategy.py</font>
</center></td></tr></table>
<pre><span class="s0">&quot;&quot;&quot;Hessian update strategies for quasi-Newton optimization methods.&quot;&quot;&quot;</span>
<span class="s2">import </span><span class="s1">numpy </span><span class="s2">as </span><span class="s1">np</span>
<span class="s2">from </span><span class="s1">numpy.linalg </span><span class="s2">import </span><span class="s1">norm</span>
<span class="s2">from </span><span class="s1">scipy.linalg </span><span class="s2">import </span><span class="s1">get_blas_funcs</span>
<span class="s2">from </span><span class="s1">warnings </span><span class="s2">import </span><span class="s1">warn</span>


<span class="s1">__all__ = [</span><span class="s3">'HessianUpdateStrategy'</span><span class="s2">, </span><span class="s3">'BFGS'</span><span class="s2">, </span><span class="s3">'SR1'</span><span class="s1">]</span>


<span class="s2">class </span><span class="s1">HessianUpdateStrategy:</span>
    <span class="s0">&quot;&quot;&quot;Interface for implementing Hessian update strategies. 
 
    Many optimization methods make use of Hessian (or inverse Hessian) 
    approximations, such as the quasi-Newton methods BFGS, SR1, L-BFGS. 
    Some of these  approximations, however, do not actually need to store 
    the entire matrix or can compute the internal matrix product with a 
    given vector in a very efficiently manner. This class serves as an 
    abstract interface between the optimization algorithm and the 
    quasi-Newton update strategies, giving freedom of implementation 
    to store and update the internal matrix as efficiently as possible. 
    Different choices of initialization and update procedure will result 
    in different quasi-Newton strategies. 
 
    Four methods should be implemented in derived classes: ``initialize``, 
    ``update``, ``dot`` and ``get_matrix``. 
 
    Notes 
    ----- 
    Any instance of a class that implements this interface, 
    can be accepted by the method ``minimize`` and used by 
    the compatible solvers to approximate the Hessian (or 
    inverse Hessian) used by the optimization algorithms. 
    &quot;&quot;&quot;</span>

    <span class="s2">def </span><span class="s1">initialize(self</span><span class="s2">, </span><span class="s1">n</span><span class="s2">, </span><span class="s1">approx_type):</span>
        <span class="s0">&quot;&quot;&quot;Initialize internal matrix. 
 
        Allocate internal memory for storing and updating 
        the Hessian or its inverse. 
 
        Parameters 
        ---------- 
        n : int 
            Problem dimension. 
        approx_type : {'hess', 'inv_hess'} 
            Selects either the Hessian or the inverse Hessian. 
            When set to 'hess' the Hessian will be stored and updated. 
            When set to 'inv_hess' its inverse will be used instead. 
        &quot;&quot;&quot;</span>
        <span class="s2">raise </span><span class="s1">NotImplementedError(</span><span class="s3">&quot;The method ``initialize(n, approx_type)``&quot;</span>
                                  <span class="s3">&quot; is not implemented.&quot;</span><span class="s1">)</span>

    <span class="s2">def </span><span class="s1">update(self</span><span class="s2">, </span><span class="s1">delta_x</span><span class="s2">, </span><span class="s1">delta_grad):</span>
        <span class="s0">&quot;&quot;&quot;Update internal matrix. 
 
        Update Hessian matrix or its inverse (depending on how 'approx_type' 
        is defined) using information about the last evaluated points. 
 
        Parameters 
        ---------- 
        delta_x : ndarray 
            The difference between two points the gradient 
            function have been evaluated at: ``delta_x = x2 - x1``. 
        delta_grad : ndarray 
            The difference between the gradients: 
            ``delta_grad = grad(x2) - grad(x1)``. 
        &quot;&quot;&quot;</span>
        <span class="s2">raise </span><span class="s1">NotImplementedError(</span><span class="s3">&quot;The method ``update(delta_x, delta_grad)``&quot;</span>
                                  <span class="s3">&quot; is not implemented.&quot;</span><span class="s1">)</span>

    <span class="s2">def </span><span class="s1">dot(self</span><span class="s2">, </span><span class="s1">p):</span>
        <span class="s0">&quot;&quot;&quot;Compute the product of the internal matrix with the given vector. 
 
        Parameters 
        ---------- 
        p : array_like 
            1-D array representing a vector. 
 
        Returns 
        ------- 
        Hp : array 
            1-D represents the result of multiplying the approximation matrix 
            by vector p. 
        &quot;&quot;&quot;</span>
        <span class="s2">raise </span><span class="s1">NotImplementedError(</span><span class="s3">&quot;The method ``dot(p)``&quot;</span>
                                  <span class="s3">&quot; is not implemented.&quot;</span><span class="s1">)</span>

    <span class="s2">def </span><span class="s1">get_matrix(self):</span>
        <span class="s0">&quot;&quot;&quot;Return current internal matrix. 
 
        Returns 
        ------- 
        H : ndarray, shape (n, n) 
            Dense matrix containing either the Hessian 
            or its inverse (depending on how 'approx_type' 
            is defined). 
        &quot;&quot;&quot;</span>
        <span class="s2">raise </span><span class="s1">NotImplementedError(</span><span class="s3">&quot;The method ``get_matrix(p)``&quot;</span>
                                  <span class="s3">&quot; is not implemented.&quot;</span><span class="s1">)</span>


<span class="s2">class </span><span class="s1">FullHessianUpdateStrategy(HessianUpdateStrategy):</span>
    <span class="s0">&quot;&quot;&quot;Hessian update strategy with full dimensional internal representation. 
    &quot;&quot;&quot;</span>
    <span class="s1">_syr = get_blas_funcs(</span><span class="s3">'syr'</span><span class="s2">, </span><span class="s1">dtype=</span><span class="s3">'d'</span><span class="s1">)  </span><span class="s4"># Symmetric rank 1 update</span>
    <span class="s1">_syr2 = get_blas_funcs(</span><span class="s3">'syr2'</span><span class="s2">, </span><span class="s1">dtype=</span><span class="s3">'d'</span><span class="s1">)  </span><span class="s4"># Symmetric rank 2 update</span>
    <span class="s4"># Symmetric matrix-vector product</span>
    <span class="s1">_symv = get_blas_funcs(</span><span class="s3">'symv'</span><span class="s2">, </span><span class="s1">dtype=</span><span class="s3">'d'</span><span class="s1">)</span>

    <span class="s2">def </span><span class="s1">__init__(self</span><span class="s2">, </span><span class="s1">init_scale=</span><span class="s3">'auto'</span><span class="s1">):</span>
        <span class="s1">self.init_scale = init_scale</span>
        <span class="s4"># Until initialize is called we can't really use the class,</span>
        <span class="s4"># so it makes sense to set everything to None.</span>
        <span class="s1">self.first_iteration = </span><span class="s2">None</span>
        <span class="s1">self.approx_type = </span><span class="s2">None</span>
        <span class="s1">self.B = </span><span class="s2">None</span>
        <span class="s1">self.H = </span><span class="s2">None</span>

    <span class="s2">def </span><span class="s1">initialize(self</span><span class="s2">, </span><span class="s1">n</span><span class="s2">, </span><span class="s1">approx_type):</span>
        <span class="s0">&quot;&quot;&quot;Initialize internal matrix. 
 
        Allocate internal memory for storing and updating 
        the Hessian or its inverse. 
 
        Parameters 
        ---------- 
        n : int 
            Problem dimension. 
        approx_type : {'hess', 'inv_hess'} 
            Selects either the Hessian or the inverse Hessian. 
            When set to 'hess' the Hessian will be stored and updated. 
            When set to 'inv_hess' its inverse will be used instead. 
        &quot;&quot;&quot;</span>
        <span class="s1">self.first_iteration = </span><span class="s2">True</span>
        <span class="s1">self.n = n</span>
        <span class="s1">self.approx_type = approx_type</span>
        <span class="s2">if </span><span class="s1">approx_type </span><span class="s2">not in </span><span class="s1">(</span><span class="s3">'hess'</span><span class="s2">, </span><span class="s3">'inv_hess'</span><span class="s1">):</span>
            <span class="s2">raise </span><span class="s1">ValueError(</span><span class="s3">&quot;`approx_type` must be 'hess' or 'inv_hess'.&quot;</span><span class="s1">)</span>
        <span class="s4"># Create matrix</span>
        <span class="s2">if </span><span class="s1">self.approx_type == </span><span class="s3">'hess'</span><span class="s1">:</span>
            <span class="s1">self.B = np.eye(n</span><span class="s2">, </span><span class="s1">dtype=float)</span>
        <span class="s2">else</span><span class="s1">:</span>
            <span class="s1">self.H = np.eye(n</span><span class="s2">, </span><span class="s1">dtype=float)</span>

    <span class="s2">def </span><span class="s1">_auto_scale(self</span><span class="s2">, </span><span class="s1">delta_x</span><span class="s2">, </span><span class="s1">delta_grad):</span>
        <span class="s4"># Heuristic to scale matrix at first iteration.</span>
        <span class="s4"># Described in Nocedal and Wright &quot;Numerical Optimization&quot;</span>
        <span class="s4"># p.143 formula (6.20).</span>
        <span class="s1">s_norm2 = np.dot(delta_x</span><span class="s2">, </span><span class="s1">delta_x)</span>
        <span class="s1">y_norm2 = np.dot(delta_grad</span><span class="s2">, </span><span class="s1">delta_grad)</span>
        <span class="s1">ys = np.abs(np.dot(delta_grad</span><span class="s2">, </span><span class="s1">delta_x))</span>
        <span class="s2">if </span><span class="s1">ys == </span><span class="s5">0.0 </span><span class="s2">or </span><span class="s1">y_norm2 == </span><span class="s5">0 </span><span class="s2">or </span><span class="s1">s_norm2 == </span><span class="s5">0</span><span class="s1">:</span>
            <span class="s2">return </span><span class="s5">1</span>
        <span class="s2">if </span><span class="s1">self.approx_type == </span><span class="s3">'hess'</span><span class="s1">:</span>
            <span class="s2">return </span><span class="s1">y_norm2 / ys</span>
        <span class="s2">else</span><span class="s1">:</span>
            <span class="s2">return </span><span class="s1">ys / y_norm2</span>

    <span class="s2">def </span><span class="s1">_update_implementation(self</span><span class="s2">, </span><span class="s1">delta_x</span><span class="s2">, </span><span class="s1">delta_grad):</span>
        <span class="s2">raise </span><span class="s1">NotImplementedError(</span><span class="s3">&quot;The method ``_update_implementation``&quot;</span>
                                  <span class="s3">&quot; is not implemented.&quot;</span><span class="s1">)</span>

    <span class="s2">def </span><span class="s1">update(self</span><span class="s2">, </span><span class="s1">delta_x</span><span class="s2">, </span><span class="s1">delta_grad):</span>
        <span class="s0">&quot;&quot;&quot;Update internal matrix. 
 
        Update Hessian matrix or its inverse (depending on how 'approx_type' 
        is defined) using information about the last evaluated points. 
 
        Parameters 
        ---------- 
        delta_x : ndarray 
            The difference between two points the gradient 
            function have been evaluated at: ``delta_x = x2 - x1``. 
        delta_grad : ndarray 
            The difference between the gradients: 
            ``delta_grad = grad(x2) - grad(x1)``. 
        &quot;&quot;&quot;</span>
        <span class="s2">if </span><span class="s1">np.all(delta_x == </span><span class="s5">0.0</span><span class="s1">):</span>
            <span class="s2">return</span>
        <span class="s2">if </span><span class="s1">np.all(delta_grad == </span><span class="s5">0.0</span><span class="s1">):</span>
            <span class="s1">warn(</span><span class="s3">'delta_grad == 0.0. Check if the approximated '</span>
                 <span class="s3">'function is linear. If the function is linear '</span>
                 <span class="s3">'better results can be obtained by defining the '</span>
                 <span class="s3">'Hessian as zero instead of using quasi-Newton '</span>
                 <span class="s3">'approximations.'</span><span class="s2">, </span><span class="s1">UserWarning)</span>
            <span class="s2">return</span>
        <span class="s2">if </span><span class="s1">self.first_iteration:</span>
            <span class="s4"># Get user specific scale</span>
            <span class="s2">if </span><span class="s1">self.init_scale == </span><span class="s3">&quot;auto&quot;</span><span class="s1">:</span>
                <span class="s1">scale = self._auto_scale(delta_x</span><span class="s2">, </span><span class="s1">delta_grad)</span>
            <span class="s2">else</span><span class="s1">:</span>
                <span class="s1">scale = float(self.init_scale)</span>
            <span class="s4"># Scale initial matrix with ``scale * np.eye(n)``</span>
            <span class="s2">if </span><span class="s1">self.approx_type == </span><span class="s3">'hess'</span><span class="s1">:</span>
                <span class="s1">self.B *= scale</span>
            <span class="s2">else</span><span class="s1">:</span>
                <span class="s1">self.H *= scale</span>
            <span class="s1">self.first_iteration = </span><span class="s2">False</span>
        <span class="s1">self._update_implementation(delta_x</span><span class="s2">, </span><span class="s1">delta_grad)</span>

    <span class="s2">def </span><span class="s1">dot(self</span><span class="s2">, </span><span class="s1">p):</span>
        <span class="s0">&quot;&quot;&quot;Compute the product of the internal matrix with the given vector. 
 
        Parameters 
        ---------- 
        p : array_like 
            1-D array representing a vector. 
 
        Returns 
        ------- 
        Hp : array 
            1-D represents the result of multiplying the approximation matrix 
            by vector p. 
        &quot;&quot;&quot;</span>
        <span class="s2">if </span><span class="s1">self.approx_type == </span><span class="s3">'hess'</span><span class="s1">:</span>
            <span class="s2">return </span><span class="s1">self._symv(</span><span class="s5">1</span><span class="s2">, </span><span class="s1">self.B</span><span class="s2">, </span><span class="s1">p)</span>
        <span class="s2">else</span><span class="s1">:</span>
            <span class="s2">return </span><span class="s1">self._symv(</span><span class="s5">1</span><span class="s2">, </span><span class="s1">self.H</span><span class="s2">, </span><span class="s1">p)</span>

    <span class="s2">def </span><span class="s1">get_matrix(self):</span>
        <span class="s0">&quot;&quot;&quot;Return the current internal matrix. 
 
        Returns 
        ------- 
        M : ndarray, shape (n, n) 
            Dense matrix containing either the Hessian or its inverse 
            (depending on how `approx_type` was defined). 
        &quot;&quot;&quot;</span>
        <span class="s2">if </span><span class="s1">self.approx_type == </span><span class="s3">'hess'</span><span class="s1">:</span>
            <span class="s1">M = np.copy(self.B)</span>
        <span class="s2">else</span><span class="s1">:</span>
            <span class="s1">M = np.copy(self.H)</span>
        <span class="s1">li = np.tril_indices_from(M</span><span class="s2">, </span><span class="s1">k=-</span><span class="s5">1</span><span class="s1">)</span>
        <span class="s1">M[li] = M.T[li]</span>
        <span class="s2">return </span><span class="s1">M</span>


<span class="s2">class </span><span class="s1">BFGS(FullHessianUpdateStrategy):</span>
    <span class="s0">&quot;&quot;&quot;Broyden-Fletcher-Goldfarb-Shanno (BFGS) Hessian update strategy. 
 
    Parameters 
    ---------- 
    exception_strategy : {'skip_update', 'damp_update'}, optional 
        Define how to proceed when the curvature condition is violated. 
        Set it to 'skip_update' to just skip the update. Or, alternatively, 
        set it to 'damp_update' to interpolate between the actual BFGS 
        result and the unmodified matrix. Both exceptions strategies 
        are explained  in [1]_, p.536-537. 
    min_curvature : float 
        This number, scaled by a normalization factor, defines the 
        minimum curvature ``dot(delta_grad, delta_x)`` allowed to go 
        unaffected by the exception strategy. By default is equal to 
        1e-8 when ``exception_strategy = 'skip_update'`` and equal 
        to 0.2 when ``exception_strategy = 'damp_update'``. 
    init_scale : {float, 'auto'} 
        Matrix scale at first iteration. At the first 
        iteration the Hessian matrix or its inverse will be initialized 
        with ``init_scale*np.eye(n)``, where ``n`` is the problem dimension. 
        Set it to 'auto' in order to use an automatic heuristic for choosing 
        the initial scale. The heuristic is described in [1]_, p.143. 
        By default uses 'auto'. 
 
    Notes 
    ----- 
    The update is based on the description in [1]_, p.140. 
 
    References 
    ---------- 
    .. [1] Nocedal, Jorge, and Stephen J. Wright. &quot;Numerical optimization&quot; 
           Second Edition (2006). 
    &quot;&quot;&quot;</span>

    <span class="s2">def </span><span class="s1">__init__(self</span><span class="s2">, </span><span class="s1">exception_strategy=</span><span class="s3">'skip_update'</span><span class="s2">, </span><span class="s1">min_curvature=</span><span class="s2">None,</span>
                 <span class="s1">init_scale=</span><span class="s3">'auto'</span><span class="s1">):</span>
        <span class="s2">if </span><span class="s1">exception_strategy == </span><span class="s3">'skip_update'</span><span class="s1">:</span>
            <span class="s2">if </span><span class="s1">min_curvature </span><span class="s2">is not None</span><span class="s1">:</span>
                <span class="s1">self.min_curvature = min_curvature</span>
            <span class="s2">else</span><span class="s1">:</span>
                <span class="s1">self.min_curvature = </span><span class="s5">1e-8</span>
        <span class="s2">elif </span><span class="s1">exception_strategy == </span><span class="s3">'damp_update'</span><span class="s1">:</span>
            <span class="s2">if </span><span class="s1">min_curvature </span><span class="s2">is not None</span><span class="s1">:</span>
                <span class="s1">self.min_curvature = min_curvature</span>
            <span class="s2">else</span><span class="s1">:</span>
                <span class="s1">self.min_curvature = </span><span class="s5">0.2</span>
        <span class="s2">else</span><span class="s1">:</span>
            <span class="s2">raise </span><span class="s1">ValueError(</span><span class="s3">&quot;`exception_strategy` must be 'skip_update' &quot;</span>
                             <span class="s3">&quot;or 'damp_update'.&quot;</span><span class="s1">)</span>

        <span class="s1">super().__init__(init_scale)</span>
        <span class="s1">self.exception_strategy = exception_strategy</span>

    <span class="s2">def </span><span class="s1">_update_inverse_hessian(self</span><span class="s2">, </span><span class="s1">ys</span><span class="s2">, </span><span class="s1">Hy</span><span class="s2">, </span><span class="s1">yHy</span><span class="s2">, </span><span class="s1">s):</span>
        <span class="s0">&quot;&quot;&quot;Update the inverse Hessian matrix. 
 
        BFGS update using the formula: 
 
            ``H &lt;- H + ((H*y).T*y + s.T*y)/(s.T*y)^2 * (s*s.T) 
                     - 1/(s.T*y) * ((H*y)*s.T + s*(H*y).T)`` 
 
        where ``s = delta_x`` and ``y = delta_grad``. This formula is 
        equivalent to (6.17) in [1]_ written in a more efficient way 
        for implementation. 
 
        References 
        ---------- 
        .. [1] Nocedal, Jorge, and Stephen J. Wright. &quot;Numerical optimization&quot; 
               Second Edition (2006). 
        &quot;&quot;&quot;</span>
        <span class="s1">self.H = self._syr2(-</span><span class="s5">1.0 </span><span class="s1">/ ys</span><span class="s2">, </span><span class="s1">s</span><span class="s2">, </span><span class="s1">Hy</span><span class="s2">, </span><span class="s1">a=self.H)</span>
        <span class="s1">self.H = self._syr((ys+yHy)/ys**</span><span class="s5">2</span><span class="s2">, </span><span class="s1">s</span><span class="s2">, </span><span class="s1">a=self.H)</span>

    <span class="s2">def </span><span class="s1">_update_hessian(self</span><span class="s2">, </span><span class="s1">ys</span><span class="s2">, </span><span class="s1">Bs</span><span class="s2">, </span><span class="s1">sBs</span><span class="s2">, </span><span class="s1">y):</span>
        <span class="s0">&quot;&quot;&quot;Update the Hessian matrix. 
 
        BFGS update using the formula: 
 
            ``B &lt;- B - (B*s)*(B*s).T/s.T*(B*s) + y*y^T/s.T*y`` 
 
        where ``s`` is short for ``delta_x`` and ``y`` is short 
        for ``delta_grad``. Formula (6.19) in [1]_. 
 
        References 
        ---------- 
        .. [1] Nocedal, Jorge, and Stephen J. Wright. &quot;Numerical optimization&quot; 
               Second Edition (2006). 
        &quot;&quot;&quot;</span>
        <span class="s1">self.B = self._syr(</span><span class="s5">1.0 </span><span class="s1">/ ys</span><span class="s2">, </span><span class="s1">y</span><span class="s2">, </span><span class="s1">a=self.B)</span>
        <span class="s1">self.B = self._syr(-</span><span class="s5">1.0 </span><span class="s1">/ sBs</span><span class="s2">, </span><span class="s1">Bs</span><span class="s2">, </span><span class="s1">a=self.B)</span>

    <span class="s2">def </span><span class="s1">_update_implementation(self</span><span class="s2">, </span><span class="s1">delta_x</span><span class="s2">, </span><span class="s1">delta_grad):</span>
        <span class="s4"># Auxiliary variables w and z</span>
        <span class="s2">if </span><span class="s1">self.approx_type == </span><span class="s3">'hess'</span><span class="s1">:</span>
            <span class="s1">w = delta_x</span>
            <span class="s1">z = delta_grad</span>
        <span class="s2">else</span><span class="s1">:</span>
            <span class="s1">w = delta_grad</span>
            <span class="s1">z = delta_x</span>
        <span class="s4"># Do some common operations</span>
        <span class="s1">wz = np.dot(w</span><span class="s2">, </span><span class="s1">z)</span>
        <span class="s1">Mw = self.dot(w)</span>
        <span class="s1">wMw = Mw.dot(w)</span>
        <span class="s4"># Guarantee that wMw &gt; 0 by reinitializing matrix.</span>
        <span class="s4"># While this is always true in exact arithmetics,</span>
        <span class="s4"># indefinite matrix may appear due to roundoff errors.</span>
        <span class="s2">if </span><span class="s1">wMw &lt;= </span><span class="s5">0.0</span><span class="s1">:</span>
            <span class="s1">scale = self._auto_scale(delta_x</span><span class="s2">, </span><span class="s1">delta_grad)</span>
            <span class="s4"># Reinitialize matrix</span>
            <span class="s2">if </span><span class="s1">self.approx_type == </span><span class="s3">'hess'</span><span class="s1">:</span>
                <span class="s1">self.B = scale * np.eye(self.n</span><span class="s2">, </span><span class="s1">dtype=float)</span>
            <span class="s2">else</span><span class="s1">:</span>
                <span class="s1">self.H = scale * np.eye(self.n</span><span class="s2">, </span><span class="s1">dtype=float)</span>
            <span class="s4"># Do common operations for new matrix</span>
            <span class="s1">Mw = self.dot(w)</span>
            <span class="s1">wMw = Mw.dot(w)</span>
        <span class="s4"># Check if curvature condition is violated</span>
        <span class="s2">if </span><span class="s1">wz &lt;= self.min_curvature * wMw:</span>
            <span class="s4"># If the option 'skip_update' is set</span>
            <span class="s4"># we just skip the update when the condion</span>
            <span class="s4"># is violated.</span>
            <span class="s2">if </span><span class="s1">self.exception_strategy == </span><span class="s3">'skip_update'</span><span class="s1">:</span>
                <span class="s2">return</span>
            <span class="s4"># If the option 'damp_update' is set we</span>
            <span class="s4"># interpolate between the actual BFGS</span>
            <span class="s4"># result and the unmodified matrix.</span>
            <span class="s2">elif </span><span class="s1">self.exception_strategy == </span><span class="s3">'damp_update'</span><span class="s1">:</span>
                <span class="s1">update_factor = (</span><span class="s5">1</span><span class="s1">-self.min_curvature) / (</span><span class="s5">1 </span><span class="s1">- wz/wMw)</span>
                <span class="s1">z = update_factor*z + (</span><span class="s5">1</span><span class="s1">-update_factor)*Mw</span>
                <span class="s1">wz = np.dot(w</span><span class="s2">, </span><span class="s1">z)</span>
        <span class="s4"># Update matrix</span>
        <span class="s2">if </span><span class="s1">self.approx_type == </span><span class="s3">'hess'</span><span class="s1">:</span>
            <span class="s1">self._update_hessian(wz</span><span class="s2">, </span><span class="s1">Mw</span><span class="s2">, </span><span class="s1">wMw</span><span class="s2">, </span><span class="s1">z)</span>
        <span class="s2">else</span><span class="s1">:</span>
            <span class="s1">self._update_inverse_hessian(wz</span><span class="s2">, </span><span class="s1">Mw</span><span class="s2">, </span><span class="s1">wMw</span><span class="s2">, </span><span class="s1">z)</span>


<span class="s2">class </span><span class="s1">SR1(FullHessianUpdateStrategy):</span>
    <span class="s0">&quot;&quot;&quot;Symmetric-rank-1 Hessian update strategy. 
 
    Parameters 
    ---------- 
    min_denominator : float 
        This number, scaled by a normalization factor, 
        defines the minimum denominator magnitude allowed 
        in the update. When the condition is violated we skip 
        the update. By default uses ``1e-8``. 
    init_scale : {float, 'auto'}, optional 
        Matrix scale at first iteration. At the first 
        iteration the Hessian matrix or its inverse will be initialized 
        with ``init_scale*np.eye(n)``, where ``n`` is the problem dimension. 
        Set it to 'auto' in order to use an automatic heuristic for choosing 
        the initial scale. The heuristic is described in [1]_, p.143. 
        By default uses 'auto'. 
 
    Notes 
    ----- 
    The update is based on the description in [1]_, p.144-146. 
 
    References 
    ---------- 
    .. [1] Nocedal, Jorge, and Stephen J. Wright. &quot;Numerical optimization&quot; 
           Second Edition (2006). 
    &quot;&quot;&quot;</span>

    <span class="s2">def </span><span class="s1">__init__(self</span><span class="s2">, </span><span class="s1">min_denominator=</span><span class="s5">1e-8</span><span class="s2">, </span><span class="s1">init_scale=</span><span class="s3">'auto'</span><span class="s1">):</span>
        <span class="s1">self.min_denominator = min_denominator</span>
        <span class="s1">super().__init__(init_scale)</span>

    <span class="s2">def </span><span class="s1">_update_implementation(self</span><span class="s2">, </span><span class="s1">delta_x</span><span class="s2">, </span><span class="s1">delta_grad):</span>
        <span class="s4"># Auxiliary variables w and z</span>
        <span class="s2">if </span><span class="s1">self.approx_type == </span><span class="s3">'hess'</span><span class="s1">:</span>
            <span class="s1">w = delta_x</span>
            <span class="s1">z = delta_grad</span>
        <span class="s2">else</span><span class="s1">:</span>
            <span class="s1">w = delta_grad</span>
            <span class="s1">z = delta_x</span>
        <span class="s4"># Do some common operations</span>
        <span class="s1">Mw = self.dot(w)</span>
        <span class="s1">z_minus_Mw = z - Mw</span>
        <span class="s1">denominator = np.dot(w</span><span class="s2">, </span><span class="s1">z_minus_Mw)</span>
        <span class="s4"># If the denominator is too small</span>
        <span class="s4"># we just skip the update.</span>
        <span class="s2">if </span><span class="s1">np.abs(denominator) &lt;= self.min_denominator*norm(w)*norm(z_minus_Mw):</span>
            <span class="s2">return</span>
        <span class="s4"># Update matrix</span>
        <span class="s2">if </span><span class="s1">self.approx_type == </span><span class="s3">'hess'</span><span class="s1">:</span>
            <span class="s1">self.B = self._syr(</span><span class="s5">1</span><span class="s1">/denominator</span><span class="s2">, </span><span class="s1">z_minus_Mw</span><span class="s2">, </span><span class="s1">a=self.B)</span>
        <span class="s2">else</span><span class="s1">:</span>
            <span class="s1">self.H = self._syr(</span><span class="s5">1</span><span class="s1">/denominator</span><span class="s2">, </span><span class="s1">z_minus_Mw</span><span class="s2">, </span><span class="s1">a=self.H)</span>
</pre>
</body>
</html>