<html>
<head>
<title>_multivariate.py</title>
<meta http-equiv="Content-Type" content="text/html; charset=utf-8">
<style type="text/css">
.s0 { color: #808080;}
.s1 { color: #a9b7c6;}
.s2 { color: #cc7832;}
.s3 { color: #6a8759;}
.s4 { color: #6897bb;}
.s5 { color: #629755; font-style: italic;}
</style>
</head>
<body bgcolor="#2b2b2b">
<table CELLSPACING=0 CELLPADDING=5 COLS=1 WIDTH="100%" BGCOLOR="#606060" >
<tr><td><center>
<font face="Arial, Helvetica" color="#000000">
_multivariate.py</font>
</center></td></tr></table>
<pre><span class="s0">#</span>
<span class="s0"># Author: Joris Vankerschaver 2013</span>
<span class="s0">#</span>
<span class="s2">import </span><span class="s1">math</span>
<span class="s2">import </span><span class="s1">numpy </span><span class="s2">as </span><span class="s1">np</span>
<span class="s2">from </span><span class="s1">numpy </span><span class="s2">import </span><span class="s1">asarray_chkfinite</span><span class="s2">, </span><span class="s1">asarray</span>
<span class="s2">from </span><span class="s1">numpy.lib </span><span class="s2">import </span><span class="s1">NumpyVersion</span>
<span class="s2">import </span><span class="s1">scipy.linalg</span>
<span class="s2">from </span><span class="s1">scipy._lib </span><span class="s2">import </span><span class="s1">doccer</span>
<span class="s2">from </span><span class="s1">scipy.special </span><span class="s2">import </span><span class="s1">(gammaln</span><span class="s2">, </span><span class="s1">psi</span><span class="s2">, </span><span class="s1">multigammaln</span><span class="s2">, </span><span class="s1">xlogy</span><span class="s2">, </span><span class="s1">entr</span><span class="s2">, </span><span class="s1">betaln</span><span class="s2">,</span>
                           <span class="s1">ive</span><span class="s2">, </span><span class="s1">loggamma)</span>
<span class="s2">from </span><span class="s1">scipy._lib._util </span><span class="s2">import </span><span class="s1">check_random_state</span>
<span class="s2">from </span><span class="s1">scipy.linalg.blas </span><span class="s2">import </span><span class="s1">drot</span>
<span class="s2">from </span><span class="s1">scipy.linalg._misc </span><span class="s2">import </span><span class="s1">LinAlgError</span>
<span class="s2">from </span><span class="s1">scipy.linalg.lapack </span><span class="s2">import </span><span class="s1">get_lapack_funcs</span>
<span class="s2">from </span><span class="s1">._discrete_distns </span><span class="s2">import </span><span class="s1">binom</span>
<span class="s2">from </span><span class="s1">. </span><span class="s2">import </span><span class="s1">_mvn</span><span class="s2">, </span><span class="s1">_covariance</span><span class="s2">, </span><span class="s1">_rcont</span>
<span class="s2">from </span><span class="s1">._qmvnt </span><span class="s2">import </span><span class="s1">_qmvt</span>
<span class="s2">from </span><span class="s1">._morestats </span><span class="s2">import </span><span class="s1">directional_stats</span>
<span class="s2">from </span><span class="s1">scipy.optimize </span><span class="s2">import </span><span class="s1">root_scalar</span>

<span class="s1">__all__ = [</span><span class="s3">'multivariate_normal'</span><span class="s2">,</span>
           <span class="s3">'matrix_normal'</span><span class="s2">,</span>
           <span class="s3">'dirichlet'</span><span class="s2">,</span>
           <span class="s3">'dirichlet_multinomial'</span><span class="s2">,</span>
           <span class="s3">'wishart'</span><span class="s2">,</span>
           <span class="s3">'invwishart'</span><span class="s2">,</span>
           <span class="s3">'multinomial'</span><span class="s2">,</span>
           <span class="s3">'special_ortho_group'</span><span class="s2">,</span>
           <span class="s3">'ortho_group'</span><span class="s2">,</span>
           <span class="s3">'random_correlation'</span><span class="s2">,</span>
           <span class="s3">'unitary_group'</span><span class="s2">,</span>
           <span class="s3">'multivariate_t'</span><span class="s2">,</span>
           <span class="s3">'multivariate_hypergeom'</span><span class="s2">,</span>
           <span class="s3">'random_table'</span><span class="s2">,</span>
           <span class="s3">'uniform_direction'</span><span class="s2">,</span>
           <span class="s3">'vonmises_fisher'</span><span class="s1">]</span>

<span class="s1">_LOG_2PI = np.log(</span><span class="s4">2 </span><span class="s1">* np.pi)</span>
<span class="s1">_LOG_2 = np.log(</span><span class="s4">2</span><span class="s1">)</span>
<span class="s1">_LOG_PI = np.log(np.pi)</span>


<span class="s1">_doc_random_state = </span><span class="s3">&quot;&quot;&quot;</span><span class="s2">\ 
</span><span class="s3">seed : {None, int, np.random.RandomState, np.random.Generator}, optional 
    Used for drawing random variates. 
    If `seed` is `None`, the `~np.random.RandomState` singleton is used. 
    If `seed` is an int, a new ``RandomState`` instance is used, seeded 
    with seed. 
    If `seed` is already a ``RandomState`` or ``Generator`` instance, 
    then that object is used. 
    Default is `None`. 
&quot;&quot;&quot;</span>


<span class="s2">def </span><span class="s1">_squeeze_output(out):</span>
    <span class="s5">&quot;&quot;&quot; 
    Remove single-dimensional entries from array and convert to scalar, 
    if necessary. 
    &quot;&quot;&quot;</span>
    <span class="s1">out = out.squeeze()</span>
    <span class="s2">if </span><span class="s1">out.ndim == </span><span class="s4">0</span><span class="s1">:</span>
        <span class="s1">out = out[()]</span>
    <span class="s2">return </span><span class="s1">out</span>


<span class="s2">def </span><span class="s1">_eigvalsh_to_eps(spectrum</span><span class="s2">, </span><span class="s1">cond=</span><span class="s2">None, </span><span class="s1">rcond=</span><span class="s2">None</span><span class="s1">):</span>
    <span class="s5">&quot;&quot;&quot;Determine which eigenvalues are &quot;small&quot; given the spectrum. 
 
    This is for compatibility across various linear algebra functions 
    that should agree about whether or not a Hermitian matrix is numerically 
    singular and what is its numerical matrix rank. 
    This is designed to be compatible with scipy.linalg.pinvh. 
 
    Parameters 
    ---------- 
    spectrum : 1d ndarray 
        Array of eigenvalues of a Hermitian matrix. 
    cond, rcond : float, optional 
        Cutoff for small eigenvalues. 
        Singular values smaller than rcond * largest_eigenvalue are 
        considered zero. 
        If None or -1, suitable machine precision is used. 
 
    Returns 
    ------- 
    eps : float 
        Magnitude cutoff for numerical negligibility. 
 
    &quot;&quot;&quot;</span>
    <span class="s2">if </span><span class="s1">rcond </span><span class="s2">is not None</span><span class="s1">:</span>
        <span class="s1">cond = rcond</span>
    <span class="s2">if </span><span class="s1">cond </span><span class="s2">in </span><span class="s1">[</span><span class="s2">None, </span><span class="s1">-</span><span class="s4">1</span><span class="s1">]:</span>
        <span class="s1">t = spectrum.dtype.char.lower()</span>
        <span class="s1">factor = {</span><span class="s3">'f'</span><span class="s1">: </span><span class="s4">1E3</span><span class="s2">, </span><span class="s3">'d'</span><span class="s1">: </span><span class="s4">1E6</span><span class="s1">}</span>
        <span class="s1">cond = factor[t] * np.finfo(t).eps</span>
    <span class="s1">eps = cond * np.max(abs(spectrum))</span>
    <span class="s2">return </span><span class="s1">eps</span>


<span class="s2">def </span><span class="s1">_pinv_1d(v</span><span class="s2">, </span><span class="s1">eps=</span><span class="s4">1e-5</span><span class="s1">):</span>
    <span class="s5">&quot;&quot;&quot;A helper function for computing the pseudoinverse. 
 
    Parameters 
    ---------- 
    v : iterable of numbers 
        This may be thought of as a vector of eigenvalues or singular values. 
    eps : float 
        Values with magnitude no greater than eps are considered negligible. 
 
    Returns 
    ------- 
    v_pinv : 1d float ndarray 
        A vector of pseudo-inverted numbers. 
 
    &quot;&quot;&quot;</span>
    <span class="s2">return </span><span class="s1">np.array([</span><span class="s4">0 </span><span class="s2">if </span><span class="s1">abs(x) &lt;= eps </span><span class="s2">else </span><span class="s4">1</span><span class="s1">/x </span><span class="s2">for </span><span class="s1">x </span><span class="s2">in </span><span class="s1">v]</span><span class="s2">, </span><span class="s1">dtype=float)</span>


<span class="s2">class </span><span class="s1">_PSD:</span>
    <span class="s5">&quot;&quot;&quot; 
    Compute coordinated functions of a symmetric positive semidefinite matrix. 
 
    This class addresses two issues.  Firstly it allows the pseudoinverse, 
    the logarithm of the pseudo-determinant, and the rank of the matrix 
    to be computed using one call to eigh instead of three. 
    Secondly it allows these functions to be computed in a way 
    that gives mutually compatible results. 
    All of the functions are computed with a common understanding as to 
    which of the eigenvalues are to be considered negligibly small. 
    The functions are designed to coordinate with scipy.linalg.pinvh() 
    but not necessarily with np.linalg.det() or with np.linalg.matrix_rank(). 
 
    Parameters 
    ---------- 
    M : array_like 
        Symmetric positive semidefinite matrix (2-D). 
    cond, rcond : float, optional 
        Cutoff for small eigenvalues. 
        Singular values smaller than rcond * largest_eigenvalue are 
        considered zero. 
        If None or -1, suitable machine precision is used. 
    lower : bool, optional 
        Whether the pertinent array data is taken from the lower 
        or upper triangle of M. (Default: lower) 
    check_finite : bool, optional 
        Whether to check that the input matrices contain only finite 
        numbers. Disabling may give a performance gain, but may result 
        in problems (crashes, non-termination) if the inputs do contain 
        infinities or NaNs. 
    allow_singular : bool, optional 
        Whether to allow a singular matrix.  (Default: True) 
 
    Notes 
    ----- 
    The arguments are similar to those of scipy.linalg.pinvh(). 
 
    &quot;&quot;&quot;</span>

    <span class="s2">def </span><span class="s1">__init__(self</span><span class="s2">, </span><span class="s1">M</span><span class="s2">, </span><span class="s1">cond=</span><span class="s2">None, </span><span class="s1">rcond=</span><span class="s2">None, </span><span class="s1">lower=</span><span class="s2">True,</span>
                 <span class="s1">check_finite=</span><span class="s2">True, </span><span class="s1">allow_singular=</span><span class="s2">True</span><span class="s1">):</span>
        <span class="s1">self._M = np.asarray(M)</span>

        <span class="s0"># Compute the symmetric eigendecomposition.</span>
        <span class="s0"># Note that eigh takes care of array conversion, chkfinite,</span>
        <span class="s0"># and assertion that the matrix is square.</span>
        <span class="s1">s</span><span class="s2">, </span><span class="s1">u = scipy.linalg.eigh(M</span><span class="s2">, </span><span class="s1">lower=lower</span><span class="s2">, </span><span class="s1">check_finite=check_finite)</span>

        <span class="s1">eps = _eigvalsh_to_eps(s</span><span class="s2">, </span><span class="s1">cond</span><span class="s2">, </span><span class="s1">rcond)</span>
        <span class="s2">if </span><span class="s1">np.min(s) &lt; -eps:</span>
            <span class="s1">msg = </span><span class="s3">&quot;The input matrix must be symmetric positive semidefinite.&quot;</span>
            <span class="s2">raise </span><span class="s1">ValueError(msg)</span>
        <span class="s1">d = s[s &gt; eps]</span>
        <span class="s2">if </span><span class="s1">len(d) &lt; len(s) </span><span class="s2">and not </span><span class="s1">allow_singular:</span>
            <span class="s1">msg = (</span><span class="s3">&quot;When `allow_singular is False`, the input matrix must be &quot;</span>
                   <span class="s3">&quot;symmetric positive definite.&quot;</span><span class="s1">)</span>
            <span class="s2">raise </span><span class="s1">np.linalg.LinAlgError(msg)</span>
        <span class="s1">s_pinv = _pinv_1d(s</span><span class="s2">, </span><span class="s1">eps)</span>
        <span class="s1">U = np.multiply(u</span><span class="s2">, </span><span class="s1">np.sqrt(s_pinv))</span>

        <span class="s0"># Save the eigenvector basis, and tolerance for testing support</span>
        <span class="s1">self.eps = </span><span class="s4">1e3</span><span class="s1">*eps</span>
        <span class="s1">self.V = u[:</span><span class="s2">, </span><span class="s1">s &lt;= eps]</span>

        <span class="s0"># Initialize the eagerly precomputed attributes.</span>
        <span class="s1">self.rank = len(d)</span>
        <span class="s1">self.U = U</span>
        <span class="s1">self.log_pdet = np.sum(np.log(d))</span>

        <span class="s0"># Initialize attributes to be lazily computed.</span>
        <span class="s1">self._pinv = </span><span class="s2">None</span>

    <span class="s2">def </span><span class="s1">_support_mask(self</span><span class="s2">, </span><span class="s1">x):</span>
        <span class="s5">&quot;&quot;&quot; 
        Check whether x lies in the support of the distribution. 
        &quot;&quot;&quot;</span>
        <span class="s1">residual = np.linalg.norm(x @ self.V</span><span class="s2">, </span><span class="s1">axis=-</span><span class="s4">1</span><span class="s1">)</span>
        <span class="s1">in_support = residual &lt; self.eps</span>
        <span class="s2">return </span><span class="s1">in_support</span>

    <span class="s1">@property</span>
    <span class="s2">def </span><span class="s1">pinv(self):</span>
        <span class="s2">if </span><span class="s1">self._pinv </span><span class="s2">is None</span><span class="s1">:</span>
            <span class="s1">self._pinv = np.dot(self.U</span><span class="s2">, </span><span class="s1">self.U.T)</span>
        <span class="s2">return </span><span class="s1">self._pinv</span>


<span class="s2">class </span><span class="s1">multi_rv_generic:</span>
    <span class="s5">&quot;&quot;&quot; 
    Class which encapsulates common functionality between all multivariate 
    distributions. 
    &quot;&quot;&quot;</span>
    <span class="s2">def </span><span class="s1">__init__(self</span><span class="s2">, </span><span class="s1">seed=</span><span class="s2">None</span><span class="s1">):</span>
        <span class="s1">super().__init__()</span>
        <span class="s1">self._random_state = check_random_state(seed)</span>

    <span class="s1">@property</span>
    <span class="s2">def </span><span class="s1">random_state(self):</span>
        <span class="s5">&quot;&quot;&quot; Get or set the Generator object for generating random variates. 
 
        If `seed` is None (or `np.random`), the `numpy.random.RandomState` 
        singleton is used. 
        If `seed` is an int, a new ``RandomState`` instance is used, 
        seeded with `seed`. 
        If `seed` is already a ``Generator`` or ``RandomState`` instance then 
        that instance is used. 
 
        &quot;&quot;&quot;</span>
        <span class="s2">return </span><span class="s1">self._random_state</span>

    <span class="s1">@random_state.setter</span>
    <span class="s2">def </span><span class="s1">random_state(self</span><span class="s2">, </span><span class="s1">seed):</span>
        <span class="s1">self._random_state = check_random_state(seed)</span>

    <span class="s2">def </span><span class="s1">_get_random_state(self</span><span class="s2">, </span><span class="s1">random_state):</span>
        <span class="s2">if </span><span class="s1">random_state </span><span class="s2">is not None</span><span class="s1">:</span>
            <span class="s2">return </span><span class="s1">check_random_state(random_state)</span>
        <span class="s2">else</span><span class="s1">:</span>
            <span class="s2">return </span><span class="s1">self._random_state</span>


<span class="s2">class </span><span class="s1">multi_rv_frozen:</span>
    <span class="s5">&quot;&quot;&quot; 
    Class which encapsulates common functionality between all frozen 
    multivariate distributions. 
    &quot;&quot;&quot;</span>
    <span class="s1">@property</span>
    <span class="s2">def </span><span class="s1">random_state(self):</span>
        <span class="s2">return </span><span class="s1">self._dist._random_state</span>

    <span class="s1">@random_state.setter</span>
    <span class="s2">def </span><span class="s1">random_state(self</span><span class="s2">, </span><span class="s1">seed):</span>
        <span class="s1">self._dist._random_state = check_random_state(seed)</span>


<span class="s1">_mvn_doc_default_callparams = </span><span class="s3">&quot;&quot;&quot;</span><span class="s2">\ 
</span><span class="s3">mean : array_like, default: ``[0]`` 
    Mean of the distribution. 
cov : array_like or `Covariance`, default: ``[1]`` 
    Symmetric positive (semi)definite covariance matrix of the distribution. 
allow_singular : bool, default: ``False`` 
    Whether to allow a singular covariance matrix. This is ignored if `cov` is 
    a `Covariance` object. 
&quot;&quot;&quot;</span>

<span class="s1">_mvn_doc_callparams_note = </span><span class="s3">&quot;&quot;&quot;</span><span class="s2">\ 
</span><span class="s3">Setting the parameter `mean` to `None` is equivalent to having `mean` 
be the zero-vector. The parameter `cov` can be a scalar, in which case 
the covariance matrix is the identity times that value, a vector of 
diagonal entries for the covariance matrix, a two-dimensional array_like, 
or a `Covariance` object. 
&quot;&quot;&quot;</span>

<span class="s1">_mvn_doc_frozen_callparams = </span><span class="s3">&quot;&quot;</span>

<span class="s1">_mvn_doc_frozen_callparams_note = </span><span class="s3">&quot;&quot;&quot;</span><span class="s2">\ 
</span><span class="s3">See class definition for a detailed description of parameters.&quot;&quot;&quot;</span>

<span class="s1">mvn_docdict_params = {</span>
    <span class="s3">'_mvn_doc_default_callparams'</span><span class="s1">: _mvn_doc_default_callparams</span><span class="s2">,</span>
    <span class="s3">'_mvn_doc_callparams_note'</span><span class="s1">: _mvn_doc_callparams_note</span><span class="s2">,</span>
    <span class="s3">'_doc_random_state'</span><span class="s1">: _doc_random_state</span>
<span class="s1">}</span>

<span class="s1">mvn_docdict_noparams = {</span>
    <span class="s3">'_mvn_doc_default_callparams'</span><span class="s1">: _mvn_doc_frozen_callparams</span><span class="s2">,</span>
    <span class="s3">'_mvn_doc_callparams_note'</span><span class="s1">: _mvn_doc_frozen_callparams_note</span><span class="s2">,</span>
    <span class="s3">'_doc_random_state'</span><span class="s1">: _doc_random_state</span>
<span class="s1">}</span>


<span class="s2">class </span><span class="s1">multivariate_normal_gen(multi_rv_generic):</span>
    <span class="s5">r&quot;&quot;&quot;A multivariate normal random variable. 
 
    The `mean` keyword specifies the mean. The `cov` keyword specifies the 
    covariance matrix. 
 
    Methods 
    ------- 
    pdf(x, mean=None, cov=1, allow_singular=False) 
        Probability density function. 
    logpdf(x, mean=None, cov=1, allow_singular=False) 
        Log of the probability density function. 
    cdf(x, mean=None, cov=1, allow_singular=False, maxpts=1000000*dim, abseps=1e-5, releps=1e-5, lower_limit=None)  # noqa 
        Cumulative distribution function. 
    logcdf(x, mean=None, cov=1, allow_singular=False, maxpts=1000000*dim, abseps=1e-5, releps=1e-5) 
        Log of the cumulative distribution function. 
    rvs(mean=None, cov=1, size=1, random_state=None) 
        Draw random samples from a multivariate normal distribution. 
    entropy() 
        Compute the differential entropy of the multivariate normal. 
 
    Parameters 
    ---------- 
    %(_mvn_doc_default_callparams)s 
    %(_doc_random_state)s 
 
    Notes 
    ----- 
    %(_mvn_doc_callparams_note)s 
 
    The covariance matrix `cov` may be an instance of a subclass of 
    `Covariance`, e.g. `scipy.stats.CovViaPrecision`. If so, `allow_singular` 
    is ignored. 
 
    Otherwise, `cov` must be a symmetric positive semidefinite 
    matrix when `allow_singular` is True; it must be (strictly) positive 
    definite when `allow_singular` is False. 
    Symmetry is not checked; only the lower triangular portion is used. 
    The determinant and inverse of `cov` are computed 
    as the pseudo-determinant and pseudo-inverse, respectively, so 
    that `cov` does not need to have full rank. 
 
    The probability density function for `multivariate_normal` is 
 
    .. math:: 
 
        f(x) = \frac{1}{\sqrt{(2 \pi)^k \det \Sigma}} 
               \exp\left( -\frac{1}{2} (x - \mu)^T \Sigma^{-1} (x - \mu) \right), 
 
    where :math:`\mu` is the mean, :math:`\Sigma` the covariance matrix, 
    :math:`k` the rank of :math:`\Sigma`. In case of singular :math:`\Sigma`, 
    SciPy extends this definition according to [1]_. 
 
    .. versionadded:: 0.14.0 
 
    References 
    ---------- 
    .. [1] Multivariate Normal Distribution - Degenerate Case, Wikipedia, 
           https://en.wikipedia.org/wiki/Multivariate_normal_distribution#Degenerate_case 
 
    Examples 
    -------- 
    &gt;&gt;&gt; import numpy as np 
    &gt;&gt;&gt; import matplotlib.pyplot as plt 
    &gt;&gt;&gt; from scipy.stats import multivariate_normal 
 
    &gt;&gt;&gt; x = np.linspace(0, 5, 10, endpoint=False) 
    &gt;&gt;&gt; y = multivariate_normal.pdf(x, mean=2.5, cov=0.5); y 
    array([ 0.00108914,  0.01033349,  0.05946514,  0.20755375,  0.43939129, 
            0.56418958,  0.43939129,  0.20755375,  0.05946514,  0.01033349]) 
    &gt;&gt;&gt; fig1 = plt.figure() 
    &gt;&gt;&gt; ax = fig1.add_subplot(111) 
    &gt;&gt;&gt; ax.plot(x, y) 
    &gt;&gt;&gt; plt.show() 
 
    Alternatively, the object may be called (as a function) to fix the mean 
    and covariance parameters, returning a &quot;frozen&quot; multivariate normal 
    random variable: 
 
    &gt;&gt;&gt; rv = multivariate_normal(mean=None, cov=1, allow_singular=False) 
    &gt;&gt;&gt; # Frozen object with the same methods but holding the given 
    &gt;&gt;&gt; # mean and covariance fixed. 
 
    The input quantiles can be any shape of array, as long as the last 
    axis labels the components.  This allows us for instance to 
    display the frozen pdf for a non-isotropic random variable in 2D as 
    follows: 
 
    &gt;&gt;&gt; x, y = np.mgrid[-1:1:.01, -1:1:.01] 
    &gt;&gt;&gt; pos = np.dstack((x, y)) 
    &gt;&gt;&gt; rv = multivariate_normal([0.5, -0.2], [[2.0, 0.3], [0.3, 0.5]]) 
    &gt;&gt;&gt; fig2 = plt.figure() 
    &gt;&gt;&gt; ax2 = fig2.add_subplot(111) 
    &gt;&gt;&gt; ax2.contourf(x, y, rv.pdf(pos)) 
 
    &quot;&quot;&quot;</span>

    <span class="s2">def </span><span class="s1">__init__(self</span><span class="s2">, </span><span class="s1">seed=</span><span class="s2">None</span><span class="s1">):</span>
        <span class="s1">super().__init__(seed)</span>
        <span class="s1">self.__doc__ = doccer.docformat(self.__doc__</span><span class="s2">, </span><span class="s1">mvn_docdict_params)</span>

    <span class="s2">def </span><span class="s1">__call__(self</span><span class="s2">, </span><span class="s1">mean=</span><span class="s2">None, </span><span class="s1">cov=</span><span class="s4">1</span><span class="s2">, </span><span class="s1">allow_singular=</span><span class="s2">False, </span><span class="s1">seed=</span><span class="s2">None</span><span class="s1">):</span>
        <span class="s5">&quot;&quot;&quot;Create a frozen multivariate normal distribution. 
 
        See `multivariate_normal_frozen` for more information. 
        &quot;&quot;&quot;</span>
        <span class="s2">return </span><span class="s1">multivariate_normal_frozen(mean</span><span class="s2">, </span><span class="s1">cov</span><span class="s2">,</span>
                                          <span class="s1">allow_singular=allow_singular</span><span class="s2">,</span>
                                          <span class="s1">seed=seed)</span>

    <span class="s2">def </span><span class="s1">_process_parameters(self</span><span class="s2">, </span><span class="s1">mean</span><span class="s2">, </span><span class="s1">cov</span><span class="s2">, </span><span class="s1">allow_singular=</span><span class="s2">True</span><span class="s1">):</span>
        <span class="s5">&quot;&quot;&quot; 
        Infer dimensionality from mean or covariance matrix, ensure that 
        mean and covariance are full vector resp. matrix. 
        &quot;&quot;&quot;</span>
        <span class="s2">if </span><span class="s1">isinstance(cov</span><span class="s2">, </span><span class="s1">_covariance.Covariance):</span>
            <span class="s2">return </span><span class="s1">self._process_parameters_Covariance(mean</span><span class="s2">, </span><span class="s1">cov)</span>
        <span class="s2">else</span><span class="s1">:</span>
            <span class="s0"># Before `Covariance` classes were introduced,</span>
            <span class="s0"># `multivariate_normal` accepted plain arrays as `cov` and used the</span>
            <span class="s0"># following input validation. To avoid disturbing the behavior of</span>
            <span class="s0"># `multivariate_normal` when plain arrays are used, we use the</span>
            <span class="s0"># original input validation here.</span>
            <span class="s1">dim</span><span class="s2">, </span><span class="s1">mean</span><span class="s2">, </span><span class="s1">cov = self._process_parameters_psd(</span><span class="s2">None, </span><span class="s1">mean</span><span class="s2">, </span><span class="s1">cov)</span>
            <span class="s0"># After input validation, some methods then processed the arrays</span>
            <span class="s0"># with a `_PSD` object and used that to perform computation.</span>
            <span class="s0"># To avoid branching statements in each method depending on whether</span>
            <span class="s0"># `cov` is an array or `Covariance` object, we always process the</span>
            <span class="s0"># array with `_PSD`, and then use wrapper that satisfies the</span>
            <span class="s0"># `Covariance` interface, `CovViaPSD`.</span>
            <span class="s1">psd = _PSD(cov</span><span class="s2">, </span><span class="s1">allow_singular=allow_singular)</span>
            <span class="s1">cov_object = _covariance.CovViaPSD(psd)</span>
            <span class="s2">return </span><span class="s1">dim</span><span class="s2">, </span><span class="s1">mean</span><span class="s2">, </span><span class="s1">cov_object</span>

    <span class="s2">def </span><span class="s1">_process_parameters_Covariance(self</span><span class="s2">, </span><span class="s1">mean</span><span class="s2">, </span><span class="s1">cov):</span>
        <span class="s1">dim = cov.shape[-</span><span class="s4">1</span><span class="s1">]</span>
        <span class="s1">mean = np.array([</span><span class="s4">0.</span><span class="s1">]) </span><span class="s2">if </span><span class="s1">mean </span><span class="s2">is None else </span><span class="s1">mean</span>
        <span class="s1">message = (</span><span class="s3">f&quot;`cov` represents a covariance matrix in </span><span class="s2">{</span><span class="s1">dim</span><span class="s2">} </span><span class="s3">dimensions,&quot;</span>
                   <span class="s3">f&quot;and so `mean` must be broadcastable to shape </span><span class="s2">{</span><span class="s1">(dim</span><span class="s2">,</span><span class="s1">)</span><span class="s2">}</span><span class="s3">&quot;</span><span class="s1">)</span>
        <span class="s2">try</span><span class="s1">:</span>
            <span class="s1">mean = np.broadcast_to(mean</span><span class="s2">, </span><span class="s1">dim)</span>
        <span class="s2">except </span><span class="s1">ValueError </span><span class="s2">as </span><span class="s1">e:</span>
            <span class="s2">raise </span><span class="s1">ValueError(message) </span><span class="s2">from </span><span class="s1">e</span>
        <span class="s2">return </span><span class="s1">dim</span><span class="s2">, </span><span class="s1">mean</span><span class="s2">, </span><span class="s1">cov</span>

    <span class="s2">def </span><span class="s1">_process_parameters_psd(self</span><span class="s2">, </span><span class="s1">dim</span><span class="s2">, </span><span class="s1">mean</span><span class="s2">, </span><span class="s1">cov):</span>
        <span class="s0"># Try to infer dimensionality</span>
        <span class="s2">if </span><span class="s1">dim </span><span class="s2">is None</span><span class="s1">:</span>
            <span class="s2">if </span><span class="s1">mean </span><span class="s2">is None</span><span class="s1">:</span>
                <span class="s2">if </span><span class="s1">cov </span><span class="s2">is None</span><span class="s1">:</span>
                    <span class="s1">dim = </span><span class="s4">1</span>
                <span class="s2">else</span><span class="s1">:</span>
                    <span class="s1">cov = np.asarray(cov</span><span class="s2">, </span><span class="s1">dtype=float)</span>
                    <span class="s2">if </span><span class="s1">cov.ndim &lt; </span><span class="s4">2</span><span class="s1">:</span>
                        <span class="s1">dim = </span><span class="s4">1</span>
                    <span class="s2">else</span><span class="s1">:</span>
                        <span class="s1">dim = cov.shape[</span><span class="s4">0</span><span class="s1">]</span>
            <span class="s2">else</span><span class="s1">:</span>
                <span class="s1">mean = np.asarray(mean</span><span class="s2">, </span><span class="s1">dtype=float)</span>
                <span class="s1">dim = mean.size</span>
        <span class="s2">else</span><span class="s1">:</span>
            <span class="s2">if not </span><span class="s1">np.isscalar(dim):</span>
                <span class="s2">raise </span><span class="s1">ValueError(</span><span class="s3">&quot;Dimension of random variable must be &quot;</span>
                                 <span class="s3">&quot;a scalar.&quot;</span><span class="s1">)</span>

        <span class="s0"># Check input sizes and return full arrays for mean and cov if</span>
        <span class="s0"># necessary</span>
        <span class="s2">if </span><span class="s1">mean </span><span class="s2">is None</span><span class="s1">:</span>
            <span class="s1">mean = np.zeros(dim)</span>
        <span class="s1">mean = np.asarray(mean</span><span class="s2">, </span><span class="s1">dtype=float)</span>

        <span class="s2">if </span><span class="s1">cov </span><span class="s2">is None</span><span class="s1">:</span>
            <span class="s1">cov = </span><span class="s4">1.0</span>
        <span class="s1">cov = np.asarray(cov</span><span class="s2">, </span><span class="s1">dtype=float)</span>

        <span class="s2">if </span><span class="s1">dim == </span><span class="s4">1</span><span class="s1">:</span>
            <span class="s1">mean = mean.reshape(</span><span class="s4">1</span><span class="s1">)</span>
            <span class="s1">cov = cov.reshape(</span><span class="s4">1</span><span class="s2">, </span><span class="s4">1</span><span class="s1">)</span>

        <span class="s2">if </span><span class="s1">mean.ndim != </span><span class="s4">1 </span><span class="s2">or </span><span class="s1">mean.shape[</span><span class="s4">0</span><span class="s1">] != dim:</span>
            <span class="s2">raise </span><span class="s1">ValueError(</span><span class="s3">&quot;Array 'mean' must be a vector of length %d.&quot; </span><span class="s1">%</span>
                             <span class="s1">dim)</span>
        <span class="s2">if </span><span class="s1">cov.ndim == </span><span class="s4">0</span><span class="s1">:</span>
            <span class="s1">cov = cov * np.eye(dim)</span>
        <span class="s2">elif </span><span class="s1">cov.ndim == </span><span class="s4">1</span><span class="s1">:</span>
            <span class="s1">cov = np.diag(cov)</span>
        <span class="s2">elif </span><span class="s1">cov.ndim == </span><span class="s4">2 </span><span class="s2">and </span><span class="s1">cov.shape != (dim</span><span class="s2">, </span><span class="s1">dim):</span>
            <span class="s1">rows</span><span class="s2">, </span><span class="s1">cols = cov.shape</span>
            <span class="s2">if </span><span class="s1">rows != cols:</span>
                <span class="s1">msg = (</span><span class="s3">&quot;Array 'cov' must be square if it is two dimensional,&quot;</span>
                       <span class="s3">&quot; but cov.shape = %s.&quot; </span><span class="s1">% str(cov.shape))</span>
            <span class="s2">else</span><span class="s1">:</span>
                <span class="s1">msg = (</span><span class="s3">&quot;Dimension mismatch: array 'cov' is of shape %s,&quot;</span>
                       <span class="s3">&quot; but 'mean' is a vector of length %d.&quot;</span><span class="s1">)</span>
                <span class="s1">msg = msg % (str(cov.shape)</span><span class="s2">, </span><span class="s1">len(mean))</span>
            <span class="s2">raise </span><span class="s1">ValueError(msg)</span>
        <span class="s2">elif </span><span class="s1">cov.ndim &gt; </span><span class="s4">2</span><span class="s1">:</span>
            <span class="s2">raise </span><span class="s1">ValueError(</span><span class="s3">&quot;Array 'cov' must be at most two-dimensional,&quot;</span>
                             <span class="s3">&quot; but cov.ndim = %d&quot; </span><span class="s1">% cov.ndim)</span>

        <span class="s2">return </span><span class="s1">dim</span><span class="s2">, </span><span class="s1">mean</span><span class="s2">, </span><span class="s1">cov</span>

    <span class="s2">def </span><span class="s1">_process_quantiles(self</span><span class="s2">, </span><span class="s1">x</span><span class="s2">, </span><span class="s1">dim):</span>
        <span class="s5">&quot;&quot;&quot; 
        Adjust quantiles array so that last axis labels the components of 
        each data point. 
        &quot;&quot;&quot;</span>
        <span class="s1">x = np.asarray(x</span><span class="s2">, </span><span class="s1">dtype=float)</span>

        <span class="s2">if </span><span class="s1">x.ndim == </span><span class="s4">0</span><span class="s1">:</span>
            <span class="s1">x = x[np.newaxis]</span>
        <span class="s2">elif </span><span class="s1">x.ndim == </span><span class="s4">1</span><span class="s1">:</span>
            <span class="s2">if </span><span class="s1">dim == </span><span class="s4">1</span><span class="s1">:</span>
                <span class="s1">x = x[:</span><span class="s2">, </span><span class="s1">np.newaxis]</span>
            <span class="s2">else</span><span class="s1">:</span>
                <span class="s1">x = x[np.newaxis</span><span class="s2">, </span><span class="s1">:]</span>

        <span class="s2">return </span><span class="s1">x</span>

    <span class="s2">def </span><span class="s1">_logpdf(self</span><span class="s2">, </span><span class="s1">x</span><span class="s2">, </span><span class="s1">mean</span><span class="s2">, </span><span class="s1">cov_object):</span>
        <span class="s5">&quot;&quot;&quot;Log of the multivariate normal probability density function. 
 
        Parameters 
        ---------- 
        x : ndarray 
            Points at which to evaluate the log of the probability 
            density function 
        mean : ndarray 
            Mean of the distribution 
        cov_object : Covariance 
            An object representing the Covariance matrix 
 
        Notes 
        ----- 
        As this function does no argument checking, it should not be 
        called directly; use 'logpdf' instead. 
 
        &quot;&quot;&quot;</span>
        <span class="s1">log_det_cov</span><span class="s2">, </span><span class="s1">rank = cov_object.log_pdet</span><span class="s2">, </span><span class="s1">cov_object.rank</span>
        <span class="s1">dev = x - mean</span>
        <span class="s2">if </span><span class="s1">dev.ndim &gt; </span><span class="s4">1</span><span class="s1">:</span>
            <span class="s1">log_det_cov = log_det_cov[...</span><span class="s2">, </span><span class="s1">np.newaxis]</span>
            <span class="s1">rank = rank[...</span><span class="s2">, </span><span class="s1">np.newaxis]</span>
        <span class="s1">maha = np.sum(np.square(cov_object.whiten(dev))</span><span class="s2">, </span><span class="s1">axis=-</span><span class="s4">1</span><span class="s1">)</span>
        <span class="s2">return </span><span class="s1">-</span><span class="s4">0.5 </span><span class="s1">* (rank * _LOG_2PI + log_det_cov + maha)</span>

    <span class="s2">def </span><span class="s1">logpdf(self</span><span class="s2">, </span><span class="s1">x</span><span class="s2">, </span><span class="s1">mean=</span><span class="s2">None, </span><span class="s1">cov=</span><span class="s4">1</span><span class="s2">, </span><span class="s1">allow_singular=</span><span class="s2">False</span><span class="s1">):</span>
        <span class="s5">&quot;&quot;&quot;Log of the multivariate normal probability density function. 
 
        Parameters 
        ---------- 
        x : array_like 
            Quantiles, with the last axis of `x` denoting the components. 
        %(_mvn_doc_default_callparams)s 
 
        Returns 
        ------- 
        pdf : ndarray or scalar 
            Log of the probability density function evaluated at `x` 
 
        Notes 
        ----- 
        %(_mvn_doc_callparams_note)s 
 
        &quot;&quot;&quot;</span>
        <span class="s1">params = self._process_parameters(mean</span><span class="s2">, </span><span class="s1">cov</span><span class="s2">, </span><span class="s1">allow_singular)</span>
        <span class="s1">dim</span><span class="s2">, </span><span class="s1">mean</span><span class="s2">, </span><span class="s1">cov_object = params</span>
        <span class="s1">x = self._process_quantiles(x</span><span class="s2">, </span><span class="s1">dim)</span>
        <span class="s1">out = self._logpdf(x</span><span class="s2">, </span><span class="s1">mean</span><span class="s2">, </span><span class="s1">cov_object)</span>
        <span class="s2">if </span><span class="s1">np.any(cov_object.rank &lt; dim):</span>
            <span class="s1">out_of_bounds = ~cov_object._support_mask(x-mean)</span>
            <span class="s1">out[out_of_bounds] = -np.inf</span>
        <span class="s2">return </span><span class="s1">_squeeze_output(out)</span>

    <span class="s2">def </span><span class="s1">pdf(self</span><span class="s2">, </span><span class="s1">x</span><span class="s2">, </span><span class="s1">mean=</span><span class="s2">None, </span><span class="s1">cov=</span><span class="s4">1</span><span class="s2">, </span><span class="s1">allow_singular=</span><span class="s2">False</span><span class="s1">):</span>
        <span class="s5">&quot;&quot;&quot;Multivariate normal probability density function. 
 
        Parameters 
        ---------- 
        x : array_like 
            Quantiles, with the last axis of `x` denoting the components. 
        %(_mvn_doc_default_callparams)s 
 
        Returns 
        ------- 
        pdf : ndarray or scalar 
            Probability density function evaluated at `x` 
 
        Notes 
        ----- 
        %(_mvn_doc_callparams_note)s 
 
        &quot;&quot;&quot;</span>
        <span class="s1">params = self._process_parameters(mean</span><span class="s2">, </span><span class="s1">cov</span><span class="s2">, </span><span class="s1">allow_singular)</span>
        <span class="s1">dim</span><span class="s2">, </span><span class="s1">mean</span><span class="s2">, </span><span class="s1">cov_object = params</span>
        <span class="s1">x = self._process_quantiles(x</span><span class="s2">, </span><span class="s1">dim)</span>
        <span class="s1">out = np.exp(self._logpdf(x</span><span class="s2">, </span><span class="s1">mean</span><span class="s2">, </span><span class="s1">cov_object))</span>
        <span class="s2">if </span><span class="s1">np.any(cov_object.rank &lt; dim):</span>
            <span class="s1">out_of_bounds = ~cov_object._support_mask(x-mean)</span>
            <span class="s1">out[out_of_bounds] = </span><span class="s4">0.0</span>
        <span class="s2">return </span><span class="s1">_squeeze_output(out)</span>

    <span class="s2">def </span><span class="s1">_cdf(self</span><span class="s2">, </span><span class="s1">x</span><span class="s2">, </span><span class="s1">mean</span><span class="s2">, </span><span class="s1">cov</span><span class="s2">, </span><span class="s1">maxpts</span><span class="s2">, </span><span class="s1">abseps</span><span class="s2">, </span><span class="s1">releps</span><span class="s2">, </span><span class="s1">lower_limit):</span>
        <span class="s5">&quot;&quot;&quot;Multivariate normal cumulative distribution function. 
 
        Parameters 
        ---------- 
        x : ndarray 
            Points at which to evaluate the cumulative distribution function. 
        mean : ndarray 
            Mean of the distribution 
        cov : array_like 
            Covariance matrix of the distribution 
        maxpts : integer 
            The maximum number of points to use for integration 
        abseps : float 
            Absolute error tolerance 
        releps : float 
            Relative error tolerance 
        lower_limit : array_like, optional 
            Lower limit of integration of the cumulative distribution function. 
            Default is negative infinity. Must be broadcastable with `x`. 
 
        Notes 
        ----- 
        As this function does no argument checking, it should not be 
        called directly; use 'cdf' instead. 
 
 
        .. versionadded:: 1.0.0 
 
        &quot;&quot;&quot;</span>
        <span class="s1">lower = (np.full(mean.shape</span><span class="s2">, </span><span class="s1">-np.inf)</span>
                 <span class="s2">if </span><span class="s1">lower_limit </span><span class="s2">is None else </span><span class="s1">lower_limit)</span>
        <span class="s0"># In 2d, _mvn.mvnun accepts input in which `lower` bound elements</span>
        <span class="s0"># are greater than `x`. Not so in other dimensions. Fix this by</span>
        <span class="s0"># ensuring that lower bounds are indeed lower when passed, then</span>
        <span class="s0"># set signs of resulting CDF manually.</span>
        <span class="s1">b</span><span class="s2">, </span><span class="s1">a = np.broadcast_arrays(x</span><span class="s2">, </span><span class="s1">lower)</span>
        <span class="s1">i_swap = b &lt; a</span>
        <span class="s1">signs = (-</span><span class="s4">1</span><span class="s1">)**(i_swap.sum(axis=-</span><span class="s4">1</span><span class="s1">))  </span><span class="s0"># odd # of swaps -&gt; negative</span>
        <span class="s1">a</span><span class="s2">, </span><span class="s1">b = a.copy()</span><span class="s2">, </span><span class="s1">b.copy()</span>
        <span class="s1">a[i_swap]</span><span class="s2">, </span><span class="s1">b[i_swap] = b[i_swap]</span><span class="s2">, </span><span class="s1">a[i_swap]</span>
        <span class="s1">n = x.shape[-</span><span class="s4">1</span><span class="s1">]</span>
        <span class="s1">limits = np.concatenate((a</span><span class="s2">, </span><span class="s1">b)</span><span class="s2">, </span><span class="s1">axis=-</span><span class="s4">1</span><span class="s1">)</span>

        <span class="s0"># mvnun expects 1-d arguments, so process points sequentially</span>
        <span class="s2">def </span><span class="s1">func1d(limits):</span>
            <span class="s2">return </span><span class="s1">_mvn.mvnun(limits[:n]</span><span class="s2">, </span><span class="s1">limits[n:]</span><span class="s2">, </span><span class="s1">mean</span><span class="s2">, </span><span class="s1">cov</span><span class="s2">,</span>
                              <span class="s1">maxpts</span><span class="s2">, </span><span class="s1">abseps</span><span class="s2">, </span><span class="s1">releps)[</span><span class="s4">0</span><span class="s1">]</span>

        <span class="s1">out = np.apply_along_axis(func1d</span><span class="s2">, </span><span class="s1">-</span><span class="s4">1</span><span class="s2">, </span><span class="s1">limits) * signs</span>
        <span class="s2">return </span><span class="s1">_squeeze_output(out)</span>

    <span class="s2">def </span><span class="s1">logcdf(self</span><span class="s2">, </span><span class="s1">x</span><span class="s2">, </span><span class="s1">mean=</span><span class="s2">None, </span><span class="s1">cov=</span><span class="s4">1</span><span class="s2">, </span><span class="s1">allow_singular=</span><span class="s2">False, </span><span class="s1">maxpts=</span><span class="s2">None,</span>
               <span class="s1">abseps=</span><span class="s4">1e-5</span><span class="s2">, </span><span class="s1">releps=</span><span class="s4">1e-5</span><span class="s2">, </span><span class="s1">*</span><span class="s2">, </span><span class="s1">lower_limit=</span><span class="s2">None</span><span class="s1">):</span>
        <span class="s5">&quot;&quot;&quot;Log of the multivariate normal cumulative distribution function. 
 
        Parameters 
        ---------- 
        x : array_like 
            Quantiles, with the last axis of `x` denoting the components. 
        %(_mvn_doc_default_callparams)s 
        maxpts : integer, optional 
            The maximum number of points to use for integration 
            (default `1000000*dim`) 
        abseps : float, optional 
            Absolute error tolerance (default 1e-5) 
        releps : float, optional 
            Relative error tolerance (default 1e-5) 
        lower_limit : array_like, optional 
            Lower limit of integration of the cumulative distribution function. 
            Default is negative infinity. Must be broadcastable with `x`. 
 
        Returns 
        ------- 
        cdf : ndarray or scalar 
            Log of the cumulative distribution function evaluated at `x` 
 
        Notes 
        ----- 
        %(_mvn_doc_callparams_note)s 
 
        .. versionadded:: 1.0.0 
 
        &quot;&quot;&quot;</span>
        <span class="s1">params = self._process_parameters(mean</span><span class="s2">, </span><span class="s1">cov</span><span class="s2">, </span><span class="s1">allow_singular)</span>
        <span class="s1">dim</span><span class="s2">, </span><span class="s1">mean</span><span class="s2">, </span><span class="s1">cov_object = params</span>
        <span class="s1">cov = cov_object.covariance</span>
        <span class="s1">x = self._process_quantiles(x</span><span class="s2">, </span><span class="s1">dim)</span>
        <span class="s2">if not </span><span class="s1">maxpts:</span>
            <span class="s1">maxpts = </span><span class="s4">1000000 </span><span class="s1">* dim</span>
        <span class="s1">cdf = self._cdf(x</span><span class="s2">, </span><span class="s1">mean</span><span class="s2">, </span><span class="s1">cov</span><span class="s2">, </span><span class="s1">maxpts</span><span class="s2">, </span><span class="s1">abseps</span><span class="s2">, </span><span class="s1">releps</span><span class="s2">, </span><span class="s1">lower_limit)</span>
        <span class="s0"># the log of a negative real is complex, and cdf can be negative</span>
        <span class="s0"># if lower limit is greater than upper limit</span>
        <span class="s1">cdf = cdf + </span><span class="s4">0j </span><span class="s2">if </span><span class="s1">np.any(cdf &lt; </span><span class="s4">0</span><span class="s1">) </span><span class="s2">else </span><span class="s1">cdf</span>
        <span class="s1">out = np.log(cdf)</span>
        <span class="s2">return </span><span class="s1">out</span>

    <span class="s2">def </span><span class="s1">cdf(self</span><span class="s2">, </span><span class="s1">x</span><span class="s2">, </span><span class="s1">mean=</span><span class="s2">None, </span><span class="s1">cov=</span><span class="s4">1</span><span class="s2">, </span><span class="s1">allow_singular=</span><span class="s2">False, </span><span class="s1">maxpts=</span><span class="s2">None,</span>
            <span class="s1">abseps=</span><span class="s4">1e-5</span><span class="s2">, </span><span class="s1">releps=</span><span class="s4">1e-5</span><span class="s2">, </span><span class="s1">*</span><span class="s2">, </span><span class="s1">lower_limit=</span><span class="s2">None</span><span class="s1">):</span>
        <span class="s5">&quot;&quot;&quot;Multivariate normal cumulative distribution function. 
 
        Parameters 
        ---------- 
        x : array_like 
            Quantiles, with the last axis of `x` denoting the components. 
        %(_mvn_doc_default_callparams)s 
        maxpts : integer, optional 
            The maximum number of points to use for integration 
            (default `1000000*dim`) 
        abseps : float, optional 
            Absolute error tolerance (default 1e-5) 
        releps : float, optional 
            Relative error tolerance (default 1e-5) 
        lower_limit : array_like, optional 
            Lower limit of integration of the cumulative distribution function. 
            Default is negative infinity. Must be broadcastable with `x`. 
 
        Returns 
        ------- 
        cdf : ndarray or scalar 
            Cumulative distribution function evaluated at `x` 
 
        Notes 
        ----- 
        %(_mvn_doc_callparams_note)s 
 
        .. versionadded:: 1.0.0 
 
        &quot;&quot;&quot;</span>
        <span class="s1">params = self._process_parameters(mean</span><span class="s2">, </span><span class="s1">cov</span><span class="s2">, </span><span class="s1">allow_singular)</span>
        <span class="s1">dim</span><span class="s2">, </span><span class="s1">mean</span><span class="s2">, </span><span class="s1">cov_object = params</span>
        <span class="s1">cov = cov_object.covariance</span>
        <span class="s1">x = self._process_quantiles(x</span><span class="s2">, </span><span class="s1">dim)</span>
        <span class="s2">if not </span><span class="s1">maxpts:</span>
            <span class="s1">maxpts = </span><span class="s4">1000000 </span><span class="s1">* dim</span>
        <span class="s1">out = self._cdf(x</span><span class="s2">, </span><span class="s1">mean</span><span class="s2">, </span><span class="s1">cov</span><span class="s2">, </span><span class="s1">maxpts</span><span class="s2">, </span><span class="s1">abseps</span><span class="s2">, </span><span class="s1">releps</span><span class="s2">, </span><span class="s1">lower_limit)</span>
        <span class="s2">return </span><span class="s1">out</span>

    <span class="s2">def </span><span class="s1">rvs(self</span><span class="s2">, </span><span class="s1">mean=</span><span class="s2">None, </span><span class="s1">cov=</span><span class="s4">1</span><span class="s2">, </span><span class="s1">size=</span><span class="s4">1</span><span class="s2">, </span><span class="s1">random_state=</span><span class="s2">None</span><span class="s1">):</span>
        <span class="s5">&quot;&quot;&quot;Draw random samples from a multivariate normal distribution. 
 
        Parameters 
        ---------- 
        %(_mvn_doc_default_callparams)s 
        size : integer, optional 
            Number of samples to draw (default 1). 
        %(_doc_random_state)s 
 
        Returns 
        ------- 
        rvs : ndarray or scalar 
            Random variates of size (`size`, `N`), where `N` is the 
            dimension of the random variable. 
 
        Notes 
        ----- 
        %(_mvn_doc_callparams_note)s 
 
        &quot;&quot;&quot;</span>
        <span class="s1">dim</span><span class="s2">, </span><span class="s1">mean</span><span class="s2">, </span><span class="s1">cov_object = self._process_parameters(mean</span><span class="s2">, </span><span class="s1">cov)</span>
        <span class="s1">random_state = self._get_random_state(random_state)</span>

        <span class="s2">if </span><span class="s1">isinstance(cov_object</span><span class="s2">, </span><span class="s1">_covariance.CovViaPSD):</span>
            <span class="s1">cov = cov_object.covariance</span>
            <span class="s1">out = random_state.multivariate_normal(mean</span><span class="s2">, </span><span class="s1">cov</span><span class="s2">, </span><span class="s1">size)</span>
            <span class="s1">out = _squeeze_output(out)</span>
        <span class="s2">else</span><span class="s1">:</span>
            <span class="s1">size = size </span><span class="s2">or </span><span class="s1">tuple()</span>
            <span class="s2">if not </span><span class="s1">np.iterable(size):</span>
                <span class="s1">size = (size</span><span class="s2">,</span><span class="s1">)</span>
            <span class="s1">shape = tuple(size) + (cov_object.shape[-</span><span class="s4">1</span><span class="s1">]</span><span class="s2">,</span><span class="s1">)</span>
            <span class="s1">x = random_state.normal(size=shape)</span>
            <span class="s1">out = mean + cov_object.colorize(x)</span>
        <span class="s2">return </span><span class="s1">out</span>

    <span class="s2">def </span><span class="s1">entropy(self</span><span class="s2">, </span><span class="s1">mean=</span><span class="s2">None, </span><span class="s1">cov=</span><span class="s4">1</span><span class="s1">):</span>
        <span class="s5">&quot;&quot;&quot;Compute the differential entropy of the multivariate normal. 
 
        Parameters 
        ---------- 
        %(_mvn_doc_default_callparams)s 
 
        Returns 
        ------- 
        h : scalar 
            Entropy of the multivariate normal distribution 
 
        Notes 
        ----- 
        %(_mvn_doc_callparams_note)s 
 
        &quot;&quot;&quot;</span>
        <span class="s1">dim</span><span class="s2">, </span><span class="s1">mean</span><span class="s2">, </span><span class="s1">cov_object = self._process_parameters(mean</span><span class="s2">, </span><span class="s1">cov)</span>
        <span class="s2">return </span><span class="s4">0.5 </span><span class="s1">* (cov_object.rank * (_LOG_2PI + </span><span class="s4">1</span><span class="s1">) + cov_object.log_pdet)</span>


<span class="s1">multivariate_normal = multivariate_normal_gen()</span>


<span class="s2">class </span><span class="s1">multivariate_normal_frozen(multi_rv_frozen):</span>
    <span class="s2">def </span><span class="s1">__init__(self</span><span class="s2">, </span><span class="s1">mean=</span><span class="s2">None, </span><span class="s1">cov=</span><span class="s4">1</span><span class="s2">, </span><span class="s1">allow_singular=</span><span class="s2">False, </span><span class="s1">seed=</span><span class="s2">None,</span>
                 <span class="s1">maxpts=</span><span class="s2">None, </span><span class="s1">abseps=</span><span class="s4">1e-5</span><span class="s2">, </span><span class="s1">releps=</span><span class="s4">1e-5</span><span class="s1">):</span>
        <span class="s5">&quot;&quot;&quot;Create a frozen multivariate normal distribution. 
 
        Parameters 
        ---------- 
        mean : array_like, default: ``[0]`` 
            Mean of the distribution. 
        cov : array_like, default: ``[1]`` 
            Symmetric positive (semi)definite covariance matrix of the 
            distribution. 
        allow_singular : bool, default: ``False`` 
            Whether to allow a singular covariance matrix. 
        seed : {None, int, `numpy.random.Generator`, `numpy.random.RandomState`}, optional 
            If `seed` is None (or `np.random`), the `numpy.random.RandomState` 
            singleton is used. 
            If `seed` is an int, a new ``RandomState`` instance is used, 
            seeded with `seed`. 
            If `seed` is already a ``Generator`` or ``RandomState`` instance 
            then that instance is used. 
        maxpts : integer, optional 
            The maximum number of points to use for integration of the 
            cumulative distribution function (default `1000000*dim`) 
        abseps : float, optional 
            Absolute error tolerance for the cumulative distribution function 
            (default 1e-5) 
        releps : float, optional 
            Relative error tolerance for the cumulative distribution function 
            (default 1e-5) 
 
        Examples 
        -------- 
        When called with the default parameters, this will create a 1D random 
        variable with mean 0 and covariance 1: 
 
        &gt;&gt;&gt; from scipy.stats import multivariate_normal 
        &gt;&gt;&gt; r = multivariate_normal() 
        &gt;&gt;&gt; r.mean 
        array([ 0.]) 
        &gt;&gt;&gt; r.cov 
        array([[1.]]) 
 
        &quot;&quot;&quot;</span>
        <span class="s1">self._dist = multivariate_normal_gen(seed)</span>
        <span class="s1">self.dim</span><span class="s2">, </span><span class="s1">self.mean</span><span class="s2">, </span><span class="s1">self.cov_object = (</span>
            <span class="s1">self._dist._process_parameters(mean</span><span class="s2">, </span><span class="s1">cov</span><span class="s2">, </span><span class="s1">allow_singular))</span>
        <span class="s1">self.allow_singular = allow_singular </span><span class="s2">or </span><span class="s1">self.cov_object._allow_singular</span>
        <span class="s2">if not </span><span class="s1">maxpts:</span>
            <span class="s1">maxpts = </span><span class="s4">1000000 </span><span class="s1">* self.dim</span>
        <span class="s1">self.maxpts = maxpts</span>
        <span class="s1">self.abseps = abseps</span>
        <span class="s1">self.releps = releps</span>

    <span class="s1">@property</span>
    <span class="s2">def </span><span class="s1">cov(self):</span>
        <span class="s2">return </span><span class="s1">self.cov_object.covariance</span>

    <span class="s2">def </span><span class="s1">logpdf(self</span><span class="s2">, </span><span class="s1">x):</span>
        <span class="s1">x = self._dist._process_quantiles(x</span><span class="s2">, </span><span class="s1">self.dim)</span>
        <span class="s1">out = self._dist._logpdf(x</span><span class="s2">, </span><span class="s1">self.mean</span><span class="s2">, </span><span class="s1">self.cov_object)</span>
        <span class="s2">if </span><span class="s1">np.any(self.cov_object.rank &lt; self.dim):</span>
            <span class="s1">out_of_bounds = ~self.cov_object._support_mask(x-self.mean)</span>
            <span class="s1">out[out_of_bounds] = -np.inf</span>
        <span class="s2">return </span><span class="s1">_squeeze_output(out)</span>

    <span class="s2">def </span><span class="s1">pdf(self</span><span class="s2">, </span><span class="s1">x):</span>
        <span class="s2">return </span><span class="s1">np.exp(self.logpdf(x))</span>

    <span class="s2">def </span><span class="s1">logcdf(self</span><span class="s2">, </span><span class="s1">x</span><span class="s2">, </span><span class="s1">*</span><span class="s2">, </span><span class="s1">lower_limit=</span><span class="s2">None</span><span class="s1">):</span>
        <span class="s1">cdf = self.cdf(x</span><span class="s2">, </span><span class="s1">lower_limit=lower_limit)</span>
        <span class="s0"># the log of a negative real is complex, and cdf can be negative</span>
        <span class="s0"># if lower limit is greater than upper limit</span>
        <span class="s1">cdf = cdf + </span><span class="s4">0j </span><span class="s2">if </span><span class="s1">np.any(cdf &lt; </span><span class="s4">0</span><span class="s1">) </span><span class="s2">else </span><span class="s1">cdf</span>
        <span class="s1">out = np.log(cdf)</span>
        <span class="s2">return </span><span class="s1">out</span>

    <span class="s2">def </span><span class="s1">cdf(self</span><span class="s2">, </span><span class="s1">x</span><span class="s2">, </span><span class="s1">*</span><span class="s2">, </span><span class="s1">lower_limit=</span><span class="s2">None</span><span class="s1">):</span>
        <span class="s1">x = self._dist._process_quantiles(x</span><span class="s2">, </span><span class="s1">self.dim)</span>
        <span class="s1">out = self._dist._cdf(x</span><span class="s2">, </span><span class="s1">self.mean</span><span class="s2">, </span><span class="s1">self.cov_object.covariance</span><span class="s2">,</span>
                              <span class="s1">self.maxpts</span><span class="s2">, </span><span class="s1">self.abseps</span><span class="s2">, </span><span class="s1">self.releps</span><span class="s2">,</span>
                              <span class="s1">lower_limit)</span>
        <span class="s2">return </span><span class="s1">_squeeze_output(out)</span>

    <span class="s2">def </span><span class="s1">rvs(self</span><span class="s2">, </span><span class="s1">size=</span><span class="s4">1</span><span class="s2">, </span><span class="s1">random_state=</span><span class="s2">None</span><span class="s1">):</span>
        <span class="s2">return </span><span class="s1">self._dist.rvs(self.mean</span><span class="s2">, </span><span class="s1">self.cov_object</span><span class="s2">, </span><span class="s1">size</span><span class="s2">, </span><span class="s1">random_state)</span>

    <span class="s2">def </span><span class="s1">entropy(self):</span>
        <span class="s5">&quot;&quot;&quot;Computes the differential entropy of the multivariate normal. 
 
        Returns 
        ------- 
        h : scalar 
            Entropy of the multivariate normal distribution 
 
        &quot;&quot;&quot;</span>
        <span class="s1">log_pdet = self.cov_object.log_pdet</span>
        <span class="s1">rank = self.cov_object.rank</span>
        <span class="s2">return </span><span class="s4">0.5 </span><span class="s1">* (rank * (_LOG_2PI + </span><span class="s4">1</span><span class="s1">) + log_pdet)</span>


<span class="s0"># Set frozen generator docstrings from corresponding docstrings in</span>
<span class="s0"># multivariate_normal_gen and fill in default strings in class docstrings</span>
<span class="s2">for </span><span class="s1">name </span><span class="s2">in </span><span class="s1">[</span><span class="s3">'logpdf'</span><span class="s2">, </span><span class="s3">'pdf'</span><span class="s2">, </span><span class="s3">'logcdf'</span><span class="s2">, </span><span class="s3">'cdf'</span><span class="s2">, </span><span class="s3">'rvs'</span><span class="s1">]:</span>
    <span class="s1">method = multivariate_normal_gen.__dict__[name]</span>
    <span class="s1">method_frozen = multivariate_normal_frozen.__dict__[name]</span>
    <span class="s1">method_frozen.__doc__ = doccer.docformat(method.__doc__</span><span class="s2">,</span>
                                             <span class="s1">mvn_docdict_noparams)</span>
    <span class="s1">method.__doc__ = doccer.docformat(method.__doc__</span><span class="s2">, </span><span class="s1">mvn_docdict_params)</span>

<span class="s1">_matnorm_doc_default_callparams = </span><span class="s3">&quot;&quot;&quot;</span><span class="s2">\ 
</span><span class="s3">mean : array_like, optional 
    Mean of the distribution (default: `None`) 
rowcov : array_like, optional 
    Among-row covariance matrix of the distribution (default: `1`) 
colcov : array_like, optional 
    Among-column covariance matrix of the distribution (default: `1`) 
&quot;&quot;&quot;</span>

<span class="s1">_matnorm_doc_callparams_note = </span><span class="s3">&quot;&quot;&quot;</span><span class="s2">\ 
</span><span class="s3">If `mean` is set to `None` then a matrix of zeros is used for the mean. 
The dimensions of this matrix are inferred from the shape of `rowcov` and 
`colcov`, if these are provided, or set to `1` if ambiguous. 
 
`rowcov` and `colcov` can be two-dimensional array_likes specifying the 
covariance matrices directly. Alternatively, a one-dimensional array will 
be be interpreted as the entries of a diagonal matrix, and a scalar or 
zero-dimensional array will be interpreted as this value times the 
identity matrix. 
&quot;&quot;&quot;</span>

<span class="s1">_matnorm_doc_frozen_callparams = </span><span class="s3">&quot;&quot;</span>

<span class="s1">_matnorm_doc_frozen_callparams_note = </span><span class="s3">&quot;&quot;&quot;</span><span class="s2">\ 
</span><span class="s3">See class definition for a detailed description of parameters.&quot;&quot;&quot;</span>

<span class="s1">matnorm_docdict_params = {</span>
    <span class="s3">'_matnorm_doc_default_callparams'</span><span class="s1">: _matnorm_doc_default_callparams</span><span class="s2">,</span>
    <span class="s3">'_matnorm_doc_callparams_note'</span><span class="s1">: _matnorm_doc_callparams_note</span><span class="s2">,</span>
    <span class="s3">'_doc_random_state'</span><span class="s1">: _doc_random_state</span>
<span class="s1">}</span>

<span class="s1">matnorm_docdict_noparams = {</span>
    <span class="s3">'_matnorm_doc_default_callparams'</span><span class="s1">: _matnorm_doc_frozen_callparams</span><span class="s2">,</span>
    <span class="s3">'_matnorm_doc_callparams_note'</span><span class="s1">: _matnorm_doc_frozen_callparams_note</span><span class="s2">,</span>
    <span class="s3">'_doc_random_state'</span><span class="s1">: _doc_random_state</span>
<span class="s1">}</span>


<span class="s2">class </span><span class="s1">matrix_normal_gen(multi_rv_generic):</span>
    <span class="s5">r&quot;&quot;&quot;A matrix normal random variable. 
 
    The `mean` keyword specifies the mean. The `rowcov` keyword specifies the 
    among-row covariance matrix. The 'colcov' keyword specifies the 
    among-column covariance matrix. 
 
    Methods 
    ------- 
    pdf(X, mean=None, rowcov=1, colcov=1) 
        Probability density function. 
    logpdf(X, mean=None, rowcov=1, colcov=1) 
        Log of the probability density function. 
    rvs(mean=None, rowcov=1, colcov=1, size=1, random_state=None) 
        Draw random samples. 
    entropy(rowcol=1, colcov=1) 
        Differential entropy. 
 
    Parameters 
    ---------- 
    %(_matnorm_doc_default_callparams)s 
    %(_doc_random_state)s 
 
    Notes 
    ----- 
    %(_matnorm_doc_callparams_note)s 
 
    The covariance matrices specified by `rowcov` and `colcov` must be 
    (symmetric) positive definite. If the samples in `X` are 
    :math:`m \times n`, then `rowcov` must be :math:`m \times m` and 
    `colcov` must be :math:`n \times n`. `mean` must be the same shape as `X`. 
 
    The probability density function for `matrix_normal` is 
 
    .. math:: 
 
        f(X) = (2 \pi)^{-\frac{mn}{2}}|U|^{-\frac{n}{2}} |V|^{-\frac{m}{2}} 
               \exp\left( -\frac{1}{2} \mathrm{Tr}\left[ U^{-1} (X-M) V^{-1} 
               (X-M)^T \right] \right), 
 
    where :math:`M` is the mean, :math:`U` the among-row covariance matrix, 
    :math:`V` the among-column covariance matrix. 
 
    The `allow_singular` behaviour of the `multivariate_normal` 
    distribution is not currently supported. Covariance matrices must be 
    full rank. 
 
    The `matrix_normal` distribution is closely related to the 
    `multivariate_normal` distribution. Specifically, :math:`\mathrm{Vec}(X)` 
    (the vector formed by concatenating the columns  of :math:`X`) has a 
    multivariate normal distribution with mean :math:`\mathrm{Vec}(M)` 
    and covariance :math:`V \otimes U` (where :math:`\otimes` is the Kronecker 
    product). Sampling and pdf evaluation are 
    :math:`\mathcal{O}(m^3 + n^3 + m^2 n + m n^2)` for the matrix normal, but 
    :math:`\mathcal{O}(m^3 n^3)` for the equivalent multivariate normal, 
    making this equivalent form algorithmically inefficient. 
 
    .. versionadded:: 0.17.0 
 
    Examples 
    -------- 
 
    &gt;&gt;&gt; import numpy as np 
    &gt;&gt;&gt; from scipy.stats import matrix_normal 
 
    &gt;&gt;&gt; M = np.arange(6).reshape(3,2); M 
    array([[0, 1], 
           [2, 3], 
           [4, 5]]) 
    &gt;&gt;&gt; U = np.diag([1,2,3]); U 
    array([[1, 0, 0], 
           [0, 2, 0], 
           [0, 0, 3]]) 
    &gt;&gt;&gt; V = 0.3*np.identity(2); V 
    array([[ 0.3,  0. ], 
           [ 0. ,  0.3]]) 
    &gt;&gt;&gt; X = M + 0.1; X 
    array([[ 0.1,  1.1], 
           [ 2.1,  3.1], 
           [ 4.1,  5.1]]) 
    &gt;&gt;&gt; matrix_normal.pdf(X, mean=M, rowcov=U, colcov=V) 
    0.023410202050005054 
 
    &gt;&gt;&gt; # Equivalent multivariate normal 
    &gt;&gt;&gt; from scipy.stats import multivariate_normal 
    &gt;&gt;&gt; vectorised_X = X.T.flatten() 
    &gt;&gt;&gt; equiv_mean = M.T.flatten() 
    &gt;&gt;&gt; equiv_cov = np.kron(V,U) 
    &gt;&gt;&gt; multivariate_normal.pdf(vectorised_X, mean=equiv_mean, cov=equiv_cov) 
    0.023410202050005054 
 
    Alternatively, the object may be called (as a function) to fix the mean 
    and covariance parameters, returning a &quot;frozen&quot; matrix normal 
    random variable: 
 
    &gt;&gt;&gt; rv = matrix_normal(mean=None, rowcov=1, colcov=1) 
    &gt;&gt;&gt; # Frozen object with the same methods but holding the given 
    &gt;&gt;&gt; # mean and covariance fixed. 
 
    &quot;&quot;&quot;</span>

    <span class="s2">def </span><span class="s1">__init__(self</span><span class="s2">, </span><span class="s1">seed=</span><span class="s2">None</span><span class="s1">):</span>
        <span class="s1">super().__init__(seed)</span>
        <span class="s1">self.__doc__ = doccer.docformat(self.__doc__</span><span class="s2">, </span><span class="s1">matnorm_docdict_params)</span>

    <span class="s2">def </span><span class="s1">__call__(self</span><span class="s2">, </span><span class="s1">mean=</span><span class="s2">None, </span><span class="s1">rowcov=</span><span class="s4">1</span><span class="s2">, </span><span class="s1">colcov=</span><span class="s4">1</span><span class="s2">, </span><span class="s1">seed=</span><span class="s2">None</span><span class="s1">):</span>
        <span class="s5">&quot;&quot;&quot;Create a frozen matrix normal distribution. 
 
        See `matrix_normal_frozen` for more information. 
 
        &quot;&quot;&quot;</span>
        <span class="s2">return </span><span class="s1">matrix_normal_frozen(mean</span><span class="s2">, </span><span class="s1">rowcov</span><span class="s2">, </span><span class="s1">colcov</span><span class="s2">, </span><span class="s1">seed=seed)</span>

    <span class="s2">def </span><span class="s1">_process_parameters(self</span><span class="s2">, </span><span class="s1">mean</span><span class="s2">, </span><span class="s1">rowcov</span><span class="s2">, </span><span class="s1">colcov):</span>
        <span class="s5">&quot;&quot;&quot; 
        Infer dimensionality from mean or covariance matrices. Handle 
        defaults. Ensure compatible dimensions. 
        &quot;&quot;&quot;</span>

        <span class="s0"># Process mean</span>
        <span class="s2">if </span><span class="s1">mean </span><span class="s2">is not None</span><span class="s1">:</span>
            <span class="s1">mean = np.asarray(mean</span><span class="s2">, </span><span class="s1">dtype=float)</span>
            <span class="s1">meanshape = mean.shape</span>
            <span class="s2">if </span><span class="s1">len(meanshape) != </span><span class="s4">2</span><span class="s1">:</span>
                <span class="s2">raise </span><span class="s1">ValueError(</span><span class="s3">&quot;Array `mean` must be two dimensional.&quot;</span><span class="s1">)</span>
            <span class="s2">if </span><span class="s1">np.any(meanshape == </span><span class="s4">0</span><span class="s1">):</span>
                <span class="s2">raise </span><span class="s1">ValueError(</span><span class="s3">&quot;Array `mean` has invalid shape.&quot;</span><span class="s1">)</span>

        <span class="s0"># Process among-row covariance</span>
        <span class="s1">rowcov = np.asarray(rowcov</span><span class="s2">, </span><span class="s1">dtype=float)</span>
        <span class="s2">if </span><span class="s1">rowcov.ndim == </span><span class="s4">0</span><span class="s1">:</span>
            <span class="s2">if </span><span class="s1">mean </span><span class="s2">is not None</span><span class="s1">:</span>
                <span class="s1">rowcov = rowcov * np.identity(meanshape[</span><span class="s4">0</span><span class="s1">])</span>
            <span class="s2">else</span><span class="s1">:</span>
                <span class="s1">rowcov = rowcov * np.identity(</span><span class="s4">1</span><span class="s1">)</span>
        <span class="s2">elif </span><span class="s1">rowcov.ndim == </span><span class="s4">1</span><span class="s1">:</span>
            <span class="s1">rowcov = np.diag(rowcov)</span>
        <span class="s1">rowshape = rowcov.shape</span>
        <span class="s2">if </span><span class="s1">len(rowshape) != </span><span class="s4">2</span><span class="s1">:</span>
            <span class="s2">raise </span><span class="s1">ValueError(</span><span class="s3">&quot;`rowcov` must be a scalar or a 2D array.&quot;</span><span class="s1">)</span>
        <span class="s2">if </span><span class="s1">rowshape[</span><span class="s4">0</span><span class="s1">] != rowshape[</span><span class="s4">1</span><span class="s1">]:</span>
            <span class="s2">raise </span><span class="s1">ValueError(</span><span class="s3">&quot;Array `rowcov` must be square.&quot;</span><span class="s1">)</span>
        <span class="s2">if </span><span class="s1">rowshape[</span><span class="s4">0</span><span class="s1">] == </span><span class="s4">0</span><span class="s1">:</span>
            <span class="s2">raise </span><span class="s1">ValueError(</span><span class="s3">&quot;Array `rowcov` has invalid shape.&quot;</span><span class="s1">)</span>
        <span class="s1">numrows = rowshape[</span><span class="s4">0</span><span class="s1">]</span>

        <span class="s0"># Process among-column covariance</span>
        <span class="s1">colcov = np.asarray(colcov</span><span class="s2">, </span><span class="s1">dtype=float)</span>
        <span class="s2">if </span><span class="s1">colcov.ndim == </span><span class="s4">0</span><span class="s1">:</span>
            <span class="s2">if </span><span class="s1">mean </span><span class="s2">is not None</span><span class="s1">:</span>
                <span class="s1">colcov = colcov * np.identity(meanshape[</span><span class="s4">1</span><span class="s1">])</span>
            <span class="s2">else</span><span class="s1">:</span>
                <span class="s1">colcov = colcov * np.identity(</span><span class="s4">1</span><span class="s1">)</span>
        <span class="s2">elif </span><span class="s1">colcov.ndim == </span><span class="s4">1</span><span class="s1">:</span>
            <span class="s1">colcov = np.diag(colcov)</span>
        <span class="s1">colshape = colcov.shape</span>
        <span class="s2">if </span><span class="s1">len(colshape) != </span><span class="s4">2</span><span class="s1">:</span>
            <span class="s2">raise </span><span class="s1">ValueError(</span><span class="s3">&quot;`colcov` must be a scalar or a 2D array.&quot;</span><span class="s1">)</span>
        <span class="s2">if </span><span class="s1">colshape[</span><span class="s4">0</span><span class="s1">] != colshape[</span><span class="s4">1</span><span class="s1">]:</span>
            <span class="s2">raise </span><span class="s1">ValueError(</span><span class="s3">&quot;Array `colcov` must be square.&quot;</span><span class="s1">)</span>
        <span class="s2">if </span><span class="s1">colshape[</span><span class="s4">0</span><span class="s1">] == </span><span class="s4">0</span><span class="s1">:</span>
            <span class="s2">raise </span><span class="s1">ValueError(</span><span class="s3">&quot;Array `colcov` has invalid shape.&quot;</span><span class="s1">)</span>
        <span class="s1">numcols = colshape[</span><span class="s4">0</span><span class="s1">]</span>

        <span class="s0"># Ensure mean and covariances compatible</span>
        <span class="s2">if </span><span class="s1">mean </span><span class="s2">is not None</span><span class="s1">:</span>
            <span class="s2">if </span><span class="s1">meanshape[</span><span class="s4">0</span><span class="s1">] != numrows:</span>
                <span class="s2">raise </span><span class="s1">ValueError(</span><span class="s3">&quot;Arrays `mean` and `rowcov` must have the &quot;</span>
                                 <span class="s3">&quot;same number of rows.&quot;</span><span class="s1">)</span>
            <span class="s2">if </span><span class="s1">meanshape[</span><span class="s4">1</span><span class="s1">] != numcols:</span>
                <span class="s2">raise </span><span class="s1">ValueError(</span><span class="s3">&quot;Arrays `mean` and `colcov` must have the &quot;</span>
                                 <span class="s3">&quot;same number of columns.&quot;</span><span class="s1">)</span>
        <span class="s2">else</span><span class="s1">:</span>
            <span class="s1">mean = np.zeros((numrows</span><span class="s2">, </span><span class="s1">numcols))</span>

        <span class="s1">dims = (numrows</span><span class="s2">, </span><span class="s1">numcols)</span>

        <span class="s2">return </span><span class="s1">dims</span><span class="s2">, </span><span class="s1">mean</span><span class="s2">, </span><span class="s1">rowcov</span><span class="s2">, </span><span class="s1">colcov</span>

    <span class="s2">def </span><span class="s1">_process_quantiles(self</span><span class="s2">, </span><span class="s1">X</span><span class="s2">, </span><span class="s1">dims):</span>
        <span class="s5">&quot;&quot;&quot; 
        Adjust quantiles array so that last two axes labels the components of 
        each data point. 
        &quot;&quot;&quot;</span>
        <span class="s1">X = np.asarray(X</span><span class="s2">, </span><span class="s1">dtype=float)</span>
        <span class="s2">if </span><span class="s1">X.ndim == </span><span class="s4">2</span><span class="s1">:</span>
            <span class="s1">X = X[np.newaxis</span><span class="s2">, </span><span class="s1">:]</span>
        <span class="s2">if </span><span class="s1">X.shape[-</span><span class="s4">2</span><span class="s1">:] != dims:</span>
            <span class="s2">raise </span><span class="s1">ValueError(</span><span class="s3">&quot;The shape of array `X` is not compatible &quot;</span>
                             <span class="s3">&quot;with the distribution parameters.&quot;</span><span class="s1">)</span>
        <span class="s2">return </span><span class="s1">X</span>

    <span class="s2">def </span><span class="s1">_logpdf(self</span><span class="s2">, </span><span class="s1">dims</span><span class="s2">, </span><span class="s1">X</span><span class="s2">, </span><span class="s1">mean</span><span class="s2">, </span><span class="s1">row_prec_rt</span><span class="s2">, </span><span class="s1">log_det_rowcov</span><span class="s2">,</span>
                <span class="s1">col_prec_rt</span><span class="s2">, </span><span class="s1">log_det_colcov):</span>
        <span class="s5">&quot;&quot;&quot;Log of the matrix normal probability density function. 
 
        Parameters 
        ---------- 
        dims : tuple 
            Dimensions of the matrix variates 
        X : ndarray 
            Points at which to evaluate the log of the probability 
            density function 
        mean : ndarray 
            Mean of the distribution 
        row_prec_rt : ndarray 
            A decomposition such that np.dot(row_prec_rt, row_prec_rt.T) 
            is the inverse of the among-row covariance matrix 
        log_det_rowcov : float 
            Logarithm of the determinant of the among-row covariance matrix 
        col_prec_rt : ndarray 
            A decomposition such that np.dot(col_prec_rt, col_prec_rt.T) 
            is the inverse of the among-column covariance matrix 
        log_det_colcov : float 
            Logarithm of the determinant of the among-column covariance matrix 
 
        Notes 
        ----- 
        As this function does no argument checking, it should not be 
        called directly; use 'logpdf' instead. 
 
        &quot;&quot;&quot;</span>
        <span class="s1">numrows</span><span class="s2">, </span><span class="s1">numcols = dims</span>
        <span class="s1">roll_dev = np.moveaxis(X-mean</span><span class="s2">, </span><span class="s1">-</span><span class="s4">1</span><span class="s2">, </span><span class="s4">0</span><span class="s1">)</span>
        <span class="s1">scale_dev = np.tensordot(col_prec_rt.T</span><span class="s2">,</span>
                                 <span class="s1">np.dot(roll_dev</span><span class="s2">, </span><span class="s1">row_prec_rt)</span><span class="s2">, </span><span class="s4">1</span><span class="s1">)</span>
        <span class="s1">maha = np.sum(np.sum(np.square(scale_dev)</span><span class="s2">, </span><span class="s1">axis=-</span><span class="s4">1</span><span class="s1">)</span><span class="s2">, </span><span class="s1">axis=</span><span class="s4">0</span><span class="s1">)</span>
        <span class="s2">return </span><span class="s1">-</span><span class="s4">0.5 </span><span class="s1">* (numrows*numcols*_LOG_2PI + numcols*log_det_rowcov</span>
                       <span class="s1">+ numrows*log_det_colcov + maha)</span>

    <span class="s2">def </span><span class="s1">logpdf(self</span><span class="s2">, </span><span class="s1">X</span><span class="s2">, </span><span class="s1">mean=</span><span class="s2">None, </span><span class="s1">rowcov=</span><span class="s4">1</span><span class="s2">, </span><span class="s1">colcov=</span><span class="s4">1</span><span class="s1">):</span>
        <span class="s5">&quot;&quot;&quot;Log of the matrix normal probability density function. 
 
        Parameters 
        ---------- 
        X : array_like 
            Quantiles, with the last two axes of `X` denoting the components. 
        %(_matnorm_doc_default_callparams)s 
 
        Returns 
        ------- 
        logpdf : ndarray 
            Log of the probability density function evaluated at `X` 
 
        Notes 
        ----- 
        %(_matnorm_doc_callparams_note)s 
 
        &quot;&quot;&quot;</span>
        <span class="s1">dims</span><span class="s2">, </span><span class="s1">mean</span><span class="s2">, </span><span class="s1">rowcov</span><span class="s2">, </span><span class="s1">colcov = self._process_parameters(mean</span><span class="s2">, </span><span class="s1">rowcov</span><span class="s2">,</span>
                                                              <span class="s1">colcov)</span>
        <span class="s1">X = self._process_quantiles(X</span><span class="s2">, </span><span class="s1">dims)</span>
        <span class="s1">rowpsd = _PSD(rowcov</span><span class="s2">, </span><span class="s1">allow_singular=</span><span class="s2">False</span><span class="s1">)</span>
        <span class="s1">colpsd = _PSD(colcov</span><span class="s2">, </span><span class="s1">allow_singular=</span><span class="s2">False</span><span class="s1">)</span>
        <span class="s1">out = self._logpdf(dims</span><span class="s2">, </span><span class="s1">X</span><span class="s2">, </span><span class="s1">mean</span><span class="s2">, </span><span class="s1">rowpsd.U</span><span class="s2">, </span><span class="s1">rowpsd.log_pdet</span><span class="s2">, </span><span class="s1">colpsd.U</span><span class="s2">,</span>
                           <span class="s1">colpsd.log_pdet)</span>
        <span class="s2">return </span><span class="s1">_squeeze_output(out)</span>

    <span class="s2">def </span><span class="s1">pdf(self</span><span class="s2">, </span><span class="s1">X</span><span class="s2">, </span><span class="s1">mean=</span><span class="s2">None, </span><span class="s1">rowcov=</span><span class="s4">1</span><span class="s2">, </span><span class="s1">colcov=</span><span class="s4">1</span><span class="s1">):</span>
        <span class="s5">&quot;&quot;&quot;Matrix normal probability density function. 
 
        Parameters 
        ---------- 
        X : array_like 
            Quantiles, with the last two axes of `X` denoting the components. 
        %(_matnorm_doc_default_callparams)s 
 
        Returns 
        ------- 
        pdf : ndarray 
            Probability density function evaluated at `X` 
 
        Notes 
        ----- 
        %(_matnorm_doc_callparams_note)s 
 
        &quot;&quot;&quot;</span>
        <span class="s2">return </span><span class="s1">np.exp(self.logpdf(X</span><span class="s2">, </span><span class="s1">mean</span><span class="s2">, </span><span class="s1">rowcov</span><span class="s2">, </span><span class="s1">colcov))</span>

    <span class="s2">def </span><span class="s1">rvs(self</span><span class="s2">, </span><span class="s1">mean=</span><span class="s2">None, </span><span class="s1">rowcov=</span><span class="s4">1</span><span class="s2">, </span><span class="s1">colcov=</span><span class="s4">1</span><span class="s2">, </span><span class="s1">size=</span><span class="s4">1</span><span class="s2">, </span><span class="s1">random_state=</span><span class="s2">None</span><span class="s1">):</span>
        <span class="s5">&quot;&quot;&quot;Draw random samples from a matrix normal distribution. 
 
        Parameters 
        ---------- 
        %(_matnorm_doc_default_callparams)s 
        size : integer, optional 
            Number of samples to draw (default 1). 
        %(_doc_random_state)s 
 
        Returns 
        ------- 
        rvs : ndarray or scalar 
            Random variates of size (`size`, `dims`), where `dims` is the 
            dimension of the random matrices. 
 
        Notes 
        ----- 
        %(_matnorm_doc_callparams_note)s 
 
        &quot;&quot;&quot;</span>
        <span class="s1">size = int(size)</span>
        <span class="s1">dims</span><span class="s2">, </span><span class="s1">mean</span><span class="s2">, </span><span class="s1">rowcov</span><span class="s2">, </span><span class="s1">colcov = self._process_parameters(mean</span><span class="s2">, </span><span class="s1">rowcov</span><span class="s2">,</span>
                                                              <span class="s1">colcov)</span>
        <span class="s1">rowchol = scipy.linalg.cholesky(rowcov</span><span class="s2">, </span><span class="s1">lower=</span><span class="s2">True</span><span class="s1">)</span>
        <span class="s1">colchol = scipy.linalg.cholesky(colcov</span><span class="s2">, </span><span class="s1">lower=</span><span class="s2">True</span><span class="s1">)</span>
        <span class="s1">random_state = self._get_random_state(random_state)</span>
        <span class="s0"># We aren't generating standard normal variates with size=(size,</span>
        <span class="s0"># dims[0], dims[1]) directly to ensure random variates remain backwards</span>
        <span class="s0"># compatible. See https://github.com/scipy/scipy/pull/12312 for more</span>
        <span class="s0"># details.</span>
        <span class="s1">std_norm = random_state.standard_normal(</span>
            <span class="s1">size=(dims[</span><span class="s4">1</span><span class="s1">]</span><span class="s2">, </span><span class="s1">size</span><span class="s2">, </span><span class="s1">dims[</span><span class="s4">0</span><span class="s1">])</span>
        <span class="s1">).transpose(</span><span class="s4">1</span><span class="s2">, </span><span class="s4">2</span><span class="s2">, </span><span class="s4">0</span><span class="s1">)</span>
        <span class="s1">out = mean + np.einsum(</span><span class="s3">'jp,ipq,kq-&gt;ijk'</span><span class="s2">,</span>
                               <span class="s1">rowchol</span><span class="s2">, </span><span class="s1">std_norm</span><span class="s2">, </span><span class="s1">colchol</span><span class="s2">,</span>
                               <span class="s1">optimize=</span><span class="s2">True</span><span class="s1">)</span>
        <span class="s2">if </span><span class="s1">size == </span><span class="s4">1</span><span class="s1">:</span>
            <span class="s1">out = out.reshape(mean.shape)</span>
        <span class="s2">return </span><span class="s1">out</span>

    <span class="s2">def </span><span class="s1">entropy(self</span><span class="s2">, </span><span class="s1">rowcov=</span><span class="s4">1</span><span class="s2">, </span><span class="s1">colcov=</span><span class="s4">1</span><span class="s1">):</span>
        <span class="s5">&quot;&quot;&quot;Log of the matrix normal probability density function. 
 
        Parameters 
        ---------- 
        rowcov : array_like, optional 
            Among-row covariance matrix of the distribution (default: `1`) 
        colcov : array_like, optional 
            Among-column covariance matrix of the distribution (default: `1`) 
 
        Returns 
        ------- 
        entropy : float 
            Entropy of the distribution 
 
        Notes 
        ----- 
        %(_matnorm_doc_callparams_note)s 
 
        &quot;&quot;&quot;</span>
        <span class="s1">dummy_mean = np.zeros((rowcov.shape[</span><span class="s4">0</span><span class="s1">]</span><span class="s2">, </span><span class="s1">colcov.shape[</span><span class="s4">0</span><span class="s1">]))</span>
        <span class="s1">dims</span><span class="s2">, </span><span class="s1">_</span><span class="s2">, </span><span class="s1">rowcov</span><span class="s2">, </span><span class="s1">colcov = self._process_parameters(dummy_mean</span><span class="s2">,</span>
                                                           <span class="s1">rowcov</span><span class="s2">,</span>
                                                           <span class="s1">colcov)</span>
        <span class="s1">rowpsd = _PSD(rowcov</span><span class="s2">, </span><span class="s1">allow_singular=</span><span class="s2">False</span><span class="s1">)</span>
        <span class="s1">colpsd = _PSD(colcov</span><span class="s2">, </span><span class="s1">allow_singular=</span><span class="s2">False</span><span class="s1">)</span>

        <span class="s2">return </span><span class="s1">self._entropy(dims</span><span class="s2">, </span><span class="s1">rowpsd.log_pdet</span><span class="s2">, </span><span class="s1">colpsd.log_pdet)</span>

    <span class="s2">def </span><span class="s1">_entropy(self</span><span class="s2">, </span><span class="s1">dims</span><span class="s2">, </span><span class="s1">row_cov_logdet</span><span class="s2">, </span><span class="s1">col_cov_logdet):</span>
        <span class="s1">n</span><span class="s2">, </span><span class="s1">p = dims</span>
        <span class="s2">return </span><span class="s1">(</span><span class="s4">0.5 </span><span class="s1">* n * p * (</span><span class="s4">1 </span><span class="s1">+ _LOG_2PI) + </span><span class="s4">0.5 </span><span class="s1">* p * row_cov_logdet +</span>
                <span class="s4">0.5 </span><span class="s1">* n * col_cov_logdet)</span>


<span class="s1">matrix_normal = matrix_normal_gen()</span>


<span class="s2">class </span><span class="s1">matrix_normal_frozen(multi_rv_frozen):</span>
    <span class="s5">&quot;&quot;&quot; 
    Create a frozen matrix normal distribution. 
 
    Parameters 
    ---------- 
    %(_matnorm_doc_default_callparams)s 
    seed : {None, int, `numpy.random.Generator`, `numpy.random.RandomState`}, optional 
        If `seed` is `None` the `~np.random.RandomState` singleton is used. 
        If `seed` is an int, a new ``RandomState`` instance is used, seeded 
        with seed. 
        If `seed` is already a ``RandomState`` or ``Generator`` instance, 
        then that object is used. 
        Default is `None`. 
 
    Examples 
    -------- 
    &gt;&gt;&gt; import numpy as np 
    &gt;&gt;&gt; from scipy.stats import matrix_normal 
 
    &gt;&gt;&gt; distn = matrix_normal(mean=np.zeros((3,3))) 
    &gt;&gt;&gt; X = distn.rvs(); X 
    array([[-0.02976962,  0.93339138, -0.09663178], 
           [ 0.67405524,  0.28250467, -0.93308929], 
           [-0.31144782,  0.74535536,  1.30412916]]) 
    &gt;&gt;&gt; distn.pdf(X) 
    2.5160642368346784e-05 
    &gt;&gt;&gt; distn.logpdf(X) 
    -10.590229595124615 
    &quot;&quot;&quot;</span>

    <span class="s2">def </span><span class="s1">__init__(self</span><span class="s2">, </span><span class="s1">mean=</span><span class="s2">None, </span><span class="s1">rowcov=</span><span class="s4">1</span><span class="s2">, </span><span class="s1">colcov=</span><span class="s4">1</span><span class="s2">, </span><span class="s1">seed=</span><span class="s2">None</span><span class="s1">):</span>
        <span class="s1">self._dist = matrix_normal_gen(seed)</span>
        <span class="s1">self.dims</span><span class="s2">, </span><span class="s1">self.mean</span><span class="s2">, </span><span class="s1">self.rowcov</span><span class="s2">, </span><span class="s1">self.colcov = \</span>
            <span class="s1">self._dist._process_parameters(mean</span><span class="s2">, </span><span class="s1">rowcov</span><span class="s2">, </span><span class="s1">colcov)</span>
        <span class="s1">self.rowpsd = _PSD(self.rowcov</span><span class="s2">, </span><span class="s1">allow_singular=</span><span class="s2">False</span><span class="s1">)</span>
        <span class="s1">self.colpsd = _PSD(self.colcov</span><span class="s2">, </span><span class="s1">allow_singular=</span><span class="s2">False</span><span class="s1">)</span>

    <span class="s2">def </span><span class="s1">logpdf(self</span><span class="s2">, </span><span class="s1">X):</span>
        <span class="s1">X = self._dist._process_quantiles(X</span><span class="s2">, </span><span class="s1">self.dims)</span>
        <span class="s1">out = self._dist._logpdf(self.dims</span><span class="s2">, </span><span class="s1">X</span><span class="s2">, </span><span class="s1">self.mean</span><span class="s2">, </span><span class="s1">self.rowpsd.U</span><span class="s2">,</span>
                                 <span class="s1">self.rowpsd.log_pdet</span><span class="s2">, </span><span class="s1">self.colpsd.U</span><span class="s2">,</span>
                                 <span class="s1">self.colpsd.log_pdet)</span>
        <span class="s2">return </span><span class="s1">_squeeze_output(out)</span>

    <span class="s2">def </span><span class="s1">pdf(self</span><span class="s2">, </span><span class="s1">X):</span>
        <span class="s2">return </span><span class="s1">np.exp(self.logpdf(X))</span>

    <span class="s2">def </span><span class="s1">rvs(self</span><span class="s2">, </span><span class="s1">size=</span><span class="s4">1</span><span class="s2">, </span><span class="s1">random_state=</span><span class="s2">None</span><span class="s1">):</span>
        <span class="s2">return </span><span class="s1">self._dist.rvs(self.mean</span><span class="s2">, </span><span class="s1">self.rowcov</span><span class="s2">, </span><span class="s1">self.colcov</span><span class="s2">, </span><span class="s1">size</span><span class="s2">,</span>
                              <span class="s1">random_state)</span>

    <span class="s2">def </span><span class="s1">entropy(self):</span>
        <span class="s2">return </span><span class="s1">self._dist._entropy(self.dims</span><span class="s2">, </span><span class="s1">self.rowpsd.log_pdet</span><span class="s2">,</span>
                                   <span class="s1">self.colpsd.log_pdet)</span>


<span class="s0"># Set frozen generator docstrings from corresponding docstrings in</span>
<span class="s0"># matrix_normal_gen and fill in default strings in class docstrings</span>
<span class="s2">for </span><span class="s1">name </span><span class="s2">in </span><span class="s1">[</span><span class="s3">'logpdf'</span><span class="s2">, </span><span class="s3">'pdf'</span><span class="s2">, </span><span class="s3">'rvs'</span><span class="s2">, </span><span class="s3">'entropy'</span><span class="s1">]:</span>
    <span class="s1">method = matrix_normal_gen.__dict__[name]</span>
    <span class="s1">method_frozen = matrix_normal_frozen.__dict__[name]</span>
    <span class="s1">method_frozen.__doc__ = doccer.docformat(method.__doc__</span><span class="s2">,</span>
                                             <span class="s1">matnorm_docdict_noparams)</span>
    <span class="s1">method.__doc__ = doccer.docformat(method.__doc__</span><span class="s2">, </span><span class="s1">matnorm_docdict_params)</span>

<span class="s1">_dirichlet_doc_default_callparams = </span><span class="s3">&quot;&quot;&quot;</span><span class="s2">\ 
</span><span class="s3">alpha : array_like 
    The concentration parameters. The number of entries determines the 
    dimensionality of the distribution. 
&quot;&quot;&quot;</span>
<span class="s1">_dirichlet_doc_frozen_callparams = </span><span class="s3">&quot;&quot;</span>

<span class="s1">_dirichlet_doc_frozen_callparams_note = </span><span class="s3">&quot;&quot;&quot;</span><span class="s2">\ 
</span><span class="s3">See class definition for a detailed description of parameters.&quot;&quot;&quot;</span>

<span class="s1">dirichlet_docdict_params = {</span>
    <span class="s3">'_dirichlet_doc_default_callparams'</span><span class="s1">: _dirichlet_doc_default_callparams</span><span class="s2">,</span>
    <span class="s3">'_doc_random_state'</span><span class="s1">: _doc_random_state</span>
<span class="s1">}</span>

<span class="s1">dirichlet_docdict_noparams = {</span>
    <span class="s3">'_dirichlet_doc_default_callparams'</span><span class="s1">: _dirichlet_doc_frozen_callparams</span><span class="s2">,</span>
    <span class="s3">'_doc_random_state'</span><span class="s1">: _doc_random_state</span>
<span class="s1">}</span>


<span class="s2">def </span><span class="s1">_dirichlet_check_parameters(alpha):</span>
    <span class="s1">alpha = np.asarray(alpha)</span>
    <span class="s2">if </span><span class="s1">np.min(alpha) &lt;= </span><span class="s4">0</span><span class="s1">:</span>
        <span class="s2">raise </span><span class="s1">ValueError(</span><span class="s3">&quot;All parameters must be greater than 0&quot;</span><span class="s1">)</span>
    <span class="s2">elif </span><span class="s1">alpha.ndim != </span><span class="s4">1</span><span class="s1">:</span>
        <span class="s2">raise </span><span class="s1">ValueError(</span><span class="s3">&quot;Parameter vector 'a' must be one dimensional, &quot;</span>
                         <span class="s3">&quot;but a.shape = {}.&quot;</span><span class="s1">.format(alpha.shape))</span>
    <span class="s2">return </span><span class="s1">alpha</span>


<span class="s2">def </span><span class="s1">_dirichlet_check_input(alpha</span><span class="s2">, </span><span class="s1">x):</span>
    <span class="s1">x = np.asarray(x)</span>

    <span class="s2">if </span><span class="s1">x.shape[</span><span class="s4">0</span><span class="s1">] + </span><span class="s4">1 </span><span class="s1">!= alpha.shape[</span><span class="s4">0</span><span class="s1">] </span><span class="s2">and </span><span class="s1">x.shape[</span><span class="s4">0</span><span class="s1">] != alpha.shape[</span><span class="s4">0</span><span class="s1">]:</span>
        <span class="s2">raise </span><span class="s1">ValueError(</span><span class="s3">&quot;Vector 'x' must have either the same number &quot;</span>
                         <span class="s3">&quot;of entries as, or one entry fewer than, &quot;</span>
                         <span class="s3">&quot;parameter vector 'a', but alpha.shape = {} &quot;</span>
                         <span class="s3">&quot;and x.shape = {}.&quot;</span><span class="s1">.format(alpha.shape</span><span class="s2">, </span><span class="s1">x.shape))</span>

    <span class="s2">if </span><span class="s1">x.shape[</span><span class="s4">0</span><span class="s1">] != alpha.shape[</span><span class="s4">0</span><span class="s1">]:</span>
        <span class="s1">xk = np.array([</span><span class="s4">1 </span><span class="s1">- np.sum(x</span><span class="s2">, </span><span class="s4">0</span><span class="s1">)])</span>
        <span class="s2">if </span><span class="s1">xk.ndim == </span><span class="s4">1</span><span class="s1">:</span>
            <span class="s1">x = np.append(x</span><span class="s2">, </span><span class="s1">xk)</span>
        <span class="s2">elif </span><span class="s1">xk.ndim == </span><span class="s4">2</span><span class="s1">:</span>
            <span class="s1">x = np.vstack((x</span><span class="s2">, </span><span class="s1">xk))</span>
        <span class="s2">else</span><span class="s1">:</span>
            <span class="s2">raise </span><span class="s1">ValueError(</span><span class="s3">&quot;The input must be one dimensional or a two &quot;</span>
                             <span class="s3">&quot;dimensional matrix containing the entries.&quot;</span><span class="s1">)</span>

    <span class="s2">if </span><span class="s1">np.min(x) &lt; </span><span class="s4">0</span><span class="s1">:</span>
        <span class="s2">raise </span><span class="s1">ValueError(</span><span class="s3">&quot;Each entry in 'x' must be greater than or equal &quot;</span>
                         <span class="s3">&quot;to zero.&quot;</span><span class="s1">)</span>

    <span class="s2">if </span><span class="s1">np.max(x) &gt; </span><span class="s4">1</span><span class="s1">:</span>
        <span class="s2">raise </span><span class="s1">ValueError(</span><span class="s3">&quot;Each entry in 'x' must be smaller or equal one.&quot;</span><span class="s1">)</span>

    <span class="s0"># Check x_i &gt; 0 or alpha_i &gt; 1</span>
    <span class="s1">xeq0 = (x == </span><span class="s4">0</span><span class="s1">)</span>
    <span class="s1">alphalt1 = (alpha &lt; </span><span class="s4">1</span><span class="s1">)</span>
    <span class="s2">if </span><span class="s1">x.shape != alpha.shape:</span>
        <span class="s1">alphalt1 = np.repeat(alphalt1</span><span class="s2">, </span><span class="s1">x.shape[-</span><span class="s4">1</span><span class="s1">]</span><span class="s2">, </span><span class="s1">axis=-</span><span class="s4">1</span><span class="s1">).reshape(x.shape)</span>
    <span class="s1">chk = np.logical_and(xeq0</span><span class="s2">, </span><span class="s1">alphalt1)</span>

    <span class="s2">if </span><span class="s1">np.sum(chk):</span>
        <span class="s2">raise </span><span class="s1">ValueError(</span><span class="s3">&quot;Each entry in 'x' must be greater than zero if its &quot;</span>
                         <span class="s3">&quot;alpha is less than one.&quot;</span><span class="s1">)</span>

    <span class="s2">if </span><span class="s1">(np.abs(np.sum(x</span><span class="s2">, </span><span class="s4">0</span><span class="s1">) - </span><span class="s4">1.0</span><span class="s1">) &gt; </span><span class="s4">10e-10</span><span class="s1">).any():</span>
        <span class="s2">raise </span><span class="s1">ValueError(</span><span class="s3">&quot;The input vector 'x' must lie within the normal &quot;</span>
                         <span class="s3">&quot;simplex. but np.sum(x, 0) = %s.&quot; </span><span class="s1">% np.sum(x</span><span class="s2">, </span><span class="s4">0</span><span class="s1">))</span>

    <span class="s2">return </span><span class="s1">x</span>


<span class="s2">def </span><span class="s1">_lnB(alpha):</span>
    <span class="s5">r&quot;&quot;&quot;Internal helper function to compute the log of the useful quotient. 
 
    .. math:: 
 
        B(\alpha) = \frac{\prod_{i=1}{K}\Gamma(\alpha_i)} 
                         {\Gamma\left(\sum_{i=1}^{K} \alpha_i \right)} 
 
    Parameters 
    ---------- 
    %(_dirichlet_doc_default_callparams)s 
 
    Returns 
    ------- 
    B : scalar 
        Helper quotient, internal use only 
 
    &quot;&quot;&quot;</span>
    <span class="s2">return </span><span class="s1">np.sum(gammaln(alpha)) - gammaln(np.sum(alpha))</span>


<span class="s2">class </span><span class="s1">dirichlet_gen(multi_rv_generic):</span>
    <span class="s5">r&quot;&quot;&quot;A Dirichlet random variable. 
 
    The ``alpha`` keyword specifies the concentration parameters of the 
    distribution. 
 
    .. versionadded:: 0.15.0 
 
    Methods 
    ------- 
    pdf(x, alpha) 
        Probability density function. 
    logpdf(x, alpha) 
        Log of the probability density function. 
    rvs(alpha, size=1, random_state=None) 
        Draw random samples from a Dirichlet distribution. 
    mean(alpha) 
        The mean of the Dirichlet distribution 
    var(alpha) 
        The variance of the Dirichlet distribution 
    entropy(alpha) 
        Compute the differential entropy of the Dirichlet distribution. 
 
    Parameters 
    ---------- 
    %(_dirichlet_doc_default_callparams)s 
    %(_doc_random_state)s 
 
    Notes 
    ----- 
    Each :math:`\alpha` entry must be positive. The distribution has only 
    support on the simplex defined by 
 
    .. math:: 
        \sum_{i=1}^{K} x_i = 1 
 
    where :math:`0 &lt; x_i &lt; 1`. 
 
    If the quantiles don't lie within the simplex, a ValueError is raised. 
 
    The probability density function for `dirichlet` is 
 
    .. math:: 
 
        f(x) = \frac{1}{\mathrm{B}(\boldsymbol\alpha)} \prod_{i=1}^K x_i^{\alpha_i - 1} 
 
    where 
 
    .. math:: 
 
        \mathrm{B}(\boldsymbol\alpha) = \frac{\prod_{i=1}^K \Gamma(\alpha_i)} 
                                     {\Gamma\bigl(\sum_{i=1}^K \alpha_i\bigr)} 
 
    and :math:`\boldsymbol\alpha=(\alpha_1,\ldots,\alpha_K)`, the 
    concentration parameters and :math:`K` is the dimension of the space 
    where :math:`x` takes values. 
 
    Note that the `dirichlet` interface is somewhat inconsistent. 
    The array returned by the rvs function is transposed 
    with respect to the format expected by the pdf and logpdf. 
 
    Examples 
    -------- 
    &gt;&gt;&gt; import numpy as np 
    &gt;&gt;&gt; from scipy.stats import dirichlet 
 
    Generate a dirichlet random variable 
 
    &gt;&gt;&gt; quantiles = np.array([0.2, 0.2, 0.6])  # specify quantiles 
    &gt;&gt;&gt; alpha = np.array([0.4, 5, 15])  # specify concentration parameters 
    &gt;&gt;&gt; dirichlet.pdf(quantiles, alpha) 
    0.2843831684937255 
 
    The same PDF but following a log scale 
 
    &gt;&gt;&gt; dirichlet.logpdf(quantiles, alpha) 
    -1.2574327653159187 
 
    Once we specify the dirichlet distribution 
    we can then calculate quantities of interest 
 
    &gt;&gt;&gt; dirichlet.mean(alpha)  # get the mean of the distribution 
    array([0.01960784, 0.24509804, 0.73529412]) 
    &gt;&gt;&gt; dirichlet.var(alpha) # get variance 
    array([0.00089829, 0.00864603, 0.00909517]) 
    &gt;&gt;&gt; dirichlet.entropy(alpha)  # calculate the differential entropy 
    -4.3280162474082715 
 
    We can also return random samples from the distribution 
 
    &gt;&gt;&gt; dirichlet.rvs(alpha, size=1, random_state=1) 
    array([[0.00766178, 0.24670518, 0.74563305]]) 
    &gt;&gt;&gt; dirichlet.rvs(alpha, size=2, random_state=2) 
    array([[0.01639427, 0.1292273 , 0.85437844], 
           [0.00156917, 0.19033695, 0.80809388]]) 
 
    Alternatively, the object may be called (as a function) to fix 
    concentration parameters, returning a &quot;frozen&quot; Dirichlet 
    random variable: 
 
    &gt;&gt;&gt; rv = dirichlet(alpha) 
    &gt;&gt;&gt; # Frozen object with the same methods but holding the given 
    &gt;&gt;&gt; # concentration parameters fixed. 
 
    &quot;&quot;&quot;</span>

    <span class="s2">def </span><span class="s1">__init__(self</span><span class="s2">, </span><span class="s1">seed=</span><span class="s2">None</span><span class="s1">):</span>
        <span class="s1">super().__init__(seed)</span>
        <span class="s1">self.__doc__ = doccer.docformat(self.__doc__</span><span class="s2">, </span><span class="s1">dirichlet_docdict_params)</span>

    <span class="s2">def </span><span class="s1">__call__(self</span><span class="s2">, </span><span class="s1">alpha</span><span class="s2">, </span><span class="s1">seed=</span><span class="s2">None</span><span class="s1">):</span>
        <span class="s2">return </span><span class="s1">dirichlet_frozen(alpha</span><span class="s2">, </span><span class="s1">seed=seed)</span>

    <span class="s2">def </span><span class="s1">_logpdf(self</span><span class="s2">, </span><span class="s1">x</span><span class="s2">, </span><span class="s1">alpha):</span>
        <span class="s5">&quot;&quot;&quot;Log of the Dirichlet probability density function. 
 
        Parameters 
        ---------- 
        x : ndarray 
            Points at which to evaluate the log of the probability 
            density function 
        %(_dirichlet_doc_default_callparams)s 
 
        Notes 
        ----- 
        As this function does no argument checking, it should not be 
        called directly; use 'logpdf' instead. 
 
        &quot;&quot;&quot;</span>
        <span class="s1">lnB = _lnB(alpha)</span>
        <span class="s2">return </span><span class="s1">- lnB + np.sum((xlogy(alpha - </span><span class="s4">1</span><span class="s2">, </span><span class="s1">x.T)).T</span><span class="s2">, </span><span class="s4">0</span><span class="s1">)</span>

    <span class="s2">def </span><span class="s1">logpdf(self</span><span class="s2">, </span><span class="s1">x</span><span class="s2">, </span><span class="s1">alpha):</span>
        <span class="s5">&quot;&quot;&quot;Log of the Dirichlet probability density function. 
 
        Parameters 
        ---------- 
        x : array_like 
            Quantiles, with the last axis of `x` denoting the components. 
        %(_dirichlet_doc_default_callparams)s 
 
        Returns 
        ------- 
        pdf : ndarray or scalar 
            Log of the probability density function evaluated at `x`. 
 
        &quot;&quot;&quot;</span>
        <span class="s1">alpha = _dirichlet_check_parameters(alpha)</span>
        <span class="s1">x = _dirichlet_check_input(alpha</span><span class="s2">, </span><span class="s1">x)</span>

        <span class="s1">out = self._logpdf(x</span><span class="s2">, </span><span class="s1">alpha)</span>
        <span class="s2">return </span><span class="s1">_squeeze_output(out)</span>

    <span class="s2">def </span><span class="s1">pdf(self</span><span class="s2">, </span><span class="s1">x</span><span class="s2">, </span><span class="s1">alpha):</span>
        <span class="s5">&quot;&quot;&quot;The Dirichlet probability density function. 
 
        Parameters 
        ---------- 
        x : array_like 
            Quantiles, with the last axis of `x` denoting the components. 
        %(_dirichlet_doc_default_callparams)s 
 
        Returns 
        ------- 
        pdf : ndarray or scalar 
            The probability density function evaluated at `x`. 
 
        &quot;&quot;&quot;</span>
        <span class="s1">alpha = _dirichlet_check_parameters(alpha)</span>
        <span class="s1">x = _dirichlet_check_input(alpha</span><span class="s2">, </span><span class="s1">x)</span>

        <span class="s1">out = np.exp(self._logpdf(x</span><span class="s2">, </span><span class="s1">alpha))</span>
        <span class="s2">return </span><span class="s1">_squeeze_output(out)</span>

    <span class="s2">def </span><span class="s1">mean(self</span><span class="s2">, </span><span class="s1">alpha):</span>
        <span class="s5">&quot;&quot;&quot;Mean of the Dirichlet distribution. 
 
        Parameters 
        ---------- 
        %(_dirichlet_doc_default_callparams)s 
 
        Returns 
        ------- 
        mu : ndarray or scalar 
            Mean of the Dirichlet distribution. 
 
        &quot;&quot;&quot;</span>
        <span class="s1">alpha = _dirichlet_check_parameters(alpha)</span>

        <span class="s1">out = alpha / (np.sum(alpha))</span>
        <span class="s2">return </span><span class="s1">_squeeze_output(out)</span>

    <span class="s2">def </span><span class="s1">var(self</span><span class="s2">, </span><span class="s1">alpha):</span>
        <span class="s5">&quot;&quot;&quot;Variance of the Dirichlet distribution. 
 
        Parameters 
        ---------- 
        %(_dirichlet_doc_default_callparams)s 
 
        Returns 
        ------- 
        v : ndarray or scalar 
            Variance of the Dirichlet distribution. 
 
        &quot;&quot;&quot;</span>

        <span class="s1">alpha = _dirichlet_check_parameters(alpha)</span>

        <span class="s1">alpha0 = np.sum(alpha)</span>
        <span class="s1">out = (alpha * (alpha0 - alpha)) / ((alpha0 * alpha0) * (alpha0 + </span><span class="s4">1</span><span class="s1">))</span>
        <span class="s2">return </span><span class="s1">_squeeze_output(out)</span>

    <span class="s2">def </span><span class="s1">entropy(self</span><span class="s2">, </span><span class="s1">alpha):</span>
        <span class="s5">&quot;&quot;&quot; 
        Differential entropy of the Dirichlet distribution. 
 
        Parameters 
        ---------- 
        %(_dirichlet_doc_default_callparams)s 
 
        Returns 
        ------- 
        h : scalar 
            Entropy of the Dirichlet distribution 
 
        &quot;&quot;&quot;</span>

        <span class="s1">alpha = _dirichlet_check_parameters(alpha)</span>

        <span class="s1">alpha0 = np.sum(alpha)</span>
        <span class="s1">lnB = _lnB(alpha)</span>
        <span class="s1">K = alpha.shape[</span><span class="s4">0</span><span class="s1">]</span>

        <span class="s1">out = lnB + (alpha0 - K) * scipy.special.psi(alpha0) - np.sum(</span>
            <span class="s1">(alpha - </span><span class="s4">1</span><span class="s1">) * scipy.special.psi(alpha))</span>
        <span class="s2">return </span><span class="s1">_squeeze_output(out)</span>

    <span class="s2">def </span><span class="s1">rvs(self</span><span class="s2">, </span><span class="s1">alpha</span><span class="s2">, </span><span class="s1">size=</span><span class="s4">1</span><span class="s2">, </span><span class="s1">random_state=</span><span class="s2">None</span><span class="s1">):</span>
        <span class="s5">&quot;&quot;&quot; 
        Draw random samples from a Dirichlet distribution. 
 
        Parameters 
        ---------- 
        %(_dirichlet_doc_default_callparams)s 
        size : int, optional 
            Number of samples to draw (default 1). 
        %(_doc_random_state)s 
 
        Returns 
        ------- 
        rvs : ndarray or scalar 
            Random variates of size (`size`, `N`), where `N` is the 
            dimension of the random variable. 
 
        &quot;&quot;&quot;</span>
        <span class="s1">alpha = _dirichlet_check_parameters(alpha)</span>
        <span class="s1">random_state = self._get_random_state(random_state)</span>
        <span class="s2">return </span><span class="s1">random_state.dirichlet(alpha</span><span class="s2">, </span><span class="s1">size=size)</span>


<span class="s1">dirichlet = dirichlet_gen()</span>


<span class="s2">class </span><span class="s1">dirichlet_frozen(multi_rv_frozen):</span>
    <span class="s2">def </span><span class="s1">__init__(self</span><span class="s2">, </span><span class="s1">alpha</span><span class="s2">, </span><span class="s1">seed=</span><span class="s2">None</span><span class="s1">):</span>
        <span class="s1">self.alpha = _dirichlet_check_parameters(alpha)</span>
        <span class="s1">self._dist = dirichlet_gen(seed)</span>

    <span class="s2">def </span><span class="s1">logpdf(self</span><span class="s2">, </span><span class="s1">x):</span>
        <span class="s2">return </span><span class="s1">self._dist.logpdf(x</span><span class="s2">, </span><span class="s1">self.alpha)</span>

    <span class="s2">def </span><span class="s1">pdf(self</span><span class="s2">, </span><span class="s1">x):</span>
        <span class="s2">return </span><span class="s1">self._dist.pdf(x</span><span class="s2">, </span><span class="s1">self.alpha)</span>

    <span class="s2">def </span><span class="s1">mean(self):</span>
        <span class="s2">return </span><span class="s1">self._dist.mean(self.alpha)</span>

    <span class="s2">def </span><span class="s1">var(self):</span>
        <span class="s2">return </span><span class="s1">self._dist.var(self.alpha)</span>

    <span class="s2">def </span><span class="s1">entropy(self):</span>
        <span class="s2">return </span><span class="s1">self._dist.entropy(self.alpha)</span>

    <span class="s2">def </span><span class="s1">rvs(self</span><span class="s2">, </span><span class="s1">size=</span><span class="s4">1</span><span class="s2">, </span><span class="s1">random_state=</span><span class="s2">None</span><span class="s1">):</span>
        <span class="s2">return </span><span class="s1">self._dist.rvs(self.alpha</span><span class="s2">, </span><span class="s1">size</span><span class="s2">, </span><span class="s1">random_state)</span>


<span class="s0"># Set frozen generator docstrings from corresponding docstrings in</span>
<span class="s0"># multivariate_normal_gen and fill in default strings in class docstrings</span>
<span class="s2">for </span><span class="s1">name </span><span class="s2">in </span><span class="s1">[</span><span class="s3">'logpdf'</span><span class="s2">, </span><span class="s3">'pdf'</span><span class="s2">, </span><span class="s3">'rvs'</span><span class="s2">, </span><span class="s3">'mean'</span><span class="s2">, </span><span class="s3">'var'</span><span class="s2">, </span><span class="s3">'entropy'</span><span class="s1">]:</span>
    <span class="s1">method = dirichlet_gen.__dict__[name]</span>
    <span class="s1">method_frozen = dirichlet_frozen.__dict__[name]</span>
    <span class="s1">method_frozen.__doc__ = doccer.docformat(</span>
        <span class="s1">method.__doc__</span><span class="s2">, </span><span class="s1">dirichlet_docdict_noparams)</span>
    <span class="s1">method.__doc__ = doccer.docformat(method.__doc__</span><span class="s2">, </span><span class="s1">dirichlet_docdict_params)</span>


<span class="s1">_wishart_doc_default_callparams = </span><span class="s3">&quot;&quot;&quot;</span><span class="s2">\ 
</span><span class="s3">df : int 
    Degrees of freedom, must be greater than or equal to dimension of the 
    scale matrix 
scale : array_like 
    Symmetric positive definite scale matrix of the distribution 
&quot;&quot;&quot;</span>

<span class="s1">_wishart_doc_callparams_note = </span><span class="s3">&quot;&quot;</span>

<span class="s1">_wishart_doc_frozen_callparams = </span><span class="s3">&quot;&quot;</span>

<span class="s1">_wishart_doc_frozen_callparams_note = </span><span class="s3">&quot;&quot;&quot;</span><span class="s2">\ 
</span><span class="s3">See class definition for a detailed description of parameters.&quot;&quot;&quot;</span>

<span class="s1">wishart_docdict_params = {</span>
    <span class="s3">'_doc_default_callparams'</span><span class="s1">: _wishart_doc_default_callparams</span><span class="s2">,</span>
    <span class="s3">'_doc_callparams_note'</span><span class="s1">: _wishart_doc_callparams_note</span><span class="s2">,</span>
    <span class="s3">'_doc_random_state'</span><span class="s1">: _doc_random_state</span>
<span class="s1">}</span>

<span class="s1">wishart_docdict_noparams = {</span>
    <span class="s3">'_doc_default_callparams'</span><span class="s1">: _wishart_doc_frozen_callparams</span><span class="s2">,</span>
    <span class="s3">'_doc_callparams_note'</span><span class="s1">: _wishart_doc_frozen_callparams_note</span><span class="s2">,</span>
    <span class="s3">'_doc_random_state'</span><span class="s1">: _doc_random_state</span>
<span class="s1">}</span>


<span class="s2">class </span><span class="s1">wishart_gen(multi_rv_generic):</span>
    <span class="s5">r&quot;&quot;&quot;A Wishart random variable. 
 
    The `df` keyword specifies the degrees of freedom. The `scale` keyword 
    specifies the scale matrix, which must be symmetric and positive definite. 
    In this context, the scale matrix is often interpreted in terms of a 
    multivariate normal precision matrix (the inverse of the covariance 
    matrix). These arguments must satisfy the relationship 
    ``df &gt; scale.ndim - 1``, but see notes on using the `rvs` method with 
    ``df &lt; scale.ndim``. 
 
    Methods 
    ------- 
    pdf(x, df, scale) 
        Probability density function. 
    logpdf(x, df, scale) 
        Log of the probability density function. 
    rvs(df, scale, size=1, random_state=None) 
        Draw random samples from a Wishart distribution. 
    entropy() 
        Compute the differential entropy of the Wishart distribution. 
 
    Parameters 
    ---------- 
    %(_doc_default_callparams)s 
    %(_doc_random_state)s 
 
    Raises 
    ------ 
    scipy.linalg.LinAlgError 
        If the scale matrix `scale` is not positive definite. 
 
    See Also 
    -------- 
    invwishart, chi2 
 
    Notes 
    ----- 
    %(_doc_callparams_note)s 
 
    The scale matrix `scale` must be a symmetric positive definite 
    matrix. Singular matrices, including the symmetric positive semi-definite 
    case, are not supported. Symmetry is not checked; only the lower triangular 
    portion is used. 
 
    The Wishart distribution is often denoted 
 
    .. math:: 
 
        W_p(\nu, \Sigma) 
 
    where :math:`\nu` is the degrees of freedom and :math:`\Sigma` is the 
    :math:`p \times p` scale matrix. 
 
    The probability density function for `wishart` has support over positive 
    definite matrices :math:`S`; if :math:`S \sim W_p(\nu, \Sigma)`, then 
    its PDF is given by: 
 
    .. math:: 
 
        f(S) = \frac{|S|^{\frac{\nu - p - 1}{2}}}{2^{ \frac{\nu p}{2} } 
               |\Sigma|^\frac{\nu}{2} \Gamma_p \left ( \frac{\nu}{2} \right )} 
               \exp\left( -tr(\Sigma^{-1} S) / 2 \right) 
 
    If :math:`S \sim W_p(\nu, \Sigma)` (Wishart) then 
    :math:`S^{-1} \sim W_p^{-1}(\nu, \Sigma^{-1})` (inverse Wishart). 
 
    If the scale matrix is 1-dimensional and equal to one, then the Wishart 
    distribution :math:`W_1(\nu, 1)` collapses to the :math:`\chi^2(\nu)` 
    distribution. 
 
    The algorithm [2]_ implemented by the `rvs` method may 
    produce numerically singular matrices with :math:`p - 1 &lt; \nu &lt; p`; the 
    user may wish to check for this condition and generate replacement samples 
    as necessary. 
 
 
    .. versionadded:: 0.16.0 
 
    References 
    ---------- 
    .. [1] M.L. Eaton, &quot;Multivariate Statistics: A Vector Space Approach&quot;, 
           Wiley, 1983. 
    .. [2] W.B. Smith and R.R. Hocking, &quot;Algorithm AS 53: Wishart Variate 
           Generator&quot;, Applied Statistics, vol. 21, pp. 341-345, 1972. 
 
    Examples 
    -------- 
    &gt;&gt;&gt; import numpy as np 
    &gt;&gt;&gt; import matplotlib.pyplot as plt 
    &gt;&gt;&gt; from scipy.stats import wishart, chi2 
    &gt;&gt;&gt; x = np.linspace(1e-5, 8, 100) 
    &gt;&gt;&gt; w = wishart.pdf(x, df=3, scale=1); w[:5] 
    array([ 0.00126156,  0.10892176,  0.14793434,  0.17400548,  0.1929669 ]) 
    &gt;&gt;&gt; c = chi2.pdf(x, 3); c[:5] 
    array([ 0.00126156,  0.10892176,  0.14793434,  0.17400548,  0.1929669 ]) 
    &gt;&gt;&gt; plt.plot(x, w) 
    &gt;&gt;&gt; plt.show() 
 
    The input quantiles can be any shape of array, as long as the last 
    axis labels the components. 
 
    Alternatively, the object may be called (as a function) to fix the degrees 
    of freedom and scale parameters, returning a &quot;frozen&quot; Wishart random 
    variable: 
 
    &gt;&gt;&gt; rv = wishart(df=1, scale=1) 
    &gt;&gt;&gt; # Frozen object with the same methods but holding the given 
    &gt;&gt;&gt; # degrees of freedom and scale fixed. 
 
    &quot;&quot;&quot;</span>

    <span class="s2">def </span><span class="s1">__init__(self</span><span class="s2">, </span><span class="s1">seed=</span><span class="s2">None</span><span class="s1">):</span>
        <span class="s1">super().__init__(seed)</span>
        <span class="s1">self.__doc__ = doccer.docformat(self.__doc__</span><span class="s2">, </span><span class="s1">wishart_docdict_params)</span>

    <span class="s2">def </span><span class="s1">__call__(self</span><span class="s2">, </span><span class="s1">df=</span><span class="s2">None, </span><span class="s1">scale=</span><span class="s2">None, </span><span class="s1">seed=</span><span class="s2">None</span><span class="s1">):</span>
        <span class="s5">&quot;&quot;&quot;Create a frozen Wishart distribution. 
 
        See `wishart_frozen` for more information. 
        &quot;&quot;&quot;</span>
        <span class="s2">return </span><span class="s1">wishart_frozen(df</span><span class="s2">, </span><span class="s1">scale</span><span class="s2">, </span><span class="s1">seed)</span>

    <span class="s2">def </span><span class="s1">_process_parameters(self</span><span class="s2">, </span><span class="s1">df</span><span class="s2">, </span><span class="s1">scale):</span>
        <span class="s2">if </span><span class="s1">scale </span><span class="s2">is None</span><span class="s1">:</span>
            <span class="s1">scale = </span><span class="s4">1.0</span>
        <span class="s1">scale = np.asarray(scale</span><span class="s2">, </span><span class="s1">dtype=float)</span>

        <span class="s2">if </span><span class="s1">scale.ndim == </span><span class="s4">0</span><span class="s1">:</span>
            <span class="s1">scale = scale[np.newaxis</span><span class="s2">, </span><span class="s1">np.newaxis]</span>
        <span class="s2">elif </span><span class="s1">scale.ndim == </span><span class="s4">1</span><span class="s1">:</span>
            <span class="s1">scale = np.diag(scale)</span>
        <span class="s2">elif </span><span class="s1">scale.ndim == </span><span class="s4">2 </span><span class="s2">and not </span><span class="s1">scale.shape[</span><span class="s4">0</span><span class="s1">] == scale.shape[</span><span class="s4">1</span><span class="s1">]:</span>
            <span class="s2">raise </span><span class="s1">ValueError(</span><span class="s3">&quot;Array 'scale' must be square if it is two&quot;</span>
                             <span class="s3">&quot; dimensional, but scale.scale = %s.&quot;</span>
                             <span class="s1">% str(scale.shape))</span>
        <span class="s2">elif </span><span class="s1">scale.ndim &gt; </span><span class="s4">2</span><span class="s1">:</span>
            <span class="s2">raise </span><span class="s1">ValueError(</span><span class="s3">&quot;Array 'scale' must be at most two-dimensional,&quot;</span>
                             <span class="s3">&quot; but scale.ndim = %d&quot; </span><span class="s1">% scale.ndim)</span>

        <span class="s1">dim = scale.shape[</span><span class="s4">0</span><span class="s1">]</span>

        <span class="s2">if </span><span class="s1">df </span><span class="s2">is None</span><span class="s1">:</span>
            <span class="s1">df = dim</span>
        <span class="s2">elif not </span><span class="s1">np.isscalar(df):</span>
            <span class="s2">raise </span><span class="s1">ValueError(</span><span class="s3">&quot;Degrees of freedom must be a scalar.&quot;</span><span class="s1">)</span>
        <span class="s2">elif </span><span class="s1">df &lt;= dim - </span><span class="s4">1</span><span class="s1">:</span>
            <span class="s2">raise </span><span class="s1">ValueError(</span><span class="s3">&quot;Degrees of freedom must be greater than the &quot;</span>
                             <span class="s3">&quot;dimension of scale matrix minus 1.&quot;</span><span class="s1">)</span>

        <span class="s2">return </span><span class="s1">dim</span><span class="s2">, </span><span class="s1">df</span><span class="s2">, </span><span class="s1">scale</span>

    <span class="s2">def </span><span class="s1">_process_quantiles(self</span><span class="s2">, </span><span class="s1">x</span><span class="s2">, </span><span class="s1">dim):</span>
        <span class="s5">&quot;&quot;&quot; 
        Adjust quantiles array so that last axis labels the components of 
        each data point. 
        &quot;&quot;&quot;</span>
        <span class="s1">x = np.asarray(x</span><span class="s2">, </span><span class="s1">dtype=float)</span>

        <span class="s2">if </span><span class="s1">x.ndim == </span><span class="s4">0</span><span class="s1">:</span>
            <span class="s1">x = x * np.eye(dim)[:</span><span class="s2">, </span><span class="s1">:</span><span class="s2">, </span><span class="s1">np.newaxis]</span>
        <span class="s2">if </span><span class="s1">x.ndim == </span><span class="s4">1</span><span class="s1">:</span>
            <span class="s2">if </span><span class="s1">dim == </span><span class="s4">1</span><span class="s1">:</span>
                <span class="s1">x = x[np.newaxis</span><span class="s2">, </span><span class="s1">np.newaxis</span><span class="s2">, </span><span class="s1">:]</span>
            <span class="s2">else</span><span class="s1">:</span>
                <span class="s1">x = np.diag(x)[:</span><span class="s2">, </span><span class="s1">:</span><span class="s2">, </span><span class="s1">np.newaxis]</span>
        <span class="s2">elif </span><span class="s1">x.ndim == </span><span class="s4">2</span><span class="s1">:</span>
            <span class="s2">if not </span><span class="s1">x.shape[</span><span class="s4">0</span><span class="s1">] == x.shape[</span><span class="s4">1</span><span class="s1">]:</span>
                <span class="s2">raise </span><span class="s1">ValueError(</span><span class="s3">&quot;Quantiles must be square if they are two&quot;</span>
                                 <span class="s3">&quot; dimensional, but x.shape = %s.&quot;</span>
                                 <span class="s1">% str(x.shape))</span>
            <span class="s1">x = x[:</span><span class="s2">, </span><span class="s1">:</span><span class="s2">, </span><span class="s1">np.newaxis]</span>
        <span class="s2">elif </span><span class="s1">x.ndim == </span><span class="s4">3</span><span class="s1">:</span>
            <span class="s2">if not </span><span class="s1">x.shape[</span><span class="s4">0</span><span class="s1">] == x.shape[</span><span class="s4">1</span><span class="s1">]:</span>
                <span class="s2">raise </span><span class="s1">ValueError(</span><span class="s3">&quot;Quantiles must be square in the first two&quot;</span>
                                 <span class="s3">&quot; dimensions if they are three dimensional&quot;</span>
                                 <span class="s3">&quot;, but x.shape = %s.&quot; </span><span class="s1">% str(x.shape))</span>
        <span class="s2">elif </span><span class="s1">x.ndim &gt; </span><span class="s4">3</span><span class="s1">:</span>
            <span class="s2">raise </span><span class="s1">ValueError(</span><span class="s3">&quot;Quantiles must be at most two-dimensional with&quot;</span>
                             <span class="s3">&quot; an additional dimension for multiple&quot;</span>
                             <span class="s3">&quot;components, but x.ndim = %d&quot; </span><span class="s1">% x.ndim)</span>

        <span class="s0"># Now we have 3-dim array; should have shape [dim, dim, *]</span>
        <span class="s2">if not </span><span class="s1">x.shape[</span><span class="s4">0</span><span class="s1">:</span><span class="s4">2</span><span class="s1">] == (dim</span><span class="s2">, </span><span class="s1">dim):</span>
            <span class="s2">raise </span><span class="s1">ValueError(</span><span class="s3">'Quantiles have incompatible dimensions: should'</span>
                             <span class="s3">' be {}, got {}.'</span><span class="s1">.format((dim</span><span class="s2">, </span><span class="s1">dim)</span><span class="s2">, </span><span class="s1">x.shape[</span><span class="s4">0</span><span class="s1">:</span><span class="s4">2</span><span class="s1">]))</span>

        <span class="s2">return </span><span class="s1">x</span>

    <span class="s2">def </span><span class="s1">_process_size(self</span><span class="s2">, </span><span class="s1">size):</span>
        <span class="s1">size = np.asarray(size)</span>

        <span class="s2">if </span><span class="s1">size.ndim == </span><span class="s4">0</span><span class="s1">:</span>
            <span class="s1">size = size[np.newaxis]</span>
        <span class="s2">elif </span><span class="s1">size.ndim &gt; </span><span class="s4">1</span><span class="s1">:</span>
            <span class="s2">raise </span><span class="s1">ValueError(</span><span class="s3">'Size must be an integer or tuple of integers;'</span>
                             <span class="s3">' thus must have dimension &lt;= 1.'</span>
                             <span class="s3">' Got size.ndim = %s' </span><span class="s1">% str(tuple(size)))</span>
        <span class="s1">n = size.prod()</span>
        <span class="s1">shape = tuple(size)</span>

        <span class="s2">return </span><span class="s1">n</span><span class="s2">, </span><span class="s1">shape</span>

    <span class="s2">def </span><span class="s1">_logpdf(self</span><span class="s2">, </span><span class="s1">x</span><span class="s2">, </span><span class="s1">dim</span><span class="s2">, </span><span class="s1">df</span><span class="s2">, </span><span class="s1">scale</span><span class="s2">, </span><span class="s1">log_det_scale</span><span class="s2">, </span><span class="s1">C):</span>
        <span class="s5">&quot;&quot;&quot;Log of the Wishart probability density function. 
 
        Parameters 
        ---------- 
        x : ndarray 
            Points at which to evaluate the log of the probability 
            density function 
        dim : int 
            Dimension of the scale matrix 
        df : int 
            Degrees of freedom 
        scale : ndarray 
            Scale matrix 
        log_det_scale : float 
            Logarithm of the determinant of the scale matrix 
        C : ndarray 
            Cholesky factorization of the scale matrix, lower triagular. 
 
        Notes 
        ----- 
        As this function does no argument checking, it should not be 
        called directly; use 'logpdf' instead. 
 
        &quot;&quot;&quot;</span>
        <span class="s0"># log determinant of x</span>
        <span class="s0"># Note: x has components along the last axis, so that x.T has</span>
        <span class="s0"># components alone the 0-th axis. Then since det(A) = det(A'), this</span>
        <span class="s0"># gives us a 1-dim vector of determinants</span>

        <span class="s0"># Retrieve tr(scale^{-1} x)</span>
        <span class="s1">log_det_x = np.empty(x.shape[-</span><span class="s4">1</span><span class="s1">])</span>
        <span class="s1">scale_inv_x = np.empty(x.shape)</span>
        <span class="s1">tr_scale_inv_x = np.empty(x.shape[-</span><span class="s4">1</span><span class="s1">])</span>
        <span class="s2">for </span><span class="s1">i </span><span class="s2">in </span><span class="s1">range(x.shape[-</span><span class="s4">1</span><span class="s1">]):</span>
            <span class="s1">_</span><span class="s2">, </span><span class="s1">log_det_x[i] = self._cholesky_logdet(x[:</span><span class="s2">, </span><span class="s1">:</span><span class="s2">, </span><span class="s1">i])</span>
            <span class="s1">scale_inv_x[:</span><span class="s2">, </span><span class="s1">:</span><span class="s2">, </span><span class="s1">i] = scipy.linalg.cho_solve((C</span><span class="s2">, True</span><span class="s1">)</span><span class="s2">, </span><span class="s1">x[:</span><span class="s2">, </span><span class="s1">:</span><span class="s2">, </span><span class="s1">i])</span>
            <span class="s1">tr_scale_inv_x[i] = scale_inv_x[:</span><span class="s2">, </span><span class="s1">:</span><span class="s2">, </span><span class="s1">i].trace()</span>

        <span class="s0"># Log PDF</span>
        <span class="s1">out = ((</span><span class="s4">0.5 </span><span class="s1">* (df - dim - </span><span class="s4">1</span><span class="s1">) * log_det_x - </span><span class="s4">0.5 </span><span class="s1">* tr_scale_inv_x) -</span>
               <span class="s1">(</span><span class="s4">0.5 </span><span class="s1">* df * dim * _LOG_2 + </span><span class="s4">0.5 </span><span class="s1">* df * log_det_scale +</span>
                <span class="s1">multigammaln(</span><span class="s4">0.5</span><span class="s1">*df</span><span class="s2">, </span><span class="s1">dim)))</span>

        <span class="s2">return </span><span class="s1">out</span>

    <span class="s2">def </span><span class="s1">logpdf(self</span><span class="s2">, </span><span class="s1">x</span><span class="s2">, </span><span class="s1">df</span><span class="s2">, </span><span class="s1">scale):</span>
        <span class="s5">&quot;&quot;&quot;Log of the Wishart probability density function. 
 
        Parameters 
        ---------- 
        x : array_like 
            Quantiles, with the last axis of `x` denoting the components. 
            Each quantile must be a symmetric positive definite matrix. 
        %(_doc_default_callparams)s 
 
        Returns 
        ------- 
        pdf : ndarray 
            Log of the probability density function evaluated at `x` 
 
        Notes 
        ----- 
        %(_doc_callparams_note)s 
 
        &quot;&quot;&quot;</span>
        <span class="s1">dim</span><span class="s2">, </span><span class="s1">df</span><span class="s2">, </span><span class="s1">scale = self._process_parameters(df</span><span class="s2">, </span><span class="s1">scale)</span>
        <span class="s1">x = self._process_quantiles(x</span><span class="s2">, </span><span class="s1">dim)</span>

        <span class="s0"># Cholesky decomposition of scale, get log(det(scale))</span>
        <span class="s1">C</span><span class="s2">, </span><span class="s1">log_det_scale = self._cholesky_logdet(scale)</span>

        <span class="s1">out = self._logpdf(x</span><span class="s2">, </span><span class="s1">dim</span><span class="s2">, </span><span class="s1">df</span><span class="s2">, </span><span class="s1">scale</span><span class="s2">, </span><span class="s1">log_det_scale</span><span class="s2">, </span><span class="s1">C)</span>
        <span class="s2">return </span><span class="s1">_squeeze_output(out)</span>

    <span class="s2">def </span><span class="s1">pdf(self</span><span class="s2">, </span><span class="s1">x</span><span class="s2">, </span><span class="s1">df</span><span class="s2">, </span><span class="s1">scale):</span>
        <span class="s5">&quot;&quot;&quot;Wishart probability density function. 
 
        Parameters 
        ---------- 
        x : array_like 
            Quantiles, with the last axis of `x` denoting the components. 
            Each quantile must be a symmetric positive definite matrix. 
        %(_doc_default_callparams)s 
 
        Returns 
        ------- 
        pdf : ndarray 
            Probability density function evaluated at `x` 
 
        Notes 
        ----- 
        %(_doc_callparams_note)s 
 
        &quot;&quot;&quot;</span>
        <span class="s2">return </span><span class="s1">np.exp(self.logpdf(x</span><span class="s2">, </span><span class="s1">df</span><span class="s2">, </span><span class="s1">scale))</span>

    <span class="s2">def </span><span class="s1">_mean(self</span><span class="s2">, </span><span class="s1">dim</span><span class="s2">, </span><span class="s1">df</span><span class="s2">, </span><span class="s1">scale):</span>
        <span class="s5">&quot;&quot;&quot;Mean of the Wishart distribution. 
 
        Parameters 
        ---------- 
        dim : int 
            Dimension of the scale matrix 
        %(_doc_default_callparams)s 
 
        Notes 
        ----- 
        As this function does no argument checking, it should not be 
        called directly; use 'mean' instead. 
 
        &quot;&quot;&quot;</span>
        <span class="s2">return </span><span class="s1">df * scale</span>

    <span class="s2">def </span><span class="s1">mean(self</span><span class="s2">, </span><span class="s1">df</span><span class="s2">, </span><span class="s1">scale):</span>
        <span class="s5">&quot;&quot;&quot;Mean of the Wishart distribution. 
 
        Parameters 
        ---------- 
        %(_doc_default_callparams)s 
 
        Returns 
        ------- 
        mean : float 
            The mean of the distribution 
        &quot;&quot;&quot;</span>
        <span class="s1">dim</span><span class="s2">, </span><span class="s1">df</span><span class="s2">, </span><span class="s1">scale = self._process_parameters(df</span><span class="s2">, </span><span class="s1">scale)</span>
        <span class="s1">out = self._mean(dim</span><span class="s2">, </span><span class="s1">df</span><span class="s2">, </span><span class="s1">scale)</span>
        <span class="s2">return </span><span class="s1">_squeeze_output(out)</span>

    <span class="s2">def </span><span class="s1">_mode(self</span><span class="s2">, </span><span class="s1">dim</span><span class="s2">, </span><span class="s1">df</span><span class="s2">, </span><span class="s1">scale):</span>
        <span class="s5">&quot;&quot;&quot;Mode of the Wishart distribution. 
 
        Parameters 
        ---------- 
        dim : int 
            Dimension of the scale matrix 
        %(_doc_default_callparams)s 
 
        Notes 
        ----- 
        As this function does no argument checking, it should not be 
        called directly; use 'mode' instead. 
 
        &quot;&quot;&quot;</span>
        <span class="s2">if </span><span class="s1">df &gt;= dim + </span><span class="s4">1</span><span class="s1">:</span>
            <span class="s1">out = (df-dim-</span><span class="s4">1</span><span class="s1">) * scale</span>
        <span class="s2">else</span><span class="s1">:</span>
            <span class="s1">out = </span><span class="s2">None</span>
        <span class="s2">return </span><span class="s1">out</span>

    <span class="s2">def </span><span class="s1">mode(self</span><span class="s2">, </span><span class="s1">df</span><span class="s2">, </span><span class="s1">scale):</span>
        <span class="s5">&quot;&quot;&quot;Mode of the Wishart distribution 
 
        Only valid if the degrees of freedom are greater than the dimension of 
        the scale matrix. 
 
        Parameters 
        ---------- 
        %(_doc_default_callparams)s 
 
        Returns 
        ------- 
        mode : float or None 
            The Mode of the distribution 
        &quot;&quot;&quot;</span>
        <span class="s1">dim</span><span class="s2">, </span><span class="s1">df</span><span class="s2">, </span><span class="s1">scale = self._process_parameters(df</span><span class="s2">, </span><span class="s1">scale)</span>
        <span class="s1">out = self._mode(dim</span><span class="s2">, </span><span class="s1">df</span><span class="s2">, </span><span class="s1">scale)</span>
        <span class="s2">return </span><span class="s1">_squeeze_output(out) </span><span class="s2">if </span><span class="s1">out </span><span class="s2">is not None else </span><span class="s1">out</span>

    <span class="s2">def </span><span class="s1">_var(self</span><span class="s2">, </span><span class="s1">dim</span><span class="s2">, </span><span class="s1">df</span><span class="s2">, </span><span class="s1">scale):</span>
        <span class="s5">&quot;&quot;&quot;Variance of the Wishart distribution. 
 
        Parameters 
        ---------- 
        dim : int 
            Dimension of the scale matrix 
        %(_doc_default_callparams)s 
 
        Notes 
        ----- 
        As this function does no argument checking, it should not be 
        called directly; use 'var' instead. 
 
        &quot;&quot;&quot;</span>
        <span class="s1">var = scale**</span><span class="s4">2</span>
        <span class="s1">diag = scale.diagonal()  </span><span class="s0"># 1 x dim array</span>
        <span class="s1">var += np.outer(diag</span><span class="s2">, </span><span class="s1">diag)</span>
        <span class="s1">var *= df</span>
        <span class="s2">return </span><span class="s1">var</span>

    <span class="s2">def </span><span class="s1">var(self</span><span class="s2">, </span><span class="s1">df</span><span class="s2">, </span><span class="s1">scale):</span>
        <span class="s5">&quot;&quot;&quot;Variance of the Wishart distribution. 
 
        Parameters 
        ---------- 
        %(_doc_default_callparams)s 
 
        Returns 
        ------- 
        var : float 
            The variance of the distribution 
        &quot;&quot;&quot;</span>
        <span class="s1">dim</span><span class="s2">, </span><span class="s1">df</span><span class="s2">, </span><span class="s1">scale = self._process_parameters(df</span><span class="s2">, </span><span class="s1">scale)</span>
        <span class="s1">out = self._var(dim</span><span class="s2">, </span><span class="s1">df</span><span class="s2">, </span><span class="s1">scale)</span>
        <span class="s2">return </span><span class="s1">_squeeze_output(out)</span>

    <span class="s2">def </span><span class="s1">_standard_rvs(self</span><span class="s2">, </span><span class="s1">n</span><span class="s2">, </span><span class="s1">shape</span><span class="s2">, </span><span class="s1">dim</span><span class="s2">, </span><span class="s1">df</span><span class="s2">, </span><span class="s1">random_state):</span>
        <span class="s5">&quot;&quot;&quot; 
        Parameters 
        ---------- 
        n : integer 
            Number of variates to generate 
        shape : iterable 
            Shape of the variates to generate 
        dim : int 
            Dimension of the scale matrix 
        df : int 
            Degrees of freedom 
        random_state : {None, int, `numpy.random.Generator`, 
                        `numpy.random.RandomState`}, optional 
 
            If `seed` is None (or `np.random`), the `numpy.random.RandomState` 
            singleton is used. 
            If `seed` is an int, a new ``RandomState`` instance is used, 
            seeded with `seed`. 
            If `seed` is already a ``Generator`` or ``RandomState`` instance 
            then that instance is used. 
 
        Notes 
        ----- 
        As this function does no argument checking, it should not be 
        called directly; use 'rvs' instead. 
 
        &quot;&quot;&quot;</span>
        <span class="s0"># Random normal variates for off-diagonal elements</span>
        <span class="s1">n_tril = dim * (dim-</span><span class="s4">1</span><span class="s1">) // </span><span class="s4">2</span>
        <span class="s1">covariances = random_state.normal(</span>
            <span class="s1">size=n*n_tril).reshape(shape+(n_tril</span><span class="s2">,</span><span class="s1">))</span>

        <span class="s0"># Random chi-square variates for diagonal elements</span>
        <span class="s1">variances = (np.r_[[random_state.chisquare(df-(i+</span><span class="s4">1</span><span class="s1">)+</span><span class="s4">1</span><span class="s2">, </span><span class="s1">size=n)**</span><span class="s4">0.5</span>
                            <span class="s2">for </span><span class="s1">i </span><span class="s2">in </span><span class="s1">range(dim)]].reshape((dim</span><span class="s2">,</span><span class="s1">) +</span>
                                                          <span class="s1">shape[::-</span><span class="s4">1</span><span class="s1">]).T)</span>

        <span class="s0"># Create the A matri(ces) - lower triangular</span>
        <span class="s1">A = np.zeros(shape + (dim</span><span class="s2">, </span><span class="s1">dim))</span>

        <span class="s0"># Input the covariances</span>
        <span class="s1">size_idx = tuple([slice(</span><span class="s2">None, None, None</span><span class="s1">)]*len(shape))</span>
        <span class="s1">tril_idx = np.tril_indices(dim</span><span class="s2">, </span><span class="s1">k=-</span><span class="s4">1</span><span class="s1">)</span>
        <span class="s1">A[size_idx + tril_idx] = covariances</span>

        <span class="s0"># Input the variances</span>
        <span class="s1">diag_idx = np.diag_indices(dim)</span>
        <span class="s1">A[size_idx + diag_idx] = variances</span>

        <span class="s2">return </span><span class="s1">A</span>

    <span class="s2">def </span><span class="s1">_rvs(self</span><span class="s2">, </span><span class="s1">n</span><span class="s2">, </span><span class="s1">shape</span><span class="s2">, </span><span class="s1">dim</span><span class="s2">, </span><span class="s1">df</span><span class="s2">, </span><span class="s1">C</span><span class="s2">, </span><span class="s1">random_state):</span>
        <span class="s5">&quot;&quot;&quot;Draw random samples from a Wishart distribution. 
 
        Parameters 
        ---------- 
        n : integer 
            Number of variates to generate 
        shape : iterable 
            Shape of the variates to generate 
        dim : int 
            Dimension of the scale matrix 
        df : int 
            Degrees of freedom 
        C : ndarray 
            Cholesky factorization of the scale matrix, lower triangular. 
        %(_doc_random_state)s 
 
        Notes 
        ----- 
        As this function does no argument checking, it should not be 
        called directly; use 'rvs' instead. 
 
        &quot;&quot;&quot;</span>
        <span class="s1">random_state = self._get_random_state(random_state)</span>
        <span class="s0"># Calculate the matrices A, which are actually lower triangular</span>
        <span class="s0"># Cholesky factorizations of a matrix B such that B ~ W(df, I)</span>
        <span class="s1">A = self._standard_rvs(n</span><span class="s2">, </span><span class="s1">shape</span><span class="s2">, </span><span class="s1">dim</span><span class="s2">, </span><span class="s1">df</span><span class="s2">, </span><span class="s1">random_state)</span>

        <span class="s0"># Calculate SA = C A A' C', where SA ~ W(df, scale)</span>
        <span class="s0"># Note: this is the product of a (lower) (lower) (lower)' (lower)'</span>
        <span class="s0">#       or, denoting B = AA', it is C B C' where C is the lower</span>
        <span class="s0">#       triangular Cholesky factorization of the scale matrix.</span>
        <span class="s0">#       this appears to conflict with the instructions in [1]_, which</span>
        <span class="s0">#       suggest that it should be D' B D where D is the lower</span>
        <span class="s0">#       triangular factorization of the scale matrix. However, it is</span>
        <span class="s0">#       meant to refer to the Bartlett (1933) representation of a</span>
        <span class="s0">#       Wishart random variate as L A A' L' where L is lower triangular</span>
        <span class="s0">#       so it appears that understanding D' to be upper triangular</span>
        <span class="s0">#       is either a typo in or misreading of [1]_.</span>
        <span class="s2">for </span><span class="s1">index </span><span class="s2">in </span><span class="s1">np.ndindex(shape):</span>
            <span class="s1">CA = np.dot(C</span><span class="s2">, </span><span class="s1">A[index])</span>
            <span class="s1">A[index] = np.dot(CA</span><span class="s2">, </span><span class="s1">CA.T)</span>

        <span class="s2">return </span><span class="s1">A</span>

    <span class="s2">def </span><span class="s1">rvs(self</span><span class="s2">, </span><span class="s1">df</span><span class="s2">, </span><span class="s1">scale</span><span class="s2">, </span><span class="s1">size=</span><span class="s4">1</span><span class="s2">, </span><span class="s1">random_state=</span><span class="s2">None</span><span class="s1">):</span>
        <span class="s5">&quot;&quot;&quot;Draw random samples from a Wishart distribution. 
 
        Parameters 
        ---------- 
        %(_doc_default_callparams)s 
        size : integer or iterable of integers, optional 
            Number of samples to draw (default 1). 
        %(_doc_random_state)s 
 
        Returns 
        ------- 
        rvs : ndarray 
            Random variates of shape (`size`) + (`dim`, `dim), where `dim` is 
            the dimension of the scale matrix. 
 
        Notes 
        ----- 
        %(_doc_callparams_note)s 
 
        &quot;&quot;&quot;</span>
        <span class="s1">n</span><span class="s2">, </span><span class="s1">shape = self._process_size(size)</span>
        <span class="s1">dim</span><span class="s2">, </span><span class="s1">df</span><span class="s2">, </span><span class="s1">scale = self._process_parameters(df</span><span class="s2">, </span><span class="s1">scale)</span>

        <span class="s0"># Cholesky decomposition of scale</span>
        <span class="s1">C = scipy.linalg.cholesky(scale</span><span class="s2">, </span><span class="s1">lower=</span><span class="s2">True</span><span class="s1">)</span>

        <span class="s1">out = self._rvs(n</span><span class="s2">, </span><span class="s1">shape</span><span class="s2">, </span><span class="s1">dim</span><span class="s2">, </span><span class="s1">df</span><span class="s2">, </span><span class="s1">C</span><span class="s2">, </span><span class="s1">random_state)</span>

        <span class="s2">return </span><span class="s1">_squeeze_output(out)</span>

    <span class="s2">def </span><span class="s1">_entropy(self</span><span class="s2">, </span><span class="s1">dim</span><span class="s2">, </span><span class="s1">df</span><span class="s2">, </span><span class="s1">log_det_scale):</span>
        <span class="s5">&quot;&quot;&quot;Compute the differential entropy of the Wishart. 
 
        Parameters 
        ---------- 
        dim : int 
            Dimension of the scale matrix 
        df : int 
            Degrees of freedom 
        log_det_scale : float 
            Logarithm of the determinant of the scale matrix 
 
        Notes 
        ----- 
        As this function does no argument checking, it should not be 
        called directly; use 'entropy' instead. 
 
        &quot;&quot;&quot;</span>
        <span class="s2">return </span><span class="s1">(</span>
            <span class="s4">0.5 </span><span class="s1">* (dim+</span><span class="s4">1</span><span class="s1">) * log_det_scale +</span>
            <span class="s4">0.5 </span><span class="s1">* dim * (dim+</span><span class="s4">1</span><span class="s1">) * _LOG_2 +</span>
            <span class="s1">multigammaln(</span><span class="s4">0.5</span><span class="s1">*df</span><span class="s2">, </span><span class="s1">dim) -</span>
            <span class="s4">0.5 </span><span class="s1">* (df - dim - </span><span class="s4">1</span><span class="s1">) * np.sum(</span>
                <span class="s1">[psi(</span><span class="s4">0.5</span><span class="s1">*(df + </span><span class="s4">1 </span><span class="s1">- (i+</span><span class="s4">1</span><span class="s1">))) </span><span class="s2">for </span><span class="s1">i </span><span class="s2">in </span><span class="s1">range(dim)]</span>
            <span class="s1">) +</span>
            <span class="s4">0.5 </span><span class="s1">* df * dim</span>
        <span class="s1">)</span>

    <span class="s2">def </span><span class="s1">entropy(self</span><span class="s2">, </span><span class="s1">df</span><span class="s2">, </span><span class="s1">scale):</span>
        <span class="s5">&quot;&quot;&quot;Compute the differential entropy of the Wishart. 
 
        Parameters 
        ---------- 
        %(_doc_default_callparams)s 
 
        Returns 
        ------- 
        h : scalar 
            Entropy of the Wishart distribution 
 
        Notes 
        ----- 
        %(_doc_callparams_note)s 
 
        &quot;&quot;&quot;</span>
        <span class="s1">dim</span><span class="s2">, </span><span class="s1">df</span><span class="s2">, </span><span class="s1">scale = self._process_parameters(df</span><span class="s2">, </span><span class="s1">scale)</span>
        <span class="s1">_</span><span class="s2">, </span><span class="s1">log_det_scale = self._cholesky_logdet(scale)</span>
        <span class="s2">return </span><span class="s1">self._entropy(dim</span><span class="s2">, </span><span class="s1">df</span><span class="s2">, </span><span class="s1">log_det_scale)</span>

    <span class="s2">def </span><span class="s1">_cholesky_logdet(self</span><span class="s2">, </span><span class="s1">scale):</span>
        <span class="s5">&quot;&quot;&quot;Compute Cholesky decomposition and determine (log(det(scale)). 
 
        Parameters 
        ---------- 
        scale : ndarray 
            Scale matrix. 
 
        Returns 
        ------- 
        c_decomp : ndarray 
            The Cholesky decomposition of `scale`. 
        logdet : scalar 
            The log of the determinant of `scale`. 
 
        Notes 
        ----- 
        This computation of ``logdet`` is equivalent to 
        ``np.linalg.slogdet(scale)``.  It is ~2x faster though. 
 
        &quot;&quot;&quot;</span>
        <span class="s1">c_decomp = scipy.linalg.cholesky(scale</span><span class="s2">, </span><span class="s1">lower=</span><span class="s2">True</span><span class="s1">)</span>
        <span class="s1">logdet = </span><span class="s4">2 </span><span class="s1">* np.sum(np.log(c_decomp.diagonal()))</span>
        <span class="s2">return </span><span class="s1">c_decomp</span><span class="s2">, </span><span class="s1">logdet</span>


<span class="s1">wishart = wishart_gen()</span>


<span class="s2">class </span><span class="s1">wishart_frozen(multi_rv_frozen):</span>
    <span class="s5">&quot;&quot;&quot;Create a frozen Wishart distribution. 
 
    Parameters 
    ---------- 
    df : array_like 
        Degrees of freedom of the distribution 
    scale : array_like 
        Scale matrix of the distribution 
    seed : {None, int, `numpy.random.Generator`, `numpy.random.RandomState`}, optional 
        If `seed` is None (or `np.random`), the `numpy.random.RandomState` 
        singleton is used. 
        If `seed` is an int, a new ``RandomState`` instance is used, 
        seeded with `seed`. 
        If `seed` is already a ``Generator`` or ``RandomState`` instance then 
        that instance is used. 
 
    &quot;&quot;&quot;</span>
    <span class="s2">def </span><span class="s1">__init__(self</span><span class="s2">, </span><span class="s1">df</span><span class="s2">, </span><span class="s1">scale</span><span class="s2">, </span><span class="s1">seed=</span><span class="s2">None</span><span class="s1">):</span>
        <span class="s1">self._dist = wishart_gen(seed)</span>
        <span class="s1">self.dim</span><span class="s2">, </span><span class="s1">self.df</span><span class="s2">, </span><span class="s1">self.scale = self._dist._process_parameters(</span>
            <span class="s1">df</span><span class="s2">, </span><span class="s1">scale)</span>
        <span class="s1">self.C</span><span class="s2">, </span><span class="s1">self.log_det_scale = self._dist._cholesky_logdet(self.scale)</span>

    <span class="s2">def </span><span class="s1">logpdf(self</span><span class="s2">, </span><span class="s1">x):</span>
        <span class="s1">x = self._dist._process_quantiles(x</span><span class="s2">, </span><span class="s1">self.dim)</span>

        <span class="s1">out = self._dist._logpdf(x</span><span class="s2">, </span><span class="s1">self.dim</span><span class="s2">, </span><span class="s1">self.df</span><span class="s2">, </span><span class="s1">self.scale</span><span class="s2">,</span>
                                 <span class="s1">self.log_det_scale</span><span class="s2">, </span><span class="s1">self.C)</span>
        <span class="s2">return </span><span class="s1">_squeeze_output(out)</span>

    <span class="s2">def </span><span class="s1">pdf(self</span><span class="s2">, </span><span class="s1">x):</span>
        <span class="s2">return </span><span class="s1">np.exp(self.logpdf(x))</span>

    <span class="s2">def </span><span class="s1">mean(self):</span>
        <span class="s1">out = self._dist._mean(self.dim</span><span class="s2">, </span><span class="s1">self.df</span><span class="s2">, </span><span class="s1">self.scale)</span>
        <span class="s2">return </span><span class="s1">_squeeze_output(out)</span>

    <span class="s2">def </span><span class="s1">mode(self):</span>
        <span class="s1">out = self._dist._mode(self.dim</span><span class="s2">, </span><span class="s1">self.df</span><span class="s2">, </span><span class="s1">self.scale)</span>
        <span class="s2">return </span><span class="s1">_squeeze_output(out) </span><span class="s2">if </span><span class="s1">out </span><span class="s2">is not None else </span><span class="s1">out</span>

    <span class="s2">def </span><span class="s1">var(self):</span>
        <span class="s1">out = self._dist._var(self.dim</span><span class="s2">, </span><span class="s1">self.df</span><span class="s2">, </span><span class="s1">self.scale)</span>
        <span class="s2">return </span><span class="s1">_squeeze_output(out)</span>

    <span class="s2">def </span><span class="s1">rvs(self</span><span class="s2">, </span><span class="s1">size=</span><span class="s4">1</span><span class="s2">, </span><span class="s1">random_state=</span><span class="s2">None</span><span class="s1">):</span>
        <span class="s1">n</span><span class="s2">, </span><span class="s1">shape = self._dist._process_size(size)</span>
        <span class="s1">out = self._dist._rvs(n</span><span class="s2">, </span><span class="s1">shape</span><span class="s2">, </span><span class="s1">self.dim</span><span class="s2">, </span><span class="s1">self.df</span><span class="s2">,</span>
                              <span class="s1">self.C</span><span class="s2">, </span><span class="s1">random_state)</span>
        <span class="s2">return </span><span class="s1">_squeeze_output(out)</span>

    <span class="s2">def </span><span class="s1">entropy(self):</span>
        <span class="s2">return </span><span class="s1">self._dist._entropy(self.dim</span><span class="s2">, </span><span class="s1">self.df</span><span class="s2">, </span><span class="s1">self.log_det_scale)</span>


<span class="s0"># Set frozen generator docstrings from corresponding docstrings in</span>
<span class="s0"># Wishart and fill in default strings in class docstrings</span>
<span class="s2">for </span><span class="s1">name </span><span class="s2">in </span><span class="s1">[</span><span class="s3">'logpdf'</span><span class="s2">, </span><span class="s3">'pdf'</span><span class="s2">, </span><span class="s3">'mean'</span><span class="s2">, </span><span class="s3">'mode'</span><span class="s2">, </span><span class="s3">'var'</span><span class="s2">, </span><span class="s3">'rvs'</span><span class="s2">, </span><span class="s3">'entropy'</span><span class="s1">]:</span>
    <span class="s1">method = wishart_gen.__dict__[name]</span>
    <span class="s1">method_frozen = wishart_frozen.__dict__[name]</span>
    <span class="s1">method_frozen.__doc__ = doccer.docformat(</span>
        <span class="s1">method.__doc__</span><span class="s2">, </span><span class="s1">wishart_docdict_noparams)</span>
    <span class="s1">method.__doc__ = doccer.docformat(method.__doc__</span><span class="s2">, </span><span class="s1">wishart_docdict_params)</span>


<span class="s2">def </span><span class="s1">_cho_inv_batch(a</span><span class="s2">, </span><span class="s1">check_finite=</span><span class="s2">True</span><span class="s1">):</span>
    <span class="s5">&quot;&quot;&quot; 
    Invert the matrices a_i, using a Cholesky factorization of A, where 
    a_i resides in the last two dimensions of a and the other indices describe 
    the index i. 
 
    Overwrites the data in a. 
 
    Parameters 
    ---------- 
    a : array 
        Array of matrices to invert, where the matrices themselves are stored 
        in the last two dimensions. 
    check_finite : bool, optional 
        Whether to check that the input matrices contain only finite numbers. 
        Disabling may give a performance gain, but may result in problems 
        (crashes, non-termination) if the inputs do contain infinities or NaNs. 
 
    Returns 
    ------- 
    x : array 
        Array of inverses of the matrices ``a_i``. 
 
    See Also 
    -------- 
    scipy.linalg.cholesky : Cholesky factorization of a matrix 
 
    &quot;&quot;&quot;</span>
    <span class="s2">if </span><span class="s1">check_finite:</span>
        <span class="s1">a1 = asarray_chkfinite(a)</span>
    <span class="s2">else</span><span class="s1">:</span>
        <span class="s1">a1 = asarray(a)</span>
    <span class="s2">if </span><span class="s1">len(a1.shape) &lt; </span><span class="s4">2 </span><span class="s2">or </span><span class="s1">a1.shape[-</span><span class="s4">2</span><span class="s1">] != a1.shape[-</span><span class="s4">1</span><span class="s1">]:</span>
        <span class="s2">raise </span><span class="s1">ValueError(</span><span class="s3">'expected square matrix in last two dimensions'</span><span class="s1">)</span>

    <span class="s1">potrf</span><span class="s2">, </span><span class="s1">potri = get_lapack_funcs((</span><span class="s3">'potrf'</span><span class="s2">, </span><span class="s3">'potri'</span><span class="s1">)</span><span class="s2">, </span><span class="s1">(a1</span><span class="s2">,</span><span class="s1">))</span>

    <span class="s1">triu_rows</span><span class="s2">, </span><span class="s1">triu_cols = np.triu_indices(a.shape[-</span><span class="s4">2</span><span class="s1">]</span><span class="s2">, </span><span class="s1">k=</span><span class="s4">1</span><span class="s1">)</span>
    <span class="s2">for </span><span class="s1">index </span><span class="s2">in </span><span class="s1">np.ndindex(a1.shape[:-</span><span class="s4">2</span><span class="s1">]):</span>

        <span class="s0"># Cholesky decomposition</span>
        <span class="s1">a1[index]</span><span class="s2">, </span><span class="s1">info = potrf(a1[index]</span><span class="s2">, </span><span class="s1">lower=</span><span class="s2">True, </span><span class="s1">overwrite_a=</span><span class="s2">False,</span>
                                <span class="s1">clean=</span><span class="s2">False</span><span class="s1">)</span>
        <span class="s2">if </span><span class="s1">info &gt; </span><span class="s4">0</span><span class="s1">:</span>
            <span class="s2">raise </span><span class="s1">LinAlgError(</span><span class="s3">&quot;%d-th leading minor not positive definite&quot;</span>
                              <span class="s1">% info)</span>
        <span class="s2">if </span><span class="s1">info &lt; </span><span class="s4">0</span><span class="s1">:</span>
            <span class="s2">raise </span><span class="s1">ValueError(</span><span class="s3">'illegal value in %d-th argument of internal'</span>
                             <span class="s3">' potrf' </span><span class="s1">% -info)</span>
        <span class="s0"># Inversion</span>
        <span class="s1">a1[index]</span><span class="s2">, </span><span class="s1">info = potri(a1[index]</span><span class="s2">, </span><span class="s1">lower=</span><span class="s2">True, </span><span class="s1">overwrite_c=</span><span class="s2">False</span><span class="s1">)</span>
        <span class="s2">if </span><span class="s1">info &gt; </span><span class="s4">0</span><span class="s1">:</span>
            <span class="s2">raise </span><span class="s1">LinAlgError(</span><span class="s3">&quot;the inverse could not be computed&quot;</span><span class="s1">)</span>
        <span class="s2">if </span><span class="s1">info &lt; </span><span class="s4">0</span><span class="s1">:</span>
            <span class="s2">raise </span><span class="s1">ValueError(</span><span class="s3">'illegal value in %d-th argument of internal'</span>
                             <span class="s3">' potrf' </span><span class="s1">% -info)</span>

        <span class="s0"># Make symmetric (dpotri only fills in the lower triangle)</span>
        <span class="s1">a1[index][triu_rows</span><span class="s2">, </span><span class="s1">triu_cols] = a1[index][triu_cols</span><span class="s2">, </span><span class="s1">triu_rows]</span>

    <span class="s2">return </span><span class="s1">a1</span>


<span class="s2">class </span><span class="s1">invwishart_gen(wishart_gen):</span>
    <span class="s5">r&quot;&quot;&quot;An inverse Wishart random variable. 
 
    The `df` keyword specifies the degrees of freedom. The `scale` keyword 
    specifies the scale matrix, which must be symmetric and positive definite. 
    In this context, the scale matrix is often interpreted in terms of a 
    multivariate normal covariance matrix. 
 
    Methods 
    ------- 
    pdf(x, df, scale) 
        Probability density function. 
    logpdf(x, df, scale) 
        Log of the probability density function. 
    rvs(df, scale, size=1, random_state=None) 
        Draw random samples from an inverse Wishart distribution. 
    entropy(df, scale) 
        Differential entropy of the distribution. 
 
    Parameters 
    ---------- 
    %(_doc_default_callparams)s 
    %(_doc_random_state)s 
 
    Raises 
    ------ 
    scipy.linalg.LinAlgError 
        If the scale matrix `scale` is not positive definite. 
 
    See Also 
    -------- 
    wishart 
 
    Notes 
    ----- 
    %(_doc_callparams_note)s 
 
    The scale matrix `scale` must be a symmetric positive definite 
    matrix. Singular matrices, including the symmetric positive semi-definite 
    case, are not supported. Symmetry is not checked; only the lower triangular 
    portion is used. 
 
    The inverse Wishart distribution is often denoted 
 
    .. math:: 
 
        W_p^{-1}(\nu, \Psi) 
 
    where :math:`\nu` is the degrees of freedom and :math:`\Psi` is the 
    :math:`p \times p` scale matrix. 
 
    The probability density function for `invwishart` has support over positive 
    definite matrices :math:`S`; if :math:`S \sim W^{-1}_p(\nu, \Sigma)`, 
    then its PDF is given by: 
 
    .. math:: 
 
        f(S) = \frac{|\Sigma|^\frac{\nu}{2}}{2^{ \frac{\nu p}{2} } 
               |S|^{\frac{\nu + p + 1}{2}} \Gamma_p \left(\frac{\nu}{2} \right)} 
               \exp\left( -tr(\Sigma S^{-1}) / 2 \right) 
 
    If :math:`S \sim W_p^{-1}(\nu, \Psi)` (inverse Wishart) then 
    :math:`S^{-1} \sim W_p(\nu, \Psi^{-1})` (Wishart). 
 
    If the scale matrix is 1-dimensional and equal to one, then the inverse 
    Wishart distribution :math:`W_1(\nu, 1)` collapses to the 
    inverse Gamma distribution with parameters shape = :math:`\frac{\nu}{2}` 
    and scale = :math:`\frac{1}{2}`. 
 
    .. versionadded:: 0.16.0 
 
    References 
    ---------- 
    .. [1] M.L. Eaton, &quot;Multivariate Statistics: A Vector Space Approach&quot;, 
           Wiley, 1983. 
    .. [2] M.C. Jones, &quot;Generating Inverse Wishart Matrices&quot;, Communications 
           in Statistics - Simulation and Computation, vol. 14.2, pp.511-514, 
           1985. 
    .. [3] Gupta, M. and Srivastava, S. &quot;Parametric Bayesian Estimation of 
           Differential Entropy and Relative Entropy&quot;. Entropy 12, 818 - 843. 
           2010. 
 
    Examples 
    -------- 
    &gt;&gt;&gt; import numpy as np 
    &gt;&gt;&gt; import matplotlib.pyplot as plt 
    &gt;&gt;&gt; from scipy.stats import invwishart, invgamma 
    &gt;&gt;&gt; x = np.linspace(0.01, 1, 100) 
    &gt;&gt;&gt; iw = invwishart.pdf(x, df=6, scale=1) 
    &gt;&gt;&gt; iw[:3] 
    array([  1.20546865e-15,   5.42497807e-06,   4.45813929e-03]) 
    &gt;&gt;&gt; ig = invgamma.pdf(x, 6/2., scale=1./2) 
    &gt;&gt;&gt; ig[:3] 
    array([  1.20546865e-15,   5.42497807e-06,   4.45813929e-03]) 
    &gt;&gt;&gt; plt.plot(x, iw) 
    &gt;&gt;&gt; plt.show() 
 
    The input quantiles can be any shape of array, as long as the last 
    axis labels the components. 
 
    Alternatively, the object may be called (as a function) to fix the degrees 
    of freedom and scale parameters, returning a &quot;frozen&quot; inverse Wishart 
    random variable: 
 
    &gt;&gt;&gt; rv = invwishart(df=1, scale=1) 
    &gt;&gt;&gt; # Frozen object with the same methods but holding the given 
    &gt;&gt;&gt; # degrees of freedom and scale fixed. 
 
    &quot;&quot;&quot;</span>

    <span class="s2">def </span><span class="s1">__init__(self</span><span class="s2">, </span><span class="s1">seed=</span><span class="s2">None</span><span class="s1">):</span>
        <span class="s1">super().__init__(seed)</span>
        <span class="s1">self.__doc__ = doccer.docformat(self.__doc__</span><span class="s2">, </span><span class="s1">wishart_docdict_params)</span>

    <span class="s2">def </span><span class="s1">__call__(self</span><span class="s2">, </span><span class="s1">df=</span><span class="s2">None, </span><span class="s1">scale=</span><span class="s2">None, </span><span class="s1">seed=</span><span class="s2">None</span><span class="s1">):</span>
        <span class="s5">&quot;&quot;&quot;Create a frozen inverse Wishart distribution. 
 
        See `invwishart_frozen` for more information. 
 
        &quot;&quot;&quot;</span>
        <span class="s2">return </span><span class="s1">invwishart_frozen(df</span><span class="s2">, </span><span class="s1">scale</span><span class="s2">, </span><span class="s1">seed)</span>

    <span class="s2">def </span><span class="s1">_logpdf(self</span><span class="s2">, </span><span class="s1">x</span><span class="s2">, </span><span class="s1">dim</span><span class="s2">, </span><span class="s1">df</span><span class="s2">, </span><span class="s1">scale</span><span class="s2">, </span><span class="s1">log_det_scale):</span>
        <span class="s5">&quot;&quot;&quot;Log of the inverse Wishart probability density function. 
 
        Parameters 
        ---------- 
        x : ndarray 
            Points at which to evaluate the log of the probability 
            density function. 
        dim : int 
            Dimension of the scale matrix 
        df : int 
            Degrees of freedom 
        scale : ndarray 
            Scale matrix 
        log_det_scale : float 
            Logarithm of the determinant of the scale matrix 
 
        Notes 
        ----- 
        As this function does no argument checking, it should not be 
        called directly; use 'logpdf' instead. 
 
        &quot;&quot;&quot;</span>
        <span class="s1">log_det_x = np.empty(x.shape[-</span><span class="s4">1</span><span class="s1">])</span>
        <span class="s1">x_inv = np.copy(x).T</span>
        <span class="s2">if </span><span class="s1">dim &gt; </span><span class="s4">1</span><span class="s1">:</span>
            <span class="s1">_cho_inv_batch(x_inv)  </span><span class="s0"># works in-place</span>
        <span class="s2">else</span><span class="s1">:</span>
            <span class="s1">x_inv = </span><span class="s4">1.</span><span class="s1">/x_inv</span>
        <span class="s1">tr_scale_x_inv = np.empty(x.shape[-</span><span class="s4">1</span><span class="s1">])</span>

        <span class="s2">for </span><span class="s1">i </span><span class="s2">in </span><span class="s1">range(x.shape[-</span><span class="s4">1</span><span class="s1">]):</span>
            <span class="s1">C</span><span class="s2">, </span><span class="s1">lower = scipy.linalg.cho_factor(x[:</span><span class="s2">, </span><span class="s1">:</span><span class="s2">, </span><span class="s1">i]</span><span class="s2">, </span><span class="s1">lower=</span><span class="s2">True</span><span class="s1">)</span>

            <span class="s1">log_det_x[i] = </span><span class="s4">2 </span><span class="s1">* np.sum(np.log(C.diagonal()))</span>

            <span class="s1">tr_scale_x_inv[i] = np.dot(scale</span><span class="s2">, </span><span class="s1">x_inv[i]).trace()</span>

        <span class="s0"># Log PDF</span>
        <span class="s1">out = ((</span><span class="s4">0.5 </span><span class="s1">* df * log_det_scale - </span><span class="s4">0.5 </span><span class="s1">* tr_scale_x_inv) -</span>
               <span class="s1">(</span><span class="s4">0.5 </span><span class="s1">* df * dim * _LOG_2 + </span><span class="s4">0.5 </span><span class="s1">* (df + dim + </span><span class="s4">1</span><span class="s1">) * log_det_x) -</span>
               <span class="s1">multigammaln(</span><span class="s4">0.5</span><span class="s1">*df</span><span class="s2">, </span><span class="s1">dim))</span>

        <span class="s2">return </span><span class="s1">out</span>

    <span class="s2">def </span><span class="s1">logpdf(self</span><span class="s2">, </span><span class="s1">x</span><span class="s2">, </span><span class="s1">df</span><span class="s2">, </span><span class="s1">scale):</span>
        <span class="s5">&quot;&quot;&quot;Log of the inverse Wishart probability density function. 
 
        Parameters 
        ---------- 
        x : array_like 
            Quantiles, with the last axis of `x` denoting the components. 
            Each quantile must be a symmetric positive definite matrix. 
        %(_doc_default_callparams)s 
 
        Returns 
        ------- 
        pdf : ndarray 
            Log of the probability density function evaluated at `x` 
 
        Notes 
        ----- 
        %(_doc_callparams_note)s 
 
        &quot;&quot;&quot;</span>
        <span class="s1">dim</span><span class="s2">, </span><span class="s1">df</span><span class="s2">, </span><span class="s1">scale = self._process_parameters(df</span><span class="s2">, </span><span class="s1">scale)</span>
        <span class="s1">x = self._process_quantiles(x</span><span class="s2">, </span><span class="s1">dim)</span>
        <span class="s1">_</span><span class="s2">, </span><span class="s1">log_det_scale = self._cholesky_logdet(scale)</span>
        <span class="s1">out = self._logpdf(x</span><span class="s2">, </span><span class="s1">dim</span><span class="s2">, </span><span class="s1">df</span><span class="s2">, </span><span class="s1">scale</span><span class="s2">, </span><span class="s1">log_det_scale)</span>
        <span class="s2">return </span><span class="s1">_squeeze_output(out)</span>

    <span class="s2">def </span><span class="s1">pdf(self</span><span class="s2">, </span><span class="s1">x</span><span class="s2">, </span><span class="s1">df</span><span class="s2">, </span><span class="s1">scale):</span>
        <span class="s5">&quot;&quot;&quot;Inverse Wishart probability density function. 
 
        Parameters 
        ---------- 
        x : array_like 
            Quantiles, with the last axis of `x` denoting the components. 
            Each quantile must be a symmetric positive definite matrix. 
        %(_doc_default_callparams)s 
 
        Returns 
        ------- 
        pdf : ndarray 
            Probability density function evaluated at `x` 
 
        Notes 
        ----- 
        %(_doc_callparams_note)s 
 
        &quot;&quot;&quot;</span>
        <span class="s2">return </span><span class="s1">np.exp(self.logpdf(x</span><span class="s2">, </span><span class="s1">df</span><span class="s2">, </span><span class="s1">scale))</span>

    <span class="s2">def </span><span class="s1">_mean(self</span><span class="s2">, </span><span class="s1">dim</span><span class="s2">, </span><span class="s1">df</span><span class="s2">, </span><span class="s1">scale):</span>
        <span class="s5">&quot;&quot;&quot;Mean of the inverse Wishart distribution. 
 
        Parameters 
        ---------- 
        dim : int 
            Dimension of the scale matrix 
        %(_doc_default_callparams)s 
 
        Notes 
        ----- 
        As this function does no argument checking, it should not be 
        called directly; use 'mean' instead. 
 
        &quot;&quot;&quot;</span>
        <span class="s2">if </span><span class="s1">df &gt; dim + </span><span class="s4">1</span><span class="s1">:</span>
            <span class="s1">out = scale / (df - dim - </span><span class="s4">1</span><span class="s1">)</span>
        <span class="s2">else</span><span class="s1">:</span>
            <span class="s1">out = </span><span class="s2">None</span>
        <span class="s2">return </span><span class="s1">out</span>

    <span class="s2">def </span><span class="s1">mean(self</span><span class="s2">, </span><span class="s1">df</span><span class="s2">, </span><span class="s1">scale):</span>
        <span class="s5">&quot;&quot;&quot;Mean of the inverse Wishart distribution. 
 
        Only valid if the degrees of freedom are greater than the dimension of 
        the scale matrix plus one. 
 
        Parameters 
        ---------- 
        %(_doc_default_callparams)s 
 
        Returns 
        ------- 
        mean : float or None 
            The mean of the distribution 
 
        &quot;&quot;&quot;</span>
        <span class="s1">dim</span><span class="s2">, </span><span class="s1">df</span><span class="s2">, </span><span class="s1">scale = self._process_parameters(df</span><span class="s2">, </span><span class="s1">scale)</span>
        <span class="s1">out = self._mean(dim</span><span class="s2">, </span><span class="s1">df</span><span class="s2">, </span><span class="s1">scale)</span>
        <span class="s2">return </span><span class="s1">_squeeze_output(out) </span><span class="s2">if </span><span class="s1">out </span><span class="s2">is not None else </span><span class="s1">out</span>

    <span class="s2">def </span><span class="s1">_mode(self</span><span class="s2">, </span><span class="s1">dim</span><span class="s2">, </span><span class="s1">df</span><span class="s2">, </span><span class="s1">scale):</span>
        <span class="s5">&quot;&quot;&quot;Mode of the inverse Wishart distribution. 
 
        Parameters 
        ---------- 
        dim : int 
            Dimension of the scale matrix 
        %(_doc_default_callparams)s 
 
        Notes 
        ----- 
        As this function does no argument checking, it should not be 
        called directly; use 'mode' instead. 
 
        &quot;&quot;&quot;</span>
        <span class="s2">return </span><span class="s1">scale / (df + dim + </span><span class="s4">1</span><span class="s1">)</span>

    <span class="s2">def </span><span class="s1">mode(self</span><span class="s2">, </span><span class="s1">df</span><span class="s2">, </span><span class="s1">scale):</span>
        <span class="s5">&quot;&quot;&quot;Mode of the inverse Wishart distribution. 
 
        Parameters 
        ---------- 
        %(_doc_default_callparams)s 
 
        Returns 
        ------- 
        mode : float 
            The Mode of the distribution 
 
        &quot;&quot;&quot;</span>
        <span class="s1">dim</span><span class="s2">, </span><span class="s1">df</span><span class="s2">, </span><span class="s1">scale = self._process_parameters(df</span><span class="s2">, </span><span class="s1">scale)</span>
        <span class="s1">out = self._mode(dim</span><span class="s2">, </span><span class="s1">df</span><span class="s2">, </span><span class="s1">scale)</span>
        <span class="s2">return </span><span class="s1">_squeeze_output(out)</span>

    <span class="s2">def </span><span class="s1">_var(self</span><span class="s2">, </span><span class="s1">dim</span><span class="s2">, </span><span class="s1">df</span><span class="s2">, </span><span class="s1">scale):</span>
        <span class="s5">&quot;&quot;&quot;Variance of the inverse Wishart distribution. 
 
        Parameters 
        ---------- 
        dim : int 
            Dimension of the scale matrix 
        %(_doc_default_callparams)s 
 
        Notes 
        ----- 
        As this function does no argument checking, it should not be 
        called directly; use 'var' instead. 
 
        &quot;&quot;&quot;</span>
        <span class="s2">if </span><span class="s1">df &gt; dim + </span><span class="s4">3</span><span class="s1">:</span>
            <span class="s1">var = (df - dim + </span><span class="s4">1</span><span class="s1">) * scale**</span><span class="s4">2</span>
            <span class="s1">diag = scale.diagonal()  </span><span class="s0"># 1 x dim array</span>
            <span class="s1">var += (df - dim - </span><span class="s4">1</span><span class="s1">) * np.outer(diag</span><span class="s2">, </span><span class="s1">diag)</span>
            <span class="s1">var /= (df - dim) * (df - dim - </span><span class="s4">1</span><span class="s1">)**</span><span class="s4">2 </span><span class="s1">* (df - dim - </span><span class="s4">3</span><span class="s1">)</span>
        <span class="s2">else</span><span class="s1">:</span>
            <span class="s1">var = </span><span class="s2">None</span>
        <span class="s2">return </span><span class="s1">var</span>

    <span class="s2">def </span><span class="s1">var(self</span><span class="s2">, </span><span class="s1">df</span><span class="s2">, </span><span class="s1">scale):</span>
        <span class="s5">&quot;&quot;&quot;Variance of the inverse Wishart distribution. 
 
        Only valid if the degrees of freedom are greater than the dimension of 
        the scale matrix plus three. 
 
        Parameters 
        ---------- 
        %(_doc_default_callparams)s 
 
        Returns 
        ------- 
        var : float 
            The variance of the distribution 
        &quot;&quot;&quot;</span>
        <span class="s1">dim</span><span class="s2">, </span><span class="s1">df</span><span class="s2">, </span><span class="s1">scale = self._process_parameters(df</span><span class="s2">, </span><span class="s1">scale)</span>
        <span class="s1">out = self._var(dim</span><span class="s2">, </span><span class="s1">df</span><span class="s2">, </span><span class="s1">scale)</span>
        <span class="s2">return </span><span class="s1">_squeeze_output(out) </span><span class="s2">if </span><span class="s1">out </span><span class="s2">is not None else </span><span class="s1">out</span>

    <span class="s2">def </span><span class="s1">_rvs(self</span><span class="s2">, </span><span class="s1">n</span><span class="s2">, </span><span class="s1">shape</span><span class="s2">, </span><span class="s1">dim</span><span class="s2">, </span><span class="s1">df</span><span class="s2">, </span><span class="s1">C</span><span class="s2">, </span><span class="s1">random_state):</span>
        <span class="s5">&quot;&quot;&quot;Draw random samples from an inverse Wishart distribution. 
 
        Parameters 
        ---------- 
        n : integer 
            Number of variates to generate 
        shape : iterable 
            Shape of the variates to generate 
        dim : int 
            Dimension of the scale matrix 
        df : int 
            Degrees of freedom 
        C : ndarray 
            Cholesky factorization of the scale matrix, lower triagular. 
        %(_doc_random_state)s 
 
        Notes 
        ----- 
        As this function does no argument checking, it should not be 
        called directly; use 'rvs' instead. 
 
        &quot;&quot;&quot;</span>
        <span class="s1">random_state = self._get_random_state(random_state)</span>
        <span class="s0"># Get random draws A such that A ~ W(df, I)</span>
        <span class="s1">A = super()._standard_rvs(n</span><span class="s2">, </span><span class="s1">shape</span><span class="s2">, </span><span class="s1">dim</span><span class="s2">, </span><span class="s1">df</span><span class="s2">, </span><span class="s1">random_state)</span>

        <span class="s0"># Calculate SA = (CA)'^{-1} (CA)^{-1} ~ iW(df, scale)</span>
        <span class="s1">eye = np.eye(dim)</span>
        <span class="s1">trtrs = get_lapack_funcs((</span><span class="s3">'trtrs'</span><span class="s1">)</span><span class="s2">, </span><span class="s1">(A</span><span class="s2">,</span><span class="s1">))</span>

        <span class="s2">for </span><span class="s1">index </span><span class="s2">in </span><span class="s1">np.ndindex(A.shape[:-</span><span class="s4">2</span><span class="s1">]):</span>
            <span class="s0"># Calculate CA</span>
            <span class="s1">CA = np.dot(C</span><span class="s2">, </span><span class="s1">A[index])</span>
            <span class="s0"># Get (C A)^{-1} via triangular solver</span>
            <span class="s2">if </span><span class="s1">dim &gt; </span><span class="s4">1</span><span class="s1">:</span>
                <span class="s1">CA</span><span class="s2">, </span><span class="s1">info = trtrs(CA</span><span class="s2">, </span><span class="s1">eye</span><span class="s2">, </span><span class="s1">lower=</span><span class="s2">True</span><span class="s1">)</span>
                <span class="s2">if </span><span class="s1">info &gt; </span><span class="s4">0</span><span class="s1">:</span>
                    <span class="s2">raise </span><span class="s1">LinAlgError(</span><span class="s3">&quot;Singular matrix.&quot;</span><span class="s1">)</span>
                <span class="s2">if </span><span class="s1">info &lt; </span><span class="s4">0</span><span class="s1">:</span>
                    <span class="s2">raise </span><span class="s1">ValueError(</span><span class="s3">'Illegal value in %d-th argument of'</span>
                                     <span class="s3">' internal trtrs' </span><span class="s1">% -info)</span>
            <span class="s2">else</span><span class="s1">:</span>
                <span class="s1">CA = </span><span class="s4">1. </span><span class="s1">/ CA</span>
            <span class="s0"># Get SA</span>
            <span class="s1">A[index] = np.dot(CA.T</span><span class="s2">, </span><span class="s1">CA)</span>

        <span class="s2">return </span><span class="s1">A</span>

    <span class="s2">def </span><span class="s1">rvs(self</span><span class="s2">, </span><span class="s1">df</span><span class="s2">, </span><span class="s1">scale</span><span class="s2">, </span><span class="s1">size=</span><span class="s4">1</span><span class="s2">, </span><span class="s1">random_state=</span><span class="s2">None</span><span class="s1">):</span>
        <span class="s5">&quot;&quot;&quot;Draw random samples from an inverse Wishart distribution. 
 
        Parameters 
        ---------- 
        %(_doc_default_callparams)s 
        size : integer or iterable of integers, optional 
            Number of samples to draw (default 1). 
        %(_doc_random_state)s 
 
        Returns 
        ------- 
        rvs : ndarray 
            Random variates of shape (`size`) + (`dim`, `dim), where `dim` is 
            the dimension of the scale matrix. 
 
        Notes 
        ----- 
        %(_doc_callparams_note)s 
 
        &quot;&quot;&quot;</span>
        <span class="s1">n</span><span class="s2">, </span><span class="s1">shape = self._process_size(size)</span>
        <span class="s1">dim</span><span class="s2">, </span><span class="s1">df</span><span class="s2">, </span><span class="s1">scale = self._process_parameters(df</span><span class="s2">, </span><span class="s1">scale)</span>

        <span class="s0"># Invert the scale</span>
        <span class="s1">eye = np.eye(dim)</span>
        <span class="s1">L</span><span class="s2">, </span><span class="s1">lower = scipy.linalg.cho_factor(scale</span><span class="s2">, </span><span class="s1">lower=</span><span class="s2">True</span><span class="s1">)</span>
        <span class="s1">inv_scale = scipy.linalg.cho_solve((L</span><span class="s2">, </span><span class="s1">lower)</span><span class="s2">, </span><span class="s1">eye)</span>
        <span class="s0"># Cholesky decomposition of inverted scale</span>
        <span class="s1">C = scipy.linalg.cholesky(inv_scale</span><span class="s2">, </span><span class="s1">lower=</span><span class="s2">True</span><span class="s1">)</span>

        <span class="s1">out = self._rvs(n</span><span class="s2">, </span><span class="s1">shape</span><span class="s2">, </span><span class="s1">dim</span><span class="s2">, </span><span class="s1">df</span><span class="s2">, </span><span class="s1">C</span><span class="s2">, </span><span class="s1">random_state)</span>

        <span class="s2">return </span><span class="s1">_squeeze_output(out)</span>

    <span class="s2">def </span><span class="s1">_entropy(self</span><span class="s2">, </span><span class="s1">dim</span><span class="s2">, </span><span class="s1">df</span><span class="s2">, </span><span class="s1">log_det_scale):</span>
        <span class="s0"># reference: eq. (17) from ref. 3</span>
        <span class="s1">psi_eval_points = [</span><span class="s4">0.5 </span><span class="s1">* (df - dim + i) </span><span class="s2">for </span><span class="s1">i </span><span class="s2">in </span><span class="s1">range(</span><span class="s4">1</span><span class="s2">, </span><span class="s1">dim + </span><span class="s4">1</span><span class="s1">)]</span>
        <span class="s1">psi_eval_points = np.asarray(psi_eval_points)</span>
        <span class="s2">return </span><span class="s1">multigammaln(</span><span class="s4">0.5 </span><span class="s1">* df</span><span class="s2">, </span><span class="s1">dim) + </span><span class="s4">0.5 </span><span class="s1">* dim * df + \</span>
            <span class="s4">0.5 </span><span class="s1">* (dim + </span><span class="s4">1</span><span class="s1">) * (log_det_scale - _LOG_2) - \</span>
            <span class="s4">0.5 </span><span class="s1">* (df + dim + </span><span class="s4">1</span><span class="s1">) * \</span>
            <span class="s1">psi(psi_eval_points</span><span class="s2">, </span><span class="s1">out=psi_eval_points).sum()</span>

    <span class="s2">def </span><span class="s1">entropy(self</span><span class="s2">, </span><span class="s1">df</span><span class="s2">, </span><span class="s1">scale):</span>
        <span class="s1">dim</span><span class="s2">, </span><span class="s1">df</span><span class="s2">, </span><span class="s1">scale = self._process_parameters(df</span><span class="s2">, </span><span class="s1">scale)</span>
        <span class="s1">_</span><span class="s2">, </span><span class="s1">log_det_scale = self._cholesky_logdet(scale)</span>
        <span class="s2">return </span><span class="s1">self._entropy(dim</span><span class="s2">, </span><span class="s1">df</span><span class="s2">, </span><span class="s1">log_det_scale)</span>


<span class="s1">invwishart = invwishart_gen()</span>


<span class="s2">class </span><span class="s1">invwishart_frozen(multi_rv_frozen):</span>
    <span class="s2">def </span><span class="s1">__init__(self</span><span class="s2">, </span><span class="s1">df</span><span class="s2">, </span><span class="s1">scale</span><span class="s2">, </span><span class="s1">seed=</span><span class="s2">None</span><span class="s1">):</span>
        <span class="s5">&quot;&quot;&quot;Create a frozen inverse Wishart distribution. 
 
        Parameters 
        ---------- 
        df : array_like 
            Degrees of freedom of the distribution 
        scale : array_like 
            Scale matrix of the distribution 
        seed : {None, int, `numpy.random.Generator`}, optional 
            If `seed` is None the `numpy.random.Generator` singleton is used. 
            If `seed` is an int, a new ``Generator`` instance is used, 
            seeded with `seed`. 
            If `seed` is already a ``Generator`` instance then that instance is 
            used. 
 
        &quot;&quot;&quot;</span>
        <span class="s1">self._dist = invwishart_gen(seed)</span>
        <span class="s1">self.dim</span><span class="s2">, </span><span class="s1">self.df</span><span class="s2">, </span><span class="s1">self.scale = self._dist._process_parameters(</span>
            <span class="s1">df</span><span class="s2">, </span><span class="s1">scale</span>
        <span class="s1">)</span>

        <span class="s0"># Get the determinant via Cholesky factorization</span>
        <span class="s1">C</span><span class="s2">, </span><span class="s1">lower = scipy.linalg.cho_factor(self.scale</span><span class="s2">, </span><span class="s1">lower=</span><span class="s2">True</span><span class="s1">)</span>
        <span class="s1">self.log_det_scale = </span><span class="s4">2 </span><span class="s1">* np.sum(np.log(C.diagonal()))</span>

        <span class="s0"># Get the inverse using the Cholesky factorization</span>
        <span class="s1">eye = np.eye(self.dim)</span>
        <span class="s1">self.inv_scale = scipy.linalg.cho_solve((C</span><span class="s2">, </span><span class="s1">lower)</span><span class="s2">, </span><span class="s1">eye)</span>

        <span class="s0"># Get the Cholesky factorization of the inverse scale</span>
        <span class="s1">self.C = scipy.linalg.cholesky(self.inv_scale</span><span class="s2">, </span><span class="s1">lower=</span><span class="s2">True</span><span class="s1">)</span>

    <span class="s2">def </span><span class="s1">logpdf(self</span><span class="s2">, </span><span class="s1">x):</span>
        <span class="s1">x = self._dist._process_quantiles(x</span><span class="s2">, </span><span class="s1">self.dim)</span>
        <span class="s1">out = self._dist._logpdf(x</span><span class="s2">, </span><span class="s1">self.dim</span><span class="s2">, </span><span class="s1">self.df</span><span class="s2">, </span><span class="s1">self.scale</span><span class="s2">,</span>
                                 <span class="s1">self.log_det_scale)</span>
        <span class="s2">return </span><span class="s1">_squeeze_output(out)</span>

    <span class="s2">def </span><span class="s1">pdf(self</span><span class="s2">, </span><span class="s1">x):</span>
        <span class="s2">return </span><span class="s1">np.exp(self.logpdf(x))</span>

    <span class="s2">def </span><span class="s1">mean(self):</span>
        <span class="s1">out = self._dist._mean(self.dim</span><span class="s2">, </span><span class="s1">self.df</span><span class="s2">, </span><span class="s1">self.scale)</span>
        <span class="s2">return </span><span class="s1">_squeeze_output(out) </span><span class="s2">if </span><span class="s1">out </span><span class="s2">is not None else </span><span class="s1">out</span>

    <span class="s2">def </span><span class="s1">mode(self):</span>
        <span class="s1">out = self._dist._mode(self.dim</span><span class="s2">, </span><span class="s1">self.df</span><span class="s2">, </span><span class="s1">self.scale)</span>
        <span class="s2">return </span><span class="s1">_squeeze_output(out)</span>

    <span class="s2">def </span><span class="s1">var(self):</span>
        <span class="s1">out = self._dist._var(self.dim</span><span class="s2">, </span><span class="s1">self.df</span><span class="s2">, </span><span class="s1">self.scale)</span>
        <span class="s2">return </span><span class="s1">_squeeze_output(out) </span><span class="s2">if </span><span class="s1">out </span><span class="s2">is not None else </span><span class="s1">out</span>

    <span class="s2">def </span><span class="s1">rvs(self</span><span class="s2">, </span><span class="s1">size=</span><span class="s4">1</span><span class="s2">, </span><span class="s1">random_state=</span><span class="s2">None</span><span class="s1">):</span>
        <span class="s1">n</span><span class="s2">, </span><span class="s1">shape = self._dist._process_size(size)</span>

        <span class="s1">out = self._dist._rvs(n</span><span class="s2">, </span><span class="s1">shape</span><span class="s2">, </span><span class="s1">self.dim</span><span class="s2">, </span><span class="s1">self.df</span><span class="s2">,</span>
                              <span class="s1">self.C</span><span class="s2">, </span><span class="s1">random_state)</span>

        <span class="s2">return </span><span class="s1">_squeeze_output(out)</span>

    <span class="s2">def </span><span class="s1">entropy(self):</span>
        <span class="s2">return </span><span class="s1">self._dist._entropy(self.dim</span><span class="s2">, </span><span class="s1">self.df</span><span class="s2">, </span><span class="s1">self.log_det_scale)</span>


<span class="s0"># Set frozen generator docstrings from corresponding docstrings in</span>
<span class="s0"># inverse Wishart and fill in default strings in class docstrings</span>
<span class="s2">for </span><span class="s1">name </span><span class="s2">in </span><span class="s1">[</span><span class="s3">'logpdf'</span><span class="s2">, </span><span class="s3">'pdf'</span><span class="s2">, </span><span class="s3">'mean'</span><span class="s2">, </span><span class="s3">'mode'</span><span class="s2">, </span><span class="s3">'var'</span><span class="s2">, </span><span class="s3">'rvs'</span><span class="s1">]:</span>
    <span class="s1">method = invwishart_gen.__dict__[name]</span>
    <span class="s1">method_frozen = wishart_frozen.__dict__[name]</span>
    <span class="s1">method_frozen.__doc__ = doccer.docformat(</span>
        <span class="s1">method.__doc__</span><span class="s2">, </span><span class="s1">wishart_docdict_noparams)</span>
    <span class="s1">method.__doc__ = doccer.docformat(method.__doc__</span><span class="s2">, </span><span class="s1">wishart_docdict_params)</span>

<span class="s1">_multinomial_doc_default_callparams = </span><span class="s3">&quot;&quot;&quot;</span><span class="s2">\ 
</span><span class="s3">n : int 
    Number of trials 
p : array_like 
    Probability of a trial falling into each category; should sum to 1 
&quot;&quot;&quot;</span>

<span class="s1">_multinomial_doc_callparams_note = </span><span class="s3">&quot;&quot;&quot;</span><span class="s2">\ 
</span><span class="s3">`n` should be a nonnegative integer. Each element of `p` should be in the 
interval :math:`[0,1]` and the elements should sum to 1. If they do not sum to 
1, the last element of the `p` array is not used and is replaced with the 
remaining probability left over from the earlier elements. 
&quot;&quot;&quot;</span>

<span class="s1">_multinomial_doc_frozen_callparams = </span><span class="s3">&quot;&quot;</span>

<span class="s1">_multinomial_doc_frozen_callparams_note = </span><span class="s3">&quot;&quot;&quot;</span><span class="s2">\ 
</span><span class="s3">See class definition for a detailed description of parameters.&quot;&quot;&quot;</span>

<span class="s1">multinomial_docdict_params = {</span>
    <span class="s3">'_doc_default_callparams'</span><span class="s1">: _multinomial_doc_default_callparams</span><span class="s2">,</span>
    <span class="s3">'_doc_callparams_note'</span><span class="s1">: _multinomial_doc_callparams_note</span><span class="s2">,</span>
    <span class="s3">'_doc_random_state'</span><span class="s1">: _doc_random_state</span>
<span class="s1">}</span>

<span class="s1">multinomial_docdict_noparams = {</span>
    <span class="s3">'_doc_default_callparams'</span><span class="s1">: _multinomial_doc_frozen_callparams</span><span class="s2">,</span>
    <span class="s3">'_doc_callparams_note'</span><span class="s1">: _multinomial_doc_frozen_callparams_note</span><span class="s2">,</span>
    <span class="s3">'_doc_random_state'</span><span class="s1">: _doc_random_state</span>
<span class="s1">}</span>


<span class="s2">class </span><span class="s1">multinomial_gen(multi_rv_generic):</span>
    <span class="s3">r&quot;&quot;&quot;A multinomial random variable. 
 
    Methods 
    ------- 
    pmf(x, n, p) 
        Probability mass function. 
    logpmf(x, n, p) 
        Log of the probability mass function. 
    rvs(n, p, size=1, random_state=None) 
        Draw random samples from a multinomial distribution. 
    entropy(n, p) 
        Compute the entropy of the multinomial distribution. 
    cov(n, p) 
        Compute the covariance matrix of the multinomial distribution. 
 
    Parameters 
    ---------- 
    %(_doc_default_callparams)s 
    %(_doc_random_state)s 
 
    Notes 
    ----- 
    %(_doc_callparams_note)s 
 
    The probability mass function for `multinomial` is 
 
    .. math:: 
 
        f(x) = \frac{n!}{x_1! \cdots x_k!} p_1^{x_1} \cdots p_k^{x_k}, 
 
    supported on :math:`x=(x_1, \ldots, x_k)` where each :math:`x_i` is a 
    nonnegative integer and their sum is :math:`n`. 
 
    .. versionadded:: 0.19.0 
 
    Examples 
    -------- 
 
    &gt;&gt;&gt; from scipy.stats import multinomial 
    &gt;&gt;&gt; rv = multinomial(8, [0.3, 0.2, 0.5]) 
    &gt;&gt;&gt; rv.pmf([1, 3, 4]) 
    0.042000000000000072 
 
    The multinomial distribution for :math:`k=2` is identical to the 
    corresponding binomial distribution (tiny numerical differences 
    notwithstanding): 
 
    &gt;&gt;&gt; from scipy.stats import binom 
    &gt;&gt;&gt; multinomial.pmf([3, 4], n=7, p=[0.4, 0.6]) 
    0.29030399999999973 
    &gt;&gt;&gt; binom.pmf(3, 7, 0.4) 
    0.29030400000000012 
 
    The functions ``pmf``, ``logpmf``, ``entropy``, and ``cov`` support 
    broadcasting, under the convention that the vector parameters (``x`` and 
    ``p``) are interpreted as if each row along the last axis is a single 
    object. For instance: 
 
    &gt;&gt;&gt; multinomial.pmf([[3, 4], [3, 5]], n=[7, 8], p=[.3, .7]) 
    array([0.2268945,  0.25412184]) 
 
    Here, ``x.shape == (2, 2)``, ``n.shape == (2,)``, and ``p.shape == (2,)``, 
    but following the rules mentioned above they behave as if the rows 
    ``[3, 4]`` and ``[3, 5]`` in ``x`` and ``[.3, .7]`` in ``p`` were a single 
    object, and as if we had ``x.shape = (2,)``, ``n.shape = (2,)``, and 
    ``p.shape = ()``. To obtain the individual elements without broadcasting, 
    we would do this: 
 
    &gt;&gt;&gt; multinomial.pmf([3, 4], n=7, p=[.3, .7]) 
    0.2268945 
    &gt;&gt;&gt; multinomial.pmf([3, 5], 8, p=[.3, .7]) 
    0.25412184 
 
    This broadcasting also works for ``cov``, where the output objects are 
    square matrices of size ``p.shape[-1]``. For example: 
 
    &gt;&gt;&gt; multinomial.cov([4, 5], [[.3, .7], [.4, .6]]) 
    array([[[ 0.84, -0.84], 
            [-0.84,  0.84]], 
           [[ 1.2 , -1.2 ], 
            [-1.2 ,  1.2 ]]]) 
 
    In this example, ``n.shape == (2,)`` and ``p.shape == (2, 2)``, and 
    following the rules above, these broadcast as if ``p.shape == (2,)``. 
    Thus the result should also be of shape ``(2,)``, but since each output is 
    a :math:`2 \times 2` matrix, the result in fact has shape ``(2, 2, 2)``, 
    where ``result[0]`` is equal to ``multinomial.cov(n=4, p=[.3, .7])`` and 
    ``result[1]`` is equal to ``multinomial.cov(n=5, p=[.4, .6])``. 
 
    Alternatively, the object may be called (as a function) to fix the `n` and 
    `p` parameters, returning a &quot;frozen&quot; multinomial random variable: 
 
    &gt;&gt;&gt; rv = multinomial(n=7, p=[.3, .7]) 
    &gt;&gt;&gt; # Frozen object with the same methods but holding the given 
    &gt;&gt;&gt; # degrees of freedom and scale fixed. 
 
    See also 
    -------- 
    scipy.stats.binom : The binomial distribution. 
    numpy.random.Generator.multinomial : Sampling from the multinomial distribution. 
    scipy.stats.multivariate_hypergeom : 
        The multivariate hypergeometric distribution. 
    &quot;&quot;&quot;  </span><span class="s0"># noqa: E501</span>

    <span class="s2">def </span><span class="s1">__init__(self</span><span class="s2">, </span><span class="s1">seed=</span><span class="s2">None</span><span class="s1">):</span>
        <span class="s1">super().__init__(seed)</span>
        <span class="s1">self.__doc__ = \</span>
            <span class="s1">doccer.docformat(self.__doc__</span><span class="s2">, </span><span class="s1">multinomial_docdict_params)</span>

    <span class="s2">def </span><span class="s1">__call__(self</span><span class="s2">, </span><span class="s1">n</span><span class="s2">, </span><span class="s1">p</span><span class="s2">, </span><span class="s1">seed=</span><span class="s2">None</span><span class="s1">):</span>
        <span class="s5">&quot;&quot;&quot;Create a frozen multinomial distribution. 
 
        See `multinomial_frozen` for more information. 
        &quot;&quot;&quot;</span>
        <span class="s2">return </span><span class="s1">multinomial_frozen(n</span><span class="s2">, </span><span class="s1">p</span><span class="s2">, </span><span class="s1">seed)</span>

    <span class="s2">def </span><span class="s1">_process_parameters(self</span><span class="s2">, </span><span class="s1">n</span><span class="s2">, </span><span class="s1">p</span><span class="s2">, </span><span class="s1">eps=</span><span class="s4">1e-15</span><span class="s1">):</span>
        <span class="s5">&quot;&quot;&quot;Returns: n_, p_, npcond. 
 
        n_ and p_ are arrays of the correct shape; npcond is a boolean array 
        flagging values out of the domain. 
        &quot;&quot;&quot;</span>
        <span class="s1">p = np.array(p</span><span class="s2">, </span><span class="s1">dtype=np.float64</span><span class="s2">, </span><span class="s1">copy=</span><span class="s2">True</span><span class="s1">)</span>
        <span class="s1">p_adjusted = </span><span class="s4">1. </span><span class="s1">- p[...</span><span class="s2">, </span><span class="s1">:-</span><span class="s4">1</span><span class="s1">].sum(axis=-</span><span class="s4">1</span><span class="s1">)</span>
        <span class="s1">i_adjusted = np.abs(p_adjusted) &gt; eps</span>
        <span class="s1">p[i_adjusted</span><span class="s2">, </span><span class="s1">-</span><span class="s4">1</span><span class="s1">] = p_adjusted[i_adjusted]</span>

        <span class="s0"># true for bad p</span>
        <span class="s1">pcond = np.any(p &lt; </span><span class="s4">0</span><span class="s2">, </span><span class="s1">axis=-</span><span class="s4">1</span><span class="s1">)</span>
        <span class="s1">pcond |= np.any(p &gt; </span><span class="s4">1</span><span class="s2">, </span><span class="s1">axis=-</span><span class="s4">1</span><span class="s1">)</span>

        <span class="s1">n = np.array(n</span><span class="s2">, </span><span class="s1">dtype=np.int_</span><span class="s2">, </span><span class="s1">copy=</span><span class="s2">True</span><span class="s1">)</span>

        <span class="s0"># true for bad n</span>
        <span class="s1">ncond = n &lt; </span><span class="s4">0</span>

        <span class="s2">return </span><span class="s1">n</span><span class="s2">, </span><span class="s1">p</span><span class="s2">, </span><span class="s1">ncond | pcond</span>

    <span class="s2">def </span><span class="s1">_process_quantiles(self</span><span class="s2">, </span><span class="s1">x</span><span class="s2">, </span><span class="s1">n</span><span class="s2">, </span><span class="s1">p):</span>
        <span class="s5">&quot;&quot;&quot;Returns: x_, xcond. 
 
        x_ is an int array; xcond is a boolean array flagging values out of the 
        domain. 
        &quot;&quot;&quot;</span>
        <span class="s1">xx = np.asarray(x</span><span class="s2">, </span><span class="s1">dtype=np.int_)</span>

        <span class="s2">if </span><span class="s1">xx.ndim == </span><span class="s4">0</span><span class="s1">:</span>
            <span class="s2">raise </span><span class="s1">ValueError(</span><span class="s3">&quot;x must be an array.&quot;</span><span class="s1">)</span>

        <span class="s2">if </span><span class="s1">xx.size != </span><span class="s4">0 </span><span class="s2">and not </span><span class="s1">xx.shape[-</span><span class="s4">1</span><span class="s1">] == p.shape[-</span><span class="s4">1</span><span class="s1">]:</span>
            <span class="s2">raise </span><span class="s1">ValueError(</span><span class="s3">&quot;Size of each quantile should be size of p: &quot;</span>
                             <span class="s3">&quot;received %d, but expected %d.&quot; </span><span class="s1">%</span>
                             <span class="s1">(xx.shape[-</span><span class="s4">1</span><span class="s1">]</span><span class="s2">, </span><span class="s1">p.shape[-</span><span class="s4">1</span><span class="s1">]))</span>

        <span class="s0"># true for x out of the domain</span>
        <span class="s1">cond = np.any(xx != x</span><span class="s2">, </span><span class="s1">axis=-</span><span class="s4">1</span><span class="s1">)</span>
        <span class="s1">cond |= np.any(xx &lt; </span><span class="s4">0</span><span class="s2">, </span><span class="s1">axis=-</span><span class="s4">1</span><span class="s1">)</span>
        <span class="s1">cond = cond | (np.sum(xx</span><span class="s2">, </span><span class="s1">axis=-</span><span class="s4">1</span><span class="s1">) != n)</span>

        <span class="s2">return </span><span class="s1">xx</span><span class="s2">, </span><span class="s1">cond</span>

    <span class="s2">def </span><span class="s1">_checkresult(self</span><span class="s2">, </span><span class="s1">result</span><span class="s2">, </span><span class="s1">cond</span><span class="s2">, </span><span class="s1">bad_value):</span>
        <span class="s1">result = np.asarray(result)</span>

        <span class="s2">if </span><span class="s1">cond.ndim != </span><span class="s4">0</span><span class="s1">:</span>
            <span class="s1">result[cond] = bad_value</span>
        <span class="s2">elif </span><span class="s1">cond:</span>
            <span class="s2">if </span><span class="s1">result.ndim == </span><span class="s4">0</span><span class="s1">:</span>
                <span class="s2">return </span><span class="s1">bad_value</span>
            <span class="s1">result[...] = bad_value</span>
        <span class="s2">return </span><span class="s1">result</span>

    <span class="s2">def </span><span class="s1">_logpmf(self</span><span class="s2">, </span><span class="s1">x</span><span class="s2">, </span><span class="s1">n</span><span class="s2">, </span><span class="s1">p):</span>
        <span class="s2">return </span><span class="s1">gammaln(n+</span><span class="s4">1</span><span class="s1">) + np.sum(xlogy(x</span><span class="s2">, </span><span class="s1">p) - gammaln(x+</span><span class="s4">1</span><span class="s1">)</span><span class="s2">, </span><span class="s1">axis=-</span><span class="s4">1</span><span class="s1">)</span>

    <span class="s2">def </span><span class="s1">logpmf(self</span><span class="s2">, </span><span class="s1">x</span><span class="s2">, </span><span class="s1">n</span><span class="s2">, </span><span class="s1">p):</span>
        <span class="s5">&quot;&quot;&quot;Log of the Multinomial probability mass function. 
 
        Parameters 
        ---------- 
        x : array_like 
            Quantiles, with the last axis of `x` denoting the components. 
        %(_doc_default_callparams)s 
 
        Returns 
        ------- 
        logpmf : ndarray or scalar 
            Log of the probability mass function evaluated at `x` 
 
        Notes 
        ----- 
        %(_doc_callparams_note)s 
        &quot;&quot;&quot;</span>
        <span class="s1">n</span><span class="s2">, </span><span class="s1">p</span><span class="s2">, </span><span class="s1">npcond = self._process_parameters(n</span><span class="s2">, </span><span class="s1">p)</span>
        <span class="s1">x</span><span class="s2">, </span><span class="s1">xcond = self._process_quantiles(x</span><span class="s2">, </span><span class="s1">n</span><span class="s2">, </span><span class="s1">p)</span>

        <span class="s1">result = self._logpmf(x</span><span class="s2">, </span><span class="s1">n</span><span class="s2">, </span><span class="s1">p)</span>

        <span class="s0"># replace values for which x was out of the domain; broadcast</span>
        <span class="s0"># xcond to the right shape</span>
        <span class="s1">xcond_ = xcond | np.zeros(npcond.shape</span><span class="s2">, </span><span class="s1">dtype=np.bool_)</span>
        <span class="s1">result = self._checkresult(result</span><span class="s2">, </span><span class="s1">xcond_</span><span class="s2">, </span><span class="s1">-np.inf)</span>

        <span class="s0"># replace values bad for n or p; broadcast npcond to the right shape</span>
        <span class="s1">npcond_ = npcond | np.zeros(xcond.shape</span><span class="s2">, </span><span class="s1">dtype=np.bool_)</span>
        <span class="s2">return </span><span class="s1">self._checkresult(result</span><span class="s2">, </span><span class="s1">npcond_</span><span class="s2">, </span><span class="s1">np.nan)</span>

    <span class="s2">def </span><span class="s1">pmf(self</span><span class="s2">, </span><span class="s1">x</span><span class="s2">, </span><span class="s1">n</span><span class="s2">, </span><span class="s1">p):</span>
        <span class="s5">&quot;&quot;&quot;Multinomial probability mass function. 
 
        Parameters 
        ---------- 
        x : array_like 
            Quantiles, with the last axis of `x` denoting the components. 
        %(_doc_default_callparams)s 
 
        Returns 
        ------- 
        pmf : ndarray or scalar 
            Probability density function evaluated at `x` 
 
        Notes 
        ----- 
        %(_doc_callparams_note)s 
        &quot;&quot;&quot;</span>
        <span class="s2">return </span><span class="s1">np.exp(self.logpmf(x</span><span class="s2">, </span><span class="s1">n</span><span class="s2">, </span><span class="s1">p))</span>

    <span class="s2">def </span><span class="s1">mean(self</span><span class="s2">, </span><span class="s1">n</span><span class="s2">, </span><span class="s1">p):</span>
        <span class="s5">&quot;&quot;&quot;Mean of the Multinomial distribution. 
 
        Parameters 
        ---------- 
        %(_doc_default_callparams)s 
 
        Returns 
        ------- 
        mean : float 
            The mean of the distribution 
        &quot;&quot;&quot;</span>
        <span class="s1">n</span><span class="s2">, </span><span class="s1">p</span><span class="s2">, </span><span class="s1">npcond = self._process_parameters(n</span><span class="s2">, </span><span class="s1">p)</span>
        <span class="s1">result = n[...</span><span class="s2">, </span><span class="s1">np.newaxis]*p</span>
        <span class="s2">return </span><span class="s1">self._checkresult(result</span><span class="s2">, </span><span class="s1">npcond</span><span class="s2">, </span><span class="s1">np.nan)</span>

    <span class="s2">def </span><span class="s1">cov(self</span><span class="s2">, </span><span class="s1">n</span><span class="s2">, </span><span class="s1">p):</span>
        <span class="s5">&quot;&quot;&quot;Covariance matrix of the multinomial distribution. 
 
        Parameters 
        ---------- 
        %(_doc_default_callparams)s 
 
        Returns 
        ------- 
        cov : ndarray 
            The covariance matrix of the distribution 
        &quot;&quot;&quot;</span>
        <span class="s1">n</span><span class="s2">, </span><span class="s1">p</span><span class="s2">, </span><span class="s1">npcond = self._process_parameters(n</span><span class="s2">, </span><span class="s1">p)</span>

        <span class="s1">nn = n[...</span><span class="s2">, </span><span class="s1">np.newaxis</span><span class="s2">, </span><span class="s1">np.newaxis]</span>
        <span class="s1">result = nn * np.einsum(</span><span class="s3">'...j,...k-&gt;...jk'</span><span class="s2">, </span><span class="s1">-p</span><span class="s2">, </span><span class="s1">p)</span>

        <span class="s0"># change the diagonal</span>
        <span class="s2">for </span><span class="s1">i </span><span class="s2">in </span><span class="s1">range(p.shape[-</span><span class="s4">1</span><span class="s1">]):</span>
            <span class="s1">result[...</span><span class="s2">, </span><span class="s1">i</span><span class="s2">, </span><span class="s1">i] += n*p[...</span><span class="s2">, </span><span class="s1">i]</span>

        <span class="s2">return </span><span class="s1">self._checkresult(result</span><span class="s2">, </span><span class="s1">npcond</span><span class="s2">, </span><span class="s1">np.nan)</span>

    <span class="s2">def </span><span class="s1">entropy(self</span><span class="s2">, </span><span class="s1">n</span><span class="s2">, </span><span class="s1">p):</span>
        <span class="s5">r&quot;&quot;&quot;Compute the entropy of the multinomial distribution. 
 
        The entropy is computed using this expression: 
 
        .. math:: 
 
            f(x) = - \log n! - n\sum_{i=1}^k p_i \log p_i + 
            \sum_{i=1}^k \sum_{x=0}^n \binom n x p_i^x(1-p_i)^{n-x} \log x! 
 
        Parameters 
        ---------- 
        %(_doc_default_callparams)s 
 
        Returns 
        ------- 
        h : scalar 
            Entropy of the Multinomial distribution 
 
        Notes 
        ----- 
        %(_doc_callparams_note)s 
        &quot;&quot;&quot;</span>
        <span class="s1">n</span><span class="s2">, </span><span class="s1">p</span><span class="s2">, </span><span class="s1">npcond = self._process_parameters(n</span><span class="s2">, </span><span class="s1">p)</span>

        <span class="s1">x = np.r_[</span><span class="s4">1</span><span class="s1">:np.max(n)+</span><span class="s4">1</span><span class="s1">]</span>

        <span class="s1">term1 = n*np.sum(entr(p)</span><span class="s2">, </span><span class="s1">axis=-</span><span class="s4">1</span><span class="s1">)</span>
        <span class="s1">term1 -= gammaln(n+</span><span class="s4">1</span><span class="s1">)</span>

        <span class="s1">n = n[...</span><span class="s2">, </span><span class="s1">np.newaxis]</span>
        <span class="s1">new_axes_needed = max(p.ndim</span><span class="s2">, </span><span class="s1">n.ndim) - x.ndim + </span><span class="s4">1</span>
        <span class="s1">x.shape += (</span><span class="s4">1</span><span class="s2">,</span><span class="s1">)*new_axes_needed</span>

        <span class="s1">term2 = np.sum(binom.pmf(x</span><span class="s2">, </span><span class="s1">n</span><span class="s2">, </span><span class="s1">p)*gammaln(x+</span><span class="s4">1</span><span class="s1">)</span><span class="s2">,</span>
                       <span class="s1">axis=(-</span><span class="s4">1</span><span class="s2">, </span><span class="s1">-</span><span class="s4">1</span><span class="s1">-new_axes_needed))</span>

        <span class="s2">return </span><span class="s1">self._checkresult(term1 + term2</span><span class="s2">, </span><span class="s1">npcond</span><span class="s2">, </span><span class="s1">np.nan)</span>

    <span class="s2">def </span><span class="s1">rvs(self</span><span class="s2">, </span><span class="s1">n</span><span class="s2">, </span><span class="s1">p</span><span class="s2">, </span><span class="s1">size=</span><span class="s2">None, </span><span class="s1">random_state=</span><span class="s2">None</span><span class="s1">):</span>
        <span class="s5">&quot;&quot;&quot;Draw random samples from a Multinomial distribution. 
 
        Parameters 
        ---------- 
        %(_doc_default_callparams)s 
        size : integer or iterable of integers, optional 
            Number of samples to draw (default 1). 
        %(_doc_random_state)s 
 
        Returns 
        ------- 
        rvs : ndarray or scalar 
            Random variates of shape (`size`, `len(p)`) 
 
        Notes 
        ----- 
        %(_doc_callparams_note)s 
        &quot;&quot;&quot;</span>
        <span class="s1">n</span><span class="s2">, </span><span class="s1">p</span><span class="s2">, </span><span class="s1">npcond = self._process_parameters(n</span><span class="s2">, </span><span class="s1">p)</span>
        <span class="s1">random_state = self._get_random_state(random_state)</span>
        <span class="s2">return </span><span class="s1">random_state.multinomial(n</span><span class="s2">, </span><span class="s1">p</span><span class="s2">, </span><span class="s1">size)</span>


<span class="s1">multinomial = multinomial_gen()</span>


<span class="s2">class </span><span class="s1">multinomial_frozen(multi_rv_frozen):</span>
    <span class="s5">r&quot;&quot;&quot;Create a frozen Multinomial distribution. 
 
    Parameters 
    ---------- 
    n : int 
        number of trials 
    p: array_like 
        probability of a trial falling into each category; should sum to 1 
    seed : {None, int, `numpy.random.Generator`, `numpy.random.RandomState`}, optional 
        If `seed` is None (or `np.random`), the `numpy.random.RandomState` 
        singleton is used. 
        If `seed` is an int, a new ``RandomState`` instance is used, 
        seeded with `seed`. 
        If `seed` is already a ``Generator`` or ``RandomState`` instance then 
        that instance is used. 
    &quot;&quot;&quot;</span>
    <span class="s2">def </span><span class="s1">__init__(self</span><span class="s2">, </span><span class="s1">n</span><span class="s2">, </span><span class="s1">p</span><span class="s2">, </span><span class="s1">seed=</span><span class="s2">None</span><span class="s1">):</span>
        <span class="s1">self._dist = multinomial_gen(seed)</span>
        <span class="s1">self.n</span><span class="s2">, </span><span class="s1">self.p</span><span class="s2">, </span><span class="s1">self.npcond = self._dist._process_parameters(n</span><span class="s2">, </span><span class="s1">p)</span>

        <span class="s0"># monkey patch self._dist</span>
        <span class="s2">def </span><span class="s1">_process_parameters(n</span><span class="s2">, </span><span class="s1">p):</span>
            <span class="s2">return </span><span class="s1">self.n</span><span class="s2">, </span><span class="s1">self.p</span><span class="s2">, </span><span class="s1">self.npcond</span>

        <span class="s1">self._dist._process_parameters = _process_parameters</span>

    <span class="s2">def </span><span class="s1">logpmf(self</span><span class="s2">, </span><span class="s1">x):</span>
        <span class="s2">return </span><span class="s1">self._dist.logpmf(x</span><span class="s2">, </span><span class="s1">self.n</span><span class="s2">, </span><span class="s1">self.p)</span>

    <span class="s2">def </span><span class="s1">pmf(self</span><span class="s2">, </span><span class="s1">x):</span>
        <span class="s2">return </span><span class="s1">self._dist.pmf(x</span><span class="s2">, </span><span class="s1">self.n</span><span class="s2">, </span><span class="s1">self.p)</span>

    <span class="s2">def </span><span class="s1">mean(self):</span>
        <span class="s2">return </span><span class="s1">self._dist.mean(self.n</span><span class="s2">, </span><span class="s1">self.p)</span>

    <span class="s2">def </span><span class="s1">cov(self):</span>
        <span class="s2">return </span><span class="s1">self._dist.cov(self.n</span><span class="s2">, </span><span class="s1">self.p)</span>

    <span class="s2">def </span><span class="s1">entropy(self):</span>
        <span class="s2">return </span><span class="s1">self._dist.entropy(self.n</span><span class="s2">, </span><span class="s1">self.p)</span>

    <span class="s2">def </span><span class="s1">rvs(self</span><span class="s2">, </span><span class="s1">size=</span><span class="s4">1</span><span class="s2">, </span><span class="s1">random_state=</span><span class="s2">None</span><span class="s1">):</span>
        <span class="s2">return </span><span class="s1">self._dist.rvs(self.n</span><span class="s2">, </span><span class="s1">self.p</span><span class="s2">, </span><span class="s1">size</span><span class="s2">, </span><span class="s1">random_state)</span>


<span class="s0"># Set frozen generator docstrings from corresponding docstrings in</span>
<span class="s0"># multinomial and fill in default strings in class docstrings</span>
<span class="s2">for </span><span class="s1">name </span><span class="s2">in </span><span class="s1">[</span><span class="s3">'logpmf'</span><span class="s2">, </span><span class="s3">'pmf'</span><span class="s2">, </span><span class="s3">'mean'</span><span class="s2">, </span><span class="s3">'cov'</span><span class="s2">, </span><span class="s3">'rvs'</span><span class="s1">]:</span>
    <span class="s1">method = multinomial_gen.__dict__[name]</span>
    <span class="s1">method_frozen = multinomial_frozen.__dict__[name]</span>
    <span class="s1">method_frozen.__doc__ = doccer.docformat(</span>
        <span class="s1">method.__doc__</span><span class="s2">, </span><span class="s1">multinomial_docdict_noparams)</span>
    <span class="s1">method.__doc__ = doccer.docformat(method.__doc__</span><span class="s2">,</span>
                                      <span class="s1">multinomial_docdict_params)</span>


<span class="s2">class </span><span class="s1">special_ortho_group_gen(multi_rv_generic):</span>
    <span class="s5">r&quot;&quot;&quot;A Special Orthogonal matrix (SO(N)) random variable. 
 
    Return a random rotation matrix, drawn from the Haar distribution 
    (the only uniform distribution on SO(N)) with a determinant of +1. 
 
    The `dim` keyword specifies the dimension N. 
 
    Methods 
    ------- 
    rvs(dim=None, size=1, random_state=None) 
        Draw random samples from SO(N). 
 
    Parameters 
    ---------- 
    dim : scalar 
        Dimension of matrices 
    seed : {None, int, np.random.RandomState, np.random.Generator}, optional 
        Used for drawing random variates. 
        If `seed` is `None`, the `~np.random.RandomState` singleton is used. 
        If `seed` is an int, a new ``RandomState`` instance is used, seeded 
        with seed. 
        If `seed` is already a ``RandomState`` or ``Generator`` instance, 
        then that object is used. 
        Default is `None`. 
 
    Notes 
    ----- 
    This class is wrapping the random_rot code from the MDP Toolkit, 
    https://github.com/mdp-toolkit/mdp-toolkit 
 
    Return a random rotation matrix, drawn from the Haar distribution 
    (the only uniform distribution on SO(N)). 
    The algorithm is described in the paper 
    Stewart, G.W., &quot;The efficient generation of random orthogonal 
    matrices with an application to condition estimators&quot;, SIAM Journal 
    on Numerical Analysis, 17(3), pp. 403-409, 1980. 
    For more information see 
    https://en.wikipedia.org/wiki/Orthogonal_matrix#Randomization 
 
    See also the similar `ortho_group`. For a random rotation in three 
    dimensions, see `scipy.spatial.transform.Rotation.random`. 
 
    Examples 
    -------- 
    &gt;&gt;&gt; import numpy as np 
    &gt;&gt;&gt; from scipy.stats import special_ortho_group 
    &gt;&gt;&gt; x = special_ortho_group.rvs(3) 
 
    &gt;&gt;&gt; np.dot(x, x.T) 
    array([[  1.00000000e+00,   1.13231364e-17,  -2.86852790e-16], 
           [  1.13231364e-17,   1.00000000e+00,  -1.46845020e-16], 
           [ -2.86852790e-16,  -1.46845020e-16,   1.00000000e+00]]) 
 
    &gt;&gt;&gt; import scipy.linalg 
    &gt;&gt;&gt; scipy.linalg.det(x) 
    1.0 
 
    This generates one random matrix from SO(3). It is orthogonal and 
    has a determinant of 1. 
 
    Alternatively, the object may be called (as a function) to fix the `dim` 
    parameter, returning a &quot;frozen&quot; special_ortho_group random variable: 
 
    &gt;&gt;&gt; rv = special_ortho_group(5) 
    &gt;&gt;&gt; # Frozen object with the same methods but holding the 
    &gt;&gt;&gt; # dimension parameter fixed. 
 
    See Also 
    -------- 
    ortho_group, scipy.spatial.transform.Rotation.random 
 
    &quot;&quot;&quot;</span>

    <span class="s2">def </span><span class="s1">__init__(self</span><span class="s2">, </span><span class="s1">seed=</span><span class="s2">None</span><span class="s1">):</span>
        <span class="s1">super().__init__(seed)</span>
        <span class="s1">self.__doc__ = doccer.docformat(self.__doc__)</span>

    <span class="s2">def </span><span class="s1">__call__(self</span><span class="s2">, </span><span class="s1">dim=</span><span class="s2">None, </span><span class="s1">seed=</span><span class="s2">None</span><span class="s1">):</span>
        <span class="s5">&quot;&quot;&quot;Create a frozen SO(N) distribution. 
 
        See `special_ortho_group_frozen` for more information. 
        &quot;&quot;&quot;</span>
        <span class="s2">return </span><span class="s1">special_ortho_group_frozen(dim</span><span class="s2">, </span><span class="s1">seed=seed)</span>

    <span class="s2">def </span><span class="s1">_process_parameters(self</span><span class="s2">, </span><span class="s1">dim):</span>
        <span class="s5">&quot;&quot;&quot;Dimension N must be specified; it cannot be inferred.&quot;&quot;&quot;</span>
        <span class="s2">if </span><span class="s1">dim </span><span class="s2">is None or not </span><span class="s1">np.isscalar(dim) </span><span class="s2">or </span><span class="s1">dim &lt;= </span><span class="s4">1 </span><span class="s2">or </span><span class="s1">dim != int(dim):</span>
            <span class="s2">raise </span><span class="s1">ValueError(</span><span class="s3">&quot;&quot;&quot;Dimension of rotation must be specified, 
                                and must be a scalar greater than 1.&quot;&quot;&quot;</span><span class="s1">)</span>

        <span class="s2">return </span><span class="s1">dim</span>

    <span class="s2">def </span><span class="s1">rvs(self</span><span class="s2">, </span><span class="s1">dim</span><span class="s2">, </span><span class="s1">size=</span><span class="s4">1</span><span class="s2">, </span><span class="s1">random_state=</span><span class="s2">None</span><span class="s1">):</span>
        <span class="s5">&quot;&quot;&quot;Draw random samples from SO(N). 
 
        Parameters 
        ---------- 
        dim : integer 
            Dimension of rotation space (N). 
        size : integer, optional 
            Number of samples to draw (default 1). 
 
        Returns 
        ------- 
        rvs : ndarray or scalar 
            Random size N-dimensional matrices, dimension (size, dim, dim) 
 
        &quot;&quot;&quot;</span>
        <span class="s1">random_state = self._get_random_state(random_state)</span>

        <span class="s1">size = int(size)</span>
        <span class="s1">size = (size</span><span class="s2">,</span><span class="s1">) </span><span class="s2">if </span><span class="s1">size &gt; </span><span class="s4">1 </span><span class="s2">else </span><span class="s1">()</span>

        <span class="s1">dim = self._process_parameters(dim)</span>

        <span class="s0"># H represents a (dim, dim) matrix, while D represents the diagonal of</span>
        <span class="s0"># a (dim, dim) diagonal matrix. The algorithm that follows is</span>
        <span class="s0"># broadcasted on the leading shape in `size` to vectorize along</span>
        <span class="s0"># samples.</span>
        <span class="s1">H = np.empty(size + (dim</span><span class="s2">, </span><span class="s1">dim))</span>
        <span class="s1">H[...</span><span class="s2">, </span><span class="s1">:</span><span class="s2">, </span><span class="s1">:] = np.eye(dim)</span>
        <span class="s1">D = np.empty(size + (dim</span><span class="s2">,</span><span class="s1">))</span>

        <span class="s2">for </span><span class="s1">n </span><span class="s2">in </span><span class="s1">range(dim-</span><span class="s4">1</span><span class="s1">):</span>

            <span class="s0"># x is a vector with length dim-n, xrow and xcol are views of it as</span>
            <span class="s0"># a row vector and column vector respectively. It's important they</span>
            <span class="s0"># are views and not copies because we are going to modify x</span>
            <span class="s0"># in-place.</span>
            <span class="s1">x = random_state.normal(size=size + (dim-n</span><span class="s2">,</span><span class="s1">))</span>
            <span class="s1">xrow = x[...</span><span class="s2">, None, </span><span class="s1">:]</span>
            <span class="s1">xcol = x[...</span><span class="s2">, </span><span class="s1">:</span><span class="s2">, None</span><span class="s1">]</span>

            <span class="s0"># This is the squared norm of x, without vectorization it would be</span>
            <span class="s0"># dot(x, x), to have proper broadcasting we use matmul and squeeze</span>
            <span class="s0"># out (convert to scalar) the resulting 1x1 matrix</span>
            <span class="s1">norm2 = np.matmul(xrow</span><span class="s2">, </span><span class="s1">xcol).squeeze((-</span><span class="s4">2</span><span class="s2">, </span><span class="s1">-</span><span class="s4">1</span><span class="s1">))</span>

            <span class="s1">x0 = x[...</span><span class="s2">, </span><span class="s4">0</span><span class="s1">].copy()</span>
            <span class="s1">D[...</span><span class="s2">, </span><span class="s1">n] = np.where(x0 != </span><span class="s4">0</span><span class="s2">, </span><span class="s1">np.sign(x0)</span><span class="s2">, </span><span class="s4">1</span><span class="s1">)</span>
            <span class="s1">x[...</span><span class="s2">, </span><span class="s4">0</span><span class="s1">] += D[...</span><span class="s2">, </span><span class="s1">n]*np.sqrt(norm2)</span>

            <span class="s0"># In renormalizing x we have to append an additional axis with</span>
            <span class="s0"># [..., None] to broadcast the scalar against the vector x</span>
            <span class="s1">x /= np.sqrt((norm2 - x0**</span><span class="s4">2 </span><span class="s1">+ x[...</span><span class="s2">, </span><span class="s4">0</span><span class="s1">]**</span><span class="s4">2</span><span class="s1">) / </span><span class="s4">2.</span><span class="s1">)[...</span><span class="s2">, None</span><span class="s1">]</span>

            <span class="s0"># Householder transformation, without vectorization the RHS can be</span>
            <span class="s0"># written as outer(H @ x, x) (apart from the slicing)</span>
            <span class="s1">H[...</span><span class="s2">, </span><span class="s1">:</span><span class="s2">, </span><span class="s1">n:] -= np.matmul(H[...</span><span class="s2">, </span><span class="s1">:</span><span class="s2">, </span><span class="s1">n:]</span><span class="s2">, </span><span class="s1">xcol) * xrow</span>

        <span class="s1">D[...</span><span class="s2">, </span><span class="s1">-</span><span class="s4">1</span><span class="s1">] = (-</span><span class="s4">1</span><span class="s1">)**(dim-</span><span class="s4">1</span><span class="s1">)*D[...</span><span class="s2">, </span><span class="s1">:-</span><span class="s4">1</span><span class="s1">].prod(axis=-</span><span class="s4">1</span><span class="s1">)</span>

        <span class="s0"># Without vectorization this could be written as H = diag(D) @ H,</span>
        <span class="s0"># left-multiplication by a diagonal matrix amounts to multiplying each</span>
        <span class="s0"># row of H by an element of the diagonal, so we add a dummy axis for</span>
        <span class="s0"># the column index</span>
        <span class="s1">H *= D[...</span><span class="s2">, </span><span class="s1">:</span><span class="s2">, None</span><span class="s1">]</span>
        <span class="s2">return </span><span class="s1">H</span>


<span class="s1">special_ortho_group = special_ortho_group_gen()</span>


<span class="s2">class </span><span class="s1">special_ortho_group_frozen(multi_rv_frozen):</span>
    <span class="s2">def </span><span class="s1">__init__(self</span><span class="s2">, </span><span class="s1">dim=</span><span class="s2">None, </span><span class="s1">seed=</span><span class="s2">None</span><span class="s1">):</span>
        <span class="s5">&quot;&quot;&quot;Create a frozen SO(N) distribution. 
 
        Parameters 
        ---------- 
        dim : scalar 
            Dimension of matrices 
        seed : {None, int, `numpy.random.Generator`, `numpy.random.RandomState`}, optional 
            If `seed` is None (or `np.random`), the `numpy.random.RandomState` 
            singleton is used. 
            If `seed` is an int, a new ``RandomState`` instance is used, 
            seeded with `seed`. 
            If `seed` is already a ``Generator`` or ``RandomState`` instance 
            then that instance is used. 
 
        Examples 
        -------- 
        &gt;&gt;&gt; from scipy.stats import special_ortho_group 
        &gt;&gt;&gt; g = special_ortho_group(5) 
        &gt;&gt;&gt; x = g.rvs() 
 
        &quot;&quot;&quot;</span>
        <span class="s1">self._dist = special_ortho_group_gen(seed)</span>
        <span class="s1">self.dim = self._dist._process_parameters(dim)</span>

    <span class="s2">def </span><span class="s1">rvs(self</span><span class="s2">, </span><span class="s1">size=</span><span class="s4">1</span><span class="s2">, </span><span class="s1">random_state=</span><span class="s2">None</span><span class="s1">):</span>
        <span class="s2">return </span><span class="s1">self._dist.rvs(self.dim</span><span class="s2">, </span><span class="s1">size</span><span class="s2">, </span><span class="s1">random_state)</span>


<span class="s2">class </span><span class="s1">ortho_group_gen(multi_rv_generic):</span>
    <span class="s5">r&quot;&quot;&quot;An Orthogonal matrix (O(N)) random variable. 
 
    Return a random orthogonal matrix, drawn from the O(N) Haar 
    distribution (the only uniform distribution on O(N)). 
 
    The `dim` keyword specifies the dimension N. 
 
    Methods 
    ------- 
    rvs(dim=None, size=1, random_state=None) 
        Draw random samples from O(N). 
 
    Parameters 
    ---------- 
    dim : scalar 
        Dimension of matrices 
    seed : {None, int, np.random.RandomState, np.random.Generator}, optional 
        Used for drawing random variates. 
        If `seed` is `None`, the `~np.random.RandomState` singleton is used. 
        If `seed` is an int, a new ``RandomState`` instance is used, seeded 
        with seed. 
        If `seed` is already a ``RandomState`` or ``Generator`` instance, 
        then that object is used. 
        Default is `None`. 
 
    Notes 
    ----- 
    This class is closely related to `special_ortho_group`. 
 
    Some care is taken to avoid numerical error, as per the paper by Mezzadri. 
 
    References 
    ---------- 
    .. [1] F. Mezzadri, &quot;How to generate random matrices from the classical 
           compact groups&quot;, :arXiv:`math-ph/0609050v2`. 
 
    Examples 
    -------- 
    &gt;&gt;&gt; import numpy as np 
    &gt;&gt;&gt; from scipy.stats import ortho_group 
    &gt;&gt;&gt; x = ortho_group.rvs(3) 
 
    &gt;&gt;&gt; np.dot(x, x.T) 
    array([[  1.00000000e+00,   1.13231364e-17,  -2.86852790e-16], 
           [  1.13231364e-17,   1.00000000e+00,  -1.46845020e-16], 
           [ -2.86852790e-16,  -1.46845020e-16,   1.00000000e+00]]) 
 
    &gt;&gt;&gt; import scipy.linalg 
    &gt;&gt;&gt; np.fabs(scipy.linalg.det(x)) 
    1.0 
 
    This generates one random matrix from O(3). It is orthogonal and 
    has a determinant of +1 or -1. 
 
    Alternatively, the object may be called (as a function) to fix the `dim` 
    parameter, returning a &quot;frozen&quot; ortho_group random variable: 
 
    &gt;&gt;&gt; rv = ortho_group(5) 
    &gt;&gt;&gt; # Frozen object with the same methods but holding the 
    &gt;&gt;&gt; # dimension parameter fixed. 
 
    See Also 
    -------- 
    special_ortho_group 
    &quot;&quot;&quot;</span>

    <span class="s2">def </span><span class="s1">__init__(self</span><span class="s2">, </span><span class="s1">seed=</span><span class="s2">None</span><span class="s1">):</span>
        <span class="s1">super().__init__(seed)</span>
        <span class="s1">self.__doc__ = doccer.docformat(self.__doc__)</span>

    <span class="s2">def </span><span class="s1">__call__(self</span><span class="s2">, </span><span class="s1">dim=</span><span class="s2">None, </span><span class="s1">seed=</span><span class="s2">None</span><span class="s1">):</span>
        <span class="s5">&quot;&quot;&quot;Create a frozen O(N) distribution. 
 
        See `ortho_group_frozen` for more information. 
        &quot;&quot;&quot;</span>
        <span class="s2">return </span><span class="s1">ortho_group_frozen(dim</span><span class="s2">, </span><span class="s1">seed=seed)</span>

    <span class="s2">def </span><span class="s1">_process_parameters(self</span><span class="s2">, </span><span class="s1">dim):</span>
        <span class="s5">&quot;&quot;&quot;Dimension N must be specified; it cannot be inferred.&quot;&quot;&quot;</span>
        <span class="s2">if </span><span class="s1">dim </span><span class="s2">is None or not </span><span class="s1">np.isscalar(dim) </span><span class="s2">or </span><span class="s1">dim &lt;= </span><span class="s4">1 </span><span class="s2">or </span><span class="s1">dim != int(dim):</span>
            <span class="s2">raise </span><span class="s1">ValueError(</span><span class="s3">&quot;Dimension of rotation must be specified,&quot;</span>
                             <span class="s3">&quot;and must be a scalar greater than 1.&quot;</span><span class="s1">)</span>

        <span class="s2">return </span><span class="s1">dim</span>

    <span class="s2">def </span><span class="s1">rvs(self</span><span class="s2">, </span><span class="s1">dim</span><span class="s2">, </span><span class="s1">size=</span><span class="s4">1</span><span class="s2">, </span><span class="s1">random_state=</span><span class="s2">None</span><span class="s1">):</span>
        <span class="s5">&quot;&quot;&quot;Draw random samples from O(N). 
 
        Parameters 
        ---------- 
        dim : integer 
            Dimension of rotation space (N). 
        size : integer, optional 
            Number of samples to draw (default 1). 
 
        Returns 
        ------- 
        rvs : ndarray or scalar 
            Random size N-dimensional matrices, dimension (size, dim, dim) 
 
        &quot;&quot;&quot;</span>
        <span class="s1">random_state = self._get_random_state(random_state)</span>

        <span class="s1">size = int(size)</span>
        <span class="s2">if </span><span class="s1">size &gt; </span><span class="s4">1 </span><span class="s2">and </span><span class="s1">NumpyVersion(np.__version__) &lt; </span><span class="s3">'1.22.0'</span><span class="s1">:</span>
            <span class="s2">return </span><span class="s1">np.array([self.rvs(dim</span><span class="s2">, </span><span class="s1">size=</span><span class="s4">1</span><span class="s2">, </span><span class="s1">random_state=random_state)</span>
                             <span class="s2">for </span><span class="s1">i </span><span class="s2">in </span><span class="s1">range(size)])</span>

        <span class="s1">dim = self._process_parameters(dim)</span>

        <span class="s1">size = (size</span><span class="s2">,</span><span class="s1">) </span><span class="s2">if </span><span class="s1">size &gt; </span><span class="s4">1 </span><span class="s2">else </span><span class="s1">()</span>
        <span class="s1">z = random_state.normal(size=size + (dim</span><span class="s2">, </span><span class="s1">dim))</span>
        <span class="s1">q</span><span class="s2">, </span><span class="s1">r = np.linalg.qr(z)</span>
        <span class="s0"># The last two dimensions are the rows and columns of R matrices.</span>
        <span class="s0"># Extract the diagonals. Note that this eliminates a dimension.</span>
        <span class="s1">d = r.diagonal(offset=</span><span class="s4">0</span><span class="s2">, </span><span class="s1">axis1=-</span><span class="s4">2</span><span class="s2">, </span><span class="s1">axis2=-</span><span class="s4">1</span><span class="s1">)</span>
        <span class="s0"># Add back a dimension for proper broadcasting: we're dividing</span>
        <span class="s0"># each row of each R matrix by the diagonal of the R matrix.</span>
        <span class="s1">q *= (d/abs(d))[...</span><span class="s2">, </span><span class="s1">np.newaxis</span><span class="s2">, </span><span class="s1">:]  </span><span class="s0"># to broadcast properly</span>
        <span class="s2">return </span><span class="s1">q</span>


<span class="s1">ortho_group = ortho_group_gen()</span>


<span class="s2">class </span><span class="s1">ortho_group_frozen(multi_rv_frozen):</span>
    <span class="s2">def </span><span class="s1">__init__(self</span><span class="s2">, </span><span class="s1">dim=</span><span class="s2">None, </span><span class="s1">seed=</span><span class="s2">None</span><span class="s1">):</span>
        <span class="s5">&quot;&quot;&quot;Create a frozen O(N) distribution. 
 
        Parameters 
        ---------- 
        dim : scalar 
            Dimension of matrices 
        seed : {None, int, `numpy.random.Generator`, `numpy.random.RandomState`}, optional 
            If `seed` is None (or `np.random`), the `numpy.random.RandomState` 
            singleton is used. 
            If `seed` is an int, a new ``RandomState`` instance is used, 
            seeded with `seed`. 
            If `seed` is already a ``Generator`` or ``RandomState`` instance 
            then that instance is used. 
 
        Examples 
        -------- 
        &gt;&gt;&gt; from scipy.stats import ortho_group 
        &gt;&gt;&gt; g = ortho_group(5) 
        &gt;&gt;&gt; x = g.rvs() 
 
        &quot;&quot;&quot;</span>
        <span class="s1">self._dist = ortho_group_gen(seed)</span>
        <span class="s1">self.dim = self._dist._process_parameters(dim)</span>

    <span class="s2">def </span><span class="s1">rvs(self</span><span class="s2">, </span><span class="s1">size=</span><span class="s4">1</span><span class="s2">, </span><span class="s1">random_state=</span><span class="s2">None</span><span class="s1">):</span>
        <span class="s2">return </span><span class="s1">self._dist.rvs(self.dim</span><span class="s2">, </span><span class="s1">size</span><span class="s2">, </span><span class="s1">random_state)</span>


<span class="s2">class </span><span class="s1">random_correlation_gen(multi_rv_generic):</span>
    <span class="s5">r&quot;&quot;&quot;A random correlation matrix. 
 
    Return a random correlation matrix, given a vector of eigenvalues. 
 
    The `eigs` keyword specifies the eigenvalues of the correlation matrix, 
    and implies the dimension. 
 
    Methods 
    ------- 
    rvs(eigs=None, random_state=None) 
        Draw random correlation matrices, all with eigenvalues eigs. 
 
    Parameters 
    ---------- 
    eigs : 1d ndarray 
        Eigenvalues of correlation matrix 
    seed : {None, int, `numpy.random.Generator`, `numpy.random.RandomState`}, optional 
        If `seed` is None (or `np.random`), the `numpy.random.RandomState` 
        singleton is used. 
        If `seed` is an int, a new ``RandomState`` instance is used, 
        seeded with `seed`. 
        If `seed` is already a ``Generator`` or ``RandomState`` instance 
        then that instance is used. 
    tol : float, optional 
        Tolerance for input parameter checks 
    diag_tol : float, optional 
        Tolerance for deviation of the diagonal of the resulting 
        matrix. Default: 1e-7 
 
    Raises 
    ------ 
    RuntimeError 
        Floating point error prevented generating a valid correlation 
        matrix. 
 
    Returns 
    ------- 
    rvs : ndarray or scalar 
        Random size N-dimensional matrices, dimension (size, dim, dim), 
        each having eigenvalues eigs. 
 
    Notes 
    ----- 
 
    Generates a random correlation matrix following a numerically stable 
    algorithm spelled out by Davies &amp; Higham. This algorithm uses a single O(N) 
    similarity transformation to construct a symmetric positive semi-definite 
    matrix, and applies a series of Givens rotations to scale it to have ones 
    on the diagonal. 
 
    References 
    ---------- 
 
    .. [1] Davies, Philip I; Higham, Nicholas J; &quot;Numerically stable generation 
           of correlation matrices and their factors&quot;, BIT 2000, Vol. 40, 
           No. 4, pp. 640 651 
 
    Examples 
    -------- 
    &gt;&gt;&gt; import numpy as np 
    &gt;&gt;&gt; from scipy.stats import random_correlation 
    &gt;&gt;&gt; rng = np.random.default_rng() 
    &gt;&gt;&gt; x = random_correlation.rvs((.5, .8, 1.2, 1.5), random_state=rng) 
    &gt;&gt;&gt; x 
    array([[ 1.        , -0.02423399,  0.03130519,  0.4946965 ], 
           [-0.02423399,  1.        ,  0.20334736,  0.04039817], 
           [ 0.03130519,  0.20334736,  1.        ,  0.02694275], 
           [ 0.4946965 ,  0.04039817,  0.02694275,  1.        ]]) 
    &gt;&gt;&gt; import scipy.linalg 
    &gt;&gt;&gt; e, v = scipy.linalg.eigh(x) 
    &gt;&gt;&gt; e 
    array([ 0.5,  0.8,  1.2,  1.5]) 
 
    &quot;&quot;&quot;</span>

    <span class="s2">def </span><span class="s1">__init__(self</span><span class="s2">, </span><span class="s1">seed=</span><span class="s2">None</span><span class="s1">):</span>
        <span class="s1">super().__init__(seed)</span>
        <span class="s1">self.__doc__ = doccer.docformat(self.__doc__)</span>

    <span class="s2">def </span><span class="s1">__call__(self</span><span class="s2">, </span><span class="s1">eigs</span><span class="s2">, </span><span class="s1">seed=</span><span class="s2">None, </span><span class="s1">tol=</span><span class="s4">1e-13</span><span class="s2">, </span><span class="s1">diag_tol=</span><span class="s4">1e-7</span><span class="s1">):</span>
        <span class="s5">&quot;&quot;&quot;Create a frozen random correlation matrix. 
 
        See `random_correlation_frozen` for more information. 
        &quot;&quot;&quot;</span>
        <span class="s2">return </span><span class="s1">random_correlation_frozen(eigs</span><span class="s2">, </span><span class="s1">seed=seed</span><span class="s2">, </span><span class="s1">tol=tol</span><span class="s2">,</span>
                                         <span class="s1">diag_tol=diag_tol)</span>

    <span class="s2">def </span><span class="s1">_process_parameters(self</span><span class="s2">, </span><span class="s1">eigs</span><span class="s2">, </span><span class="s1">tol):</span>
        <span class="s1">eigs = np.asarray(eigs</span><span class="s2">, </span><span class="s1">dtype=float)</span>
        <span class="s1">dim = eigs.size</span>

        <span class="s2">if </span><span class="s1">eigs.ndim != </span><span class="s4">1 </span><span class="s2">or </span><span class="s1">eigs.shape[</span><span class="s4">0</span><span class="s1">] != dim </span><span class="s2">or </span><span class="s1">dim &lt;= </span><span class="s4">1</span><span class="s1">:</span>
            <span class="s2">raise </span><span class="s1">ValueError(</span><span class="s3">&quot;Array 'eigs' must be a vector of length &quot;</span>
                             <span class="s3">&quot;greater than 1.&quot;</span><span class="s1">)</span>

        <span class="s2">if </span><span class="s1">np.fabs(np.sum(eigs) - dim) &gt; tol:</span>
            <span class="s2">raise </span><span class="s1">ValueError(</span><span class="s3">&quot;Sum of eigenvalues must equal dimensionality.&quot;</span><span class="s1">)</span>

        <span class="s2">for </span><span class="s1">x </span><span class="s2">in </span><span class="s1">eigs:</span>
            <span class="s2">if </span><span class="s1">x &lt; -tol:</span>
                <span class="s2">raise </span><span class="s1">ValueError(</span><span class="s3">&quot;All eigenvalues must be non-negative.&quot;</span><span class="s1">)</span>

        <span class="s2">return </span><span class="s1">dim</span><span class="s2">, </span><span class="s1">eigs</span>

    <span class="s2">def </span><span class="s1">_givens_to_1(self</span><span class="s2">, </span><span class="s1">aii</span><span class="s2">, </span><span class="s1">ajj</span><span class="s2">, </span><span class="s1">aij):</span>
        <span class="s5">&quot;&quot;&quot;Computes a 2x2 Givens matrix to put 1's on the diagonal. 
 
        The input matrix is a 2x2 symmetric matrix M = [ aii aij ; aij ajj ]. 
 
        The output matrix g is a 2x2 anti-symmetric matrix of the form 
        [ c s ; -s c ];  the elements c and s are returned. 
 
        Applying the output matrix to the input matrix (as b=g.T M g) 
        results in a matrix with bii=1, provided tr(M) - det(M) &gt;= 1 
        and floating point issues do not occur. Otherwise, some other 
        valid rotation is returned. When tr(M)==2, also bjj=1. 
 
        &quot;&quot;&quot;</span>
        <span class="s1">aiid = aii - </span><span class="s4">1.</span>
        <span class="s1">ajjd = ajj - </span><span class="s4">1.</span>

        <span class="s2">if </span><span class="s1">ajjd == </span><span class="s4">0</span><span class="s1">:</span>
            <span class="s0"># ajj==1, so swap aii and ajj to avoid division by zero</span>
            <span class="s2">return </span><span class="s4">0.</span><span class="s2">, </span><span class="s4">1.</span>

        <span class="s1">dd = math.sqrt(max(aij**</span><span class="s4">2 </span><span class="s1">- aiid*ajjd</span><span class="s2">, </span><span class="s4">0</span><span class="s1">))</span>

        <span class="s0"># The choice of t should be chosen to avoid cancellation [1]</span>
        <span class="s1">t = (aij + math.copysign(dd</span><span class="s2">, </span><span class="s1">aij)) / ajjd</span>
        <span class="s1">c = </span><span class="s4">1. </span><span class="s1">/ math.sqrt(</span><span class="s4">1. </span><span class="s1">+ t*t)</span>
        <span class="s2">if </span><span class="s1">c == </span><span class="s4">0</span><span class="s1">:</span>
            <span class="s0"># Underflow</span>
            <span class="s1">s = </span><span class="s4">1.0</span>
        <span class="s2">else</span><span class="s1">:</span>
            <span class="s1">s = c*t</span>
        <span class="s2">return </span><span class="s1">c</span><span class="s2">, </span><span class="s1">s</span>

    <span class="s2">def </span><span class="s1">_to_corr(self</span><span class="s2">, </span><span class="s1">m):</span>
        <span class="s5">&quot;&quot;&quot; 
        Given a psd matrix m, rotate to put one's on the diagonal, turning it 
        into a correlation matrix.  This also requires the trace equal the 
        dimensionality. Note: modifies input matrix 
        &quot;&quot;&quot;</span>
        <span class="s0"># Check requirements for in-place Givens</span>
        <span class="s2">if not </span><span class="s1">(m.flags.c_contiguous </span><span class="s2">and </span><span class="s1">m.dtype == np.float64 </span><span class="s2">and</span>
                <span class="s1">m.shape[</span><span class="s4">0</span><span class="s1">] == m.shape[</span><span class="s4">1</span><span class="s1">]):</span>
            <span class="s2">raise </span><span class="s1">ValueError()</span>

        <span class="s1">d = m.shape[</span><span class="s4">0</span><span class="s1">]</span>
        <span class="s2">for </span><span class="s1">i </span><span class="s2">in </span><span class="s1">range(d-</span><span class="s4">1</span><span class="s1">):</span>
            <span class="s2">if </span><span class="s1">m[i</span><span class="s2">, </span><span class="s1">i] == </span><span class="s4">1</span><span class="s1">:</span>
                <span class="s2">continue</span>
            <span class="s2">elif </span><span class="s1">m[i</span><span class="s2">, </span><span class="s1">i] &gt; </span><span class="s4">1</span><span class="s1">:</span>
                <span class="s2">for </span><span class="s1">j </span><span class="s2">in </span><span class="s1">range(i+</span><span class="s4">1</span><span class="s2">, </span><span class="s1">d):</span>
                    <span class="s2">if </span><span class="s1">m[j</span><span class="s2">, </span><span class="s1">j] &lt; </span><span class="s4">1</span><span class="s1">:</span>
                        <span class="s2">break</span>
            <span class="s2">else</span><span class="s1">:</span>
                <span class="s2">for </span><span class="s1">j </span><span class="s2">in </span><span class="s1">range(i+</span><span class="s4">1</span><span class="s2">, </span><span class="s1">d):</span>
                    <span class="s2">if </span><span class="s1">m[j</span><span class="s2">, </span><span class="s1">j] &gt; </span><span class="s4">1</span><span class="s1">:</span>
                        <span class="s2">break</span>

            <span class="s1">c</span><span class="s2">, </span><span class="s1">s = self._givens_to_1(m[i</span><span class="s2">, </span><span class="s1">i]</span><span class="s2">, </span><span class="s1">m[j</span><span class="s2">, </span><span class="s1">j]</span><span class="s2">, </span><span class="s1">m[i</span><span class="s2">, </span><span class="s1">j])</span>

            <span class="s0"># Use BLAS to apply Givens rotations in-place. Equivalent to:</span>
            <span class="s0"># g = np.eye(d)</span>
            <span class="s0"># g[i, i] = g[j,j] = c</span>
            <span class="s0"># g[j, i] = -s; g[i, j] = s</span>
            <span class="s0"># m = np.dot(g.T, np.dot(m, g))</span>
            <span class="s1">mv = m.ravel()</span>
            <span class="s1">drot(mv</span><span class="s2">, </span><span class="s1">mv</span><span class="s2">, </span><span class="s1">c</span><span class="s2">, </span><span class="s1">-s</span><span class="s2">, </span><span class="s1">n=d</span><span class="s2">,</span>
                 <span class="s1">offx=i*d</span><span class="s2">, </span><span class="s1">incx=</span><span class="s4">1</span><span class="s2">, </span><span class="s1">offy=j*d</span><span class="s2">, </span><span class="s1">incy=</span><span class="s4">1</span><span class="s2">,</span>
                 <span class="s1">overwrite_x=</span><span class="s2">True, </span><span class="s1">overwrite_y=</span><span class="s2">True</span><span class="s1">)</span>
            <span class="s1">drot(mv</span><span class="s2">, </span><span class="s1">mv</span><span class="s2">, </span><span class="s1">c</span><span class="s2">, </span><span class="s1">-s</span><span class="s2">, </span><span class="s1">n=d</span><span class="s2">,</span>
                 <span class="s1">offx=i</span><span class="s2">, </span><span class="s1">incx=d</span><span class="s2">, </span><span class="s1">offy=j</span><span class="s2">, </span><span class="s1">incy=d</span><span class="s2">,</span>
                 <span class="s1">overwrite_x=</span><span class="s2">True, </span><span class="s1">overwrite_y=</span><span class="s2">True</span><span class="s1">)</span>

        <span class="s2">return </span><span class="s1">m</span>

    <span class="s2">def </span><span class="s1">rvs(self</span><span class="s2">, </span><span class="s1">eigs</span><span class="s2">, </span><span class="s1">random_state=</span><span class="s2">None, </span><span class="s1">tol=</span><span class="s4">1e-13</span><span class="s2">, </span><span class="s1">diag_tol=</span><span class="s4">1e-7</span><span class="s1">):</span>
        <span class="s5">&quot;&quot;&quot;Draw random correlation matrices. 
 
        Parameters 
        ---------- 
        eigs : 1d ndarray 
            Eigenvalues of correlation matrix 
        tol : float, optional 
            Tolerance for input parameter checks 
        diag_tol : float, optional 
            Tolerance for deviation of the diagonal of the resulting 
            matrix. Default: 1e-7 
 
        Raises 
        ------ 
        RuntimeError 
            Floating point error prevented generating a valid correlation 
            matrix. 
 
        Returns 
        ------- 
        rvs : ndarray or scalar 
            Random size N-dimensional matrices, dimension (size, dim, dim), 
            each having eigenvalues eigs. 
 
        &quot;&quot;&quot;</span>
        <span class="s1">dim</span><span class="s2">, </span><span class="s1">eigs = self._process_parameters(eigs</span><span class="s2">, </span><span class="s1">tol=tol)</span>

        <span class="s1">random_state = self._get_random_state(random_state)</span>

        <span class="s1">m = ortho_group.rvs(dim</span><span class="s2">, </span><span class="s1">random_state=random_state)</span>
        <span class="s1">m = np.dot(np.dot(m</span><span class="s2">, </span><span class="s1">np.diag(eigs))</span><span class="s2">, </span><span class="s1">m.T)  </span><span class="s0"># Set the trace of m</span>
        <span class="s1">m = self._to_corr(m)  </span><span class="s0"># Carefully rotate to unit diagonal</span>

        <span class="s0"># Check diagonal</span>
        <span class="s2">if </span><span class="s1">abs(m.diagonal() - </span><span class="s4">1</span><span class="s1">).max() &gt; diag_tol:</span>
            <span class="s2">raise </span><span class="s1">RuntimeError(</span><span class="s3">&quot;Failed to generate a valid correlation matrix&quot;</span><span class="s1">)</span>

        <span class="s2">return </span><span class="s1">m</span>


<span class="s1">random_correlation = random_correlation_gen()</span>


<span class="s2">class </span><span class="s1">random_correlation_frozen(multi_rv_frozen):</span>
    <span class="s2">def </span><span class="s1">__init__(self</span><span class="s2">, </span><span class="s1">eigs</span><span class="s2">, </span><span class="s1">seed=</span><span class="s2">None, </span><span class="s1">tol=</span><span class="s4">1e-13</span><span class="s2">, </span><span class="s1">diag_tol=</span><span class="s4">1e-7</span><span class="s1">):</span>
        <span class="s5">&quot;&quot;&quot;Create a frozen random correlation matrix distribution. 
 
        Parameters 
        ---------- 
        eigs : 1d ndarray 
            Eigenvalues of correlation matrix 
        seed : {None, int, `numpy.random.Generator`, `numpy.random.RandomState`}, optional 
            If `seed` is None (or `np.random`), the `numpy.random.RandomState` 
            singleton is used. 
            If `seed` is an int, a new ``RandomState`` instance is used, 
            seeded with `seed`. 
            If `seed` is already a ``Generator`` or ``RandomState`` instance 
            then that instance is used. 
        tol : float, optional 
            Tolerance for input parameter checks 
        diag_tol : float, optional 
            Tolerance for deviation of the diagonal of the resulting 
            matrix. Default: 1e-7 
 
        Raises 
        ------ 
        RuntimeError 
            Floating point error prevented generating a valid correlation 
            matrix. 
 
        Returns 
        ------- 
        rvs : ndarray or scalar 
            Random size N-dimensional matrices, dimension (size, dim, dim), 
            each having eigenvalues eigs. 
        &quot;&quot;&quot;</span>

        <span class="s1">self._dist = random_correlation_gen(seed)</span>
        <span class="s1">self.tol = tol</span>
        <span class="s1">self.diag_tol = diag_tol</span>
        <span class="s1">_</span><span class="s2">, </span><span class="s1">self.eigs = self._dist._process_parameters(eigs</span><span class="s2">, </span><span class="s1">tol=self.tol)</span>

    <span class="s2">def </span><span class="s1">rvs(self</span><span class="s2">, </span><span class="s1">random_state=</span><span class="s2">None</span><span class="s1">):</span>
        <span class="s2">return </span><span class="s1">self._dist.rvs(self.eigs</span><span class="s2">, </span><span class="s1">random_state=random_state</span><span class="s2">,</span>
                              <span class="s1">tol=self.tol</span><span class="s2">, </span><span class="s1">diag_tol=self.diag_tol)</span>


<span class="s2">class </span><span class="s1">unitary_group_gen(multi_rv_generic):</span>
    <span class="s5">r&quot;&quot;&quot;A matrix-valued U(N) random variable. 
 
    Return a random unitary matrix. 
 
    The `dim` keyword specifies the dimension N. 
 
    Methods 
    ------- 
    rvs(dim=None, size=1, random_state=None) 
        Draw random samples from U(N). 
 
    Parameters 
    ---------- 
    dim : scalar 
        Dimension of matrices 
    seed : {None, int, np.random.RandomState, np.random.Generator}, optional 
        Used for drawing random variates. 
        If `seed` is `None`, the `~np.random.RandomState` singleton is used. 
        If `seed` is an int, a new ``RandomState`` instance is used, seeded 
        with seed. 
        If `seed` is already a ``RandomState`` or ``Generator`` instance, 
        then that object is used. 
        Default is `None`. 
 
    Notes 
    ----- 
    This class is similar to `ortho_group`. 
 
    References 
    ---------- 
    .. [1] F. Mezzadri, &quot;How to generate random matrices from the classical 
           compact groups&quot;, :arXiv:`math-ph/0609050v2`. 
 
    Examples 
    -------- 
    &gt;&gt;&gt; import numpy as np 
    &gt;&gt;&gt; from scipy.stats import unitary_group 
    &gt;&gt;&gt; x = unitary_group.rvs(3) 
 
    &gt;&gt;&gt; np.dot(x, x.conj().T) 
    array([[  1.00000000e+00,   1.13231364e-17,  -2.86852790e-16], 
           [  1.13231364e-17,   1.00000000e+00,  -1.46845020e-16], 
           [ -2.86852790e-16,  -1.46845020e-16,   1.00000000e+00]]) 
 
    This generates one random matrix from U(3). The dot product confirms that 
    it is unitary up to machine precision. 
 
    Alternatively, the object may be called (as a function) to fix the `dim` 
    parameter, return a &quot;frozen&quot; unitary_group random variable: 
 
    &gt;&gt;&gt; rv = unitary_group(5) 
 
    See Also 
    -------- 
    ortho_group 
 
    &quot;&quot;&quot;</span>

    <span class="s2">def </span><span class="s1">__init__(self</span><span class="s2">, </span><span class="s1">seed=</span><span class="s2">None</span><span class="s1">):</span>
        <span class="s1">super().__init__(seed)</span>
        <span class="s1">self.__doc__ = doccer.docformat(self.__doc__)</span>

    <span class="s2">def </span><span class="s1">__call__(self</span><span class="s2">, </span><span class="s1">dim=</span><span class="s2">None, </span><span class="s1">seed=</span><span class="s2">None</span><span class="s1">):</span>
        <span class="s5">&quot;&quot;&quot;Create a frozen (U(N)) n-dimensional unitary matrix distribution. 
 
        See `unitary_group_frozen` for more information. 
        &quot;&quot;&quot;</span>
        <span class="s2">return </span><span class="s1">unitary_group_frozen(dim</span><span class="s2">, </span><span class="s1">seed=seed)</span>

    <span class="s2">def </span><span class="s1">_process_parameters(self</span><span class="s2">, </span><span class="s1">dim):</span>
        <span class="s5">&quot;&quot;&quot;Dimension N must be specified; it cannot be inferred.&quot;&quot;&quot;</span>
        <span class="s2">if </span><span class="s1">dim </span><span class="s2">is None or not </span><span class="s1">np.isscalar(dim) </span><span class="s2">or </span><span class="s1">dim &lt;= </span><span class="s4">1 </span><span class="s2">or </span><span class="s1">dim != int(dim):</span>
            <span class="s2">raise </span><span class="s1">ValueError(</span><span class="s3">&quot;Dimension of rotation must be specified,&quot;</span>
                             <span class="s3">&quot;and must be a scalar greater than 1.&quot;</span><span class="s1">)</span>

        <span class="s2">return </span><span class="s1">dim</span>

    <span class="s2">def </span><span class="s1">rvs(self</span><span class="s2">, </span><span class="s1">dim</span><span class="s2">, </span><span class="s1">size=</span><span class="s4">1</span><span class="s2">, </span><span class="s1">random_state=</span><span class="s2">None</span><span class="s1">):</span>
        <span class="s5">&quot;&quot;&quot;Draw random samples from U(N). 
 
        Parameters 
        ---------- 
        dim : integer 
            Dimension of space (N). 
        size : integer, optional 
            Number of samples to draw (default 1). 
 
        Returns 
        ------- 
        rvs : ndarray or scalar 
            Random size N-dimensional matrices, dimension (size, dim, dim) 
 
        &quot;&quot;&quot;</span>
        <span class="s1">random_state = self._get_random_state(random_state)</span>

        <span class="s1">size = int(size)</span>
        <span class="s2">if </span><span class="s1">size &gt; </span><span class="s4">1 </span><span class="s2">and </span><span class="s1">NumpyVersion(np.__version__) &lt; </span><span class="s3">'1.22.0'</span><span class="s1">:</span>
            <span class="s2">return </span><span class="s1">np.array([self.rvs(dim</span><span class="s2">, </span><span class="s1">size=</span><span class="s4">1</span><span class="s2">, </span><span class="s1">random_state=random_state)</span>
                             <span class="s2">for </span><span class="s1">i </span><span class="s2">in </span><span class="s1">range(size)])</span>

        <span class="s1">dim = self._process_parameters(dim)</span>

        <span class="s1">size = (size</span><span class="s2">,</span><span class="s1">) </span><span class="s2">if </span><span class="s1">size &gt; </span><span class="s4">1 </span><span class="s2">else </span><span class="s1">()</span>
        <span class="s1">z = </span><span class="s4">1</span><span class="s1">/math.sqrt(</span><span class="s4">2</span><span class="s1">)*(random_state.normal(size=size + (dim</span><span class="s2">, </span><span class="s1">dim)) +</span>
                            <span class="s4">1j</span><span class="s1">*random_state.normal(size=size + (dim</span><span class="s2">, </span><span class="s1">dim)))</span>
        <span class="s1">q</span><span class="s2">, </span><span class="s1">r = np.linalg.qr(z)</span>
        <span class="s0"># The last two dimensions are the rows and columns of R matrices.</span>
        <span class="s0"># Extract the diagonals. Note that this eliminates a dimension.</span>
        <span class="s1">d = r.diagonal(offset=</span><span class="s4">0</span><span class="s2">, </span><span class="s1">axis1=-</span><span class="s4">2</span><span class="s2">, </span><span class="s1">axis2=-</span><span class="s4">1</span><span class="s1">)</span>
        <span class="s0"># Add back a dimension for proper broadcasting: we're dividing</span>
        <span class="s0"># each row of each R matrix by the diagonal of the R matrix.</span>
        <span class="s1">q *= (d/abs(d))[...</span><span class="s2">, </span><span class="s1">np.newaxis</span><span class="s2">, </span><span class="s1">:]  </span><span class="s0"># to broadcast properly</span>
        <span class="s2">return </span><span class="s1">q</span>


<span class="s1">unitary_group = unitary_group_gen()</span>


<span class="s2">class </span><span class="s1">unitary_group_frozen(multi_rv_frozen):</span>
    <span class="s2">def </span><span class="s1">__init__(self</span><span class="s2">, </span><span class="s1">dim=</span><span class="s2">None, </span><span class="s1">seed=</span><span class="s2">None</span><span class="s1">):</span>
        <span class="s5">&quot;&quot;&quot;Create a frozen (U(N)) n-dimensional unitary matrix distribution. 
 
        Parameters 
        ---------- 
        dim : scalar 
            Dimension of matrices 
        seed : {None, int, `numpy.random.Generator`, `numpy.random.RandomState`}, optional 
            If `seed` is None (or `np.random`), the `numpy.random.RandomState` 
            singleton is used. 
            If `seed` is an int, a new ``RandomState`` instance is used, 
            seeded with `seed`. 
            If `seed` is already a ``Generator`` or ``RandomState`` instance 
            then that instance is used. 
 
        Examples 
        -------- 
        &gt;&gt;&gt; from scipy.stats import unitary_group 
        &gt;&gt;&gt; x = unitary_group(3) 
        &gt;&gt;&gt; x.rvs() 
 
        &quot;&quot;&quot;</span>
        <span class="s1">self._dist = unitary_group_gen(seed)</span>
        <span class="s1">self.dim = self._dist._process_parameters(dim)</span>

    <span class="s2">def </span><span class="s1">rvs(self</span><span class="s2">, </span><span class="s1">size=</span><span class="s4">1</span><span class="s2">, </span><span class="s1">random_state=</span><span class="s2">None</span><span class="s1">):</span>
        <span class="s2">return </span><span class="s1">self._dist.rvs(self.dim</span><span class="s2">, </span><span class="s1">size</span><span class="s2">, </span><span class="s1">random_state)</span>


<span class="s1">_mvt_doc_default_callparams = </span><span class="s3">&quot;&quot;&quot;</span><span class="s2">\ 
</span><span class="s3">loc : array_like, optional 
    Location of the distribution. (default ``0``) 
shape : array_like, optional 
    Positive semidefinite matrix of the distribution. (default ``1``) 
df : float, optional 
    Degrees of freedom of the distribution; must be greater than zero. 
    If ``np.inf`` then results are multivariate normal. The default is ``1``. 
allow_singular : bool, optional 
    Whether to allow a singular matrix. (default ``False``) 
&quot;&quot;&quot;</span>

<span class="s1">_mvt_doc_callparams_note = </span><span class="s3">&quot;&quot;&quot;</span><span class="s2">\ 
</span><span class="s3">Setting the parameter `loc` to ``None`` is equivalent to having `loc` 
be the zero-vector. The parameter `shape` can be a scalar, in which case 
the shape matrix is the identity times that value, a vector of 
diagonal entries for the shape matrix, or a two-dimensional array_like. 
&quot;&quot;&quot;</span>

<span class="s1">_mvt_doc_frozen_callparams_note = </span><span class="s3">&quot;&quot;&quot;</span><span class="s2">\ 
</span><span class="s3">See class definition for a detailed description of parameters.&quot;&quot;&quot;</span>

<span class="s1">mvt_docdict_params = {</span>
    <span class="s3">'_mvt_doc_default_callparams'</span><span class="s1">: _mvt_doc_default_callparams</span><span class="s2">,</span>
    <span class="s3">'_mvt_doc_callparams_note'</span><span class="s1">: _mvt_doc_callparams_note</span><span class="s2">,</span>
    <span class="s3">'_doc_random_state'</span><span class="s1">: _doc_random_state</span>
<span class="s1">}</span>

<span class="s1">mvt_docdict_noparams = {</span>
    <span class="s3">'_mvt_doc_default_callparams'</span><span class="s1">: </span><span class="s3">&quot;&quot;</span><span class="s2">,</span>
    <span class="s3">'_mvt_doc_callparams_note'</span><span class="s1">: _mvt_doc_frozen_callparams_note</span><span class="s2">,</span>
    <span class="s3">'_doc_random_state'</span><span class="s1">: _doc_random_state</span>
<span class="s1">}</span>


<span class="s2">class </span><span class="s1">multivariate_t_gen(multi_rv_generic):</span>
    <span class="s5">r&quot;&quot;&quot;A multivariate t-distributed random variable. 
 
    The `loc` parameter specifies the location. The `shape` parameter specifies 
    the positive semidefinite shape matrix. The `df` parameter specifies the 
    degrees of freedom. 
 
    In addition to calling the methods below, the object itself may be called 
    as a function to fix the location, shape matrix, and degrees of freedom 
    parameters, returning a &quot;frozen&quot; multivariate t-distribution random. 
 
    Methods 
    ------- 
    pdf(x, loc=None, shape=1, df=1, allow_singular=False) 
        Probability density function. 
    logpdf(x, loc=None, shape=1, df=1, allow_singular=False) 
        Log of the probability density function. 
    cdf(x, loc=None, shape=1, df=1, allow_singular=False, *, 
        maxpts=None, lower_limit=None, random_state=None) 
        Cumulative distribution function. 
    rvs(loc=None, shape=1, df=1, size=1, random_state=None) 
        Draw random samples from a multivariate t-distribution. 
    entropy(loc=None, shape=1, df=1) 
        Differential entropy of a multivariate t-distribution. 
 
    Parameters 
    ---------- 
    %(_mvt_doc_default_callparams)s 
    %(_doc_random_state)s 
 
    Notes 
    ----- 
    %(_mvt_doc_callparams_note)s 
    The matrix `shape` must be a (symmetric) positive semidefinite matrix. The 
    determinant and inverse of `shape` are computed as the pseudo-determinant 
    and pseudo-inverse, respectively, so that `shape` does not need to have 
    full rank. 
 
    The probability density function for `multivariate_t` is 
 
    .. math:: 
 
        f(x) = \frac{\Gamma((\nu + p)/2)}{\Gamma(\nu/2)\nu^{p/2}\pi^{p/2}|\Sigma|^{1/2}} 
               \left[1 + \frac{1}{\nu} (\mathbf{x} - \boldsymbol{\mu})^{\top} 
               \boldsymbol{\Sigma}^{-1} 
               (\mathbf{x} - \boldsymbol{\mu}) \right]^{-(\nu + p)/2}, 
 
    where :math:`p` is the dimension of :math:`\mathbf{x}`, 
    :math:`\boldsymbol{\mu}` is the :math:`p`-dimensional location, 
    :math:`\boldsymbol{\Sigma}` the :math:`p \times p`-dimensional shape 
    matrix, and :math:`\nu` is the degrees of freedom. 
 
    .. versionadded:: 1.6.0 
 
    References 
    ---------- 
    [1]     Arellano-Valle et al. &quot;Shannon Entropy and Mutual Information for 
            Multivariate Skew-Elliptical Distributions&quot;. Scandinavian Journal 
            of Statistics. Vol. 40, issue 1. 
 
    Examples 
    -------- 
    The object may be called (as a function) to fix the `loc`, `shape`, 
    `df`, and `allow_singular` parameters, returning a &quot;frozen&quot; 
    multivariate_t random variable: 
 
    &gt;&gt;&gt; import numpy as np 
    &gt;&gt;&gt; from scipy.stats import multivariate_t 
    &gt;&gt;&gt; rv = multivariate_t([1.0, -0.5], [[2.1, 0.3], [0.3, 1.5]], df=2) 
    &gt;&gt;&gt; # Frozen object with the same methods but holding the given location, 
    &gt;&gt;&gt; # scale, and degrees of freedom fixed. 
 
    Create a contour plot of the PDF. 
 
    &gt;&gt;&gt; import matplotlib.pyplot as plt 
    &gt;&gt;&gt; x, y = np.mgrid[-1:3:.01, -2:1.5:.01] 
    &gt;&gt;&gt; pos = np.dstack((x, y)) 
    &gt;&gt;&gt; fig, ax = plt.subplots(1, 1) 
    &gt;&gt;&gt; ax.set_aspect('equal') 
    &gt;&gt;&gt; plt.contourf(x, y, rv.pdf(pos)) 
 
    &quot;&quot;&quot;</span>

    <span class="s2">def </span><span class="s1">__init__(self</span><span class="s2">, </span><span class="s1">seed=</span><span class="s2">None</span><span class="s1">):</span>
        <span class="s5">&quot;&quot;&quot;Initialize a multivariate t-distributed random variable. 
 
        Parameters 
        ---------- 
        seed : Random state. 
 
        &quot;&quot;&quot;</span>
        <span class="s1">super().__init__(seed)</span>
        <span class="s1">self.__doc__ = doccer.docformat(self.__doc__</span><span class="s2">, </span><span class="s1">mvt_docdict_params)</span>
        <span class="s1">self._random_state = check_random_state(seed)</span>

    <span class="s2">def </span><span class="s1">__call__(self</span><span class="s2">, </span><span class="s1">loc=</span><span class="s2">None, </span><span class="s1">shape=</span><span class="s4">1</span><span class="s2">, </span><span class="s1">df=</span><span class="s4">1</span><span class="s2">, </span><span class="s1">allow_singular=</span><span class="s2">False,</span>
                 <span class="s1">seed=</span><span class="s2">None</span><span class="s1">):</span>
        <span class="s5">&quot;&quot;&quot;Create a frozen multivariate t-distribution. 
 
        See `multivariate_t_frozen` for parameters. 
        &quot;&quot;&quot;</span>
        <span class="s2">if </span><span class="s1">df == np.inf:</span>
            <span class="s2">return </span><span class="s1">multivariate_normal_frozen(mean=loc</span><span class="s2">, </span><span class="s1">cov=shape</span><span class="s2">,</span>
                                              <span class="s1">allow_singular=allow_singular</span><span class="s2">,</span>
                                              <span class="s1">seed=seed)</span>
        <span class="s2">return </span><span class="s1">multivariate_t_frozen(loc=loc</span><span class="s2">, </span><span class="s1">shape=shape</span><span class="s2">, </span><span class="s1">df=df</span><span class="s2">,</span>
                                     <span class="s1">allow_singular=allow_singular</span><span class="s2">, </span><span class="s1">seed=seed)</span>

    <span class="s2">def </span><span class="s1">pdf(self</span><span class="s2">, </span><span class="s1">x</span><span class="s2">, </span><span class="s1">loc=</span><span class="s2">None, </span><span class="s1">shape=</span><span class="s4">1</span><span class="s2">, </span><span class="s1">df=</span><span class="s4">1</span><span class="s2">, </span><span class="s1">allow_singular=</span><span class="s2">False</span><span class="s1">):</span>
        <span class="s5">&quot;&quot;&quot;Multivariate t-distribution probability density function. 
 
        Parameters 
        ---------- 
        x : array_like 
            Points at which to evaluate the probability density function. 
        %(_mvt_doc_default_callparams)s 
 
        Returns 
        ------- 
        pdf : Probability density function evaluated at `x`. 
 
        Examples 
        -------- 
        &gt;&gt;&gt; from scipy.stats import multivariate_t 
        &gt;&gt;&gt; x = [0.4, 5] 
        &gt;&gt;&gt; loc = [0, 1] 
        &gt;&gt;&gt; shape = [[1, 0.1], [0.1, 1]] 
        &gt;&gt;&gt; df = 7 
        &gt;&gt;&gt; multivariate_t.pdf(x, loc, shape, df) 
        0.00075713 
 
        &quot;&quot;&quot;</span>
        <span class="s1">dim</span><span class="s2">, </span><span class="s1">loc</span><span class="s2">, </span><span class="s1">shape</span><span class="s2">, </span><span class="s1">df = self._process_parameters(loc</span><span class="s2">, </span><span class="s1">shape</span><span class="s2">, </span><span class="s1">df)</span>
        <span class="s1">x = self._process_quantiles(x</span><span class="s2">, </span><span class="s1">dim)</span>
        <span class="s1">shape_info = _PSD(shape</span><span class="s2">, </span><span class="s1">allow_singular=allow_singular)</span>
        <span class="s1">logpdf = self._logpdf(x</span><span class="s2">, </span><span class="s1">loc</span><span class="s2">, </span><span class="s1">shape_info.U</span><span class="s2">, </span><span class="s1">shape_info.log_pdet</span><span class="s2">, </span><span class="s1">df</span><span class="s2">,</span>
                              <span class="s1">dim</span><span class="s2">, </span><span class="s1">shape_info.rank)</span>
        <span class="s2">return </span><span class="s1">np.exp(logpdf)</span>

    <span class="s2">def </span><span class="s1">logpdf(self</span><span class="s2">, </span><span class="s1">x</span><span class="s2">, </span><span class="s1">loc=</span><span class="s2">None, </span><span class="s1">shape=</span><span class="s4">1</span><span class="s2">, </span><span class="s1">df=</span><span class="s4">1</span><span class="s1">):</span>
        <span class="s5">&quot;&quot;&quot;Log of the multivariate t-distribution probability density function. 
 
        Parameters 
        ---------- 
        x : array_like 
            Points at which to evaluate the log of the probability density 
            function. 
        %(_mvt_doc_default_callparams)s 
 
        Returns 
        ------- 
        logpdf : Log of the probability density function evaluated at `x`. 
 
        Examples 
        -------- 
        &gt;&gt;&gt; from scipy.stats import multivariate_t 
        &gt;&gt;&gt; x = [0.4, 5] 
        &gt;&gt;&gt; loc = [0, 1] 
        &gt;&gt;&gt; shape = [[1, 0.1], [0.1, 1]] 
        &gt;&gt;&gt; df = 7 
        &gt;&gt;&gt; multivariate_t.logpdf(x, loc, shape, df) 
        -7.1859802 
 
        See Also 
        -------- 
        pdf : Probability density function. 
 
        &quot;&quot;&quot;</span>
        <span class="s1">dim</span><span class="s2">, </span><span class="s1">loc</span><span class="s2">, </span><span class="s1">shape</span><span class="s2">, </span><span class="s1">df = self._process_parameters(loc</span><span class="s2">, </span><span class="s1">shape</span><span class="s2">, </span><span class="s1">df)</span>
        <span class="s1">x = self._process_quantiles(x</span><span class="s2">, </span><span class="s1">dim)</span>
        <span class="s1">shape_info = _PSD(shape)</span>
        <span class="s2">return </span><span class="s1">self._logpdf(x</span><span class="s2">, </span><span class="s1">loc</span><span class="s2">, </span><span class="s1">shape_info.U</span><span class="s2">, </span><span class="s1">shape_info.log_pdet</span><span class="s2">, </span><span class="s1">df</span><span class="s2">, </span><span class="s1">dim</span><span class="s2">,</span>
                            <span class="s1">shape_info.rank)</span>

    <span class="s2">def </span><span class="s1">_logpdf(self</span><span class="s2">, </span><span class="s1">x</span><span class="s2">, </span><span class="s1">loc</span><span class="s2">, </span><span class="s1">prec_U</span><span class="s2">, </span><span class="s1">log_pdet</span><span class="s2">, </span><span class="s1">df</span><span class="s2">, </span><span class="s1">dim</span><span class="s2">, </span><span class="s1">rank):</span>
        <span class="s5">&quot;&quot;&quot;Utility method `pdf`, `logpdf` for parameters. 
 
        Parameters 
        ---------- 
        x : ndarray 
            Points at which to evaluate the log of the probability density 
            function. 
        loc : ndarray 
            Location of the distribution. 
        prec_U : ndarray 
            A decomposition such that `np.dot(prec_U, prec_U.T)` is the inverse 
            of the shape matrix. 
        log_pdet : float 
            Logarithm of the determinant of the shape matrix. 
        df : float 
            Degrees of freedom of the distribution. 
        dim : int 
            Dimension of the quantiles x. 
        rank : int 
            Rank of the shape matrix. 
 
        Notes 
        ----- 
        As this function does no argument checking, it should not be called 
        directly; use 'logpdf' instead. 
 
        &quot;&quot;&quot;</span>
        <span class="s2">if </span><span class="s1">df == np.inf:</span>
            <span class="s2">return </span><span class="s1">multivariate_normal._logpdf(x</span><span class="s2">, </span><span class="s1">loc</span><span class="s2">, </span><span class="s1">prec_U</span><span class="s2">, </span><span class="s1">log_pdet</span><span class="s2">, </span><span class="s1">rank)</span>

        <span class="s1">dev = x - loc</span>
        <span class="s1">maha = np.square(np.dot(dev</span><span class="s2">, </span><span class="s1">prec_U)).sum(axis=-</span><span class="s4">1</span><span class="s1">)</span>

        <span class="s1">t = </span><span class="s4">0.5 </span><span class="s1">* (df + dim)</span>
        <span class="s1">A = gammaln(t)</span>
        <span class="s1">B = gammaln(</span><span class="s4">0.5 </span><span class="s1">* df)</span>
        <span class="s1">C = dim/</span><span class="s4">2. </span><span class="s1">* np.log(df * np.pi)</span>
        <span class="s1">D = </span><span class="s4">0.5 </span><span class="s1">* log_pdet</span>
        <span class="s1">E = -t * np.log(</span><span class="s4">1 </span><span class="s1">+ (</span><span class="s4">1.</span><span class="s1">/df) * maha)</span>

        <span class="s2">return </span><span class="s1">_squeeze_output(A - B - C - D + E)</span>

    <span class="s2">def </span><span class="s1">_cdf(self</span><span class="s2">, </span><span class="s1">x</span><span class="s2">, </span><span class="s1">loc</span><span class="s2">, </span><span class="s1">shape</span><span class="s2">, </span><span class="s1">df</span><span class="s2">, </span><span class="s1">dim</span><span class="s2">, </span><span class="s1">maxpts=</span><span class="s2">None, </span><span class="s1">lower_limit=</span><span class="s2">None,</span>
             <span class="s1">random_state=</span><span class="s2">None</span><span class="s1">):</span>

        <span class="s0"># All of this -  random state validation, maxpts, apply_along_axis,</span>
        <span class="s0"># etc. needs to go in this private method unless we want</span>
        <span class="s0"># frozen distribution's `cdf` method to duplicate it or call `cdf`,</span>
        <span class="s0"># which would require re-processing parameters</span>
        <span class="s2">if </span><span class="s1">random_state </span><span class="s2">is not None</span><span class="s1">:</span>
            <span class="s1">rng = check_random_state(random_state)</span>
        <span class="s2">else</span><span class="s1">:</span>
            <span class="s1">rng = self._random_state</span>

        <span class="s2">if not </span><span class="s1">maxpts:</span>
            <span class="s1">maxpts = </span><span class="s4">1000 </span><span class="s1">* dim</span>

        <span class="s1">x = self._process_quantiles(x</span><span class="s2">, </span><span class="s1">dim)</span>
        <span class="s1">lower_limit = (np.full(loc.shape</span><span class="s2">, </span><span class="s1">-np.inf)</span>
                       <span class="s2">if </span><span class="s1">lower_limit </span><span class="s2">is None else </span><span class="s1">lower_limit)</span>

        <span class="s0"># remove the mean</span>
        <span class="s1">x</span><span class="s2">, </span><span class="s1">lower_limit = x - loc</span><span class="s2">, </span><span class="s1">lower_limit - loc</span>

        <span class="s1">b</span><span class="s2">, </span><span class="s1">a = np.broadcast_arrays(x</span><span class="s2">, </span><span class="s1">lower_limit)</span>
        <span class="s1">i_swap = b &lt; a</span>
        <span class="s1">signs = (-</span><span class="s4">1</span><span class="s1">)**(i_swap.sum(axis=-</span><span class="s4">1</span><span class="s1">))  </span><span class="s0"># odd # of swaps -&gt; negative</span>
        <span class="s1">a</span><span class="s2">, </span><span class="s1">b = a.copy()</span><span class="s2">, </span><span class="s1">b.copy()</span>
        <span class="s1">a[i_swap]</span><span class="s2">, </span><span class="s1">b[i_swap] = b[i_swap]</span><span class="s2">, </span><span class="s1">a[i_swap]</span>
        <span class="s1">n = x.shape[-</span><span class="s4">1</span><span class="s1">]</span>
        <span class="s1">limits = np.concatenate((a</span><span class="s2">, </span><span class="s1">b)</span><span class="s2">, </span><span class="s1">axis=-</span><span class="s4">1</span><span class="s1">)</span>

        <span class="s2">def </span><span class="s1">func1d(limits):</span>
            <span class="s1">a</span><span class="s2">, </span><span class="s1">b = limits[:n]</span><span class="s2">, </span><span class="s1">limits[n:]</span>
            <span class="s2">return </span><span class="s1">_qmvt(maxpts</span><span class="s2">, </span><span class="s1">df</span><span class="s2">, </span><span class="s1">shape</span><span class="s2">, </span><span class="s1">a</span><span class="s2">, </span><span class="s1">b</span><span class="s2">, </span><span class="s1">rng)[</span><span class="s4">0</span><span class="s1">]</span>

        <span class="s1">res = np.apply_along_axis(func1d</span><span class="s2">, </span><span class="s1">-</span><span class="s4">1</span><span class="s2">, </span><span class="s1">limits) * signs</span>
        <span class="s0"># Fixing the output shape for existing distributions is a separate</span>
        <span class="s0"># issue. For now, let's keep this consistent with pdf.</span>
        <span class="s2">return </span><span class="s1">_squeeze_output(res)</span>

    <span class="s2">def </span><span class="s1">cdf(self</span><span class="s2">, </span><span class="s1">x</span><span class="s2">, </span><span class="s1">loc=</span><span class="s2">None, </span><span class="s1">shape=</span><span class="s4">1</span><span class="s2">, </span><span class="s1">df=</span><span class="s4">1</span><span class="s2">, </span><span class="s1">allow_singular=</span><span class="s2">False, </span><span class="s1">*</span><span class="s2">,</span>
            <span class="s1">maxpts=</span><span class="s2">None, </span><span class="s1">lower_limit=</span><span class="s2">None, </span><span class="s1">random_state=</span><span class="s2">None</span><span class="s1">):</span>
        <span class="s5">&quot;&quot;&quot;Multivariate t-distribution cumulative distribution function. 
 
        Parameters 
        ---------- 
        x : array_like 
            Points at which to evaluate the cumulative distribution function. 
        %(_mvt_doc_default_callparams)s 
        maxpts : int, optional 
            Maximum number of points to use for integration. The default is 
            1000 times the number of dimensions. 
        lower_limit : array_like, optional 
            Lower limit of integration of the cumulative distribution function. 
            Default is negative infinity. Must be broadcastable with `x`. 
        %(_doc_random_state)s 
 
        Returns 
        ------- 
        cdf : ndarray or scalar 
            Cumulative distribution function evaluated at `x`. 
 
        Examples 
        -------- 
        &gt;&gt;&gt; from scipy.stats import multivariate_t 
        &gt;&gt;&gt; x = [0.4, 5] 
        &gt;&gt;&gt; loc = [0, 1] 
        &gt;&gt;&gt; shape = [[1, 0.1], [0.1, 1]] 
        &gt;&gt;&gt; df = 7 
        &gt;&gt;&gt; multivariate_t.cdf(x, loc, shape, df) 
        0.64798491 
 
        &quot;&quot;&quot;</span>
        <span class="s1">dim</span><span class="s2">, </span><span class="s1">loc</span><span class="s2">, </span><span class="s1">shape</span><span class="s2">, </span><span class="s1">df = self._process_parameters(loc</span><span class="s2">, </span><span class="s1">shape</span><span class="s2">, </span><span class="s1">df)</span>
        <span class="s1">shape = _PSD(shape</span><span class="s2">, </span><span class="s1">allow_singular=allow_singular)._M</span>

        <span class="s2">return </span><span class="s1">self._cdf(x</span><span class="s2">, </span><span class="s1">loc</span><span class="s2">, </span><span class="s1">shape</span><span class="s2">, </span><span class="s1">df</span><span class="s2">, </span><span class="s1">dim</span><span class="s2">, </span><span class="s1">maxpts</span><span class="s2">,</span>
                         <span class="s1">lower_limit</span><span class="s2">, </span><span class="s1">random_state)</span>

    <span class="s2">def </span><span class="s1">_entropy(self</span><span class="s2">, </span><span class="s1">dim</span><span class="s2">, </span><span class="s1">df=</span><span class="s4">1</span><span class="s2">, </span><span class="s1">shape=</span><span class="s4">1</span><span class="s1">):</span>
        <span class="s2">if </span><span class="s1">df == np.inf:</span>
            <span class="s2">return </span><span class="s1">multivariate_normal(</span><span class="s2">None, </span><span class="s1">cov=shape).entropy()</span>

        <span class="s1">shape_info = _PSD(shape)</span>
        <span class="s1">halfsum = </span><span class="s4">0.5 </span><span class="s1">* (dim + df)</span>
        <span class="s1">half_df = </span><span class="s4">0.5 </span><span class="s1">* df</span>
        <span class="s2">return </span><span class="s1">(-gammaln(halfsum) + gammaln(half_df)</span>
                <span class="s1">+ </span><span class="s4">0.5 </span><span class="s1">* dim * np.log(df * np.pi) + halfsum</span>
                <span class="s1">* (psi(halfsum) - psi(half_df))</span>
                <span class="s1">+ </span><span class="s4">0.5 </span><span class="s1">* shape_info.log_pdet)</span>

    <span class="s2">def </span><span class="s1">entropy(self</span><span class="s2">, </span><span class="s1">loc=</span><span class="s2">None, </span><span class="s1">shape=</span><span class="s4">1</span><span class="s2">, </span><span class="s1">df=</span><span class="s4">1</span><span class="s1">):</span>
        <span class="s5">&quot;&quot;&quot;Calculate the differential entropy of a multivariate 
        t-distribution. 
 
        Parameters 
        ---------- 
        %(_mvt_doc_default_callparams)s 
 
        Returns 
        ------- 
        h : float 
            Differential entropy 
 
        &quot;&quot;&quot;</span>
        <span class="s1">dim</span><span class="s2">, </span><span class="s1">loc</span><span class="s2">, </span><span class="s1">shape</span><span class="s2">, </span><span class="s1">df = self._process_parameters(</span><span class="s2">None, </span><span class="s1">shape</span><span class="s2">, </span><span class="s1">df)</span>
        <span class="s2">return </span><span class="s1">self._entropy(dim</span><span class="s2">, </span><span class="s1">df</span><span class="s2">, </span><span class="s1">shape)</span>

    <span class="s2">def </span><span class="s1">rvs(self</span><span class="s2">, </span><span class="s1">loc=</span><span class="s2">None, </span><span class="s1">shape=</span><span class="s4">1</span><span class="s2">, </span><span class="s1">df=</span><span class="s4">1</span><span class="s2">, </span><span class="s1">size=</span><span class="s4">1</span><span class="s2">, </span><span class="s1">random_state=</span><span class="s2">None</span><span class="s1">):</span>
        <span class="s5">&quot;&quot;&quot;Draw random samples from a multivariate t-distribution. 
 
        Parameters 
        ---------- 
        %(_mvt_doc_default_callparams)s 
        size : integer, optional 
            Number of samples to draw (default 1). 
        %(_doc_random_state)s 
 
        Returns 
        ------- 
        rvs : ndarray or scalar 
            Random variates of size (`size`, `P`), where `P` is the 
            dimension of the random variable. 
 
        Examples 
        -------- 
        &gt;&gt;&gt; from scipy.stats import multivariate_t 
        &gt;&gt;&gt; x = [0.4, 5] 
        &gt;&gt;&gt; loc = [0, 1] 
        &gt;&gt;&gt; shape = [[1, 0.1], [0.1, 1]] 
        &gt;&gt;&gt; df = 7 
        &gt;&gt;&gt; multivariate_t.rvs(loc, shape, df) 
        array([[0.93477495, 3.00408716]]) 
 
        &quot;&quot;&quot;</span>
        <span class="s0"># For implementation details, see equation (3):</span>
        <span class="s0">#</span>
        <span class="s0">#    Hofert, &quot;On Sampling from the Multivariatet Distribution&quot;, 2013</span>
        <span class="s0">#     http://rjournal.github.io/archive/2013-2/hofert.pdf</span>
        <span class="s0">#</span>
        <span class="s1">dim</span><span class="s2">, </span><span class="s1">loc</span><span class="s2">, </span><span class="s1">shape</span><span class="s2">, </span><span class="s1">df = self._process_parameters(loc</span><span class="s2">, </span><span class="s1">shape</span><span class="s2">, </span><span class="s1">df)</span>
        <span class="s2">if </span><span class="s1">random_state </span><span class="s2">is not None</span><span class="s1">:</span>
            <span class="s1">rng = check_random_state(random_state)</span>
        <span class="s2">else</span><span class="s1">:</span>
            <span class="s1">rng = self._random_state</span>

        <span class="s2">if </span><span class="s1">np.isinf(df):</span>
            <span class="s1">x = np.ones(size)</span>
        <span class="s2">else</span><span class="s1">:</span>
            <span class="s1">x = rng.chisquare(df</span><span class="s2">, </span><span class="s1">size=size) / df</span>

        <span class="s1">z = rng.multivariate_normal(np.zeros(dim)</span><span class="s2">, </span><span class="s1">shape</span><span class="s2">, </span><span class="s1">size=size)</span>
        <span class="s1">samples = loc + z / np.sqrt(x)[...</span><span class="s2">, None</span><span class="s1">]</span>
        <span class="s2">return </span><span class="s1">_squeeze_output(samples)</span>

    <span class="s2">def </span><span class="s1">_process_quantiles(self</span><span class="s2">, </span><span class="s1">x</span><span class="s2">, </span><span class="s1">dim):</span>
        <span class="s5">&quot;&quot;&quot; 
        Adjust quantiles array so that last axis labels the components of 
        each data point. 
        &quot;&quot;&quot;</span>
        <span class="s1">x = np.asarray(x</span><span class="s2">, </span><span class="s1">dtype=float)</span>
        <span class="s2">if </span><span class="s1">x.ndim == </span><span class="s4">0</span><span class="s1">:</span>
            <span class="s1">x = x[np.newaxis]</span>
        <span class="s2">elif </span><span class="s1">x.ndim == </span><span class="s4">1</span><span class="s1">:</span>
            <span class="s2">if </span><span class="s1">dim == </span><span class="s4">1</span><span class="s1">:</span>
                <span class="s1">x = x[:</span><span class="s2">, </span><span class="s1">np.newaxis]</span>
            <span class="s2">else</span><span class="s1">:</span>
                <span class="s1">x = x[np.newaxis</span><span class="s2">, </span><span class="s1">:]</span>
        <span class="s2">return </span><span class="s1">x</span>

    <span class="s2">def </span><span class="s1">_process_parameters(self</span><span class="s2">, </span><span class="s1">loc</span><span class="s2">, </span><span class="s1">shape</span><span class="s2">, </span><span class="s1">df):</span>
        <span class="s5">&quot;&quot;&quot; 
        Infer dimensionality from location array and shape matrix, handle 
        defaults, and ensure compatible dimensions. 
        &quot;&quot;&quot;</span>
        <span class="s2">if </span><span class="s1">loc </span><span class="s2">is None and </span><span class="s1">shape </span><span class="s2">is None</span><span class="s1">:</span>
            <span class="s1">loc = np.asarray(</span><span class="s4">0</span><span class="s2">, </span><span class="s1">dtype=float)</span>
            <span class="s1">shape = np.asarray(</span><span class="s4">1</span><span class="s2">, </span><span class="s1">dtype=float)</span>
            <span class="s1">dim = </span><span class="s4">1</span>
        <span class="s2">elif </span><span class="s1">loc </span><span class="s2">is None</span><span class="s1">:</span>
            <span class="s1">shape = np.asarray(shape</span><span class="s2">, </span><span class="s1">dtype=float)</span>
            <span class="s2">if </span><span class="s1">shape.ndim &lt; </span><span class="s4">2</span><span class="s1">:</span>
                <span class="s1">dim = </span><span class="s4">1</span>
            <span class="s2">else</span><span class="s1">:</span>
                <span class="s1">dim = shape.shape[</span><span class="s4">0</span><span class="s1">]</span>
            <span class="s1">loc = np.zeros(dim)</span>
        <span class="s2">elif </span><span class="s1">shape </span><span class="s2">is None</span><span class="s1">:</span>
            <span class="s1">loc = np.asarray(loc</span><span class="s2">, </span><span class="s1">dtype=float)</span>
            <span class="s1">dim = loc.size</span>
            <span class="s1">shape = np.eye(dim)</span>
        <span class="s2">else</span><span class="s1">:</span>
            <span class="s1">shape = np.asarray(shape</span><span class="s2">, </span><span class="s1">dtype=float)</span>
            <span class="s1">loc = np.asarray(loc</span><span class="s2">, </span><span class="s1">dtype=float)</span>
            <span class="s1">dim = loc.size</span>

        <span class="s2">if </span><span class="s1">dim == </span><span class="s4">1</span><span class="s1">:</span>
            <span class="s1">loc = loc.reshape(</span><span class="s4">1</span><span class="s1">)</span>
            <span class="s1">shape = shape.reshape(</span><span class="s4">1</span><span class="s2">, </span><span class="s4">1</span><span class="s1">)</span>

        <span class="s2">if </span><span class="s1">loc.ndim != </span><span class="s4">1 </span><span class="s2">or </span><span class="s1">loc.shape[</span><span class="s4">0</span><span class="s1">] != dim:</span>
            <span class="s2">raise </span><span class="s1">ValueError(</span><span class="s3">&quot;Array 'loc' must be a vector of length %d.&quot; </span><span class="s1">%</span>
                             <span class="s1">dim)</span>
        <span class="s2">if </span><span class="s1">shape.ndim == </span><span class="s4">0</span><span class="s1">:</span>
            <span class="s1">shape = shape * np.eye(dim)</span>
        <span class="s2">elif </span><span class="s1">shape.ndim == </span><span class="s4">1</span><span class="s1">:</span>
            <span class="s1">shape = np.diag(shape)</span>
        <span class="s2">elif </span><span class="s1">shape.ndim == </span><span class="s4">2 </span><span class="s2">and </span><span class="s1">shape.shape != (dim</span><span class="s2">, </span><span class="s1">dim):</span>
            <span class="s1">rows</span><span class="s2">, </span><span class="s1">cols = shape.shape</span>
            <span class="s2">if </span><span class="s1">rows != cols:</span>
                <span class="s1">msg = (</span><span class="s3">&quot;Array 'cov' must be square if it is two dimensional,&quot;</span>
                       <span class="s3">&quot; but cov.shape = %s.&quot; </span><span class="s1">% str(shape.shape))</span>
            <span class="s2">else</span><span class="s1">:</span>
                <span class="s1">msg = (</span><span class="s3">&quot;Dimension mismatch: array 'cov' is of shape %s,&quot;</span>
                       <span class="s3">&quot; but 'loc' is a vector of length %d.&quot;</span><span class="s1">)</span>
                <span class="s1">msg = msg % (str(shape.shape)</span><span class="s2">, </span><span class="s1">len(loc))</span>
            <span class="s2">raise </span><span class="s1">ValueError(msg)</span>
        <span class="s2">elif </span><span class="s1">shape.ndim &gt; </span><span class="s4">2</span><span class="s1">:</span>
            <span class="s2">raise </span><span class="s1">ValueError(</span><span class="s3">&quot;Array 'cov' must be at most two-dimensional,&quot;</span>
                             <span class="s3">&quot; but cov.ndim = %d&quot; </span><span class="s1">% shape.ndim)</span>

        <span class="s0"># Process degrees of freedom.</span>
        <span class="s2">if </span><span class="s1">df </span><span class="s2">is None</span><span class="s1">:</span>
            <span class="s1">df = </span><span class="s4">1</span>
        <span class="s2">elif </span><span class="s1">df &lt;= </span><span class="s4">0</span><span class="s1">:</span>
            <span class="s2">raise </span><span class="s1">ValueError(</span><span class="s3">&quot;'df' must be greater than zero.&quot;</span><span class="s1">)</span>
        <span class="s2">elif </span><span class="s1">np.isnan(df):</span>
            <span class="s2">raise </span><span class="s1">ValueError(</span><span class="s3">&quot;'df' is 'nan' but must be greater than zero or 'np.inf'.&quot;</span><span class="s1">)</span>

        <span class="s2">return </span><span class="s1">dim</span><span class="s2">, </span><span class="s1">loc</span><span class="s2">, </span><span class="s1">shape</span><span class="s2">, </span><span class="s1">df</span>


<span class="s2">class </span><span class="s1">multivariate_t_frozen(multi_rv_frozen):</span>

    <span class="s2">def </span><span class="s1">__init__(self</span><span class="s2">, </span><span class="s1">loc=</span><span class="s2">None, </span><span class="s1">shape=</span><span class="s4">1</span><span class="s2">, </span><span class="s1">df=</span><span class="s4">1</span><span class="s2">, </span><span class="s1">allow_singular=</span><span class="s2">False,</span>
                 <span class="s1">seed=</span><span class="s2">None</span><span class="s1">):</span>
        <span class="s5">&quot;&quot;&quot;Create a frozen multivariate t distribution. 
 
        Parameters 
        ---------- 
        %(_mvt_doc_default_callparams)s 
 
        Examples 
        -------- 
        &gt;&gt;&gt; import numpy as np 
        &gt;&gt;&gt; loc = np.zeros(3) 
        &gt;&gt;&gt; shape = np.eye(3) 
        &gt;&gt;&gt; df = 10 
        &gt;&gt;&gt; dist = multivariate_t(loc, shape, df) 
        &gt;&gt;&gt; dist.rvs() 
        array([[ 0.81412036, -1.53612361,  0.42199647]]) 
        &gt;&gt;&gt; dist.pdf([1, 1, 1]) 
        array([0.01237803]) 
 
        &quot;&quot;&quot;</span>
        <span class="s1">self._dist = multivariate_t_gen(seed)</span>
        <span class="s1">dim</span><span class="s2">, </span><span class="s1">loc</span><span class="s2">, </span><span class="s1">shape</span><span class="s2">, </span><span class="s1">df = self._dist._process_parameters(loc</span><span class="s2">, </span><span class="s1">shape</span><span class="s2">, </span><span class="s1">df)</span>
        <span class="s1">self.dim</span><span class="s2">, </span><span class="s1">self.loc</span><span class="s2">, </span><span class="s1">self.shape</span><span class="s2">, </span><span class="s1">self.df = dim</span><span class="s2">, </span><span class="s1">loc</span><span class="s2">, </span><span class="s1">shape</span><span class="s2">, </span><span class="s1">df</span>
        <span class="s1">self.shape_info = _PSD(shape</span><span class="s2">, </span><span class="s1">allow_singular=allow_singular)</span>

    <span class="s2">def </span><span class="s1">logpdf(self</span><span class="s2">, </span><span class="s1">x):</span>
        <span class="s1">x = self._dist._process_quantiles(x</span><span class="s2">, </span><span class="s1">self.dim)</span>
        <span class="s1">U = self.shape_info.U</span>
        <span class="s1">log_pdet = self.shape_info.log_pdet</span>
        <span class="s2">return </span><span class="s1">self._dist._logpdf(x</span><span class="s2">, </span><span class="s1">self.loc</span><span class="s2">, </span><span class="s1">U</span><span class="s2">, </span><span class="s1">log_pdet</span><span class="s2">, </span><span class="s1">self.df</span><span class="s2">, </span><span class="s1">self.dim</span><span class="s2">,</span>
                                  <span class="s1">self.shape_info.rank)</span>

    <span class="s2">def </span><span class="s1">cdf(self</span><span class="s2">, </span><span class="s1">x</span><span class="s2">, </span><span class="s1">*</span><span class="s2">, </span><span class="s1">maxpts=</span><span class="s2">None, </span><span class="s1">lower_limit=</span><span class="s2">None, </span><span class="s1">random_state=</span><span class="s2">None</span><span class="s1">):</span>
        <span class="s1">x = self._dist._process_quantiles(x</span><span class="s2">, </span><span class="s1">self.dim)</span>
        <span class="s2">return </span><span class="s1">self._dist._cdf(x</span><span class="s2">, </span><span class="s1">self.loc</span><span class="s2">, </span><span class="s1">self.shape</span><span class="s2">, </span><span class="s1">self.df</span><span class="s2">, </span><span class="s1">self.dim</span><span class="s2">,</span>
                               <span class="s1">maxpts</span><span class="s2">, </span><span class="s1">lower_limit</span><span class="s2">, </span><span class="s1">random_state)</span>

    <span class="s2">def </span><span class="s1">pdf(self</span><span class="s2">, </span><span class="s1">x):</span>
        <span class="s2">return </span><span class="s1">np.exp(self.logpdf(x))</span>

    <span class="s2">def </span><span class="s1">rvs(self</span><span class="s2">, </span><span class="s1">size=</span><span class="s4">1</span><span class="s2">, </span><span class="s1">random_state=</span><span class="s2">None</span><span class="s1">):</span>
        <span class="s2">return </span><span class="s1">self._dist.rvs(loc=self.loc</span><span class="s2">,</span>
                              <span class="s1">shape=self.shape</span><span class="s2">,</span>
                              <span class="s1">df=self.df</span><span class="s2">,</span>
                              <span class="s1">size=size</span><span class="s2">,</span>
                              <span class="s1">random_state=random_state)</span>

    <span class="s2">def </span><span class="s1">entropy(self):</span>
        <span class="s2">return </span><span class="s1">self._dist._entropy(self.dim</span><span class="s2">, </span><span class="s1">self.df</span><span class="s2">, </span><span class="s1">self.shape)</span>


<span class="s1">multivariate_t = multivariate_t_gen()</span>


<span class="s0"># Set frozen generator docstrings from corresponding docstrings in</span>
<span class="s0"># multivariate_t_gen and fill in default strings in class docstrings</span>
<span class="s2">for </span><span class="s1">name </span><span class="s2">in </span><span class="s1">[</span><span class="s3">'logpdf'</span><span class="s2">, </span><span class="s3">'pdf'</span><span class="s2">, </span><span class="s3">'rvs'</span><span class="s2">, </span><span class="s3">'cdf'</span><span class="s2">, </span><span class="s3">'entropy'</span><span class="s1">]:</span>
    <span class="s1">method = multivariate_t_gen.__dict__[name]</span>
    <span class="s1">method_frozen = multivariate_t_frozen.__dict__[name]</span>
    <span class="s1">method_frozen.__doc__ = doccer.docformat(method.__doc__</span><span class="s2">,</span>
                                             <span class="s1">mvt_docdict_noparams)</span>
    <span class="s1">method.__doc__ = doccer.docformat(method.__doc__</span><span class="s2">, </span><span class="s1">mvt_docdict_params)</span>


<span class="s1">_mhg_doc_default_callparams = </span><span class="s3">&quot;&quot;&quot;</span><span class="s2">\ 
</span><span class="s3">m : array_like 
    The number of each type of object in the population. 
    That is, :math:`m[i]` is the number of objects of 
    type :math:`i`. 
n : array_like 
    The number of samples taken from the population. 
&quot;&quot;&quot;</span>

<span class="s1">_mhg_doc_callparams_note = </span><span class="s3">&quot;&quot;&quot;</span><span class="s2">\ 
</span><span class="s3">`m` must be an array of positive integers. If the quantile 
:math:`i` contains values out of the range :math:`[0, m_i]` 
where :math:`m_i` is the number of objects of type :math:`i` 
in the population or if the parameters are inconsistent with one 
another (e.g. ``x.sum() != n``), methods return the appropriate 
value (e.g. ``0`` for ``pmf``). If `m` or `n` contain negative 
values, the result will contain ``nan`` there. 
&quot;&quot;&quot;</span>

<span class="s1">_mhg_doc_frozen_callparams = </span><span class="s3">&quot;&quot;</span>

<span class="s1">_mhg_doc_frozen_callparams_note = </span><span class="s3">&quot;&quot;&quot;</span><span class="s2">\ 
</span><span class="s3">See class definition for a detailed description of parameters.&quot;&quot;&quot;</span>

<span class="s1">mhg_docdict_params = {</span>
    <span class="s3">'_doc_default_callparams'</span><span class="s1">: _mhg_doc_default_callparams</span><span class="s2">,</span>
    <span class="s3">'_doc_callparams_note'</span><span class="s1">: _mhg_doc_callparams_note</span><span class="s2">,</span>
    <span class="s3">'_doc_random_state'</span><span class="s1">: _doc_random_state</span>
<span class="s1">}</span>

<span class="s1">mhg_docdict_noparams = {</span>
    <span class="s3">'_doc_default_callparams'</span><span class="s1">: _mhg_doc_frozen_callparams</span><span class="s2">,</span>
    <span class="s3">'_doc_callparams_note'</span><span class="s1">: _mhg_doc_frozen_callparams_note</span><span class="s2">,</span>
    <span class="s3">'_doc_random_state'</span><span class="s1">: _doc_random_state</span>
<span class="s1">}</span>


<span class="s2">class </span><span class="s1">multivariate_hypergeom_gen(multi_rv_generic):</span>
    <span class="s5">r&quot;&quot;&quot;A multivariate hypergeometric random variable. 
 
    Methods 
    ------- 
    pmf(x, m, n) 
        Probability mass function. 
    logpmf(x, m, n) 
        Log of the probability mass function. 
    rvs(m, n, size=1, random_state=None) 
        Draw random samples from a multivariate hypergeometric 
        distribution. 
    mean(m, n) 
        Mean of the multivariate hypergeometric distribution. 
    var(m, n) 
        Variance of the multivariate hypergeometric distribution. 
    cov(m, n) 
        Compute the covariance matrix of the multivariate 
        hypergeometric distribution. 
 
    Parameters 
    ---------- 
    %(_doc_default_callparams)s 
    %(_doc_random_state)s 
 
    Notes 
    ----- 
    %(_doc_callparams_note)s 
 
    The probability mass function for `multivariate_hypergeom` is 
 
    .. math:: 
 
        P(X_1 = x_1, X_2 = x_2, \ldots, X_k = x_k) = \frac{\binom{m_1}{x_1} 
        \binom{m_2}{x_2} \cdots \binom{m_k}{x_k}}{\binom{M}{n}}, \\ \quad 
        (x_1, x_2, \ldots, x_k) \in \mathbb{N}^k \text{ with } 
        \sum_{i=1}^k x_i = n 
 
    where :math:`m_i` are the number of objects of type :math:`i`, :math:`M` 
    is the total number of objects in the population (sum of all the 
    :math:`m_i`), and :math:`n` is the size of the sample to be taken 
    from the population. 
 
    .. versionadded:: 1.6.0 
 
    Examples 
    -------- 
    To evaluate the probability mass function of the multivariate 
    hypergeometric distribution, with a dichotomous population of size 
    :math:`10` and :math:`20`, at a sample of size :math:`12` with 
    :math:`8` objects of the first type and :math:`4` objects of the 
    second type, use: 
 
    &gt;&gt;&gt; from scipy.stats import multivariate_hypergeom 
    &gt;&gt;&gt; multivariate_hypergeom.pmf(x=[8, 4], m=[10, 20], n=12) 
    0.0025207176631464523 
 
    The `multivariate_hypergeom` distribution is identical to the 
    corresponding `hypergeom` distribution (tiny numerical differences 
    notwithstanding) when only two types (good and bad) of objects 
    are present in the population as in the example above. Consider 
    another example for a comparison with the hypergeometric distribution: 
 
    &gt;&gt;&gt; from scipy.stats import hypergeom 
    &gt;&gt;&gt; multivariate_hypergeom.pmf(x=[3, 1], m=[10, 5], n=4) 
    0.4395604395604395 
    &gt;&gt;&gt; hypergeom.pmf(k=3, M=15, n=4, N=10) 
    0.43956043956044005 
 
    The functions ``pmf``, ``logpmf``, ``mean``, ``var``, ``cov``, and ``rvs`` 
    support broadcasting, under the convention that the vector parameters 
    (``x``, ``m``, and ``n``) are interpreted as if each row along the last 
    axis is a single object. For instance, we can combine the previous two 
    calls to `multivariate_hypergeom` as 
 
    &gt;&gt;&gt; multivariate_hypergeom.pmf(x=[[8, 4], [3, 1]], m=[[10, 20], [10, 5]], 
    ...                            n=[12, 4]) 
    array([0.00252072, 0.43956044]) 
 
    This broadcasting also works for ``cov``, where the output objects are 
    square matrices of size ``m.shape[-1]``. For example: 
 
    &gt;&gt;&gt; multivariate_hypergeom.cov(m=[[7, 9], [10, 15]], n=[8, 12]) 
    array([[[ 1.05, -1.05], 
            [-1.05,  1.05]], 
           [[ 1.56, -1.56], 
            [-1.56,  1.56]]]) 
 
    That is, ``result[0]`` is equal to 
    ``multivariate_hypergeom.cov(m=[7, 9], n=8)`` and ``result[1]`` is equal 
    to ``multivariate_hypergeom.cov(m=[10, 15], n=12)``. 
 
    Alternatively, the object may be called (as a function) to fix the `m` 
    and `n` parameters, returning a &quot;frozen&quot; multivariate hypergeometric 
    random variable. 
 
    &gt;&gt;&gt; rv = multivariate_hypergeom(m=[10, 20], n=12) 
    &gt;&gt;&gt; rv.pmf(x=[8, 4]) 
    0.0025207176631464523 
 
    See Also 
    -------- 
    scipy.stats.hypergeom : The hypergeometric distribution. 
    scipy.stats.multinomial : The multinomial distribution. 
 
    References 
    ---------- 
    .. [1] The Multivariate Hypergeometric Distribution, 
           http://www.randomservices.org/random/urn/MultiHypergeometric.html 
    .. [2] Thomas J. Sargent and John Stachurski, 2020, 
           Multivariate Hypergeometric Distribution 
           https://python.quantecon.org/_downloads/pdf/multi_hyper.pdf 
    &quot;&quot;&quot;</span>
    <span class="s2">def </span><span class="s1">__init__(self</span><span class="s2">, </span><span class="s1">seed=</span><span class="s2">None</span><span class="s1">):</span>
        <span class="s1">super().__init__(seed)</span>
        <span class="s1">self.__doc__ = doccer.docformat(self.__doc__</span><span class="s2">, </span><span class="s1">mhg_docdict_params)</span>

    <span class="s2">def </span><span class="s1">__call__(self</span><span class="s2">, </span><span class="s1">m</span><span class="s2">, </span><span class="s1">n</span><span class="s2">, </span><span class="s1">seed=</span><span class="s2">None</span><span class="s1">):</span>
        <span class="s5">&quot;&quot;&quot;Create a frozen multivariate_hypergeom distribution. 
 
        See `multivariate_hypergeom_frozen` for more information. 
        &quot;&quot;&quot;</span>
        <span class="s2">return </span><span class="s1">multivariate_hypergeom_frozen(m</span><span class="s2">, </span><span class="s1">n</span><span class="s2">, </span><span class="s1">seed=seed)</span>

    <span class="s2">def </span><span class="s1">_process_parameters(self</span><span class="s2">, </span><span class="s1">m</span><span class="s2">, </span><span class="s1">n):</span>
        <span class="s1">m = np.asarray(m)</span>
        <span class="s1">n = np.asarray(n)</span>
        <span class="s2">if </span><span class="s1">m.size == </span><span class="s4">0</span><span class="s1">:</span>
            <span class="s1">m = m.astype(int)</span>
        <span class="s2">if </span><span class="s1">n.size == </span><span class="s4">0</span><span class="s1">:</span>
            <span class="s1">n = n.astype(int)</span>
        <span class="s2">if not </span><span class="s1">np.issubdtype(m.dtype</span><span class="s2">, </span><span class="s1">np.integer):</span>
            <span class="s2">raise </span><span class="s1">TypeError(</span><span class="s3">&quot;'m' must an array of integers.&quot;</span><span class="s1">)</span>
        <span class="s2">if not </span><span class="s1">np.issubdtype(n.dtype</span><span class="s2">, </span><span class="s1">np.integer):</span>
            <span class="s2">raise </span><span class="s1">TypeError(</span><span class="s3">&quot;'n' must an array of integers.&quot;</span><span class="s1">)</span>
        <span class="s2">if </span><span class="s1">m.ndim == </span><span class="s4">0</span><span class="s1">:</span>
            <span class="s2">raise </span><span class="s1">ValueError(</span><span class="s3">&quot;'m' must be an array with&quot;</span>
                             <span class="s3">&quot; at least one dimension.&quot;</span><span class="s1">)</span>

        <span class="s0"># check for empty arrays</span>
        <span class="s2">if </span><span class="s1">m.size != </span><span class="s4">0</span><span class="s1">:</span>
            <span class="s1">n = n[...</span><span class="s2">, </span><span class="s1">np.newaxis]</span>

        <span class="s1">m</span><span class="s2">, </span><span class="s1">n = np.broadcast_arrays(m</span><span class="s2">, </span><span class="s1">n)</span>

        <span class="s0"># check for empty arrays</span>
        <span class="s2">if </span><span class="s1">m.size != </span><span class="s4">0</span><span class="s1">:</span>
            <span class="s1">n = n[...</span><span class="s2">, </span><span class="s4">0</span><span class="s1">]</span>

        <span class="s1">mcond = m &lt; </span><span class="s4">0</span>

        <span class="s1">M = m.sum(axis=-</span><span class="s4">1</span><span class="s1">)</span>

        <span class="s1">ncond = (n &lt; </span><span class="s4">0</span><span class="s1">) | (n &gt; M)</span>
        <span class="s2">return </span><span class="s1">M</span><span class="s2">, </span><span class="s1">m</span><span class="s2">, </span><span class="s1">n</span><span class="s2">, </span><span class="s1">mcond</span><span class="s2">, </span><span class="s1">ncond</span><span class="s2">, </span><span class="s1">np.any(mcond</span><span class="s2">, </span><span class="s1">axis=-</span><span class="s4">1</span><span class="s1">) | ncond</span>

    <span class="s2">def </span><span class="s1">_process_quantiles(self</span><span class="s2">, </span><span class="s1">x</span><span class="s2">, </span><span class="s1">M</span><span class="s2">, </span><span class="s1">m</span><span class="s2">, </span><span class="s1">n):</span>
        <span class="s1">x = np.asarray(x)</span>
        <span class="s2">if not </span><span class="s1">np.issubdtype(x.dtype</span><span class="s2">, </span><span class="s1">np.integer):</span>
            <span class="s2">raise </span><span class="s1">TypeError(</span><span class="s3">&quot;'x' must an array of integers.&quot;</span><span class="s1">)</span>
        <span class="s2">if </span><span class="s1">x.ndim == </span><span class="s4">0</span><span class="s1">:</span>
            <span class="s2">raise </span><span class="s1">ValueError(</span><span class="s3">&quot;'x' must be an array with&quot;</span>
                             <span class="s3">&quot; at least one dimension.&quot;</span><span class="s1">)</span>
        <span class="s2">if not </span><span class="s1">x.shape[-</span><span class="s4">1</span><span class="s1">] == m.shape[-</span><span class="s4">1</span><span class="s1">]:</span>
            <span class="s2">raise </span><span class="s1">ValueError(</span><span class="s3">f&quot;Size of each quantile must be size of 'm': &quot;</span>
                             <span class="s3">f&quot;received </span><span class="s2">{</span><span class="s1">x.shape[-</span><span class="s4">1</span><span class="s1">]</span><span class="s2">}</span><span class="s3">, &quot;</span>
                             <span class="s3">f&quot;but expected </span><span class="s2">{</span><span class="s1">m.shape[-</span><span class="s4">1</span><span class="s1">]</span><span class="s2">}</span><span class="s3">.&quot;</span><span class="s1">)</span>

        <span class="s0"># check for empty arrays</span>
        <span class="s2">if </span><span class="s1">m.size != </span><span class="s4">0</span><span class="s1">:</span>
            <span class="s1">n = n[...</span><span class="s2">, </span><span class="s1">np.newaxis]</span>
            <span class="s1">M = M[...</span><span class="s2">, </span><span class="s1">np.newaxis]</span>

        <span class="s1">x</span><span class="s2">, </span><span class="s1">m</span><span class="s2">, </span><span class="s1">n</span><span class="s2">, </span><span class="s1">M = np.broadcast_arrays(x</span><span class="s2">, </span><span class="s1">m</span><span class="s2">, </span><span class="s1">n</span><span class="s2">, </span><span class="s1">M)</span>

        <span class="s0"># check for empty arrays</span>
        <span class="s2">if </span><span class="s1">m.size != </span><span class="s4">0</span><span class="s1">:</span>
            <span class="s1">n</span><span class="s2">, </span><span class="s1">M = n[...</span><span class="s2">, </span><span class="s4">0</span><span class="s1">]</span><span class="s2">, </span><span class="s1">M[...</span><span class="s2">, </span><span class="s4">0</span><span class="s1">]</span>

        <span class="s1">xcond = (x &lt; </span><span class="s4">0</span><span class="s1">) | (x &gt; m)</span>
        <span class="s2">return </span><span class="s1">(x</span><span class="s2">, </span><span class="s1">M</span><span class="s2">, </span><span class="s1">m</span><span class="s2">, </span><span class="s1">n</span><span class="s2">, </span><span class="s1">xcond</span><span class="s2">,</span>
                <span class="s1">np.any(xcond</span><span class="s2">, </span><span class="s1">axis=-</span><span class="s4">1</span><span class="s1">) | (x.sum(axis=-</span><span class="s4">1</span><span class="s1">) != n))</span>

    <span class="s2">def </span><span class="s1">_checkresult(self</span><span class="s2">, </span><span class="s1">result</span><span class="s2">, </span><span class="s1">cond</span><span class="s2">, </span><span class="s1">bad_value):</span>
        <span class="s1">result = np.asarray(result)</span>
        <span class="s2">if </span><span class="s1">cond.ndim != </span><span class="s4">0</span><span class="s1">:</span>
            <span class="s1">result[cond] = bad_value</span>
        <span class="s2">elif </span><span class="s1">cond:</span>
            <span class="s2">return </span><span class="s1">bad_value</span>
        <span class="s2">if </span><span class="s1">result.ndim == </span><span class="s4">0</span><span class="s1">:</span>
            <span class="s2">return </span><span class="s1">result[()]</span>
        <span class="s2">return </span><span class="s1">result</span>

    <span class="s2">def </span><span class="s1">_logpmf(self</span><span class="s2">, </span><span class="s1">x</span><span class="s2">, </span><span class="s1">M</span><span class="s2">, </span><span class="s1">m</span><span class="s2">, </span><span class="s1">n</span><span class="s2">, </span><span class="s1">mxcond</span><span class="s2">, </span><span class="s1">ncond):</span>
        <span class="s0"># This equation of the pmf comes from the relation,</span>
        <span class="s0"># n combine r = beta(n+1, 1) / beta(r+1, n-r+1)</span>
        <span class="s1">num = np.zeros_like(m</span><span class="s2">, </span><span class="s1">dtype=np.float_)</span>
        <span class="s1">den = np.zeros_like(n</span><span class="s2">, </span><span class="s1">dtype=np.float_)</span>
        <span class="s1">m</span><span class="s2">, </span><span class="s1">x = m[~mxcond]</span><span class="s2">, </span><span class="s1">x[~mxcond]</span>
        <span class="s1">M</span><span class="s2">, </span><span class="s1">n = M[~ncond]</span><span class="s2">, </span><span class="s1">n[~ncond]</span>
        <span class="s1">num[~mxcond] = (betaln(m+</span><span class="s4">1</span><span class="s2">, </span><span class="s4">1</span><span class="s1">) - betaln(x+</span><span class="s4">1</span><span class="s2">, </span><span class="s1">m-x+</span><span class="s4">1</span><span class="s1">))</span>
        <span class="s1">den[~ncond] = (betaln(M+</span><span class="s4">1</span><span class="s2">, </span><span class="s4">1</span><span class="s1">) - betaln(n+</span><span class="s4">1</span><span class="s2">, </span><span class="s1">M-n+</span><span class="s4">1</span><span class="s1">))</span>
        <span class="s1">num[mxcond] = np.nan</span>
        <span class="s1">den[ncond] = np.nan</span>
        <span class="s1">num = num.sum(axis=-</span><span class="s4">1</span><span class="s1">)</span>
        <span class="s2">return </span><span class="s1">num - den</span>

    <span class="s2">def </span><span class="s1">logpmf(self</span><span class="s2">, </span><span class="s1">x</span><span class="s2">, </span><span class="s1">m</span><span class="s2">, </span><span class="s1">n):</span>
        <span class="s5">&quot;&quot;&quot;Log of the multivariate hypergeometric probability mass function. 
 
        Parameters 
        ---------- 
        x : array_like 
            Quantiles, with the last axis of `x` denoting the components. 
        %(_doc_default_callparams)s 
 
        Returns 
        ------- 
        logpmf : ndarray or scalar 
            Log of the probability mass function evaluated at `x` 
 
        Notes 
        ----- 
        %(_doc_callparams_note)s 
        &quot;&quot;&quot;</span>
        <span class="s1">M</span><span class="s2">, </span><span class="s1">m</span><span class="s2">, </span><span class="s1">n</span><span class="s2">, </span><span class="s1">mcond</span><span class="s2">, </span><span class="s1">ncond</span><span class="s2">, </span><span class="s1">mncond = self._process_parameters(m</span><span class="s2">, </span><span class="s1">n)</span>
        <span class="s1">(x</span><span class="s2">, </span><span class="s1">M</span><span class="s2">, </span><span class="s1">m</span><span class="s2">, </span><span class="s1">n</span><span class="s2">, </span><span class="s1">xcond</span><span class="s2">,</span>
         <span class="s1">xcond_reduced) = self._process_quantiles(x</span><span class="s2">, </span><span class="s1">M</span><span class="s2">, </span><span class="s1">m</span><span class="s2">, </span><span class="s1">n)</span>
        <span class="s1">mxcond = mcond | xcond</span>
        <span class="s1">ncond = ncond | np.zeros(n.shape</span><span class="s2">, </span><span class="s1">dtype=np.bool_)</span>

        <span class="s1">result = self._logpmf(x</span><span class="s2">, </span><span class="s1">M</span><span class="s2">, </span><span class="s1">m</span><span class="s2">, </span><span class="s1">n</span><span class="s2">, </span><span class="s1">mxcond</span><span class="s2">, </span><span class="s1">ncond)</span>

        <span class="s0"># replace values for which x was out of the domain; broadcast</span>
        <span class="s0"># xcond to the right shape</span>
        <span class="s1">xcond_ = xcond_reduced | np.zeros(mncond.shape</span><span class="s2">, </span><span class="s1">dtype=np.bool_)</span>
        <span class="s1">result = self._checkresult(result</span><span class="s2">, </span><span class="s1">xcond_</span><span class="s2">, </span><span class="s1">-np.inf)</span>

        <span class="s0"># replace values bad for n or m; broadcast</span>
        <span class="s0"># mncond to the right shape</span>
        <span class="s1">mncond_ = mncond | np.zeros(xcond_reduced.shape</span><span class="s2">, </span><span class="s1">dtype=np.bool_)</span>
        <span class="s2">return </span><span class="s1">self._checkresult(result</span><span class="s2">, </span><span class="s1">mncond_</span><span class="s2">, </span><span class="s1">np.nan)</span>

    <span class="s2">def </span><span class="s1">pmf(self</span><span class="s2">, </span><span class="s1">x</span><span class="s2">, </span><span class="s1">m</span><span class="s2">, </span><span class="s1">n):</span>
        <span class="s5">&quot;&quot;&quot;Multivariate hypergeometric probability mass function. 
 
        Parameters 
        ---------- 
        x : array_like 
            Quantiles, with the last axis of `x` denoting the components. 
        %(_doc_default_callparams)s 
 
        Returns 
        ------- 
        pmf : ndarray or scalar 
            Probability density function evaluated at `x` 
 
        Notes 
        ----- 
        %(_doc_callparams_note)s 
        &quot;&quot;&quot;</span>
        <span class="s1">out = np.exp(self.logpmf(x</span><span class="s2">, </span><span class="s1">m</span><span class="s2">, </span><span class="s1">n))</span>
        <span class="s2">return </span><span class="s1">out</span>

    <span class="s2">def </span><span class="s1">mean(self</span><span class="s2">, </span><span class="s1">m</span><span class="s2">, </span><span class="s1">n):</span>
        <span class="s5">&quot;&quot;&quot;Mean of the multivariate hypergeometric distribution. 
 
        Parameters 
        ---------- 
        %(_doc_default_callparams)s 
 
        Returns 
        ------- 
        mean : array_like or scalar 
            The mean of the distribution 
        &quot;&quot;&quot;</span>
        <span class="s1">M</span><span class="s2">, </span><span class="s1">m</span><span class="s2">, </span><span class="s1">n</span><span class="s2">, </span><span class="s1">_</span><span class="s2">, </span><span class="s1">_</span><span class="s2">, </span><span class="s1">mncond = self._process_parameters(m</span><span class="s2">, </span><span class="s1">n)</span>
        <span class="s0"># check for empty arrays</span>
        <span class="s2">if </span><span class="s1">m.size != </span><span class="s4">0</span><span class="s1">:</span>
            <span class="s1">M</span><span class="s2">, </span><span class="s1">n = M[...</span><span class="s2">, </span><span class="s1">np.newaxis]</span><span class="s2">, </span><span class="s1">n[...</span><span class="s2">, </span><span class="s1">np.newaxis]</span>
        <span class="s1">cond = (M == </span><span class="s4">0</span><span class="s1">)</span>
        <span class="s1">M = np.ma.masked_array(M</span><span class="s2">, </span><span class="s1">mask=cond)</span>
        <span class="s1">mu = n*(m/M)</span>
        <span class="s2">if </span><span class="s1">m.size != </span><span class="s4">0</span><span class="s1">:</span>
            <span class="s1">mncond = (mncond[...</span><span class="s2">, </span><span class="s1">np.newaxis] |</span>
                      <span class="s1">np.zeros(mu.shape</span><span class="s2">, </span><span class="s1">dtype=np.bool_))</span>
        <span class="s2">return </span><span class="s1">self._checkresult(mu</span><span class="s2">, </span><span class="s1">mncond</span><span class="s2">, </span><span class="s1">np.nan)</span>

    <span class="s2">def </span><span class="s1">var(self</span><span class="s2">, </span><span class="s1">m</span><span class="s2">, </span><span class="s1">n):</span>
        <span class="s5">&quot;&quot;&quot;Variance of the multivariate hypergeometric distribution. 
 
        Parameters 
        ---------- 
        %(_doc_default_callparams)s 
 
        Returns 
        ------- 
        array_like 
            The variances of the components of the distribution.  This is 
            the diagonal of the covariance matrix of the distribution 
        &quot;&quot;&quot;</span>
        <span class="s1">M</span><span class="s2">, </span><span class="s1">m</span><span class="s2">, </span><span class="s1">n</span><span class="s2">, </span><span class="s1">_</span><span class="s2">, </span><span class="s1">_</span><span class="s2">, </span><span class="s1">mncond = self._process_parameters(m</span><span class="s2">, </span><span class="s1">n)</span>
        <span class="s0"># check for empty arrays</span>
        <span class="s2">if </span><span class="s1">m.size != </span><span class="s4">0</span><span class="s1">:</span>
            <span class="s1">M</span><span class="s2">, </span><span class="s1">n = M[...</span><span class="s2">, </span><span class="s1">np.newaxis]</span><span class="s2">, </span><span class="s1">n[...</span><span class="s2">, </span><span class="s1">np.newaxis]</span>
        <span class="s1">cond = (M == </span><span class="s4">0</span><span class="s1">) &amp; (M-</span><span class="s4">1 </span><span class="s1">== </span><span class="s4">0</span><span class="s1">)</span>
        <span class="s1">M = np.ma.masked_array(M</span><span class="s2">, </span><span class="s1">mask=cond)</span>
        <span class="s1">output = n * m/M * (M-m)/M * (M-n)/(M-</span><span class="s4">1</span><span class="s1">)</span>
        <span class="s2">if </span><span class="s1">m.size != </span><span class="s4">0</span><span class="s1">:</span>
            <span class="s1">mncond = (mncond[...</span><span class="s2">, </span><span class="s1">np.newaxis] |</span>
                      <span class="s1">np.zeros(output.shape</span><span class="s2">, </span><span class="s1">dtype=np.bool_))</span>
        <span class="s2">return </span><span class="s1">self._checkresult(output</span><span class="s2">, </span><span class="s1">mncond</span><span class="s2">, </span><span class="s1">np.nan)</span>

    <span class="s2">def </span><span class="s1">cov(self</span><span class="s2">, </span><span class="s1">m</span><span class="s2">, </span><span class="s1">n):</span>
        <span class="s5">&quot;&quot;&quot;Covariance matrix of the multivariate hypergeometric distribution. 
 
        Parameters 
        ---------- 
        %(_doc_default_callparams)s 
 
        Returns 
        ------- 
        cov : array_like 
            The covariance matrix of the distribution 
        &quot;&quot;&quot;</span>
        <span class="s0"># see [1]_ for the formula and [2]_ for implementation</span>
        <span class="s0"># cov( x_i,x_j ) = -n * (M-n)/(M-1) * (K_i*K_j) / (M**2)</span>
        <span class="s1">M</span><span class="s2">, </span><span class="s1">m</span><span class="s2">, </span><span class="s1">n</span><span class="s2">, </span><span class="s1">_</span><span class="s2">, </span><span class="s1">_</span><span class="s2">, </span><span class="s1">mncond = self._process_parameters(m</span><span class="s2">, </span><span class="s1">n)</span>
        <span class="s0"># check for empty arrays</span>
        <span class="s2">if </span><span class="s1">m.size != </span><span class="s4">0</span><span class="s1">:</span>
            <span class="s1">M = M[...</span><span class="s2">, </span><span class="s1">np.newaxis</span><span class="s2">, </span><span class="s1">np.newaxis]</span>
            <span class="s1">n = n[...</span><span class="s2">, </span><span class="s1">np.newaxis</span><span class="s2">, </span><span class="s1">np.newaxis]</span>
        <span class="s1">cond = (M == </span><span class="s4">0</span><span class="s1">) &amp; (M-</span><span class="s4">1 </span><span class="s1">== </span><span class="s4">0</span><span class="s1">)</span>
        <span class="s1">M = np.ma.masked_array(M</span><span class="s2">, </span><span class="s1">mask=cond)</span>
        <span class="s1">output = (-n * (M-n)/(M-</span><span class="s4">1</span><span class="s1">) *</span>
                  <span class="s1">np.einsum(</span><span class="s3">&quot;...i,...j-&gt;...ij&quot;</span><span class="s2">, </span><span class="s1">m</span><span class="s2">, </span><span class="s1">m) / (M**</span><span class="s4">2</span><span class="s1">))</span>
        <span class="s0"># check for empty arrays</span>
        <span class="s2">if </span><span class="s1">m.size != </span><span class="s4">0</span><span class="s1">:</span>
            <span class="s1">M</span><span class="s2">, </span><span class="s1">n = M[...</span><span class="s2">, </span><span class="s4">0</span><span class="s2">, </span><span class="s4">0</span><span class="s1">]</span><span class="s2">, </span><span class="s1">n[...</span><span class="s2">, </span><span class="s4">0</span><span class="s2">, </span><span class="s4">0</span><span class="s1">]</span>
            <span class="s1">cond = cond[...</span><span class="s2">, </span><span class="s4">0</span><span class="s2">, </span><span class="s4">0</span><span class="s1">]</span>
        <span class="s1">dim = m.shape[-</span><span class="s4">1</span><span class="s1">]</span>
        <span class="s0"># diagonal entries need to be computed differently</span>
        <span class="s2">for </span><span class="s1">i </span><span class="s2">in </span><span class="s1">range(dim):</span>
            <span class="s1">output[...</span><span class="s2">, </span><span class="s1">i</span><span class="s2">, </span><span class="s1">i] = (n * (M-n) * m[...</span><span class="s2">, </span><span class="s1">i]*(M-m[...</span><span class="s2">, </span><span class="s1">i]))</span>
            <span class="s1">output[...</span><span class="s2">, </span><span class="s1">i</span><span class="s2">, </span><span class="s1">i] = output[...</span><span class="s2">, </span><span class="s1">i</span><span class="s2">, </span><span class="s1">i] / (M-</span><span class="s4">1</span><span class="s1">)</span>
            <span class="s1">output[...</span><span class="s2">, </span><span class="s1">i</span><span class="s2">, </span><span class="s1">i] = output[...</span><span class="s2">, </span><span class="s1">i</span><span class="s2">, </span><span class="s1">i] / (M**</span><span class="s4">2</span><span class="s1">)</span>
        <span class="s2">if </span><span class="s1">m.size != </span><span class="s4">0</span><span class="s1">:</span>
            <span class="s1">mncond = (mncond[...</span><span class="s2">, </span><span class="s1">np.newaxis</span><span class="s2">, </span><span class="s1">np.newaxis] |</span>
                      <span class="s1">np.zeros(output.shape</span><span class="s2">, </span><span class="s1">dtype=np.bool_))</span>
        <span class="s2">return </span><span class="s1">self._checkresult(output</span><span class="s2">, </span><span class="s1">mncond</span><span class="s2">, </span><span class="s1">np.nan)</span>

    <span class="s2">def </span><span class="s1">rvs(self</span><span class="s2">, </span><span class="s1">m</span><span class="s2">, </span><span class="s1">n</span><span class="s2">, </span><span class="s1">size=</span><span class="s2">None, </span><span class="s1">random_state=</span><span class="s2">None</span><span class="s1">):</span>
        <span class="s5">&quot;&quot;&quot;Draw random samples from a multivariate hypergeometric distribution. 
 
        Parameters 
        ---------- 
        %(_doc_default_callparams)s 
        size : integer or iterable of integers, optional 
            Number of samples to draw. Default is ``None``, in which case a 
            single variate is returned as an array with shape ``m.shape``. 
        %(_doc_random_state)s 
 
        Returns 
        ------- 
        rvs : array_like 
            Random variates of shape ``size`` or ``m.shape`` 
            (if ``size=None``). 
 
        Notes 
        ----- 
        %(_doc_callparams_note)s 
 
        Also note that NumPy's `multivariate_hypergeometric` sampler is not 
        used as it doesn't support broadcasting. 
        &quot;&quot;&quot;</span>
        <span class="s1">M</span><span class="s2">, </span><span class="s1">m</span><span class="s2">, </span><span class="s1">n</span><span class="s2">, </span><span class="s1">_</span><span class="s2">, </span><span class="s1">_</span><span class="s2">, </span><span class="s1">_ = self._process_parameters(m</span><span class="s2">, </span><span class="s1">n)</span>

        <span class="s1">random_state = self._get_random_state(random_state)</span>

        <span class="s2">if </span><span class="s1">size </span><span class="s2">is not None and </span><span class="s1">isinstance(size</span><span class="s2">, </span><span class="s1">int):</span>
            <span class="s1">size = (size</span><span class="s2">, </span><span class="s1">)</span>

        <span class="s2">if </span><span class="s1">size </span><span class="s2">is None</span><span class="s1">:</span>
            <span class="s1">rvs = np.empty(m.shape</span><span class="s2">, </span><span class="s1">dtype=m.dtype)</span>
        <span class="s2">else</span><span class="s1">:</span>
            <span class="s1">rvs = np.empty(size + (m.shape[-</span><span class="s4">1</span><span class="s1">]</span><span class="s2">, </span><span class="s1">)</span><span class="s2">, </span><span class="s1">dtype=m.dtype)</span>
        <span class="s1">rem = M</span>

        <span class="s0"># This sampler has been taken from numpy gh-13794</span>
        <span class="s0"># https://github.com/numpy/numpy/pull/13794</span>
        <span class="s2">for </span><span class="s1">c </span><span class="s2">in </span><span class="s1">range(m.shape[-</span><span class="s4">1</span><span class="s1">] - </span><span class="s4">1</span><span class="s1">):</span>
            <span class="s1">rem = rem - m[...</span><span class="s2">, </span><span class="s1">c]</span>
            <span class="s1">n0mask = n == </span><span class="s4">0</span>
            <span class="s1">rvs[...</span><span class="s2">, </span><span class="s1">c] = (~n0mask *</span>
                           <span class="s1">random_state.hypergeometric(m[...</span><span class="s2">, </span><span class="s1">c]</span><span class="s2">,</span>
                                                       <span class="s1">rem + n0mask</span><span class="s2">,</span>
                                                       <span class="s1">n + n0mask</span><span class="s2">,</span>
                                                       <span class="s1">size=size))</span>
            <span class="s1">n = n - rvs[...</span><span class="s2">, </span><span class="s1">c]</span>
        <span class="s1">rvs[...</span><span class="s2">, </span><span class="s1">m.shape[-</span><span class="s4">1</span><span class="s1">] - </span><span class="s4">1</span><span class="s1">] = n</span>

        <span class="s2">return </span><span class="s1">rvs</span>


<span class="s1">multivariate_hypergeom = multivariate_hypergeom_gen()</span>


<span class="s2">class </span><span class="s1">multivariate_hypergeom_frozen(multi_rv_frozen):</span>
    <span class="s2">def </span><span class="s1">__init__(self</span><span class="s2">, </span><span class="s1">m</span><span class="s2">, </span><span class="s1">n</span><span class="s2">, </span><span class="s1">seed=</span><span class="s2">None</span><span class="s1">):</span>
        <span class="s1">self._dist = multivariate_hypergeom_gen(seed)</span>
        <span class="s1">(self.M</span><span class="s2">, </span><span class="s1">self.m</span><span class="s2">, </span><span class="s1">self.n</span><span class="s2">,</span>
         <span class="s1">self.mcond</span><span class="s2">, </span><span class="s1">self.ncond</span><span class="s2">,</span>
         <span class="s1">self.mncond) = self._dist._process_parameters(m</span><span class="s2">, </span><span class="s1">n)</span>

        <span class="s0"># monkey patch self._dist</span>
        <span class="s2">def </span><span class="s1">_process_parameters(m</span><span class="s2">, </span><span class="s1">n):</span>
            <span class="s2">return </span><span class="s1">(self.M</span><span class="s2">, </span><span class="s1">self.m</span><span class="s2">, </span><span class="s1">self.n</span><span class="s2">,</span>
                    <span class="s1">self.mcond</span><span class="s2">, </span><span class="s1">self.ncond</span><span class="s2">,</span>
                    <span class="s1">self.mncond)</span>
        <span class="s1">self._dist._process_parameters = _process_parameters</span>

    <span class="s2">def </span><span class="s1">logpmf(self</span><span class="s2">, </span><span class="s1">x):</span>
        <span class="s2">return </span><span class="s1">self._dist.logpmf(x</span><span class="s2">, </span><span class="s1">self.m</span><span class="s2">, </span><span class="s1">self.n)</span>

    <span class="s2">def </span><span class="s1">pmf(self</span><span class="s2">, </span><span class="s1">x):</span>
        <span class="s2">return </span><span class="s1">self._dist.pmf(x</span><span class="s2">, </span><span class="s1">self.m</span><span class="s2">, </span><span class="s1">self.n)</span>

    <span class="s2">def </span><span class="s1">mean(self):</span>
        <span class="s2">return </span><span class="s1">self._dist.mean(self.m</span><span class="s2">, </span><span class="s1">self.n)</span>

    <span class="s2">def </span><span class="s1">var(self):</span>
        <span class="s2">return </span><span class="s1">self._dist.var(self.m</span><span class="s2">, </span><span class="s1">self.n)</span>

    <span class="s2">def </span><span class="s1">cov(self):</span>
        <span class="s2">return </span><span class="s1">self._dist.cov(self.m</span><span class="s2">, </span><span class="s1">self.n)</span>

    <span class="s2">def </span><span class="s1">rvs(self</span><span class="s2">, </span><span class="s1">size=</span><span class="s4">1</span><span class="s2">, </span><span class="s1">random_state=</span><span class="s2">None</span><span class="s1">):</span>
        <span class="s2">return </span><span class="s1">self._dist.rvs(self.m</span><span class="s2">, </span><span class="s1">self.n</span><span class="s2">,</span>
                              <span class="s1">size=size</span><span class="s2">,</span>
                              <span class="s1">random_state=random_state)</span>


<span class="s0"># Set frozen generator docstrings from corresponding docstrings in</span>
<span class="s0"># multivariate_hypergeom and fill in default strings in class docstrings</span>
<span class="s2">for </span><span class="s1">name </span><span class="s2">in </span><span class="s1">[</span><span class="s3">'logpmf'</span><span class="s2">, </span><span class="s3">'pmf'</span><span class="s2">, </span><span class="s3">'mean'</span><span class="s2">, </span><span class="s3">'var'</span><span class="s2">, </span><span class="s3">'cov'</span><span class="s2">, </span><span class="s3">'rvs'</span><span class="s1">]:</span>
    <span class="s1">method = multivariate_hypergeom_gen.__dict__[name]</span>
    <span class="s1">method_frozen = multivariate_hypergeom_frozen.__dict__[name]</span>
    <span class="s1">method_frozen.__doc__ = doccer.docformat(</span>
        <span class="s1">method.__doc__</span><span class="s2">, </span><span class="s1">mhg_docdict_noparams)</span>
    <span class="s1">method.__doc__ = doccer.docformat(method.__doc__</span><span class="s2">,</span>
                                      <span class="s1">mhg_docdict_params)</span>


<span class="s2">class </span><span class="s1">random_table_gen(multi_rv_generic):</span>
    <span class="s5">r&quot;&quot;&quot;Contingency tables from independent samples with fixed marginal sums. 
 
    This is the distribution of random tables with given row and column vector 
    sums. This distribution represents the set of random tables under the null 
    hypothesis that rows and columns are independent. It is used in hypothesis 
    tests of independence. 
 
    Because of assumed independence, the expected frequency of each table 
    element can be computed from the row and column sums, so that the 
    distribution is completely determined by these two vectors. 
 
    Methods 
    ------- 
    logpmf(x) 
        Log-probability of table `x` to occur in the distribution. 
    pmf(x) 
        Probability of table `x` to occur in the distribution. 
    mean(row, col) 
        Mean table. 
    rvs(row, col, size=None, method=None, random_state=None) 
        Draw random tables with given row and column vector sums. 
 
    Parameters 
    ---------- 
    %(_doc_row_col)s 
    %(_doc_random_state)s 
 
    Notes 
    ----- 
    %(_doc_row_col_note)s 
 
    Random elements from the distribution are generated either with Boyett's 
    [1]_ or Patefield's algorithm [2]_. Boyett's algorithm has 
    O(N) time and space complexity, where N is the total sum of entries in the 
    table. Patefield's algorithm has O(K x log(N)) time complexity, where K is 
    the number of cells in the table and requires only a small constant work 
    space. By default, the `rvs` method selects the fastest algorithm based on 
    the input, but you can specify the algorithm with the keyword `method`. 
    Allowed values are &quot;boyett&quot; and &quot;patefield&quot;. 
 
    .. versionadded:: 1.10.0 
 
    Examples 
    -------- 
    &gt;&gt;&gt; from scipy.stats import random_table 
 
    &gt;&gt;&gt; row = [1, 5] 
    &gt;&gt;&gt; col = [2, 3, 1] 
    &gt;&gt;&gt; random_table.mean(row, col) 
    array([[0.33333333, 0.5       , 0.16666667], 
           [1.66666667, 2.5       , 0.83333333]]) 
 
    Alternatively, the object may be called (as a function) to fix the row 
    and column vector sums, returning a &quot;frozen&quot; distribution. 
 
    &gt;&gt;&gt; dist = random_table(row, col) 
    &gt;&gt;&gt; dist.rvs(random_state=123) 
    array([[1., 0., 0.], 
           [1., 3., 1.]]) 
 
    References 
    ---------- 
    .. [1] J. Boyett, AS 144 Appl. Statist. 28 (1979) 329-332 
    .. [2] W.M. Patefield, AS 159 Appl. Statist. 30 (1981) 91-97 
    &quot;&quot;&quot;</span>

    <span class="s2">def </span><span class="s1">__init__(self</span><span class="s2">, </span><span class="s1">seed=</span><span class="s2">None</span><span class="s1">):</span>
        <span class="s1">super().__init__(seed)</span>

    <span class="s2">def </span><span class="s1">__call__(self</span><span class="s2">, </span><span class="s1">row</span><span class="s2">, </span><span class="s1">col</span><span class="s2">, </span><span class="s1">*</span><span class="s2">, </span><span class="s1">seed=</span><span class="s2">None</span><span class="s1">):</span>
        <span class="s5">&quot;&quot;&quot;Create a frozen distribution of tables with given marginals. 
 
        See `random_table_frozen` for more information. 
        &quot;&quot;&quot;</span>
        <span class="s2">return </span><span class="s1">random_table_frozen(row</span><span class="s2">, </span><span class="s1">col</span><span class="s2">, </span><span class="s1">seed=seed)</span>

    <span class="s2">def </span><span class="s1">logpmf(self</span><span class="s2">, </span><span class="s1">x</span><span class="s2">, </span><span class="s1">row</span><span class="s2">, </span><span class="s1">col):</span>
        <span class="s5">&quot;&quot;&quot;Log-probability of table to occur in the distribution. 
 
        Parameters 
        ---------- 
        %(_doc_x)s 
        %(_doc_row_col)s 
 
        Returns 
        ------- 
        logpmf : ndarray or scalar 
            Log of the probability mass function evaluated at `x`. 
 
        Notes 
        ----- 
        %(_doc_row_col_note)s 
 
        If row and column marginals of `x` do not match `row` and `col`, 
        negative infinity is returned. 
 
        Examples 
        -------- 
        &gt;&gt;&gt; from scipy.stats import random_table 
        &gt;&gt;&gt; import numpy as np 
 
        &gt;&gt;&gt; x = [[1, 5, 1], [2, 3, 1]] 
        &gt;&gt;&gt; row = np.sum(x, axis=1) 
        &gt;&gt;&gt; col = np.sum(x, axis=0) 
        &gt;&gt;&gt; random_table.logpmf(x, row, col) 
        -1.6306401200847027 
 
        Alternatively, the object may be called (as a function) to fix the row 
        and column vector sums, returning a &quot;frozen&quot; distribution. 
 
        &gt;&gt;&gt; d = random_table(row, col) 
        &gt;&gt;&gt; d.logpmf(x) 
        -1.6306401200847027 
        &quot;&quot;&quot;</span>
        <span class="s1">r</span><span class="s2">, </span><span class="s1">c</span><span class="s2">, </span><span class="s1">n = self._process_parameters(row</span><span class="s2">, </span><span class="s1">col)</span>
        <span class="s1">x = np.asarray(x)</span>

        <span class="s2">if </span><span class="s1">x.ndim &lt; </span><span class="s4">2</span><span class="s1">:</span>
            <span class="s2">raise </span><span class="s1">ValueError(</span><span class="s3">&quot;`x` must be at least two-dimensional&quot;</span><span class="s1">)</span>

        <span class="s1">dtype_is_int = np.issubdtype(x.dtype</span><span class="s2">, </span><span class="s1">np.integer)</span>
        <span class="s2">with </span><span class="s1">np.errstate(invalid=</span><span class="s3">'ignore'</span><span class="s1">):</span>
            <span class="s2">if not </span><span class="s1">dtype_is_int </span><span class="s2">and not </span><span class="s1">np.all(x.astype(int) == x):</span>
                <span class="s2">raise </span><span class="s1">ValueError(</span><span class="s3">&quot;`x` must contain only integral values&quot;</span><span class="s1">)</span>

        <span class="s0"># x does not contain NaN if we arrive here</span>
        <span class="s2">if </span><span class="s1">np.any(x &lt; </span><span class="s4">0</span><span class="s1">):</span>
            <span class="s2">raise </span><span class="s1">ValueError(</span><span class="s3">&quot;`x` must contain only non-negative values&quot;</span><span class="s1">)</span>

        <span class="s1">r2 = np.sum(x</span><span class="s2">, </span><span class="s1">axis=-</span><span class="s4">1</span><span class="s1">)</span>
        <span class="s1">c2 = np.sum(x</span><span class="s2">, </span><span class="s1">axis=-</span><span class="s4">2</span><span class="s1">)</span>

        <span class="s2">if </span><span class="s1">r2.shape[-</span><span class="s4">1</span><span class="s1">] != len(r):</span>
            <span class="s2">raise </span><span class="s1">ValueError(</span><span class="s3">&quot;shape of `x` must agree with `row`&quot;</span><span class="s1">)</span>

        <span class="s2">if </span><span class="s1">c2.shape[-</span><span class="s4">1</span><span class="s1">] != len(c):</span>
            <span class="s2">raise </span><span class="s1">ValueError(</span><span class="s3">&quot;shape of `x` must agree with `col`&quot;</span><span class="s1">)</span>

        <span class="s1">res = np.empty(x.shape[:-</span><span class="s4">2</span><span class="s1">])</span>

        <span class="s1">mask = np.all(r2 == r</span><span class="s2">, </span><span class="s1">axis=-</span><span class="s4">1</span><span class="s1">) &amp; np.all(c2 == c</span><span class="s2">, </span><span class="s1">axis=-</span><span class="s4">1</span><span class="s1">)</span>

        <span class="s2">def </span><span class="s1">lnfac(x):</span>
            <span class="s2">return </span><span class="s1">gammaln(x + </span><span class="s4">1</span><span class="s1">)</span>

        <span class="s1">res[mask] = (np.sum(lnfac(r)</span><span class="s2">, </span><span class="s1">axis=-</span><span class="s4">1</span><span class="s1">) + np.sum(lnfac(c)</span><span class="s2">, </span><span class="s1">axis=-</span><span class="s4">1</span><span class="s1">)</span>
                     <span class="s1">- lnfac(n) - np.sum(lnfac(x[mask])</span><span class="s2">, </span><span class="s1">axis=(-</span><span class="s4">1</span><span class="s2">, </span><span class="s1">-</span><span class="s4">2</span><span class="s1">)))</span>
        <span class="s1">res[~mask] = -np.inf</span>

        <span class="s2">return </span><span class="s1">res[()]</span>

    <span class="s2">def </span><span class="s1">pmf(self</span><span class="s2">, </span><span class="s1">x</span><span class="s2">, </span><span class="s1">row</span><span class="s2">, </span><span class="s1">col):</span>
        <span class="s5">&quot;&quot;&quot;Probability of table to occur in the distribution. 
 
        Parameters 
        ---------- 
        %(_doc_x)s 
        %(_doc_row_col)s 
 
        Returns 
        ------- 
        pmf : ndarray or scalar 
            Probability mass function evaluated at `x`. 
 
        Notes 
        ----- 
        %(_doc_row_col_note)s 
 
        If row and column marginals of `x` do not match `row` and `col`, 
        zero is returned. 
 
        Examples 
        -------- 
        &gt;&gt;&gt; from scipy.stats import random_table 
        &gt;&gt;&gt; import numpy as np 
 
        &gt;&gt;&gt; x = [[1, 5, 1], [2, 3, 1]] 
        &gt;&gt;&gt; row = np.sum(x, axis=1) 
        &gt;&gt;&gt; col = np.sum(x, axis=0) 
        &gt;&gt;&gt; random_table.pmf(x, row, col) 
        0.19580419580419592 
 
        Alternatively, the object may be called (as a function) to fix the row 
        and column vector sums, returning a &quot;frozen&quot; distribution. 
 
        &gt;&gt;&gt; d = random_table(row, col) 
        &gt;&gt;&gt; d.pmf(x) 
        0.19580419580419592 
        &quot;&quot;&quot;</span>
        <span class="s2">return </span><span class="s1">np.exp(self.logpmf(x</span><span class="s2">, </span><span class="s1">row</span><span class="s2">, </span><span class="s1">col))</span>

    <span class="s2">def </span><span class="s1">mean(self</span><span class="s2">, </span><span class="s1">row</span><span class="s2">, </span><span class="s1">col):</span>
        <span class="s5">&quot;&quot;&quot;Mean of distribution of conditional tables. 
        %(_doc_mean_params)s 
 
        Returns 
        ------- 
        mean: ndarray 
            Mean of the distribution. 
 
        Notes 
        ----- 
        %(_doc_row_col_note)s 
 
        Examples 
        -------- 
        &gt;&gt;&gt; from scipy.stats import random_table 
 
        &gt;&gt;&gt; row = [1, 5] 
        &gt;&gt;&gt; col = [2, 3, 1] 
        &gt;&gt;&gt; random_table.mean(row, col) 
        array([[0.33333333, 0.5       , 0.16666667], 
               [1.66666667, 2.5       , 0.83333333]]) 
 
        Alternatively, the object may be called (as a function) to fix the row 
        and column vector sums, returning a &quot;frozen&quot; distribution. 
 
        &gt;&gt;&gt; d = random_table(row, col) 
        &gt;&gt;&gt; d.mean() 
        array([[0.33333333, 0.5       , 0.16666667], 
               [1.66666667, 2.5       , 0.83333333]]) 
        &quot;&quot;&quot;</span>
        <span class="s1">r</span><span class="s2">, </span><span class="s1">c</span><span class="s2">, </span><span class="s1">n = self._process_parameters(row</span><span class="s2">, </span><span class="s1">col)</span>
        <span class="s2">return </span><span class="s1">np.outer(r</span><span class="s2">, </span><span class="s1">c) / n</span>

    <span class="s2">def </span><span class="s1">rvs(self</span><span class="s2">, </span><span class="s1">row</span><span class="s2">, </span><span class="s1">col</span><span class="s2">, </span><span class="s1">*</span><span class="s2">, </span><span class="s1">size=</span><span class="s2">None, </span><span class="s1">method=</span><span class="s2">None, </span><span class="s1">random_state=</span><span class="s2">None</span><span class="s1">):</span>
        <span class="s5">&quot;&quot;&quot;Draw random tables with fixed column and row marginals. 
 
        Parameters 
        ---------- 
        %(_doc_row_col)s 
        size : integer, optional 
            Number of samples to draw (default 1). 
        method : str, optional 
            Which method to use, &quot;boyett&quot; or &quot;patefield&quot;. If None (default), 
            selects the fastest method for this input. 
        %(_doc_random_state)s 
 
        Returns 
        ------- 
        rvs : ndarray 
            Random 2D tables of shape (`size`, `len(row)`, `len(col)`). 
 
        Notes 
        ----- 
        %(_doc_row_col_note)s 
 
        Examples 
        -------- 
        &gt;&gt;&gt; from scipy.stats import random_table 
 
        &gt;&gt;&gt; row = [1, 5] 
        &gt;&gt;&gt; col = [2, 3, 1] 
        &gt;&gt;&gt; random_table.rvs(row, col, random_state=123) 
        array([[1., 0., 0.], 
               [1., 3., 1.]]) 
 
        Alternatively, the object may be called (as a function) to fix the row 
        and column vector sums, returning a &quot;frozen&quot; distribution. 
 
        &gt;&gt;&gt; d = random_table(row, col) 
        &gt;&gt;&gt; d.rvs(random_state=123) 
        array([[1., 0., 0.], 
               [1., 3., 1.]]) 
        &quot;&quot;&quot;</span>
        <span class="s1">r</span><span class="s2">, </span><span class="s1">c</span><span class="s2">, </span><span class="s1">n = self._process_parameters(row</span><span class="s2">, </span><span class="s1">col)</span>
        <span class="s1">size</span><span class="s2">, </span><span class="s1">shape = self._process_size_shape(size</span><span class="s2">, </span><span class="s1">r</span><span class="s2">, </span><span class="s1">c)</span>

        <span class="s1">random_state = self._get_random_state(random_state)</span>
        <span class="s1">meth = self._process_rvs_method(method</span><span class="s2">, </span><span class="s1">r</span><span class="s2">, </span><span class="s1">c</span><span class="s2">, </span><span class="s1">n)</span>

        <span class="s2">return </span><span class="s1">meth(r</span><span class="s2">, </span><span class="s1">c</span><span class="s2">, </span><span class="s1">n</span><span class="s2">, </span><span class="s1">size</span><span class="s2">, </span><span class="s1">random_state).reshape(shape)</span>

    <span class="s1">@staticmethod</span>
    <span class="s2">def </span><span class="s1">_process_parameters(row</span><span class="s2">, </span><span class="s1">col):</span>
        <span class="s5">&quot;&quot;&quot; 
        Check that row and column vectors are one-dimensional, that they do 
        not contain negative or non-integer entries, and that the sums over 
        both vectors are equal. 
        &quot;&quot;&quot;</span>
        <span class="s1">r = np.array(row</span><span class="s2">, </span><span class="s1">dtype=np.int64</span><span class="s2">, </span><span class="s1">copy=</span><span class="s2">True</span><span class="s1">)</span>
        <span class="s1">c = np.array(col</span><span class="s2">, </span><span class="s1">dtype=np.int64</span><span class="s2">, </span><span class="s1">copy=</span><span class="s2">True</span><span class="s1">)</span>

        <span class="s2">if </span><span class="s1">np.ndim(r) != </span><span class="s4">1</span><span class="s1">:</span>
            <span class="s2">raise </span><span class="s1">ValueError(</span><span class="s3">&quot;`row` must be one-dimensional&quot;</span><span class="s1">)</span>
        <span class="s2">if </span><span class="s1">np.ndim(c) != </span><span class="s4">1</span><span class="s1">:</span>
            <span class="s2">raise </span><span class="s1">ValueError(</span><span class="s3">&quot;`col` must be one-dimensional&quot;</span><span class="s1">)</span>

        <span class="s2">if </span><span class="s1">np.any(r &lt; </span><span class="s4">0</span><span class="s1">):</span>
            <span class="s2">raise </span><span class="s1">ValueError(</span><span class="s3">&quot;each element of `row` must be non-negative&quot;</span><span class="s1">)</span>
        <span class="s2">if </span><span class="s1">np.any(c &lt; </span><span class="s4">0</span><span class="s1">):</span>
            <span class="s2">raise </span><span class="s1">ValueError(</span><span class="s3">&quot;each element of `col` must be non-negative&quot;</span><span class="s1">)</span>

        <span class="s1">n = np.sum(r)</span>
        <span class="s2">if </span><span class="s1">n != np.sum(c):</span>
            <span class="s2">raise </span><span class="s1">ValueError(</span><span class="s3">&quot;sums over `row` and `col` must be equal&quot;</span><span class="s1">)</span>

        <span class="s2">if not </span><span class="s1">np.all(r == np.asarray(row)):</span>
            <span class="s2">raise </span><span class="s1">ValueError(</span><span class="s3">&quot;each element of `row` must be an integer&quot;</span><span class="s1">)</span>
        <span class="s2">if not </span><span class="s1">np.all(c == np.asarray(col)):</span>
            <span class="s2">raise </span><span class="s1">ValueError(</span><span class="s3">&quot;each element of `col` must be an integer&quot;</span><span class="s1">)</span>

        <span class="s2">return </span><span class="s1">r</span><span class="s2">, </span><span class="s1">c</span><span class="s2">, </span><span class="s1">n</span>

    <span class="s1">@staticmethod</span>
    <span class="s2">def </span><span class="s1">_process_size_shape(size</span><span class="s2">, </span><span class="s1">r</span><span class="s2">, </span><span class="s1">c):</span>
        <span class="s5">&quot;&quot;&quot; 
        Compute the number of samples to be drawn and the shape of the output 
        &quot;&quot;&quot;</span>
        <span class="s1">shape = (len(r)</span><span class="s2">, </span><span class="s1">len(c))</span>

        <span class="s2">if </span><span class="s1">size </span><span class="s2">is None</span><span class="s1">:</span>
            <span class="s2">return </span><span class="s4">1</span><span class="s2">, </span><span class="s1">shape</span>

        <span class="s1">size = np.atleast_1d(size)</span>
        <span class="s2">if not </span><span class="s1">np.issubdtype(size.dtype</span><span class="s2">, </span><span class="s1">np.integer) </span><span class="s2">or </span><span class="s1">np.any(size &lt; </span><span class="s4">0</span><span class="s1">):</span>
            <span class="s2">raise </span><span class="s1">ValueError(</span><span class="s3">&quot;`size` must be a non-negative integer or `None`&quot;</span><span class="s1">)</span>

        <span class="s2">return </span><span class="s1">np.prod(size)</span><span class="s2">, </span><span class="s1">tuple(size) + shape</span>

    <span class="s1">@classmethod</span>
    <span class="s2">def </span><span class="s1">_process_rvs_method(cls</span><span class="s2">, </span><span class="s1">method</span><span class="s2">, </span><span class="s1">r</span><span class="s2">, </span><span class="s1">c</span><span class="s2">, </span><span class="s1">n):</span>
        <span class="s1">known_methods = {</span>
            <span class="s2">None</span><span class="s1">: cls._rvs_select(r</span><span class="s2">, </span><span class="s1">c</span><span class="s2">, </span><span class="s1">n)</span><span class="s2">,</span>
            <span class="s3">&quot;boyett&quot;</span><span class="s1">: cls._rvs_boyett</span><span class="s2">,</span>
            <span class="s3">&quot;patefield&quot;</span><span class="s1">: cls._rvs_patefield</span><span class="s2">,</span>
        <span class="s1">}</span>
        <span class="s2">try</span><span class="s1">:</span>
            <span class="s2">return </span><span class="s1">known_methods[method]</span>
        <span class="s2">except </span><span class="s1">KeyError:</span>
            <span class="s2">raise </span><span class="s1">ValueError(</span><span class="s3">f&quot;'</span><span class="s2">{</span><span class="s1">method</span><span class="s2">}</span><span class="s3">' not recognized, &quot;</span>
                             <span class="s3">f&quot;must be one of </span><span class="s2">{</span><span class="s1">set(known_methods)</span><span class="s2">}</span><span class="s3">&quot;</span><span class="s1">)</span>

    <span class="s1">@classmethod</span>
    <span class="s2">def </span><span class="s1">_rvs_select(cls</span><span class="s2">, </span><span class="s1">r</span><span class="s2">, </span><span class="s1">c</span><span class="s2">, </span><span class="s1">n):</span>
        <span class="s1">fac = </span><span class="s4">1.0  </span><span class="s0"># benchmarks show that this value is about 1</span>
        <span class="s1">k = len(r) * len(c)  </span><span class="s0"># number of cells</span>
        <span class="s0"># n + 1 guards against failure if n == 0</span>
        <span class="s2">if </span><span class="s1">n &gt; fac * np.log(n + </span><span class="s4">1</span><span class="s1">) * k:</span>
            <span class="s2">return </span><span class="s1">cls._rvs_patefield</span>
        <span class="s2">return </span><span class="s1">cls._rvs_boyett</span>

    <span class="s1">@staticmethod</span>
    <span class="s2">def </span><span class="s1">_rvs_boyett(row</span><span class="s2">, </span><span class="s1">col</span><span class="s2">, </span><span class="s1">ntot</span><span class="s2">, </span><span class="s1">size</span><span class="s2">, </span><span class="s1">random_state):</span>
        <span class="s2">return </span><span class="s1">_rcont.rvs_rcont1(row</span><span class="s2">, </span><span class="s1">col</span><span class="s2">, </span><span class="s1">ntot</span><span class="s2">, </span><span class="s1">size</span><span class="s2">, </span><span class="s1">random_state)</span>

    <span class="s1">@staticmethod</span>
    <span class="s2">def </span><span class="s1">_rvs_patefield(row</span><span class="s2">, </span><span class="s1">col</span><span class="s2">, </span><span class="s1">ntot</span><span class="s2">, </span><span class="s1">size</span><span class="s2">, </span><span class="s1">random_state):</span>
        <span class="s2">return </span><span class="s1">_rcont.rvs_rcont2(row</span><span class="s2">, </span><span class="s1">col</span><span class="s2">, </span><span class="s1">ntot</span><span class="s2">, </span><span class="s1">size</span><span class="s2">, </span><span class="s1">random_state)</span>


<span class="s1">random_table = random_table_gen()</span>


<span class="s2">class </span><span class="s1">random_table_frozen(multi_rv_frozen):</span>
    <span class="s2">def </span><span class="s1">__init__(self</span><span class="s2">, </span><span class="s1">row</span><span class="s2">, </span><span class="s1">col</span><span class="s2">, </span><span class="s1">*</span><span class="s2">, </span><span class="s1">seed=</span><span class="s2">None</span><span class="s1">):</span>
        <span class="s1">self._dist = random_table_gen(seed)</span>
        <span class="s1">self._params = self._dist._process_parameters(row</span><span class="s2">, </span><span class="s1">col)</span>

        <span class="s0"># monkey patch self._dist</span>
        <span class="s2">def </span><span class="s1">_process_parameters(r</span><span class="s2">, </span><span class="s1">c):</span>
            <span class="s2">return </span><span class="s1">self._params</span>
        <span class="s1">self._dist._process_parameters = _process_parameters</span>

    <span class="s2">def </span><span class="s1">logpmf(self</span><span class="s2">, </span><span class="s1">x):</span>
        <span class="s2">return </span><span class="s1">self._dist.logpmf(x</span><span class="s2">, None, None</span><span class="s1">)</span>

    <span class="s2">def </span><span class="s1">pmf(self</span><span class="s2">, </span><span class="s1">x):</span>
        <span class="s2">return </span><span class="s1">self._dist.pmf(x</span><span class="s2">, None, None</span><span class="s1">)</span>

    <span class="s2">def </span><span class="s1">mean(self):</span>
        <span class="s2">return </span><span class="s1">self._dist.mean(</span><span class="s2">None, None</span><span class="s1">)</span>

    <span class="s2">def </span><span class="s1">rvs(self</span><span class="s2">, </span><span class="s1">size=</span><span class="s2">None, </span><span class="s1">method=</span><span class="s2">None, </span><span class="s1">random_state=</span><span class="s2">None</span><span class="s1">):</span>
        <span class="s0"># optimisations are possible here</span>
        <span class="s2">return </span><span class="s1">self._dist.rvs(</span><span class="s2">None, None, </span><span class="s1">size=size</span><span class="s2">, </span><span class="s1">method=method</span><span class="s2">,</span>
                              <span class="s1">random_state=random_state)</span>


<span class="s1">_ctab_doc_row_col = </span><span class="s3">&quot;&quot;&quot;</span><span class="s2">\ 
</span><span class="s3">row : array_like 
    Sum of table entries in each row. 
col : array_like 
    Sum of table entries in each column.&quot;&quot;&quot;</span>

<span class="s1">_ctab_doc_x = </span><span class="s3">&quot;&quot;&quot;</span><span class="s2">\ 
</span><span class="s3">x : array-like 
   Two-dimensional table of non-negative integers, or a 
   multi-dimensional array with the last two dimensions 
   corresponding with the tables.&quot;&quot;&quot;</span>

<span class="s1">_ctab_doc_row_col_note = </span><span class="s3">&quot;&quot;&quot;</span><span class="s2">\ 
</span><span class="s3">The row and column vectors must be one-dimensional, not empty, 
and each sum up to the same value. They cannot contain negative 
or noninteger entries.&quot;&quot;&quot;</span>

<span class="s1">_ctab_doc_mean_params = </span><span class="s3">f&quot;&quot;&quot;</span>
<span class="s3">Parameters</span>
<span class="s3">----------</span>
<span class="s2">{</span><span class="s1">_ctab_doc_row_col</span><span class="s2">}</span><span class="s3">&quot;&quot;&quot;</span>

<span class="s1">_ctab_doc_row_col_note_frozen = </span><span class="s3">&quot;&quot;&quot;</span><span class="s2">\ 
</span><span class="s3">See class definition for a detailed description of parameters.&quot;&quot;&quot;</span>

<span class="s1">_ctab_docdict = {</span>
    <span class="s3">&quot;_doc_random_state&quot;</span><span class="s1">: _doc_random_state</span><span class="s2">,</span>
    <span class="s3">&quot;_doc_row_col&quot;</span><span class="s1">: _ctab_doc_row_col</span><span class="s2">,</span>
    <span class="s3">&quot;_doc_x&quot;</span><span class="s1">: _ctab_doc_x</span><span class="s2">,</span>
    <span class="s3">&quot;_doc_mean_params&quot;</span><span class="s1">: _ctab_doc_mean_params</span><span class="s2">,</span>
    <span class="s3">&quot;_doc_row_col_note&quot;</span><span class="s1">: _ctab_doc_row_col_note</span><span class="s2">,</span>
<span class="s1">}</span>

<span class="s1">_ctab_docdict_frozen = _ctab_docdict.copy()</span>
<span class="s1">_ctab_docdict_frozen.update({</span>
    <span class="s3">&quot;_doc_row_col&quot;</span><span class="s1">: </span><span class="s3">&quot;&quot;</span><span class="s2">,</span>
    <span class="s3">&quot;_doc_mean_params&quot;</span><span class="s1">: </span><span class="s3">&quot;&quot;</span><span class="s2">,</span>
    <span class="s3">&quot;_doc_row_col_note&quot;</span><span class="s1">: _ctab_doc_row_col_note_frozen</span><span class="s2">,</span>
<span class="s1">})</span>


<span class="s2">def </span><span class="s1">_docfill(obj</span><span class="s2">, </span><span class="s1">docdict</span><span class="s2">, </span><span class="s1">template=</span><span class="s2">None</span><span class="s1">):</span>
    <span class="s1">obj.__doc__ = doccer.docformat(template </span><span class="s2">or </span><span class="s1">obj.__doc__</span><span class="s2">, </span><span class="s1">docdict)</span>


<span class="s0"># Set frozen generator docstrings from corresponding docstrings in</span>
<span class="s0"># random_table and fill in default strings in class docstrings</span>
<span class="s1">_docfill(random_table_gen</span><span class="s2">, </span><span class="s1">_ctab_docdict)</span>
<span class="s2">for </span><span class="s1">name </span><span class="s2">in </span><span class="s1">[</span><span class="s3">'logpmf'</span><span class="s2">, </span><span class="s3">'pmf'</span><span class="s2">, </span><span class="s3">'mean'</span><span class="s2">, </span><span class="s3">'rvs'</span><span class="s1">]:</span>
    <span class="s1">method = random_table_gen.__dict__[name]</span>
    <span class="s1">method_frozen = random_table_frozen.__dict__[name]</span>
    <span class="s1">_docfill(method_frozen</span><span class="s2">, </span><span class="s1">_ctab_docdict_frozen</span><span class="s2">, </span><span class="s1">method.__doc__)</span>
    <span class="s1">_docfill(method</span><span class="s2">, </span><span class="s1">_ctab_docdict)</span>


<span class="s2">class </span><span class="s1">uniform_direction_gen(multi_rv_generic):</span>
    <span class="s5">r&quot;&quot;&quot;A vector-valued uniform direction. 
 
    Return a random direction (unit vector). The `dim` keyword specifies 
    the dimensionality of the space. 
 
    Methods 
    ------- 
    rvs(dim=None, size=1, random_state=None) 
        Draw random directions. 
 
    Parameters 
    ---------- 
    dim : scalar 
        Dimension of directions. 
    seed : {None, int, `numpy.random.Generator`, 
            `numpy.random.RandomState`}, optional 
 
        Used for drawing random variates. 
        If `seed` is `None`, the `~np.random.RandomState` singleton is used. 
        If `seed` is an int, a new ``RandomState`` instance is used, seeded 
        with seed. 
        If `seed` is already a ``RandomState`` or ``Generator`` instance, 
        then that object is used. 
        Default is `None`. 
 
    Notes 
    ----- 
    This distribution generates unit vectors uniformly distributed on 
    the surface of a hypersphere. These can be interpreted as random 
    directions. 
    For example, if `dim` is 3, 3D vectors from the surface of :math:`S^2` 
    will be sampled. 
 
    References 
    ---------- 
    .. [1] Marsaglia, G. (1972). &quot;Choosing a Point from the Surface of a 
           Sphere&quot;. Annals of Mathematical Statistics. 43 (2): 645-646. 
 
    Examples 
    -------- 
    &gt;&gt;&gt; import numpy as np 
    &gt;&gt;&gt; from scipy.stats import uniform_direction 
    &gt;&gt;&gt; x = uniform_direction.rvs(3) 
    &gt;&gt;&gt; np.linalg.norm(x) 
    1. 
 
    This generates one random direction, a vector on the surface of 
    :math:`S^2`. 
 
    Alternatively, the object may be called (as a function) to return a frozen 
    distribution with fixed `dim` parameter. Here, 
    we create a `uniform_direction` with ``dim=3`` and draw 5 observations. 
    The samples are then arranged in an array of shape 5x3. 
 
    &gt;&gt;&gt; rng = np.random.default_rng() 
    &gt;&gt;&gt; uniform_sphere_dist = uniform_direction(3) 
    &gt;&gt;&gt; unit_vectors = uniform_sphere_dist.rvs(5, random_state=rng) 
    &gt;&gt;&gt; unit_vectors 
    array([[ 0.56688642, -0.1332634 , -0.81294566], 
           [-0.427126  , -0.74779278,  0.50830044], 
           [ 0.3793989 ,  0.92346629,  0.05715323], 
           [ 0.36428383, -0.92449076, -0.11231259], 
           [-0.27733285,  0.94410968, -0.17816678]]) 
    &quot;&quot;&quot;</span>

    <span class="s2">def </span><span class="s1">__init__(self</span><span class="s2">, </span><span class="s1">seed=</span><span class="s2">None</span><span class="s1">):</span>
        <span class="s1">super().__init__(seed)</span>
        <span class="s1">self.__doc__ = doccer.docformat(self.__doc__)</span>

    <span class="s2">def </span><span class="s1">__call__(self</span><span class="s2">, </span><span class="s1">dim=</span><span class="s2">None, </span><span class="s1">seed=</span><span class="s2">None</span><span class="s1">):</span>
        <span class="s5">&quot;&quot;&quot;Create a frozen n-dimensional uniform direction distribution. 
 
        See `uniform_direction` for more information. 
        &quot;&quot;&quot;</span>
        <span class="s2">return </span><span class="s1">uniform_direction_frozen(dim</span><span class="s2">, </span><span class="s1">seed=seed)</span>

    <span class="s2">def </span><span class="s1">_process_parameters(self</span><span class="s2">, </span><span class="s1">dim):</span>
        <span class="s5">&quot;&quot;&quot;Dimension N must be specified; it cannot be inferred.&quot;&quot;&quot;</span>
        <span class="s2">if </span><span class="s1">dim </span><span class="s2">is None or not </span><span class="s1">np.isscalar(dim) </span><span class="s2">or </span><span class="s1">dim &lt; </span><span class="s4">1 </span><span class="s2">or </span><span class="s1">dim != int(dim):</span>
            <span class="s2">raise </span><span class="s1">ValueError(</span><span class="s3">&quot;Dimension of vector must be specified, &quot;</span>
                             <span class="s3">&quot;and must be an integer greater than 0.&quot;</span><span class="s1">)</span>

        <span class="s2">return </span><span class="s1">int(dim)</span>

    <span class="s2">def </span><span class="s1">rvs(self</span><span class="s2">, </span><span class="s1">dim</span><span class="s2">, </span><span class="s1">size=</span><span class="s2">None, </span><span class="s1">random_state=</span><span class="s2">None</span><span class="s1">):</span>
        <span class="s5">&quot;&quot;&quot;Draw random samples from S(N-1). 
 
        Parameters 
        ---------- 
        dim : integer 
            Dimension of space (N). 
        size : int or tuple of ints, optional 
            Given a shape of, for example, (m,n,k), m*n*k samples are 
            generated, and packed in an m-by-n-by-k arrangement. 
            Because each sample is N-dimensional, the output shape 
            is (m,n,k,N). If no shape is specified, a single (N-D) 
            sample is returned. 
        random_state : {None, int, `numpy.random.Generator`, 
                        `numpy.random.RandomState`}, optional 
 
            Pseudorandom number generator state used to generate resamples. 
 
            If `random_state` is ``None`` (or `np.random`), the 
            `numpy.random.RandomState` singleton is used. 
            If `random_state` is an int, a new ``RandomState`` instance is 
            used, seeded with `random_state`. 
            If `random_state` is already a ``Generator`` or ``RandomState`` 
            instance then that instance is used. 
 
        Returns 
        ------- 
        rvs : ndarray 
            Random direction vectors 
 
        &quot;&quot;&quot;</span>
        <span class="s1">random_state = self._get_random_state(random_state)</span>
        <span class="s2">if </span><span class="s1">size </span><span class="s2">is None</span><span class="s1">:</span>
            <span class="s1">size = np.array([]</span><span class="s2">, </span><span class="s1">dtype=int)</span>
        <span class="s1">size = np.atleast_1d(size)</span>

        <span class="s1">dim = self._process_parameters(dim)</span>

        <span class="s1">samples = _sample_uniform_direction(dim</span><span class="s2">, </span><span class="s1">size</span><span class="s2">, </span><span class="s1">random_state)</span>
        <span class="s2">return </span><span class="s1">samples</span>


<span class="s1">uniform_direction = uniform_direction_gen()</span>


<span class="s2">class </span><span class="s1">uniform_direction_frozen(multi_rv_frozen):</span>
    <span class="s2">def </span><span class="s1">__init__(self</span><span class="s2">, </span><span class="s1">dim=</span><span class="s2">None, </span><span class="s1">seed=</span><span class="s2">None</span><span class="s1">):</span>
        <span class="s5">&quot;&quot;&quot;Create a frozen n-dimensional uniform direction distribution. 
 
        Parameters 
        ---------- 
        dim : int 
            Dimension of matrices 
        seed : {None, int, `numpy.random.Generator`, 
                `numpy.random.RandomState`}, optional 
 
            If `seed` is None (or `np.random`), the `numpy.random.RandomState` 
            singleton is used. 
            If `seed` is an int, a new ``RandomState`` instance is used, 
            seeded with `seed`. 
            If `seed` is already a ``Generator`` or ``RandomState`` instance 
            then that instance is used. 
 
        Examples 
        -------- 
        &gt;&gt;&gt; from scipy.stats import uniform_direction 
        &gt;&gt;&gt; x = uniform_direction(3) 
        &gt;&gt;&gt; x.rvs() 
 
        &quot;&quot;&quot;</span>
        <span class="s1">self._dist = uniform_direction_gen(seed)</span>
        <span class="s1">self.dim = self._dist._process_parameters(dim)</span>

    <span class="s2">def </span><span class="s1">rvs(self</span><span class="s2">, </span><span class="s1">size=</span><span class="s2">None, </span><span class="s1">random_state=</span><span class="s2">None</span><span class="s1">):</span>
        <span class="s2">return </span><span class="s1">self._dist.rvs(self.dim</span><span class="s2">, </span><span class="s1">size</span><span class="s2">, </span><span class="s1">random_state)</span>


<span class="s2">def </span><span class="s1">_sample_uniform_direction(dim</span><span class="s2">, </span><span class="s1">size</span><span class="s2">, </span><span class="s1">random_state):</span>
    <span class="s5">&quot;&quot;&quot; 
    Private method to generate uniform directions 
    Reference: Marsaglia, G. (1972). &quot;Choosing a Point from the Surface of a 
               Sphere&quot;. Annals of Mathematical Statistics. 43 (2): 645-646. 
    &quot;&quot;&quot;</span>
    <span class="s1">samples_shape = np.append(size</span><span class="s2">, </span><span class="s1">dim)</span>
    <span class="s1">samples = random_state.standard_normal(samples_shape)</span>
    <span class="s1">samples /= np.linalg.norm(samples</span><span class="s2">, </span><span class="s1">axis=-</span><span class="s4">1</span><span class="s2">, </span><span class="s1">keepdims=</span><span class="s2">True</span><span class="s1">)</span>
    <span class="s2">return </span><span class="s1">samples</span>


<span class="s1">_dirichlet_mn_doc_default_callparams = </span><span class="s3">&quot;&quot;&quot;</span><span class="s2">\ 
</span><span class="s3">alpha : array_like 
    The concentration parameters. The number of entries along the last axis 
    determines the dimensionality of the distribution. Each entry must be 
    strictly positive. 
n : int or array_like 
    The number of trials. Each element must be a strictly positive integer. 
&quot;&quot;&quot;</span>

<span class="s1">_dirichlet_mn_doc_frozen_callparams = </span><span class="s3">&quot;&quot;</span>

<span class="s1">_dirichlet_mn_doc_frozen_callparams_note = </span><span class="s3">&quot;&quot;&quot;</span><span class="s2">\ 
</span><span class="s3">See class definition for a detailed description of parameters.&quot;&quot;&quot;</span>

<span class="s1">dirichlet_mn_docdict_params = {</span>
    <span class="s3">'_dirichlet_mn_doc_default_callparams'</span><span class="s1">: _dirichlet_mn_doc_default_callparams</span><span class="s2">,  </span><span class="s0"># noqa</span>
    <span class="s3">'_doc_random_state'</span><span class="s1">: _doc_random_state</span>
<span class="s1">}</span>

<span class="s1">dirichlet_mn_docdict_noparams = {</span>
    <span class="s3">'_dirichlet_mn_doc_default_callparams'</span><span class="s1">: _dirichlet_mn_doc_frozen_callparams</span><span class="s2">, </span><span class="s0"># noqa</span>
    <span class="s3">'_doc_random_state'</span><span class="s1">: _doc_random_state</span>
<span class="s1">}</span>


<span class="s2">def </span><span class="s1">_dirichlet_multinomial_check_parameters(alpha</span><span class="s2">, </span><span class="s1">n</span><span class="s2">, </span><span class="s1">x=</span><span class="s2">None</span><span class="s1">):</span>

    <span class="s1">alpha = np.asarray(alpha)</span>
    <span class="s1">n = np.asarray(n)</span>

    <span class="s2">if </span><span class="s1">x </span><span class="s2">is not None</span><span class="s1">:</span>
        <span class="s0"># Ensure that `x` and `alpha` are arrays. If the shapes are</span>
        <span class="s0"># incompatible, NumPy will raise an appropriate error.</span>
        <span class="s2">try</span><span class="s1">:</span>
            <span class="s1">x</span><span class="s2">, </span><span class="s1">alpha = np.broadcast_arrays(x</span><span class="s2">, </span><span class="s1">alpha)</span>
        <span class="s2">except </span><span class="s1">ValueError </span><span class="s2">as </span><span class="s1">e:</span>
            <span class="s1">msg = </span><span class="s3">&quot;`x` and `alpha` must be broadcastable.&quot;</span>
            <span class="s2">raise </span><span class="s1">ValueError(msg) </span><span class="s2">from </span><span class="s1">e</span>

        <span class="s1">x_int = np.floor(x)</span>
        <span class="s2">if </span><span class="s1">np.any(x &lt; </span><span class="s4">0</span><span class="s1">) </span><span class="s2">or </span><span class="s1">np.any(x != x_int):</span>
            <span class="s2">raise </span><span class="s1">ValueError(</span><span class="s3">&quot;`x` must contain only non-negative integers.&quot;</span><span class="s1">)</span>
        <span class="s1">x = x_int</span>

    <span class="s2">if </span><span class="s1">np.any(alpha &lt;= </span><span class="s4">0</span><span class="s1">):</span>
        <span class="s2">raise </span><span class="s1">ValueError(</span><span class="s3">&quot;`alpha` must contain only positive values.&quot;</span><span class="s1">)</span>

    <span class="s1">n_int = np.floor(n)</span>
    <span class="s2">if </span><span class="s1">np.any(n &lt;= </span><span class="s4">0</span><span class="s1">) </span><span class="s2">or </span><span class="s1">np.any(n != n_int):</span>
        <span class="s2">raise </span><span class="s1">ValueError(</span><span class="s3">&quot;`n` must be a positive integer.&quot;</span><span class="s1">)</span>
    <span class="s1">n = n_int</span>

    <span class="s1">sum_alpha = np.sum(alpha</span><span class="s2">, </span><span class="s1">axis=-</span><span class="s4">1</span><span class="s1">)</span>
    <span class="s1">sum_alpha</span><span class="s2">, </span><span class="s1">n = np.broadcast_arrays(sum_alpha</span><span class="s2">, </span><span class="s1">n)</span>

    <span class="s2">return </span><span class="s1">(alpha</span><span class="s2">, </span><span class="s1">sum_alpha</span><span class="s2">, </span><span class="s1">n) </span><span class="s2">if </span><span class="s1">x </span><span class="s2">is None else </span><span class="s1">(alpha</span><span class="s2">, </span><span class="s1">sum_alpha</span><span class="s2">, </span><span class="s1">n</span><span class="s2">, </span><span class="s1">x)</span>


<span class="s2">class </span><span class="s1">dirichlet_multinomial_gen(multi_rv_generic):</span>
    <span class="s5">r&quot;&quot;&quot;A Dirichlet multinomial random variable. 
 
    The Dirichlet multinomial distribution is a compound probability 
    distribution: it is the multinomial distribution with number of trials 
    `n` and class probabilities ``p`` randomly sampled from a Dirichlet 
    distribution with concentration parameters ``alpha``. 
 
    Methods 
    ------- 
    logpmf(x, alpha, n): 
        Log of the probability mass function. 
    pmf(x, alpha, n): 
        Probability mass function. 
    mean(alpha, n): 
        Mean of the Dirichlet multinomial distribution. 
    var(alpha, n): 
        Variance of the Dirichlet multinomial distribution. 
    cov(alpha, n): 
        The covariance of the Dirichlet multinomial distribution. 
 
    Parameters 
    ---------- 
    %(_dirichlet_mn_doc_default_callparams)s 
    %(_doc_random_state)s 
 
    See Also 
    -------- 
    scipy.stats.dirichlet : The dirichlet distribution. 
    scipy.stats.multinomial : The multinomial distribution. 
 
    References 
    ---------- 
    .. [1] Dirichlet-multinomial distribution, Wikipedia, 
           https://www.wikipedia.org/wiki/Dirichlet-multinomial_distribution 
 
    Examples 
    -------- 
    &gt;&gt;&gt; from scipy.stats import dirichlet_multinomial 
 
    Get the PMF 
 
    &gt;&gt;&gt; n = 6  # number of trials 
    &gt;&gt;&gt; alpha = [3, 4, 5]  # concentration parameters 
    &gt;&gt;&gt; x = [1, 2, 3]  # counts 
    &gt;&gt;&gt; dirichlet_multinomial.pmf(x, alpha, n) 
    0.08484162895927604 
 
    If the sum of category counts does not equal the number of trials, 
    the probability mass is zero. 
 
    &gt;&gt;&gt; dirichlet_multinomial.pmf(x, alpha, n=7) 
    0.0 
 
    Get the log of the PMF 
 
    &gt;&gt;&gt; dirichlet_multinomial.logpmf(x, alpha, n) 
    -2.4669689491013327 
 
    Get the mean 
 
    &gt;&gt;&gt; dirichlet_multinomial.mean(alpha, n) 
    array([1.5, 2. , 2.5]) 
 
    Get the variance 
 
    &gt;&gt;&gt; dirichlet_multinomial.var(alpha, n) 
    array([1.55769231, 1.84615385, 2.01923077]) 
 
    Get the covariance 
 
    &gt;&gt;&gt; dirichlet_multinomial.cov(alpha, n) 
    array([[ 1.55769231, -0.69230769, -0.86538462], 
           [-0.69230769,  1.84615385, -1.15384615], 
           [-0.86538462, -1.15384615,  2.01923077]]) 
 
    Alternatively, the object may be called (as a function) to fix the 
    `alpha` and `n` parameters, returning a &quot;frozen&quot; Dirichlet multinomial 
    random variable. 
 
    &gt;&gt;&gt; dm = dirichlet_multinomial(alpha, n) 
    &gt;&gt;&gt; dm.pmf(x) 
    0.08484162895927579 
 
    All methods are fully vectorized. Each element of `x` and `alpha` is 
    a vector (along the last axis), each element of `n` is an 
    integer (scalar), and the result is computed element-wise. 
 
    &gt;&gt;&gt; x = [[1, 2, 3], [4, 5, 6]] 
    &gt;&gt;&gt; alpha = [[1, 2, 3], [4, 5, 6]] 
    &gt;&gt;&gt; n = [6, 15] 
    &gt;&gt;&gt; dirichlet_multinomial.pmf(x, alpha, n) 
    array([0.06493506, 0.02626937]) 
 
    &gt;&gt;&gt; dirichlet_multinomial.cov(alpha, n).shape  # both covariance matrices 
    (2, 3, 3) 
 
    Broadcasting according to standard NumPy conventions is supported. Here, 
    we have four sets of concentration parameters (each a two element vector) 
    for each of three numbers of trials (each a scalar). 
 
    &gt;&gt;&gt; alpha = [[3, 4], [4, 5], [5, 6], [6, 7]] 
    &gt;&gt;&gt; n = [[6], [7], [8]] 
    &gt;&gt;&gt; dirichlet_multinomial.mean(alpha, n).shape 
    (3, 4, 2) 
 
    &quot;&quot;&quot;</span>
    <span class="s2">def </span><span class="s1">__init__(self</span><span class="s2">, </span><span class="s1">seed=</span><span class="s2">None</span><span class="s1">):</span>
        <span class="s1">super().__init__(seed)</span>
        <span class="s1">self.__doc__ = doccer.docformat(self.__doc__</span><span class="s2">,</span>
                                        <span class="s1">dirichlet_mn_docdict_params)</span>

    <span class="s2">def </span><span class="s1">__call__(self</span><span class="s2">, </span><span class="s1">alpha</span><span class="s2">, </span><span class="s1">n</span><span class="s2">, </span><span class="s1">seed=</span><span class="s2">None</span><span class="s1">):</span>
        <span class="s2">return </span><span class="s1">dirichlet_multinomial_frozen(alpha</span><span class="s2">, </span><span class="s1">n</span><span class="s2">, </span><span class="s1">seed=seed)</span>

    <span class="s2">def </span><span class="s1">logpmf(self</span><span class="s2">, </span><span class="s1">x</span><span class="s2">, </span><span class="s1">alpha</span><span class="s2">, </span><span class="s1">n):</span>
        <span class="s5">&quot;&quot;&quot;The log of the probability mass function. 
 
        Parameters 
        ---------- 
        x: ndarray 
            Category counts (non-negative integers). Must be broadcastable 
            with shape parameter ``alpha``. If multidimensional, the last axis 
            must correspond with the categories. 
        %(_dirichlet_mn_doc_default_callparams)s 
 
        Returns 
        ------- 
        out: ndarray or scalar 
            Log of the probability mass function. 
 
        &quot;&quot;&quot;</span>

        <span class="s1">a</span><span class="s2">, </span><span class="s1">Sa</span><span class="s2">, </span><span class="s1">n</span><span class="s2">, </span><span class="s1">x = _dirichlet_multinomial_check_parameters(alpha</span><span class="s2">, </span><span class="s1">n</span><span class="s2">, </span><span class="s1">x)</span>

        <span class="s1">out = np.asarray(loggamma(Sa) + loggamma(n + </span><span class="s4">1</span><span class="s1">) - loggamma(n + Sa))</span>
        <span class="s1">out += (loggamma(x + a) - (loggamma(a) + loggamma(x + </span><span class="s4">1</span><span class="s1">))).sum(axis=-</span><span class="s4">1</span><span class="s1">)</span>
        <span class="s1">np.place(out</span><span class="s2">, </span><span class="s1">n != x.sum(axis=-</span><span class="s4">1</span><span class="s1">)</span><span class="s2">, </span><span class="s1">-np.inf)</span>
        <span class="s2">return </span><span class="s1">out[()]</span>

    <span class="s2">def </span><span class="s1">pmf(self</span><span class="s2">, </span><span class="s1">x</span><span class="s2">, </span><span class="s1">alpha</span><span class="s2">, </span><span class="s1">n):</span>
        <span class="s5">&quot;&quot;&quot;Probability mass function for a Dirichlet multinomial distribution. 
 
        Parameters 
        ---------- 
        x: ndarray 
            Category counts (non-negative integers). Must be broadcastable 
            with shape parameter ``alpha``. If multidimensional, the last axis 
            must correspond with the categories. 
        %(_dirichlet_mn_doc_default_callparams)s 
 
        Returns 
        ------- 
        out: ndarray or scalar 
            Probability mass function. 
 
        &quot;&quot;&quot;</span>
        <span class="s2">return </span><span class="s1">np.exp(self.logpmf(x</span><span class="s2">, </span><span class="s1">alpha</span><span class="s2">, </span><span class="s1">n))</span>

    <span class="s2">def </span><span class="s1">mean(self</span><span class="s2">, </span><span class="s1">alpha</span><span class="s2">, </span><span class="s1">n):</span>
        <span class="s5">&quot;&quot;&quot;Mean of a Dirichlet multinomial distribution. 
 
        Parameters 
        ---------- 
        %(_dirichlet_mn_doc_default_callparams)s 
 
        Returns 
        ------- 
        out: ndarray 
            Mean of a Dirichlet multinomial distribution. 
 
        &quot;&quot;&quot;</span>
        <span class="s1">a</span><span class="s2">, </span><span class="s1">Sa</span><span class="s2">, </span><span class="s1">n = _dirichlet_multinomial_check_parameters(alpha</span><span class="s2">, </span><span class="s1">n)</span>
        <span class="s1">n</span><span class="s2">, </span><span class="s1">Sa = n[...</span><span class="s2">, </span><span class="s1">np.newaxis]</span><span class="s2">, </span><span class="s1">Sa[...</span><span class="s2">, </span><span class="s1">np.newaxis]</span>
        <span class="s2">return </span><span class="s1">n * a / Sa</span>

    <span class="s2">def </span><span class="s1">var(self</span><span class="s2">, </span><span class="s1">alpha</span><span class="s2">, </span><span class="s1">n):</span>
        <span class="s5">&quot;&quot;&quot;The variance of the Dirichlet multinomial distribution. 
 
        Parameters 
        ---------- 
        %(_dirichlet_mn_doc_default_callparams)s 
 
        Returns 
        ------- 
        out: array_like 
            The variances of the components of the distribution. This is 
            the diagonal of the covariance matrix of the distribution. 
 
        &quot;&quot;&quot;</span>
        <span class="s1">a</span><span class="s2">, </span><span class="s1">Sa</span><span class="s2">, </span><span class="s1">n = _dirichlet_multinomial_check_parameters(alpha</span><span class="s2">, </span><span class="s1">n)</span>
        <span class="s1">n</span><span class="s2">, </span><span class="s1">Sa = n[...</span><span class="s2">, </span><span class="s1">np.newaxis]</span><span class="s2">, </span><span class="s1">Sa[...</span><span class="s2">, </span><span class="s1">np.newaxis]</span>
        <span class="s2">return </span><span class="s1">n * a / Sa * (</span><span class="s4">1 </span><span class="s1">- a/Sa) * (n + Sa) / (</span><span class="s4">1 </span><span class="s1">+ Sa)</span>

    <span class="s2">def </span><span class="s1">cov(self</span><span class="s2">, </span><span class="s1">alpha</span><span class="s2">, </span><span class="s1">n):</span>
        <span class="s5">&quot;&quot;&quot;Covariance matrix of a Dirichlet multinomial distribution. 
 
        Parameters 
        ---------- 
        %(_dirichlet_mn_doc_default_callparams)s 
 
        Returns 
        ------- 
        out : array_like 
            The covariance matrix of the distribution. 
 
        &quot;&quot;&quot;</span>
        <span class="s1">a</span><span class="s2">, </span><span class="s1">Sa</span><span class="s2">, </span><span class="s1">n = _dirichlet_multinomial_check_parameters(alpha</span><span class="s2">, </span><span class="s1">n)</span>
        <span class="s1">var = dirichlet_multinomial.var(a</span><span class="s2">, </span><span class="s1">n)</span>

        <span class="s1">n</span><span class="s2">, </span><span class="s1">Sa = n[...</span><span class="s2">, </span><span class="s1">np.newaxis</span><span class="s2">, </span><span class="s1">np.newaxis]</span><span class="s2">, </span><span class="s1">Sa[...</span><span class="s2">, </span><span class="s1">np.newaxis</span><span class="s2">, </span><span class="s1">np.newaxis]</span>
        <span class="s1">aiaj = a[...</span><span class="s2">, </span><span class="s1">:</span><span class="s2">, </span><span class="s1">np.newaxis] * a[...</span><span class="s2">, </span><span class="s1">np.newaxis</span><span class="s2">, </span><span class="s1">:]</span>
        <span class="s1">cov = -n * aiaj / Sa ** </span><span class="s4">2 </span><span class="s1">* (n + Sa) / (</span><span class="s4">1 </span><span class="s1">+ Sa)</span>

        <span class="s1">ii = np.arange(cov.shape[-</span><span class="s4">1</span><span class="s1">])</span>
        <span class="s1">cov[...</span><span class="s2">, </span><span class="s1">ii</span><span class="s2">, </span><span class="s1">ii] = var</span>
        <span class="s2">return </span><span class="s1">cov</span>


<span class="s1">dirichlet_multinomial = dirichlet_multinomial_gen()</span>


<span class="s2">class </span><span class="s1">dirichlet_multinomial_frozen(multi_rv_frozen):</span>
    <span class="s2">def </span><span class="s1">__init__(self</span><span class="s2">, </span><span class="s1">alpha</span><span class="s2">, </span><span class="s1">n</span><span class="s2">, </span><span class="s1">seed=</span><span class="s2">None</span><span class="s1">):</span>
        <span class="s1">alpha</span><span class="s2">, </span><span class="s1">Sa</span><span class="s2">, </span><span class="s1">n = _dirichlet_multinomial_check_parameters(alpha</span><span class="s2">, </span><span class="s1">n)</span>
        <span class="s1">self.alpha = alpha</span>
        <span class="s1">self.n = n</span>
        <span class="s1">self._dist = dirichlet_multinomial_gen(seed)</span>

    <span class="s2">def </span><span class="s1">logpmf(self</span><span class="s2">, </span><span class="s1">x):</span>
        <span class="s2">return </span><span class="s1">self._dist.logpmf(x</span><span class="s2">, </span><span class="s1">self.alpha</span><span class="s2">, </span><span class="s1">self.n)</span>

    <span class="s2">def </span><span class="s1">pmf(self</span><span class="s2">, </span><span class="s1">x):</span>
        <span class="s2">return </span><span class="s1">self._dist.pmf(x</span><span class="s2">, </span><span class="s1">self.alpha</span><span class="s2">, </span><span class="s1">self.n)</span>

    <span class="s2">def </span><span class="s1">mean(self):</span>
        <span class="s2">return </span><span class="s1">self._dist.mean(self.alpha</span><span class="s2">, </span><span class="s1">self.n)</span>

    <span class="s2">def </span><span class="s1">var(self):</span>
        <span class="s2">return </span><span class="s1">self._dist.var(self.alpha</span><span class="s2">, </span><span class="s1">self.n)</span>

    <span class="s2">def </span><span class="s1">cov(self):</span>
        <span class="s2">return </span><span class="s1">self._dist.cov(self.alpha</span><span class="s2">, </span><span class="s1">self.n)</span>


<span class="s0"># Set frozen generator docstrings from corresponding docstrings in</span>
<span class="s0"># dirichlet_multinomial and fill in default strings in class docstrings.</span>
<span class="s2">for </span><span class="s1">name </span><span class="s2">in </span><span class="s1">[</span><span class="s3">'logpmf'</span><span class="s2">, </span><span class="s3">'pmf'</span><span class="s2">, </span><span class="s3">'mean'</span><span class="s2">, </span><span class="s3">'var'</span><span class="s2">, </span><span class="s3">'cov'</span><span class="s1">]:</span>
    <span class="s1">method = dirichlet_multinomial_gen.__dict__[name]</span>
    <span class="s1">method_frozen = dirichlet_multinomial_frozen.__dict__[name]</span>
    <span class="s1">method_frozen.__doc__ = doccer.docformat(</span>
        <span class="s1">method.__doc__</span><span class="s2">, </span><span class="s1">dirichlet_mn_docdict_noparams)</span>
    <span class="s1">method.__doc__ = doccer.docformat(method.__doc__</span><span class="s2">,</span>
                                      <span class="s1">dirichlet_mn_docdict_params)</span>


<span class="s2">class </span><span class="s1">vonmises_fisher_gen(multi_rv_generic):</span>
    <span class="s5">r&quot;&quot;&quot;A von Mises-Fisher variable. 
 
    The `mu` keyword specifies the mean direction vector. The `kappa` keyword 
    specifies the concentration parameter. 
 
    Methods 
    ------- 
    pdf(x, mu=None, kappa=1) 
        Probability density function. 
    logpdf(x, mu=None, kappa=1) 
        Log of the probability density function. 
    rvs(mu=None, kappa=1, size=1, random_state=None) 
        Draw random samples from a von Mises-Fisher distribution. 
    entropy(mu=None, kappa=1) 
        Compute the differential entropy of the von Mises-Fisher distribution. 
    fit(data) 
        Fit a von Mises-Fisher distribution to data. 
 
    Parameters 
    ---------- 
    mu : array_like 
        Mean direction of the distribution. Must be a one-dimensional unit 
        vector of norm 1. 
    kappa : float 
        Concentration parameter. Must be positive. 
    seed : {None, int, np.random.RandomState, np.random.Generator}, optional 
        Used for drawing random variates. 
        If `seed` is `None`, the `~np.random.RandomState` singleton is used. 
        If `seed` is an int, a new ``RandomState`` instance is used, seeded 
        with seed. 
        If `seed` is already a ``RandomState`` or ``Generator`` instance, 
        then that object is used. 
        Default is `None`. 
 
    See Also 
    -------- 
    scipy.stats.vonmises : Von-Mises Fisher distribution in 2D on a circle 
    uniform_direction : uniform distribution on the surface of a hypersphere 
 
    Notes 
    ----- 
    The von Mises-Fisher distribution is a directional distribution on the 
    surface of the unit hypersphere. The probability density 
    function of a unit vector :math:`\mathbf{x}` is 
 
    .. math:: 
 
        f(\mathbf{x}) = \frac{\kappa^{d/2-1}}{(2\pi)^{d/2}I_{d/2-1}(\kappa)} 
               \exp\left(\kappa \mathbf{\mu}^T\mathbf{x}\right), 
 
    where :math:`\mathbf{\mu}` is the mean direction, :math:`\kappa` the 
    concentration parameter, :math:`d` the dimension and :math:`I` the 
    modified Bessel function of the first kind. As :math:`\mu` represents 
    a direction, it must be a unit vector or in other words, a point 
    on the hypersphere: :math:`\mathbf{\mu}\in S^{d-1}`. :math:`\kappa` is a 
    concentration parameter, which means that it must be positive 
    (:math:`\kappa&gt;0`) and that the distribution becomes more narrow with 
    increasing :math:`\kappa`. In that sense, the reciprocal value 
    :math:`1/\kappa` resembles the variance parameter of the normal 
    distribution. 
 
    The von Mises-Fisher distribution often serves as an analogue of the 
    normal distribution on the sphere. Intuitively, for unit vectors, a 
    useful distance measure is given by the angle :math:`\alpha` between 
    them. This is exactly what the scalar product 
    :math:`\mathbf{\mu}^T\mathbf{x}=\cos(\alpha)` in the 
    von Mises-Fisher probability density function describes: the angle 
    between the mean direction :math:`\mathbf{\mu}` and the vector 
    :math:`\mathbf{x}`. The larger the angle between them, the smaller the 
    probability to observe :math:`\mathbf{x}` for this particular mean 
    direction :math:`\mathbf{\mu}`. 
 
    In dimensions 2 and 3, specialized algorithms are used for fast sampling 
    [2]_, [3]_. For dimenions of 4 or higher the rejection sampling algorithm 
    described in [4]_ is utilized. This implementation is partially based on 
    the geomstats package [5]_, [6]_. 
 
    .. versionadded:: 1.11 
 
    References 
    ---------- 
    .. [1] Von Mises-Fisher distribution, Wikipedia, 
           https://en.wikipedia.org/wiki/Von_Mises%E2%80%93Fisher_distribution 
    .. [2] Mardia, K., and Jupp, P. Directional statistics. Wiley, 2000. 
    .. [3] J. Wenzel. Numerically stable sampling of the von Mises Fisher 
           distribution on S2. 
           https://www.mitsuba-renderer.org/~wenzel/files/vmf.pdf 
    .. [4] Wood, A. Simulation of the von mises fisher distribution. 
           Communications in statistics-simulation and computation 23, 
           1 (1994), 157-164. https://doi.org/10.1080/03610919408813161 
    .. [5] geomstats, Github. MIT License. Accessed: 06.01.2023. 
           https://github.com/geomstats/geomstats 
    .. [6] Miolane, N. et al. Geomstats:  A Python Package for Riemannian 
           Geometry in Machine Learning. Journal of Machine Learning Research 
           21 (2020). http://jmlr.org/papers/v21/19-027.html 
 
    Examples 
    -------- 
    **Visualization of the probability density** 
 
    Plot the probability density in three dimensions for increasing 
    concentration parameter. The density is calculated by the ``pdf`` 
    method. 
 
    &gt;&gt;&gt; import numpy as np 
    &gt;&gt;&gt; import matplotlib.pyplot as plt 
    &gt;&gt;&gt; from scipy.stats import vonmises_fisher 
    &gt;&gt;&gt; from matplotlib.colors import Normalize 
    &gt;&gt;&gt; n_grid = 100 
    &gt;&gt;&gt; u = np.linspace(0, np.pi, n_grid) 
    &gt;&gt;&gt; v = np.linspace(0, 2 * np.pi, n_grid) 
    &gt;&gt;&gt; u_grid, v_grid = np.meshgrid(u, v) 
    &gt;&gt;&gt; vertices = np.stack([np.cos(v_grid) * np.sin(u_grid), 
    ...                      np.sin(v_grid) * np.sin(u_grid), 
    ...                      np.cos(u_grid)], 
    ...                     axis=2) 
    &gt;&gt;&gt; x = np.outer(np.cos(v), np.sin(u)) 
    &gt;&gt;&gt; y = np.outer(np.sin(v), np.sin(u)) 
    &gt;&gt;&gt; z = np.outer(np.ones_like(u), np.cos(u)) 
    &gt;&gt;&gt; def plot_vmf_density(ax, x, y, z, vertices, mu, kappa): 
    ...     vmf = vonmises_fisher(mu, kappa) 
    ...     pdf_values = vmf.pdf(vertices) 
    ...     pdfnorm = Normalize(vmin=pdf_values.min(), vmax=pdf_values.max()) 
    ...     ax.plot_surface(x, y, z, rstride=1, cstride=1, 
    ...                     facecolors=plt.cm.viridis(pdfnorm(pdf_values)), 
    ...                     linewidth=0) 
    ...     ax.set_aspect('equal') 
    ...     ax.view_init(azim=-130, elev=0) 
    ...     ax.axis('off') 
    ...     ax.set_title(rf&quot;$\kappa={kappa}$&quot;) 
    &gt;&gt;&gt; fig, axes = plt.subplots(nrows=1, ncols=3, figsize=(9, 4), 
    ...                          subplot_kw={&quot;projection&quot;: &quot;3d&quot;}) 
    &gt;&gt;&gt; left, middle, right = axes 
    &gt;&gt;&gt; mu = np.array([-np.sqrt(0.5), -np.sqrt(0.5), 0]) 
    &gt;&gt;&gt; plot_vmf_density(left, x, y, z, vertices, mu, 5) 
    &gt;&gt;&gt; plot_vmf_density(middle, x, y, z, vertices, mu, 20) 
    &gt;&gt;&gt; plot_vmf_density(right, x, y, z, vertices, mu, 100) 
    &gt;&gt;&gt; plt.subplots_adjust(top=1, bottom=0.0, left=0.0, right=1.0, wspace=0.) 
    &gt;&gt;&gt; plt.show() 
 
    As we increase the concentration parameter, the points are getting more 
    clustered together around the mean direction. 
 
    **Sampling** 
 
    Draw 5 samples from the distribution using the ``rvs`` method resulting 
    in a 5x3 array. 
 
    &gt;&gt;&gt; rng = np.random.default_rng() 
    &gt;&gt;&gt; mu = np.array([0, 0, 1]) 
    &gt;&gt;&gt; samples = vonmises_fisher(mu, 20).rvs(5, random_state=rng) 
    &gt;&gt;&gt; samples 
    array([[ 0.3884594 , -0.32482588,  0.86231516], 
           [ 0.00611366, -0.09878289,  0.99509023], 
           [-0.04154772, -0.01637135,  0.99900239], 
           [-0.14613735,  0.12553507,  0.98126695], 
           [-0.04429884, -0.23474054,  0.97104814]]) 
 
    These samples are unit vectors on the sphere :math:`S^2`. To verify, 
    let us calculate their euclidean norms: 
 
    &gt;&gt;&gt; np.linalg.norm(samples, axis=1) 
    array([1., 1., 1., 1., 1.]) 
 
    Plot 20 observations drawn from the von Mises-Fisher distribution for 
    increasing concentration parameter :math:`\kappa`. The red dot highlights 
    the mean direction :math:`\mu`. 
 
    &gt;&gt;&gt; def plot_vmf_samples(ax, x, y, z, mu, kappa): 
    ...     vmf = vonmises_fisher(mu, kappa) 
    ...     samples = vmf.rvs(20) 
    ...     ax.plot_surface(x, y, z, rstride=1, cstride=1, linewidth=0, 
    ...                     alpha=0.2) 
    ...     ax.scatter(samples[:, 0], samples[:, 1], samples[:, 2], c='k', s=5) 
    ...     ax.scatter(mu[0], mu[1], mu[2], c='r', s=30) 
    ...     ax.set_aspect('equal') 
    ...     ax.view_init(azim=-130, elev=0) 
    ...     ax.axis('off') 
    ...     ax.set_title(rf&quot;$\kappa={kappa}$&quot;) 
    &gt;&gt;&gt; mu = np.array([-np.sqrt(0.5), -np.sqrt(0.5), 0]) 
    &gt;&gt;&gt; fig, axes = plt.subplots(nrows=1, ncols=3, 
    ...                          subplot_kw={&quot;projection&quot;: &quot;3d&quot;}, 
    ...                          figsize=(9, 4)) 
    &gt;&gt;&gt; left, middle, right = axes 
    &gt;&gt;&gt; plot_vmf_samples(left, x, y, z, mu, 5) 
    &gt;&gt;&gt; plot_vmf_samples(middle, x, y, z, mu, 20) 
    &gt;&gt;&gt; plot_vmf_samples(right, x, y, z, mu, 100) 
    &gt;&gt;&gt; plt.subplots_adjust(top=1, bottom=0.0, left=0.0, 
    ...                     right=1.0, wspace=0.) 
    &gt;&gt;&gt; plt.show() 
 
    The plots show that with increasing concentration :math:`\kappa` the 
    resulting samples are centered more closely around the mean direction. 
 
    **Fitting the distribution parameters** 
 
    The distribution can be fitted to data using the ``fit`` method returning 
    the estimated parameters. As a toy example let's fit the distribution to 
    samples drawn from a known von Mises-Fisher distribution. 
 
    &gt;&gt;&gt; mu, kappa = np.array([0, 0, 1]), 20 
    &gt;&gt;&gt; samples = vonmises_fisher(mu, kappa).rvs(1000, random_state=rng) 
    &gt;&gt;&gt; mu_fit, kappa_fit = vonmises_fisher.fit(samples) 
    &gt;&gt;&gt; mu_fit, kappa_fit 
    (array([0.01126519, 0.01044501, 0.99988199]), 19.306398751730995) 
 
    We see that the estimated parameters `mu_fit` and `kappa_fit` are 
    very close to the ground truth parameters. 
 
    &quot;&quot;&quot;</span>
    <span class="s2">def </span><span class="s1">__init__(self</span><span class="s2">, </span><span class="s1">seed=</span><span class="s2">None</span><span class="s1">):</span>
        <span class="s1">super().__init__(seed)</span>

    <span class="s2">def </span><span class="s1">__call__(self</span><span class="s2">, </span><span class="s1">mu=</span><span class="s2">None, </span><span class="s1">kappa=</span><span class="s4">1</span><span class="s2">, </span><span class="s1">seed=</span><span class="s2">None</span><span class="s1">):</span>
        <span class="s5">&quot;&quot;&quot;Create a frozen von Mises-Fisher distribution. 
 
        See `vonmises_fisher_frozen` for more information. 
        &quot;&quot;&quot;</span>
        <span class="s2">return </span><span class="s1">vonmises_fisher_frozen(mu</span><span class="s2">, </span><span class="s1">kappa</span><span class="s2">, </span><span class="s1">seed=seed)</span>

    <span class="s2">def </span><span class="s1">_process_parameters(self</span><span class="s2">, </span><span class="s1">mu</span><span class="s2">, </span><span class="s1">kappa):</span>
        <span class="s5">&quot;&quot;&quot; 
        Infer dimensionality from mu and ensure that mu is a one-dimensional 
        unit vector and kappa positive. 
        &quot;&quot;&quot;</span>
        <span class="s1">mu = np.asarray(mu)</span>
        <span class="s2">if </span><span class="s1">mu.ndim &gt; </span><span class="s4">1</span><span class="s1">:</span>
            <span class="s2">raise </span><span class="s1">ValueError(</span><span class="s3">&quot;'mu' must have one-dimensional shape.&quot;</span><span class="s1">)</span>
        <span class="s2">if not </span><span class="s1">np.allclose(np.linalg.norm(mu)</span><span class="s2">, </span><span class="s4">1.</span><span class="s1">):</span>
            <span class="s2">raise </span><span class="s1">ValueError(</span><span class="s3">&quot;'mu' must be a unit vector of norm 1.&quot;</span><span class="s1">)</span>
        <span class="s2">if not </span><span class="s1">mu.size &gt; </span><span class="s4">1</span><span class="s1">:</span>
            <span class="s2">raise </span><span class="s1">ValueError(</span><span class="s3">&quot;'mu' must have at least two entries.&quot;</span><span class="s1">)</span>
        <span class="s1">kappa_error_msg = </span><span class="s3">&quot;'kappa' must be a positive scalar.&quot;</span>
        <span class="s2">if not </span><span class="s1">np.isscalar(kappa) </span><span class="s2">or </span><span class="s1">kappa &lt; </span><span class="s4">0</span><span class="s1">:</span>
            <span class="s2">raise </span><span class="s1">ValueError(kappa_error_msg)</span>
        <span class="s2">if </span><span class="s1">float(kappa) == </span><span class="s4">0.</span><span class="s1">:</span>
            <span class="s2">raise </span><span class="s1">ValueError(</span><span class="s3">&quot;For 'kappa=0' the von Mises-Fisher distribution &quot;</span>
                             <span class="s3">&quot;becomes the uniform distribution on the sphere &quot;</span>
                             <span class="s3">&quot;surface. Consider using &quot;</span>
                             <span class="s3">&quot;'scipy.stats.uniform_direction' instead.&quot;</span><span class="s1">)</span>
        <span class="s1">dim = mu.size</span>

        <span class="s2">return </span><span class="s1">dim</span><span class="s2">, </span><span class="s1">mu</span><span class="s2">, </span><span class="s1">kappa</span>

    <span class="s2">def </span><span class="s1">_check_data_vs_dist(self</span><span class="s2">, </span><span class="s1">x</span><span class="s2">, </span><span class="s1">dim):</span>
        <span class="s2">if </span><span class="s1">x.shape[-</span><span class="s4">1</span><span class="s1">] != dim:</span>
            <span class="s2">raise </span><span class="s1">ValueError(</span><span class="s3">&quot;The dimensionality of the last axis of 'x' must &quot;</span>
                             <span class="s3">&quot;match the dimensionality of the &quot;</span>
                             <span class="s3">&quot;von Mises Fisher distribution.&quot;</span><span class="s1">)</span>
        <span class="s2">if not </span><span class="s1">np.allclose(np.linalg.norm(x</span><span class="s2">, </span><span class="s1">axis=-</span><span class="s4">1</span><span class="s1">)</span><span class="s2">, </span><span class="s4">1.</span><span class="s1">):</span>
            <span class="s1">msg = </span><span class="s3">&quot;'x' must be unit vectors of norm 1 along last dimension.&quot;</span>
            <span class="s2">raise </span><span class="s1">ValueError(msg)</span>

    <span class="s2">def </span><span class="s1">_log_norm_factor(self</span><span class="s2">, </span><span class="s1">dim</span><span class="s2">, </span><span class="s1">kappa):</span>
        <span class="s0"># normalization factor is given by</span>
        <span class="s0"># c = kappa**(dim/2-1)/((2*pi)**(dim/2)*I[dim/2-1](kappa))</span>
        <span class="s0">#   = kappa**(dim/2-1)*exp(-kappa) /</span>
        <span class="s0">#     ((2*pi)**(dim/2)*I[dim/2-1](kappa)*exp(-kappa)</span>
        <span class="s0">#   = kappa**(dim/2-1)*exp(-kappa) /</span>
        <span class="s0">#     ((2*pi)**(dim/2)*ive[dim/2-1](kappa)</span>
        <span class="s0"># Then the log is given by</span>
        <span class="s0"># log c = 1/2*(dim -1)*log(kappa) - kappa - -1/2*dim*ln(2*pi) -</span>
        <span class="s0">#         ive[dim/2-1](kappa)</span>
        <span class="s1">halfdim = </span><span class="s4">0.5 </span><span class="s1">* dim</span>
        <span class="s2">return </span><span class="s1">(</span><span class="s4">0.5 </span><span class="s1">* (dim - </span><span class="s4">2</span><span class="s1">)*np.log(kappa) - halfdim * _LOG_2PI -</span>
                <span class="s1">np.log(ive(halfdim - </span><span class="s4">1</span><span class="s2">, </span><span class="s1">kappa)) - kappa)</span>

    <span class="s2">def </span><span class="s1">_logpdf(self</span><span class="s2">, </span><span class="s1">x</span><span class="s2">, </span><span class="s1">dim</span><span class="s2">, </span><span class="s1">mu</span><span class="s2">, </span><span class="s1">kappa):</span>
        <span class="s5">&quot;&quot;&quot;Log of the von Mises-Fisher probability density function. 
 
        As this function does no argument checking, it should not be 
        called directly; use 'logpdf' instead. 
 
        &quot;&quot;&quot;</span>
        <span class="s1">x = np.asarray(x)</span>
        <span class="s1">self._check_data_vs_dist(x</span><span class="s2">, </span><span class="s1">dim)</span>
        <span class="s1">dotproducts = np.einsum(</span><span class="s3">'i,...i-&gt;...'</span><span class="s2">, </span><span class="s1">mu</span><span class="s2">, </span><span class="s1">x)</span>
        <span class="s2">return </span><span class="s1">self._log_norm_factor(dim</span><span class="s2">, </span><span class="s1">kappa) + kappa * dotproducts</span>

    <span class="s2">def </span><span class="s1">logpdf(self</span><span class="s2">, </span><span class="s1">x</span><span class="s2">, </span><span class="s1">mu=</span><span class="s2">None, </span><span class="s1">kappa=</span><span class="s4">1</span><span class="s1">):</span>
        <span class="s5">&quot;&quot;&quot;Log of the von Mises-Fisher probability density function. 
 
        Parameters 
        ---------- 
        x : array_like 
            Points at which to evaluate the log of the probability 
            density function. The last axis of `x` must correspond 
            to unit vectors of the same dimensionality as the distribution. 
        mu : array_like, default: None 
            Mean direction of the distribution. Must be a one-dimensional unit 
            vector of norm 1. 
        kappa : float, default: 1 
            Concentration parameter. Must be positive. 
 
        Returns 
        ------- 
        logpdf : ndarray or scalar 
            Log of the probability density function evaluated at `x`. 
 
        &quot;&quot;&quot;</span>
        <span class="s1">dim</span><span class="s2">, </span><span class="s1">mu</span><span class="s2">, </span><span class="s1">kappa = self._process_parameters(mu</span><span class="s2">, </span><span class="s1">kappa)</span>
        <span class="s2">return </span><span class="s1">self._logpdf(x</span><span class="s2">, </span><span class="s1">dim</span><span class="s2">, </span><span class="s1">mu</span><span class="s2">, </span><span class="s1">kappa)</span>

    <span class="s2">def </span><span class="s1">pdf(self</span><span class="s2">, </span><span class="s1">x</span><span class="s2">, </span><span class="s1">mu=</span><span class="s2">None, </span><span class="s1">kappa=</span><span class="s4">1</span><span class="s1">):</span>
        <span class="s5">&quot;&quot;&quot;Von Mises-Fisher probability density function. 
 
        Parameters 
        ---------- 
        x : array_like 
            Points at which to evaluate the probability 
            density function. The last axis of `x` must correspond 
            to unit vectors of the same dimensionality as the distribution. 
        mu : array_like 
            Mean direction of the distribution. Must be a one-dimensional unit 
            vector of norm 1. 
        kappa : float 
            Concentration parameter. Must be positive. 
 
        Returns 
        ------- 
        pdf : ndarray or scalar 
            Probability density function evaluated at `x`. 
 
        &quot;&quot;&quot;</span>
        <span class="s1">dim</span><span class="s2">, </span><span class="s1">mu</span><span class="s2">, </span><span class="s1">kappa = self._process_parameters(mu</span><span class="s2">, </span><span class="s1">kappa)</span>
        <span class="s2">return </span><span class="s1">np.exp(self._logpdf(x</span><span class="s2">, </span><span class="s1">dim</span><span class="s2">, </span><span class="s1">mu</span><span class="s2">, </span><span class="s1">kappa))</span>

    <span class="s2">def </span><span class="s1">_rvs_2d(self</span><span class="s2">, </span><span class="s1">mu</span><span class="s2">, </span><span class="s1">kappa</span><span class="s2">, </span><span class="s1">size</span><span class="s2">, </span><span class="s1">random_state):</span>
        <span class="s5">&quot;&quot;&quot; 
        In 2D, the von Mises-Fisher distribution reduces to the 
        von Mises distribution which can be efficiently sampled by numpy. 
        This method is much faster than the general rejection 
        sampling based algorithm. 
 
        &quot;&quot;&quot;</span>
        <span class="s1">mean_angle = np.arctan2(mu[</span><span class="s4">1</span><span class="s1">]</span><span class="s2">, </span><span class="s1">mu[</span><span class="s4">0</span><span class="s1">])</span>
        <span class="s1">angle_samples = random_state.vonmises(mean_angle</span><span class="s2">, </span><span class="s1">kappa</span><span class="s2">, </span><span class="s1">size=size)</span>
        <span class="s1">samples = np.stack([np.cos(angle_samples)</span><span class="s2">, </span><span class="s1">np.sin(angle_samples)]</span><span class="s2">,</span>
                           <span class="s1">axis=-</span><span class="s4">1</span><span class="s1">)</span>
        <span class="s2">return </span><span class="s1">samples</span>

    <span class="s2">def </span><span class="s1">_rvs_3d(self</span><span class="s2">, </span><span class="s1">kappa</span><span class="s2">, </span><span class="s1">size</span><span class="s2">, </span><span class="s1">random_state):</span>
        <span class="s5">&quot;&quot;&quot; 
        Generate samples from a von Mises-Fisher distribution 
        with mu = [1, 0, 0] and kappa. Samples then have to be 
        rotated towards the desired mean direction mu. 
        This method is much faster than the general rejection 
        sampling based algorithm. 
        Reference: https://www.mitsuba-renderer.org/~wenzel/files/vmf.pdf 
 
        &quot;&quot;&quot;</span>
        <span class="s2">if </span><span class="s1">size </span><span class="s2">is None</span><span class="s1">:</span>
            <span class="s1">sample_size = </span><span class="s4">1</span>
        <span class="s2">else</span><span class="s1">:</span>
            <span class="s1">sample_size = size</span>

        <span class="s0"># compute x coordinate acc. to equation from section 3.1</span>
        <span class="s1">x = random_state.random(sample_size)</span>
        <span class="s1">x = </span><span class="s4">1. </span><span class="s1">+ np.log(x + (</span><span class="s4">1. </span><span class="s1">- x) * np.exp(-</span><span class="s4">2 </span><span class="s1">* kappa))/kappa</span>

        <span class="s0"># (y, z) are random 2D vectors that only have to be</span>
        <span class="s0"># normalized accordingly. Then (x, y z) follow a VMF distribution</span>
        <span class="s1">temp = np.sqrt(</span><span class="s4">1. </span><span class="s1">- np.square(x))</span>
        <span class="s1">uniformcircle = _sample_uniform_direction(</span><span class="s4">2</span><span class="s2">, </span><span class="s1">sample_size</span><span class="s2">, </span><span class="s1">random_state)</span>
        <span class="s1">samples = np.stack([x</span><span class="s2">, </span><span class="s1">temp * uniformcircle[...</span><span class="s2">, </span><span class="s4">0</span><span class="s1">]</span><span class="s2">,</span>
                            <span class="s1">temp * uniformcircle[...</span><span class="s2">, </span><span class="s4">1</span><span class="s1">]]</span><span class="s2">,</span>
                           <span class="s1">axis=-</span><span class="s4">1</span><span class="s1">)</span>
        <span class="s2">if </span><span class="s1">size </span><span class="s2">is None</span><span class="s1">:</span>
            <span class="s1">samples = np.squeeze(samples)</span>
        <span class="s2">return </span><span class="s1">samples</span>

    <span class="s2">def </span><span class="s1">_rejection_sampling(self</span><span class="s2">, </span><span class="s1">dim</span><span class="s2">, </span><span class="s1">kappa</span><span class="s2">, </span><span class="s1">size</span><span class="s2">, </span><span class="s1">random_state):</span>
        <span class="s5">&quot;&quot;&quot; 
        Generate samples from a n-dimensional von Mises-Fisher distribution 
        with mu = [1, 0, ..., 0] and kappa via rejection sampling. 
        Samples then have to be rotated towards the desired mean direction mu. 
        Reference: https://doi.org/10.1080/03610919408813161 
        &quot;&quot;&quot;</span>
        <span class="s1">dim_minus_one = dim - </span><span class="s4">1</span>
        <span class="s0"># calculate number of requested samples</span>
        <span class="s2">if </span><span class="s1">size </span><span class="s2">is not None</span><span class="s1">:</span>
            <span class="s2">if not </span><span class="s1">np.iterable(size):</span>
                <span class="s1">size = (size</span><span class="s2">, </span><span class="s1">)</span>
            <span class="s1">n_samples = math.prod(size)</span>
        <span class="s2">else</span><span class="s1">:</span>
            <span class="s1">n_samples = </span><span class="s4">1</span>
        <span class="s0"># calculate envelope for rejection sampler (eq. 4)</span>
        <span class="s1">sqrt = np.sqrt(</span><span class="s4">4 </span><span class="s1">* kappa ** </span><span class="s4">2. </span><span class="s1">+ dim_minus_one ** </span><span class="s4">2</span><span class="s1">)</span>
        <span class="s1">envelop_param = (-</span><span class="s4">2 </span><span class="s1">* kappa + sqrt) / dim_minus_one</span>
        <span class="s2">if </span><span class="s1">envelop_param == </span><span class="s4">0</span><span class="s1">:</span>
            <span class="s0"># the regular formula suffers from loss of precision for high</span>
            <span class="s0"># kappa. This can only be detected by checking for 0 here.</span>
            <span class="s0"># Workaround: expansion for sqrt variable</span>
            <span class="s0"># https://www.wolframalpha.com/input?i=sqrt%284*x%5E2%2Bd%5E2%29</span>
            <span class="s0"># e = (-2 * k + sqrt(k**2 + d**2)) / d</span>
            <span class="s0">#   ~ (-2 * k + 2 * k + d**2/(4 * k) - d**4/(64 * k**3)) / d</span>
            <span class="s0">#   = d/(4 * k) - d**3/(64 * k**3)</span>
            <span class="s1">envelop_param = (dim_minus_one/</span><span class="s4">4 </span><span class="s1">* kappa**-</span><span class="s4">1.</span>
                             <span class="s1">- dim_minus_one**</span><span class="s4">3</span><span class="s1">/</span><span class="s4">64 </span><span class="s1">* kappa**-</span><span class="s4">3.</span><span class="s1">)</span>
        <span class="s0"># reference step 0</span>
        <span class="s1">node = (</span><span class="s4">1. </span><span class="s1">- envelop_param) / (</span><span class="s4">1. </span><span class="s1">+ envelop_param)</span>
        <span class="s0"># t = ln(1 - ((1-x)/(1+x))**2)</span>
        <span class="s0">#   = ln(4 * x / (1+x)**2)</span>
        <span class="s0">#   = ln(4) + ln(x) - 2*log1p(x)</span>
        <span class="s1">correction = (kappa * node + dim_minus_one</span>
                      <span class="s1">* (np.log(</span><span class="s4">4</span><span class="s1">) + np.log(envelop_param)</span>
                      <span class="s1">- </span><span class="s4">2 </span><span class="s1">* np.log1p(envelop_param)))</span>
        <span class="s1">n_accepted = </span><span class="s4">0</span>
        <span class="s1">x = np.zeros((n_samples</span><span class="s2">, </span><span class="s1">))</span>
        <span class="s1">halfdim = </span><span class="s4">0.5 </span><span class="s1">* dim_minus_one</span>
        <span class="s0"># main loop</span>
        <span class="s2">while </span><span class="s1">n_accepted &lt; n_samples:</span>
            <span class="s0"># generate candidates acc. to reference step 1</span>
            <span class="s1">sym_beta = random_state.beta(halfdim</span><span class="s2">, </span><span class="s1">halfdim</span><span class="s2">,</span>
                                         <span class="s1">size=n_samples - n_accepted)</span>
            <span class="s1">coord_x = (</span><span class="s4">1 </span><span class="s1">- (</span><span class="s4">1 </span><span class="s1">+ envelop_param) * sym_beta) / (</span>
                <span class="s4">1 </span><span class="s1">- (</span><span class="s4">1 </span><span class="s1">- envelop_param) * sym_beta)</span>
            <span class="s0"># accept or reject: reference step 2</span>
            <span class="s0"># reformulation for numerical stability:</span>
            <span class="s0"># t = ln(1 - (1-x)/(1+x) * y)</span>
            <span class="s0">#   = ln((1 + x - y +x*y)/(1 +x))</span>
            <span class="s1">accept_tol = random_state.random(n_samples - n_accepted)</span>
            <span class="s1">criterion = (</span>
                <span class="s1">kappa * coord_x</span>
                <span class="s1">+ dim_minus_one * (np.log((</span><span class="s4">1 </span><span class="s1">+ envelop_param - coord_x</span>
                <span class="s1">+ coord_x * envelop_param) / (</span><span class="s4">1 </span><span class="s1">+ envelop_param)))</span>
                <span class="s1">- correction) &gt; np.log(accept_tol)</span>
            <span class="s1">accepted_iter = np.sum(criterion)</span>
            <span class="s1">x[n_accepted:n_accepted + accepted_iter] = coord_x[criterion]</span>
            <span class="s1">n_accepted += accepted_iter</span>
        <span class="s0"># concatenate x and remaining coordinates: step 3</span>
        <span class="s1">coord_rest = _sample_uniform_direction(dim_minus_one</span><span class="s2">, </span><span class="s1">n_accepted</span><span class="s2">,</span>
                                               <span class="s1">random_state)</span>
        <span class="s1">coord_rest = np.einsum(</span>
            <span class="s3">'...,...i-&gt;...i'</span><span class="s2">, </span><span class="s1">np.sqrt(</span><span class="s4">1 </span><span class="s1">- x ** </span><span class="s4">2</span><span class="s1">)</span><span class="s2">, </span><span class="s1">coord_rest)</span>
        <span class="s1">samples = np.concatenate([x[...</span><span class="s2">, None</span><span class="s1">]</span><span class="s2">, </span><span class="s1">coord_rest]</span><span class="s2">, </span><span class="s1">axis=</span><span class="s4">1</span><span class="s1">)</span>
        <span class="s0"># reshape output to (size, dim)</span>
        <span class="s2">if </span><span class="s1">size </span><span class="s2">is not None</span><span class="s1">:</span>
            <span class="s1">samples = samples.reshape(size + (dim</span><span class="s2">, </span><span class="s1">))</span>
        <span class="s2">else</span><span class="s1">:</span>
            <span class="s1">samples = np.squeeze(samples)</span>
        <span class="s2">return </span><span class="s1">samples</span>

    <span class="s2">def </span><span class="s1">_rotate_samples(self</span><span class="s2">, </span><span class="s1">samples</span><span class="s2">, </span><span class="s1">mu</span><span class="s2">, </span><span class="s1">dim):</span>
        <span class="s5">&quot;&quot;&quot;A QR decomposition is used to find the rotation that maps the 
        north pole (1, 0,...,0) to the vector mu. This rotation is then 
        applied to all samples. 
 
        Parameters 
        ---------- 
        samples: array_like, shape = [..., n] 
        mu : array-like, shape=[n, ] 
            Point to parametrise the rotation. 
 
        Returns 
        ------- 
        samples : rotated samples 
 
        &quot;&quot;&quot;</span>
        <span class="s1">base_point = np.zeros((dim</span><span class="s2">, </span><span class="s1">))</span>
        <span class="s1">base_point[</span><span class="s4">0</span><span class="s1">] = </span><span class="s4">1.</span>
        <span class="s1">embedded = np.concatenate([mu[</span><span class="s2">None, </span><span class="s1">:]</span><span class="s2">, </span><span class="s1">np.zeros((dim - </span><span class="s4">1</span><span class="s2">, </span><span class="s1">dim))])</span>
        <span class="s1">rotmatrix</span><span class="s2">, </span><span class="s1">_ = np.linalg.qr(np.transpose(embedded))</span>
        <span class="s2">if </span><span class="s1">np.allclose(np.matmul(rotmatrix</span><span class="s2">, </span><span class="s1">base_point[:</span><span class="s2">, None</span><span class="s1">])[:</span><span class="s2">, </span><span class="s4">0</span><span class="s1">]</span><span class="s2">, </span><span class="s1">mu):</span>
            <span class="s1">rotsign = </span><span class="s4">1</span>
        <span class="s2">else</span><span class="s1">:</span>
            <span class="s1">rotsign = -</span><span class="s4">1</span>

        <span class="s0"># apply rotation</span>
        <span class="s1">samples = np.einsum(</span><span class="s3">'ij,...j-&gt;...i'</span><span class="s2">, </span><span class="s1">rotmatrix</span><span class="s2">, </span><span class="s1">samples) * rotsign</span>
        <span class="s2">return </span><span class="s1">samples</span>

    <span class="s2">def </span><span class="s1">_rvs(self</span><span class="s2">, </span><span class="s1">dim</span><span class="s2">, </span><span class="s1">mu</span><span class="s2">, </span><span class="s1">kappa</span><span class="s2">, </span><span class="s1">size</span><span class="s2">, </span><span class="s1">random_state):</span>
        <span class="s2">if </span><span class="s1">dim == </span><span class="s4">2</span><span class="s1">:</span>
            <span class="s1">samples = self._rvs_2d(mu</span><span class="s2">, </span><span class="s1">kappa</span><span class="s2">, </span><span class="s1">size</span><span class="s2">, </span><span class="s1">random_state)</span>
        <span class="s2">elif </span><span class="s1">dim == </span><span class="s4">3</span><span class="s1">:</span>
            <span class="s1">samples = self._rvs_3d(kappa</span><span class="s2">, </span><span class="s1">size</span><span class="s2">, </span><span class="s1">random_state)</span>
        <span class="s2">else</span><span class="s1">:</span>
            <span class="s1">samples = self._rejection_sampling(dim</span><span class="s2">, </span><span class="s1">kappa</span><span class="s2">, </span><span class="s1">size</span><span class="s2">,</span>
                                               <span class="s1">random_state)</span>

        <span class="s2">if </span><span class="s1">dim != </span><span class="s4">2</span><span class="s1">:</span>
            <span class="s1">samples = self._rotate_samples(samples</span><span class="s2">, </span><span class="s1">mu</span><span class="s2">, </span><span class="s1">dim)</span>
        <span class="s2">return </span><span class="s1">samples</span>

    <span class="s2">def </span><span class="s1">rvs(self</span><span class="s2">, </span><span class="s1">mu=</span><span class="s2">None, </span><span class="s1">kappa=</span><span class="s4">1</span><span class="s2">, </span><span class="s1">size=</span><span class="s4">1</span><span class="s2">, </span><span class="s1">random_state=</span><span class="s2">None</span><span class="s1">):</span>
        <span class="s5">&quot;&quot;&quot;Draw random samples from a von Mises-Fisher distribution. 
 
        Parameters 
        ---------- 
        mu : array_like 
            Mean direction of the distribution. Must be a one-dimensional unit 
            vector of norm 1. 
        kappa : float 
            Concentration parameter. Must be positive. 
        size : int or tuple of ints, optional 
            Given a shape of, for example, (m,n,k), m*n*k samples are 
            generated, and packed in an m-by-n-by-k arrangement. 
            Because each sample is N-dimensional, the output shape 
            is (m,n,k,N). If no shape is specified, a single (N-D) 
            sample is returned. 
        random_state : {None, int, np.random.RandomState, np.random.Generator}, 
                        optional 
            Used for drawing random variates. 
            If `seed` is `None`, the `~np.random.RandomState` singleton is used. 
            If `seed` is an int, a new ``RandomState`` instance is used, seeded 
            with seed. 
            If `seed` is already a ``RandomState`` or ``Generator`` instance, 
            then that object is used. 
            Default is `None`. 
 
        Returns 
        ------- 
        rvs : ndarray 
            Random variates of shape (`size`, `N`), where `N` is the 
            dimension of the distribution. 
 
        &quot;&quot;&quot;</span>
        <span class="s1">dim</span><span class="s2">, </span><span class="s1">mu</span><span class="s2">, </span><span class="s1">kappa = self._process_parameters(mu</span><span class="s2">, </span><span class="s1">kappa)</span>
        <span class="s1">random_state = self._get_random_state(random_state)</span>
        <span class="s1">samples = self._rvs(dim</span><span class="s2">, </span><span class="s1">mu</span><span class="s2">, </span><span class="s1">kappa</span><span class="s2">, </span><span class="s1">size</span><span class="s2">, </span><span class="s1">random_state)</span>
        <span class="s2">return </span><span class="s1">samples</span>

    <span class="s2">def </span><span class="s1">_entropy(self</span><span class="s2">, </span><span class="s1">dim</span><span class="s2">, </span><span class="s1">kappa):</span>
        <span class="s1">halfdim = </span><span class="s4">0.5 </span><span class="s1">* dim</span>
        <span class="s2">return </span><span class="s1">(-self._log_norm_factor(dim</span><span class="s2">, </span><span class="s1">kappa) - kappa *</span>
                <span class="s1">ive(halfdim</span><span class="s2">, </span><span class="s1">kappa) / ive(halfdim - </span><span class="s4">1</span><span class="s2">, </span><span class="s1">kappa))</span>

    <span class="s2">def </span><span class="s1">entropy(self</span><span class="s2">, </span><span class="s1">mu=</span><span class="s2">None, </span><span class="s1">kappa=</span><span class="s4">1</span><span class="s1">):</span>
        <span class="s5">&quot;&quot;&quot;Compute the differential entropy of the von Mises-Fisher 
        distribution. 
 
        Parameters 
        ---------- 
        mu : array_like, default: None 
            Mean direction of the distribution. Must be a one-dimensional unit 
            vector of norm 1. 
        kappa : float, default: 1 
            Concentration parameter. Must be positive. 
 
        Returns 
        ------- 
        h : scalar 
            Entropy of the von Mises-Fisher distribution. 
 
        &quot;&quot;&quot;</span>
        <span class="s1">dim</span><span class="s2">, </span><span class="s1">_</span><span class="s2">, </span><span class="s1">kappa = self._process_parameters(mu</span><span class="s2">, </span><span class="s1">kappa)</span>
        <span class="s2">return </span><span class="s1">self._entropy(dim</span><span class="s2">, </span><span class="s1">kappa)</span>

    <span class="s2">def </span><span class="s1">fit(self</span><span class="s2">, </span><span class="s1">x):</span>
        <span class="s5">&quot;&quot;&quot;Fit the von Mises-Fisher distribution to data. 
 
        Parameters 
        ---------- 
        x : array-like 
            Data the distribution is fitted to. Must be two dimensional. 
            The second axis of `x` must be unit vectors of norm 1 and 
            determine the dimensionality of the fitted 
            von Mises-Fisher distribution. 
 
        Returns 
        ------- 
        mu : ndarray 
            Estimated mean direction. 
        kappa : float 
            Estimated concentration parameter. 
 
        &quot;&quot;&quot;</span>
        <span class="s0"># validate input data</span>
        <span class="s1">x = np.asarray(x)</span>
        <span class="s2">if </span><span class="s1">x.ndim != </span><span class="s4">2</span><span class="s1">:</span>
            <span class="s2">raise </span><span class="s1">ValueError(</span><span class="s3">&quot;'x' must be two dimensional.&quot;</span><span class="s1">)</span>
        <span class="s2">if not </span><span class="s1">np.allclose(np.linalg.norm(x</span><span class="s2">, </span><span class="s1">axis=-</span><span class="s4">1</span><span class="s1">)</span><span class="s2">, </span><span class="s4">1.</span><span class="s1">):</span>
            <span class="s1">msg = </span><span class="s3">&quot;'x' must be unit vectors of norm 1 along last dimension.&quot;</span>
            <span class="s2">raise </span><span class="s1">ValueError(msg)</span>
        <span class="s1">dim = x.shape[-</span><span class="s4">1</span><span class="s1">]</span>

        <span class="s0"># mu is simply the directional mean</span>
        <span class="s1">dirstats = directional_stats(x)</span>
        <span class="s1">mu = dirstats.mean_direction</span>
        <span class="s1">r = dirstats.mean_resultant_length</span>

        <span class="s0"># kappa is the solution to the equation:</span>
        <span class="s0"># r = I[dim/2](kappa) / I[dim/2 -1](kappa)</span>
        <span class="s0">#   = I[dim/2](kappa) * exp(-kappa) / I[dim/2 -1](kappa) * exp(-kappa)</span>
        <span class="s0">#   = ive(dim/2, kappa) / ive(dim/2 -1, kappa)</span>

        <span class="s1">halfdim = </span><span class="s4">0.5 </span><span class="s1">* dim</span>

        <span class="s2">def </span><span class="s1">solve_for_kappa(kappa):</span>
            <span class="s1">bessel_vals = ive([halfdim</span><span class="s2">, </span><span class="s1">halfdim - </span><span class="s4">1</span><span class="s1">]</span><span class="s2">, </span><span class="s1">kappa)</span>
            <span class="s2">return </span><span class="s1">bessel_vals[</span><span class="s4">0</span><span class="s1">]/bessel_vals[</span><span class="s4">1</span><span class="s1">] - r</span>

        <span class="s1">root_res = root_scalar(solve_for_kappa</span><span class="s2">, </span><span class="s1">method=</span><span class="s3">&quot;brentq&quot;</span><span class="s2">,</span>
                               <span class="s1">bracket=(</span><span class="s4">1e-8</span><span class="s2">, </span><span class="s4">1e9</span><span class="s1">))</span>
        <span class="s1">kappa = root_res.root</span>
        <span class="s2">return </span><span class="s1">mu</span><span class="s2">, </span><span class="s1">kappa</span>


<span class="s1">vonmises_fisher = vonmises_fisher_gen()</span>


<span class="s2">class </span><span class="s1">vonmises_fisher_frozen(multi_rv_frozen):</span>
    <span class="s2">def </span><span class="s1">__init__(self</span><span class="s2">, </span><span class="s1">mu=</span><span class="s2">None, </span><span class="s1">kappa=</span><span class="s4">1</span><span class="s2">, </span><span class="s1">seed=</span><span class="s2">None</span><span class="s1">):</span>
        <span class="s5">&quot;&quot;&quot;Create a frozen von Mises-Fisher distribution. 
 
        Parameters 
        ---------- 
        mu : array_like, default: None 
            Mean direction of the distribution. 
        kappa : float, default: 1 
            Concentration parameter. Must be positive. 
        seed : {None, int, `numpy.random.Generator`, 
                `numpy.random.RandomState`}, optional 
            If `seed` is None (or `np.random`), the `numpy.random.RandomState` 
            singleton is used. 
            If `seed` is an int, a new ``RandomState`` instance is used, 
            seeded with `seed`. 
            If `seed` is already a ``Generator`` or ``RandomState`` instance 
            then that instance is used. 
 
        &quot;&quot;&quot;</span>
        <span class="s1">self._dist = vonmises_fisher_gen(seed)</span>
        <span class="s1">self.dim</span><span class="s2">, </span><span class="s1">self.mu</span><span class="s2">, </span><span class="s1">self.kappa = (</span>
            <span class="s1">self._dist._process_parameters(mu</span><span class="s2">, </span><span class="s1">kappa)</span>
        <span class="s1">)</span>

    <span class="s2">def </span><span class="s1">logpdf(self</span><span class="s2">, </span><span class="s1">x):</span>
        <span class="s5">&quot;&quot;&quot; 
        Parameters 
        ---------- 
        x : array_like 
            Points at which to evaluate the log of the probability 
            density function. The last axis of `x` must correspond 
            to unit vectors of the same dimensionality as the distribution. 
 
        Returns 
        ------- 
        logpdf : ndarray or scalar 
            Log of probability density function evaluated at `x`. 
 
        &quot;&quot;&quot;</span>
        <span class="s2">return </span><span class="s1">self._dist._logpdf(x</span><span class="s2">, </span><span class="s1">self.dim</span><span class="s2">, </span><span class="s1">self.mu</span><span class="s2">, </span><span class="s1">self.kappa)</span>

    <span class="s2">def </span><span class="s1">pdf(self</span><span class="s2">, </span><span class="s1">x):</span>
        <span class="s5">&quot;&quot;&quot; 
        Parameters 
        ---------- 
        x : array_like 
            Points at which to evaluate the log of the probability 
            density function. The last axis of `x` must correspond 
            to unit vectors of the same dimensionality as the distribution. 
 
        Returns 
        ------- 
        pdf : ndarray or scalar 
            Probability density function evaluated at `x`. 
 
        &quot;&quot;&quot;</span>
        <span class="s2">return </span><span class="s1">np.exp(self.logpdf(x))</span>

    <span class="s2">def </span><span class="s1">rvs(self</span><span class="s2">, </span><span class="s1">size=</span><span class="s4">1</span><span class="s2">, </span><span class="s1">random_state=</span><span class="s2">None</span><span class="s1">):</span>
        <span class="s5">&quot;&quot;&quot;Draw random variates from the Von Mises-Fisher distribution. 
 
        Parameters 
        ---------- 
        size : int or tuple of ints, optional 
            Given a shape of, for example, (m,n,k), m*n*k samples are 
            generated, and packed in an m-by-n-by-k arrangement. 
            Because each sample is N-dimensional, the output shape 
            is (m,n,k,N). If no shape is specified, a single (N-D) 
            sample is returned. 
        random_state : {None, int, `numpy.random.Generator`, 
                        `numpy.random.RandomState`}, optional 
            If `seed` is None (or `np.random`), the `numpy.random.RandomState` 
            singleton is used. 
            If `seed` is an int, a new ``RandomState`` instance is used, 
            seeded with `seed`. 
            If `seed` is already a ``Generator`` or ``RandomState`` instance 
            then that instance is used. 
 
        Returns 
        ------- 
        rvs : ndarray or scalar 
            Random variates of size (`size`, `N`), where `N` is the 
            dimension of the distribution. 
 
        &quot;&quot;&quot;</span>
        <span class="s1">random_state = self._dist._get_random_state(random_state)</span>
        <span class="s2">return </span><span class="s1">self._dist._rvs(self.dim</span><span class="s2">, </span><span class="s1">self.mu</span><span class="s2">, </span><span class="s1">self.kappa</span><span class="s2">, </span><span class="s1">size</span><span class="s2">,</span>
                               <span class="s1">random_state)</span>

    <span class="s2">def </span><span class="s1">entropy(self):</span>
        <span class="s5">&quot;&quot;&quot; 
        Calculate the differential entropy of the von Mises-Fisher 
        distribution. 
 
        Returns 
        ------- 
        h: float 
            Entropy of the Von Mises-Fisher distribution. 
 
        &quot;&quot;&quot;</span>
        <span class="s2">return </span><span class="s1">self._dist._entropy(self.dim</span><span class="s2">, </span><span class="s1">self.kappa)</span>
</pre>
</body>
</html>