<html>
<head>
<title>elregress.py</title>
<meta http-equiv="Content-Type" content="text/html; charset=utf-8">
<style type="text/css">
.s0 { color: #629755; font-style: italic;}
.s1 { color: #a9b7c6;}
.s2 { color: #cc7832;}
.s3 { color: #6897bb;}
.s4 { color: #808080;}
</style>
</head>
<body bgcolor="#2b2b2b">
<table CELLSPACING=0 CELLPADDING=5 COLS=1 WIDTH="100%" BGCOLOR="#606060" >
<tr><td><center>
<font face="Arial, Helvetica" color="#000000">
elregress.py</font>
</center></td></tr></table>
<pre><span class="s0">&quot;&quot;&quot; 
Empirical Likelihood Linear Regression Inference 
 
The script contains the function that is optimized over nuisance parameters to 
 conduct inference on linear regression parameters.  It is called by eltest 
in OLSResults. 
 
 
General References 
----------------- 
 
Owen, A.B.(2001). Empirical Likelihood. Chapman and Hall 
 
&quot;&quot;&quot;</span>
<span class="s2">import </span><span class="s1">numpy </span><span class="s2">as </span><span class="s1">np</span>
<span class="s2">from </span><span class="s1">statsmodels.emplike.descriptive </span><span class="s2">import </span><span class="s1">_OptFuncts</span>



<span class="s2">class </span><span class="s1">_ELRegOpts(_OptFuncts):</span>
    <span class="s0">&quot;&quot;&quot; 
 
    A class that holds functions to be optimized over when conducting 
    hypothesis tests and calculating confidence intervals. 
 
    Parameters 
    ---------- 
 
    OLSResults : Results instance 
        A fitted OLS result. 
    &quot;&quot;&quot;</span>
    <span class="s2">def </span><span class="s1">__init__(self):</span>
        <span class="s2">pass</span>

    <span class="s2">def </span><span class="s1">_opt_nuis_regress(self</span><span class="s2">, </span><span class="s1">nuisance_params</span><span class="s2">, </span><span class="s1">param_nums=</span><span class="s2">None,</span>
                          <span class="s1">endog=</span><span class="s2">None, </span><span class="s1">exog=</span><span class="s2">None,</span>
                          <span class="s1">nobs=</span><span class="s2">None, </span><span class="s1">nvar=</span><span class="s2">None, </span><span class="s1">params=</span><span class="s2">None, </span><span class="s1">b0_vals=</span><span class="s2">None,</span>
                          <span class="s1">stochastic_exog=</span><span class="s2">None</span><span class="s1">):</span>
        <span class="s0">&quot;&quot;&quot; 
        A function that is optimized over nuisance parameters to conduct a 
        hypothesis test for the parameters of interest. 
 
        Parameters 
        ---------- 
        nuisance_params: 1darray 
            Parameters to be optimized over. 
 
        Returns 
        ------- 
        llr : float 
            -2 x the log-likelihood of the nuisance parameters and the 
            hypothesized value of the parameter(s) of interest. 
        &quot;&quot;&quot;</span>
        <span class="s1">params[param_nums] = b0_vals</span>
        <span class="s1">nuis_param_index = np.int_(np.delete(np.arange(nvar)</span><span class="s2">,</span>
                                             <span class="s1">param_nums))</span>
        <span class="s1">params[nuis_param_index] = nuisance_params</span>
        <span class="s1">new_params = params.reshape(nvar</span><span class="s2">, </span><span class="s3">1</span><span class="s1">)</span>
        <span class="s1">self.new_params = new_params</span>
        <span class="s1">est_vect = exog * \</span>
          <span class="s1">(endog - np.squeeze(np.dot(exog</span><span class="s2">, </span><span class="s1">new_params))).reshape(int(nobs)</span><span class="s2">, </span><span class="s3">1</span><span class="s1">)</span>
        <span class="s2">if not </span><span class="s1">stochastic_exog:</span>
            <span class="s1">exog_means = np.mean(exog</span><span class="s2">, </span><span class="s1">axis=</span><span class="s3">0</span><span class="s1">)[</span><span class="s3">1</span><span class="s1">:]</span>
            <span class="s1">exog_mom2 = (np.sum(exog * exog</span><span class="s2">, </span><span class="s1">axis=</span><span class="s3">0</span><span class="s1">))[</span><span class="s3">1</span><span class="s1">:]\</span>
                          <span class="s1">/ nobs</span>
            <span class="s1">mean_est_vect = exog[:</span><span class="s2">, </span><span class="s3">1</span><span class="s1">:] - exog_means</span>
            <span class="s1">mom2_est_vect = (exog * exog)[:</span><span class="s2">, </span><span class="s3">1</span><span class="s1">:] - exog_mom2</span>
            <span class="s1">regressor_est_vect = np.concatenate((mean_est_vect</span><span class="s2">, </span><span class="s1">mom2_est_vect)</span><span class="s2">,</span>
                                                <span class="s1">axis=</span><span class="s3">1</span><span class="s1">)</span>
            <span class="s1">est_vect = np.concatenate((est_vect</span><span class="s2">, </span><span class="s1">regressor_est_vect)</span><span class="s2">,</span>
                                           <span class="s1">axis=</span><span class="s3">1</span><span class="s1">)</span>

        <span class="s1">wts = np.ones(int(nobs)) * (</span><span class="s3">1. </span><span class="s1">/ nobs)</span>
        <span class="s1">x0 = np.zeros(est_vect.shape[</span><span class="s3">1</span><span class="s1">]).reshape(-</span><span class="s3">1</span><span class="s2">, </span><span class="s3">1</span><span class="s1">)</span>
        <span class="s2">try</span><span class="s1">:</span>
            <span class="s1">eta_star = self._modif_newton(x0</span><span class="s2">, </span><span class="s1">est_vect</span><span class="s2">, </span><span class="s1">wts)</span>
            <span class="s1">denom = </span><span class="s3">1. </span><span class="s1">+ np.dot(eta_star</span><span class="s2">, </span><span class="s1">est_vect.T)</span>
            <span class="s1">self.new_weights = </span><span class="s3">1. </span><span class="s1">/ nobs * </span><span class="s3">1. </span><span class="s1">/ denom</span>
            <span class="s4"># the following commented out code is to verify weights</span>
            <span class="s4"># see open issue #1845</span>
            <span class="s4">#self.new_weights /= self.new_weights.sum()</span>
            <span class="s4">#if not np.allclose(self.new_weights.sum(), 1., rtol=0, atol=1e-10):</span>
            <span class="s4">#    raise RuntimeError('weights do not sum to 1')</span>
            <span class="s1">llr = np.sum(np.log(nobs * self.new_weights))</span>
            <span class="s2">return </span><span class="s1">-</span><span class="s3">2 </span><span class="s1">* llr</span>
        <span class="s2">except </span><span class="s1">np.linalg.linalg.LinAlgError:</span>
            <span class="s2">return </span><span class="s1">np.inf</span>
</pre>
</body>
</html>