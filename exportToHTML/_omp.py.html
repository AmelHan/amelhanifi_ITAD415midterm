<html>
<head>
<title>_omp.py</title>
<meta http-equiv="Content-Type" content="text/html; charset=utf-8">
<style type="text/css">
.s0 { color: #629755; font-style: italic;}
.s1 { color: #a9b7c6;}
.s2 { color: #808080;}
.s3 { color: #cc7832;}
.s4 { color: #6a8759;}
.s5 { color: #6897bb;}
</style>
</head>
<body bgcolor="#2b2b2b">
<table CELLSPACING=0 CELLPADDING=5 COLS=1 WIDTH="100%" BGCOLOR="#606060" >
<tr><td><center>
<font face="Arial, Helvetica" color="#000000">
_omp.py</font>
</center></td></tr></table>
<pre><span class="s0">&quot;&quot;&quot;Orthogonal matching pursuit algorithms 
&quot;&quot;&quot;</span>

<span class="s2"># Author: Vlad Niculae</span>
<span class="s2">#</span>
<span class="s2"># License: BSD 3 clause</span>

<span class="s3">import </span><span class="s1">warnings</span>
<span class="s3">from </span><span class="s1">math </span><span class="s3">import </span><span class="s1">sqrt</span>
<span class="s3">from </span><span class="s1">numbers </span><span class="s3">import </span><span class="s1">Integral</span><span class="s3">, </span><span class="s1">Real</span>

<span class="s3">import </span><span class="s1">numpy </span><span class="s3">as </span><span class="s1">np</span>
<span class="s3">from </span><span class="s1">scipy </span><span class="s3">import </span><span class="s1">linalg</span>
<span class="s3">from </span><span class="s1">scipy.linalg.lapack </span><span class="s3">import </span><span class="s1">get_lapack_funcs</span>

<span class="s3">from </span><span class="s1">..base </span><span class="s3">import </span><span class="s1">MultiOutputMixin</span><span class="s3">, </span><span class="s1">RegressorMixin</span><span class="s3">, </span><span class="s1">_fit_context</span>
<span class="s3">from </span><span class="s1">..model_selection </span><span class="s3">import </span><span class="s1">check_cv</span>
<span class="s3">from </span><span class="s1">..utils </span><span class="s3">import </span><span class="s1">as_float_array</span><span class="s3">, </span><span class="s1">check_array</span>
<span class="s3">from </span><span class="s1">..utils._param_validation </span><span class="s3">import </span><span class="s1">Hidden</span><span class="s3">, </span><span class="s1">Interval</span><span class="s3">, </span><span class="s1">StrOptions</span><span class="s3">, </span><span class="s1">validate_params</span>
<span class="s3">from </span><span class="s1">..utils.parallel </span><span class="s3">import </span><span class="s1">Parallel</span><span class="s3">, </span><span class="s1">delayed</span>
<span class="s3">from </span><span class="s1">._base </span><span class="s3">import </span><span class="s1">LinearModel</span><span class="s3">, </span><span class="s1">_deprecate_normalize</span><span class="s3">, </span><span class="s1">_pre_fit</span>

<span class="s1">premature = (</span>
    <span class="s4">&quot;Orthogonal matching pursuit ended prematurely due to linear&quot;</span>
    <span class="s4">&quot; dependence in the dictionary. The requested precision might&quot;</span>
    <span class="s4">&quot; not have been met.&quot;</span>
<span class="s1">)</span>


<span class="s3">def </span><span class="s1">_cholesky_omp(X</span><span class="s3">, </span><span class="s1">y</span><span class="s3">, </span><span class="s1">n_nonzero_coefs</span><span class="s3">, </span><span class="s1">tol=</span><span class="s3">None, </span><span class="s1">copy_X=</span><span class="s3">True, </span><span class="s1">return_path=</span><span class="s3">False</span><span class="s1">):</span>
    <span class="s0">&quot;&quot;&quot;Orthogonal Matching Pursuit step using the Cholesky decomposition. 
 
    Parameters 
    ---------- 
    X : ndarray of shape (n_samples, n_features) 
        Input dictionary. Columns are assumed to have unit norm. 
 
    y : ndarray of shape (n_samples,) 
        Input targets. 
 
    n_nonzero_coefs : int 
        Targeted number of non-zero elements. 
 
    tol : float, default=None 
        Targeted squared error, if not None overrides n_nonzero_coefs. 
 
    copy_X : bool, default=True 
        Whether the design matrix X must be copied by the algorithm. A false 
        value is only helpful if X is already Fortran-ordered, otherwise a 
        copy is made anyway. 
 
    return_path : bool, default=False 
        Whether to return every value of the nonzero coefficients along the 
        forward path. Useful for cross-validation. 
 
    Returns 
    ------- 
    gamma : ndarray of shape (n_nonzero_coefs,) 
        Non-zero elements of the solution. 
 
    idx : ndarray of shape (n_nonzero_coefs,) 
        Indices of the positions of the elements in gamma within the solution 
        vector. 
 
    coef : ndarray of shape (n_features, n_nonzero_coefs) 
        The first k values of column k correspond to the coefficient value 
        for the active features at that step. The lower left triangle contains 
        garbage. Only returned if ``return_path=True``. 
 
    n_active : int 
        Number of active features at convergence. 
    &quot;&quot;&quot;</span>
    <span class="s3">if </span><span class="s1">copy_X:</span>
        <span class="s1">X = X.copy(</span><span class="s4">&quot;F&quot;</span><span class="s1">)</span>
    <span class="s3">else</span><span class="s1">:  </span><span class="s2"># even if we are allowed to overwrite, still copy it if bad order</span>
        <span class="s1">X = np.asfortranarray(X)</span>

    <span class="s1">min_float = np.finfo(X.dtype).eps</span>
    <span class="s1">nrm2</span><span class="s3">, </span><span class="s1">swap = linalg.get_blas_funcs((</span><span class="s4">&quot;nrm2&quot;</span><span class="s3">, </span><span class="s4">&quot;swap&quot;</span><span class="s1">)</span><span class="s3">, </span><span class="s1">(X</span><span class="s3">,</span><span class="s1">))</span>
    <span class="s1">(potrs</span><span class="s3">,</span><span class="s1">) = get_lapack_funcs((</span><span class="s4">&quot;potrs&quot;</span><span class="s3">,</span><span class="s1">)</span><span class="s3">, </span><span class="s1">(X</span><span class="s3">,</span><span class="s1">))</span>

    <span class="s1">alpha = np.dot(X.T</span><span class="s3">, </span><span class="s1">y)</span>
    <span class="s1">residual = y</span>
    <span class="s1">gamma = np.empty(</span><span class="s5">0</span><span class="s1">)</span>
    <span class="s1">n_active = </span><span class="s5">0</span>
    <span class="s1">indices = np.arange(X.shape[</span><span class="s5">1</span><span class="s1">])  </span><span class="s2"># keeping track of swapping</span>

    <span class="s1">max_features = X.shape[</span><span class="s5">1</span><span class="s1">] </span><span class="s3">if </span><span class="s1">tol </span><span class="s3">is not None else </span><span class="s1">n_nonzero_coefs</span>

    <span class="s1">L = np.empty((max_features</span><span class="s3">, </span><span class="s1">max_features)</span><span class="s3">, </span><span class="s1">dtype=X.dtype)</span>

    <span class="s3">if </span><span class="s1">return_path:</span>
        <span class="s1">coefs = np.empty_like(L)</span>

    <span class="s3">while True</span><span class="s1">:</span>
        <span class="s1">lam = np.argmax(np.abs(np.dot(X.T</span><span class="s3">, </span><span class="s1">residual)))</span>
        <span class="s3">if </span><span class="s1">lam &lt; n_active </span><span class="s3">or </span><span class="s1">alpha[lam] ** </span><span class="s5">2 </span><span class="s1">&lt; min_float:</span>
            <span class="s2"># atom already selected or inner product too small</span>
            <span class="s1">warnings.warn(premature</span><span class="s3">, </span><span class="s1">RuntimeWarning</span><span class="s3">, </span><span class="s1">stacklevel=</span><span class="s5">2</span><span class="s1">)</span>
            <span class="s3">break</span>

        <span class="s3">if </span><span class="s1">n_active &gt; </span><span class="s5">0</span><span class="s1">:</span>
            <span class="s2"># Updates the Cholesky decomposition of X' X</span>
            <span class="s1">L[n_active</span><span class="s3">, </span><span class="s1">:n_active] = np.dot(X[:</span><span class="s3">, </span><span class="s1">:n_active].T</span><span class="s3">, </span><span class="s1">X[:</span><span class="s3">, </span><span class="s1">lam])</span>
            <span class="s1">linalg.solve_triangular(</span>
                <span class="s1">L[:n_active</span><span class="s3">, </span><span class="s1">:n_active]</span><span class="s3">,</span>
                <span class="s1">L[n_active</span><span class="s3">, </span><span class="s1">:n_active]</span><span class="s3">,</span>
                <span class="s1">trans=</span><span class="s5">0</span><span class="s3">,</span>
                <span class="s1">lower=</span><span class="s5">1</span><span class="s3">,</span>
                <span class="s1">overwrite_b=</span><span class="s3">True,</span>
                <span class="s1">check_finite=</span><span class="s3">False,</span>
            <span class="s1">)</span>
            <span class="s1">v = nrm2(L[n_active</span><span class="s3">, </span><span class="s1">:n_active]) ** </span><span class="s5">2</span>
            <span class="s1">Lkk = linalg.norm(X[:</span><span class="s3">, </span><span class="s1">lam]) ** </span><span class="s5">2 </span><span class="s1">- v</span>
            <span class="s3">if </span><span class="s1">Lkk &lt;= min_float:  </span><span class="s2"># selected atoms are dependent</span>
                <span class="s1">warnings.warn(premature</span><span class="s3">, </span><span class="s1">RuntimeWarning</span><span class="s3">, </span><span class="s1">stacklevel=</span><span class="s5">2</span><span class="s1">)</span>
                <span class="s3">break</span>
            <span class="s1">L[n_active</span><span class="s3">, </span><span class="s1">n_active] = sqrt(Lkk)</span>
        <span class="s3">else</span><span class="s1">:</span>
            <span class="s1">L[</span><span class="s5">0</span><span class="s3">, </span><span class="s5">0</span><span class="s1">] = linalg.norm(X[:</span><span class="s3">, </span><span class="s1">lam])</span>

        <span class="s1">X.T[n_active]</span><span class="s3">, </span><span class="s1">X.T[lam] = swap(X.T[n_active]</span><span class="s3">, </span><span class="s1">X.T[lam])</span>
        <span class="s1">alpha[n_active]</span><span class="s3">, </span><span class="s1">alpha[lam] = alpha[lam]</span><span class="s3">, </span><span class="s1">alpha[n_active]</span>
        <span class="s1">indices[n_active]</span><span class="s3">, </span><span class="s1">indices[lam] = indices[lam]</span><span class="s3">, </span><span class="s1">indices[n_active]</span>
        <span class="s1">n_active += </span><span class="s5">1</span>

        <span class="s2"># solves LL'x = X'y as a composition of two triangular systems</span>
        <span class="s1">gamma</span><span class="s3">, </span><span class="s1">_ = potrs(</span>
            <span class="s1">L[:n_active</span><span class="s3">, </span><span class="s1">:n_active]</span><span class="s3">, </span><span class="s1">alpha[:n_active]</span><span class="s3">, </span><span class="s1">lower=</span><span class="s3">True, </span><span class="s1">overwrite_b=</span><span class="s3">False</span>
        <span class="s1">)</span>

        <span class="s3">if </span><span class="s1">return_path:</span>
            <span class="s1">coefs[:n_active</span><span class="s3">, </span><span class="s1">n_active - </span><span class="s5">1</span><span class="s1">] = gamma</span>
        <span class="s1">residual = y - np.dot(X[:</span><span class="s3">, </span><span class="s1">:n_active]</span><span class="s3">, </span><span class="s1">gamma)</span>
        <span class="s3">if </span><span class="s1">tol </span><span class="s3">is not None and </span><span class="s1">nrm2(residual) ** </span><span class="s5">2 </span><span class="s1">&lt;= tol:</span>
            <span class="s3">break</span>
        <span class="s3">elif </span><span class="s1">n_active == max_features:</span>
            <span class="s3">break</span>

    <span class="s3">if </span><span class="s1">return_path:</span>
        <span class="s3">return </span><span class="s1">gamma</span><span class="s3">, </span><span class="s1">indices[:n_active]</span><span class="s3">, </span><span class="s1">coefs[:</span><span class="s3">, </span><span class="s1">:n_active]</span><span class="s3">, </span><span class="s1">n_active</span>
    <span class="s3">else</span><span class="s1">:</span>
        <span class="s3">return </span><span class="s1">gamma</span><span class="s3">, </span><span class="s1">indices[:n_active]</span><span class="s3">, </span><span class="s1">n_active</span>


<span class="s3">def </span><span class="s1">_gram_omp(</span>
    <span class="s1">Gram</span><span class="s3">,</span>
    <span class="s1">Xy</span><span class="s3">,</span>
    <span class="s1">n_nonzero_coefs</span><span class="s3">,</span>
    <span class="s1">tol_0=</span><span class="s3">None,</span>
    <span class="s1">tol=</span><span class="s3">None,</span>
    <span class="s1">copy_Gram=</span><span class="s3">True,</span>
    <span class="s1">copy_Xy=</span><span class="s3">True,</span>
    <span class="s1">return_path=</span><span class="s3">False,</span>
<span class="s1">):</span>
    <span class="s0">&quot;&quot;&quot;Orthogonal Matching Pursuit step on a precomputed Gram matrix. 
 
    This function uses the Cholesky decomposition method. 
 
    Parameters 
    ---------- 
    Gram : ndarray of shape (n_features, n_features) 
        Gram matrix of the input data matrix. 
 
    Xy : ndarray of shape (n_features,) 
        Input targets. 
 
    n_nonzero_coefs : int 
        Targeted number of non-zero elements. 
 
    tol_0 : float, default=None 
        Squared norm of y, required if tol is not None. 
 
    tol : float, default=None 
        Targeted squared error, if not None overrides n_nonzero_coefs. 
 
    copy_Gram : bool, default=True 
        Whether the gram matrix must be copied by the algorithm. A false 
        value is only helpful if it is already Fortran-ordered, otherwise a 
        copy is made anyway. 
 
    copy_Xy : bool, default=True 
        Whether the covariance vector Xy must be copied by the algorithm. 
        If False, it may be overwritten. 
 
    return_path : bool, default=False 
        Whether to return every value of the nonzero coefficients along the 
        forward path. Useful for cross-validation. 
 
    Returns 
    ------- 
    gamma : ndarray of shape (n_nonzero_coefs,) 
        Non-zero elements of the solution. 
 
    idx : ndarray of shape (n_nonzero_coefs,) 
        Indices of the positions of the elements in gamma within the solution 
        vector. 
 
    coefs : ndarray of shape (n_features, n_nonzero_coefs) 
        The first k values of column k correspond to the coefficient value 
        for the active features at that step. The lower left triangle contains 
        garbage. Only returned if ``return_path=True``. 
 
    n_active : int 
        Number of active features at convergence. 
    &quot;&quot;&quot;</span>
    <span class="s1">Gram = Gram.copy(</span><span class="s4">&quot;F&quot;</span><span class="s1">) </span><span class="s3">if </span><span class="s1">copy_Gram </span><span class="s3">else </span><span class="s1">np.asfortranarray(Gram)</span>

    <span class="s3">if </span><span class="s1">copy_Xy </span><span class="s3">or not </span><span class="s1">Xy.flags.writeable:</span>
        <span class="s1">Xy = Xy.copy()</span>

    <span class="s1">min_float = np.finfo(Gram.dtype).eps</span>
    <span class="s1">nrm2</span><span class="s3">, </span><span class="s1">swap = linalg.get_blas_funcs((</span><span class="s4">&quot;nrm2&quot;</span><span class="s3">, </span><span class="s4">&quot;swap&quot;</span><span class="s1">)</span><span class="s3">, </span><span class="s1">(Gram</span><span class="s3">,</span><span class="s1">))</span>
    <span class="s1">(potrs</span><span class="s3">,</span><span class="s1">) = get_lapack_funcs((</span><span class="s4">&quot;potrs&quot;</span><span class="s3">,</span><span class="s1">)</span><span class="s3">, </span><span class="s1">(Gram</span><span class="s3">,</span><span class="s1">))</span>

    <span class="s1">indices = np.arange(len(Gram))  </span><span class="s2"># keeping track of swapping</span>
    <span class="s1">alpha = Xy</span>
    <span class="s1">tol_curr = tol_0</span>
    <span class="s1">delta = </span><span class="s5">0</span>
    <span class="s1">gamma = np.empty(</span><span class="s5">0</span><span class="s1">)</span>
    <span class="s1">n_active = </span><span class="s5">0</span>

    <span class="s1">max_features = len(Gram) </span><span class="s3">if </span><span class="s1">tol </span><span class="s3">is not None else </span><span class="s1">n_nonzero_coefs</span>

    <span class="s1">L = np.empty((max_features</span><span class="s3">, </span><span class="s1">max_features)</span><span class="s3">, </span><span class="s1">dtype=Gram.dtype)</span>

    <span class="s1">L[</span><span class="s5">0</span><span class="s3">, </span><span class="s5">0</span><span class="s1">] = </span><span class="s5">1.0</span>
    <span class="s3">if </span><span class="s1">return_path:</span>
        <span class="s1">coefs = np.empty_like(L)</span>

    <span class="s3">while True</span><span class="s1">:</span>
        <span class="s1">lam = np.argmax(np.abs(alpha))</span>
        <span class="s3">if </span><span class="s1">lam &lt; n_active </span><span class="s3">or </span><span class="s1">alpha[lam] ** </span><span class="s5">2 </span><span class="s1">&lt; min_float:</span>
            <span class="s2"># selected same atom twice, or inner product too small</span>
            <span class="s1">warnings.warn(premature</span><span class="s3">, </span><span class="s1">RuntimeWarning</span><span class="s3">, </span><span class="s1">stacklevel=</span><span class="s5">3</span><span class="s1">)</span>
            <span class="s3">break</span>
        <span class="s3">if </span><span class="s1">n_active &gt; </span><span class="s5">0</span><span class="s1">:</span>
            <span class="s1">L[n_active</span><span class="s3">, </span><span class="s1">:n_active] = Gram[lam</span><span class="s3">, </span><span class="s1">:n_active]</span>
            <span class="s1">linalg.solve_triangular(</span>
                <span class="s1">L[:n_active</span><span class="s3">, </span><span class="s1">:n_active]</span><span class="s3">,</span>
                <span class="s1">L[n_active</span><span class="s3">, </span><span class="s1">:n_active]</span><span class="s3">,</span>
                <span class="s1">trans=</span><span class="s5">0</span><span class="s3">,</span>
                <span class="s1">lower=</span><span class="s5">1</span><span class="s3">,</span>
                <span class="s1">overwrite_b=</span><span class="s3">True,</span>
                <span class="s1">check_finite=</span><span class="s3">False,</span>
            <span class="s1">)</span>
            <span class="s1">v = nrm2(L[n_active</span><span class="s3">, </span><span class="s1">:n_active]) ** </span><span class="s5">2</span>
            <span class="s1">Lkk = Gram[lam</span><span class="s3">, </span><span class="s1">lam] - v</span>
            <span class="s3">if </span><span class="s1">Lkk &lt;= min_float:  </span><span class="s2"># selected atoms are dependent</span>
                <span class="s1">warnings.warn(premature</span><span class="s3">, </span><span class="s1">RuntimeWarning</span><span class="s3">, </span><span class="s1">stacklevel=</span><span class="s5">3</span><span class="s1">)</span>
                <span class="s3">break</span>
            <span class="s1">L[n_active</span><span class="s3">, </span><span class="s1">n_active] = sqrt(Lkk)</span>
        <span class="s3">else</span><span class="s1">:</span>
            <span class="s1">L[</span><span class="s5">0</span><span class="s3">, </span><span class="s5">0</span><span class="s1">] = sqrt(Gram[lam</span><span class="s3">, </span><span class="s1">lam])</span>

        <span class="s1">Gram[n_active]</span><span class="s3">, </span><span class="s1">Gram[lam] = swap(Gram[n_active]</span><span class="s3">, </span><span class="s1">Gram[lam])</span>
        <span class="s1">Gram.T[n_active]</span><span class="s3">, </span><span class="s1">Gram.T[lam] = swap(Gram.T[n_active]</span><span class="s3">, </span><span class="s1">Gram.T[lam])</span>
        <span class="s1">indices[n_active]</span><span class="s3">, </span><span class="s1">indices[lam] = indices[lam]</span><span class="s3">, </span><span class="s1">indices[n_active]</span>
        <span class="s1">Xy[n_active]</span><span class="s3">, </span><span class="s1">Xy[lam] = Xy[lam]</span><span class="s3">, </span><span class="s1">Xy[n_active]</span>
        <span class="s1">n_active += </span><span class="s5">1</span>
        <span class="s2"># solves LL'x = X'y as a composition of two triangular systems</span>
        <span class="s1">gamma</span><span class="s3">, </span><span class="s1">_ = potrs(</span>
            <span class="s1">L[:n_active</span><span class="s3">, </span><span class="s1">:n_active]</span><span class="s3">, </span><span class="s1">Xy[:n_active]</span><span class="s3">, </span><span class="s1">lower=</span><span class="s3">True, </span><span class="s1">overwrite_b=</span><span class="s3">False</span>
        <span class="s1">)</span>
        <span class="s3">if </span><span class="s1">return_path:</span>
            <span class="s1">coefs[:n_active</span><span class="s3">, </span><span class="s1">n_active - </span><span class="s5">1</span><span class="s1">] = gamma</span>
        <span class="s1">beta = np.dot(Gram[:</span><span class="s3">, </span><span class="s1">:n_active]</span><span class="s3">, </span><span class="s1">gamma)</span>
        <span class="s1">alpha = Xy - beta</span>
        <span class="s3">if </span><span class="s1">tol </span><span class="s3">is not None</span><span class="s1">:</span>
            <span class="s1">tol_curr += delta</span>
            <span class="s1">delta = np.inner(gamma</span><span class="s3">, </span><span class="s1">beta[:n_active])</span>
            <span class="s1">tol_curr -= delta</span>
            <span class="s3">if </span><span class="s1">abs(tol_curr) &lt;= tol:</span>
                <span class="s3">break</span>
        <span class="s3">elif </span><span class="s1">n_active == max_features:</span>
            <span class="s3">break</span>

    <span class="s3">if </span><span class="s1">return_path:</span>
        <span class="s3">return </span><span class="s1">gamma</span><span class="s3">, </span><span class="s1">indices[:n_active]</span><span class="s3">, </span><span class="s1">coefs[:</span><span class="s3">, </span><span class="s1">:n_active]</span><span class="s3">, </span><span class="s1">n_active</span>
    <span class="s3">else</span><span class="s1">:</span>
        <span class="s3">return </span><span class="s1">gamma</span><span class="s3">, </span><span class="s1">indices[:n_active]</span><span class="s3">, </span><span class="s1">n_active</span>


<span class="s1">@validate_params(</span>
    <span class="s1">{</span>
        <span class="s4">&quot;X&quot;</span><span class="s1">: [</span><span class="s4">&quot;array-like&quot;</span><span class="s1">]</span><span class="s3">,</span>
        <span class="s4">&quot;y&quot;</span><span class="s1">: [np.ndarray]</span><span class="s3">,</span>
        <span class="s4">&quot;n_nonzero_coefs&quot;</span><span class="s1">: [Interval(Integral</span><span class="s3">, </span><span class="s5">1</span><span class="s3">, None, </span><span class="s1">closed=</span><span class="s4">&quot;left&quot;</span><span class="s1">)</span><span class="s3">, None</span><span class="s1">]</span><span class="s3">,</span>
        <span class="s4">&quot;tol&quot;</span><span class="s1">: [Interval(Real</span><span class="s3">, </span><span class="s5">0</span><span class="s3">, None, </span><span class="s1">closed=</span><span class="s4">&quot;left&quot;</span><span class="s1">)</span><span class="s3">, None</span><span class="s1">]</span><span class="s3">,</span>
        <span class="s4">&quot;precompute&quot;</span><span class="s1">: [</span><span class="s4">&quot;boolean&quot;</span><span class="s3">, </span><span class="s1">StrOptions({</span><span class="s4">&quot;auto&quot;</span><span class="s1">})]</span><span class="s3">,</span>
        <span class="s4">&quot;copy_X&quot;</span><span class="s1">: [</span><span class="s4">&quot;boolean&quot;</span><span class="s1">]</span><span class="s3">,</span>
        <span class="s4">&quot;return_path&quot;</span><span class="s1">: [</span><span class="s4">&quot;boolean&quot;</span><span class="s1">]</span><span class="s3">,</span>
        <span class="s4">&quot;return_n_iter&quot;</span><span class="s1">: [</span><span class="s4">&quot;boolean&quot;</span><span class="s1">]</span><span class="s3">,</span>
    <span class="s1">}</span><span class="s3">,</span>
    <span class="s1">prefer_skip_nested_validation=</span><span class="s3">True,</span>
<span class="s1">)</span>
<span class="s3">def </span><span class="s1">orthogonal_mp(</span>
    <span class="s1">X</span><span class="s3">,</span>
    <span class="s1">y</span><span class="s3">,</span>
    <span class="s1">*</span><span class="s3">,</span>
    <span class="s1">n_nonzero_coefs=</span><span class="s3">None,</span>
    <span class="s1">tol=</span><span class="s3">None,</span>
    <span class="s1">precompute=</span><span class="s3">False,</span>
    <span class="s1">copy_X=</span><span class="s3">True,</span>
    <span class="s1">return_path=</span><span class="s3">False,</span>
    <span class="s1">return_n_iter=</span><span class="s3">False,</span>
<span class="s1">):</span>
    <span class="s0">r&quot;&quot;&quot;Orthogonal Matching Pursuit (OMP). 
 
    Solves n_targets Orthogonal Matching Pursuit problems. 
    An instance of the problem has the form: 
 
    When parametrized by the number of non-zero coefficients using 
    `n_nonzero_coefs`: 
    argmin ||y - X\gamma||^2 subject to ||\gamma||_0 &lt;= n_{nonzero coefs} 
 
    When parametrized by error using the parameter `tol`: 
    argmin ||\gamma||_0 subject to ||y - X\gamma||^2 &lt;= tol 
 
    Read more in the :ref:`User Guide &lt;omp&gt;`. 
 
    Parameters 
    ---------- 
    X : array-like of shape (n_samples, n_features) 
        Input data. Columns are assumed to have unit norm. 
 
    y : ndarray of shape (n_samples,) or (n_samples, n_targets) 
        Input targets. 
 
    n_nonzero_coefs : int, default=None 
        Desired number of non-zero entries in the solution. If None (by 
        default) this value is set to 10% of n_features. 
 
    tol : float, default=None 
        Maximum squared norm of the residual. If not None, overrides n_nonzero_coefs. 
 
    precompute : 'auto' or bool, default=False 
        Whether to perform precomputations. Improves performance when n_targets 
        or n_samples is very large. 
 
    copy_X : bool, default=True 
        Whether the design matrix X must be copied by the algorithm. A false 
        value is only helpful if X is already Fortran-ordered, otherwise a 
        copy is made anyway. 
 
    return_path : bool, default=False 
        Whether to return every value of the nonzero coefficients along the 
        forward path. Useful for cross-validation. 
 
    return_n_iter : bool, default=False 
        Whether or not to return the number of iterations. 
 
    Returns 
    ------- 
    coef : ndarray of shape (n_features,) or (n_features, n_targets) 
        Coefficients of the OMP solution. If `return_path=True`, this contains 
        the whole coefficient path. In this case its shape is 
        (n_features, n_features) or (n_features, n_targets, n_features) and 
        iterating over the last axis generates coefficients in increasing order 
        of active features. 
 
    n_iters : array-like or int 
        Number of active features across every target. Returned only if 
        `return_n_iter` is set to True. 
 
    See Also 
    -------- 
    OrthogonalMatchingPursuit : Orthogonal Matching Pursuit model. 
    orthogonal_mp_gram : Solve OMP problems using Gram matrix and the product X.T * y. 
    lars_path : Compute Least Angle Regression or Lasso path using LARS algorithm. 
    sklearn.decomposition.sparse_encode : Sparse coding. 
 
    Notes 
    ----- 
    Orthogonal matching pursuit was introduced in S. Mallat, Z. Zhang, 
    Matching pursuits with time-frequency dictionaries, IEEE Transactions on 
    Signal Processing, Vol. 41, No. 12. (December 1993), pp. 3397-3415. 
    (https://www.di.ens.fr/~mallat/papiers/MallatPursuit93.pdf) 
 
    This implementation is based on Rubinstein, R., Zibulevsky, M. and Elad, 
    M., Efficient Implementation of the K-SVD Algorithm using Batch Orthogonal 
    Matching Pursuit Technical Report - CS Technion, April 2008. 
    https://www.cs.technion.ac.il/~ronrubin/Publications/KSVD-OMP-v2.pdf 
    &quot;&quot;&quot;</span>
    <span class="s1">X = check_array(X</span><span class="s3">, </span><span class="s1">order=</span><span class="s4">&quot;F&quot;</span><span class="s3">, </span><span class="s1">copy=copy_X)</span>
    <span class="s1">copy_X = </span><span class="s3">False</span>
    <span class="s3">if </span><span class="s1">y.ndim == </span><span class="s5">1</span><span class="s1">:</span>
        <span class="s1">y = y.reshape(-</span><span class="s5">1</span><span class="s3">, </span><span class="s5">1</span><span class="s1">)</span>
    <span class="s1">y = check_array(y)</span>
    <span class="s3">if </span><span class="s1">y.shape[</span><span class="s5">1</span><span class="s1">] &gt; </span><span class="s5">1</span><span class="s1">:  </span><span class="s2"># subsequent targets will be affected</span>
        <span class="s1">copy_X = </span><span class="s3">True</span>
    <span class="s3">if </span><span class="s1">n_nonzero_coefs </span><span class="s3">is None and </span><span class="s1">tol </span><span class="s3">is None</span><span class="s1">:</span>
        <span class="s2"># default for n_nonzero_coefs is 0.1 * n_features</span>
        <span class="s2"># but at least one.</span>
        <span class="s1">n_nonzero_coefs = max(int(</span><span class="s5">0.1 </span><span class="s1">* X.shape[</span><span class="s5">1</span><span class="s1">])</span><span class="s3">, </span><span class="s5">1</span><span class="s1">)</span>
    <span class="s3">if </span><span class="s1">tol </span><span class="s3">is None and </span><span class="s1">n_nonzero_coefs &gt; X.shape[</span><span class="s5">1</span><span class="s1">]:</span>
        <span class="s3">raise </span><span class="s1">ValueError(</span>
            <span class="s4">&quot;The number of atoms cannot be more than the number of features&quot;</span>
        <span class="s1">)</span>
    <span class="s3">if </span><span class="s1">precompute == </span><span class="s4">&quot;auto&quot;</span><span class="s1">:</span>
        <span class="s1">precompute = X.shape[</span><span class="s5">0</span><span class="s1">] &gt; X.shape[</span><span class="s5">1</span><span class="s1">]</span>
    <span class="s3">if </span><span class="s1">precompute:</span>
        <span class="s1">G = np.dot(X.T</span><span class="s3">, </span><span class="s1">X)</span>
        <span class="s1">G = np.asfortranarray(G)</span>
        <span class="s1">Xy = np.dot(X.T</span><span class="s3">, </span><span class="s1">y)</span>
        <span class="s3">if </span><span class="s1">tol </span><span class="s3">is not None</span><span class="s1">:</span>
            <span class="s1">norms_squared = np.sum((y**</span><span class="s5">2</span><span class="s1">)</span><span class="s3">, </span><span class="s1">axis=</span><span class="s5">0</span><span class="s1">)</span>
        <span class="s3">else</span><span class="s1">:</span>
            <span class="s1">norms_squared = </span><span class="s3">None</span>
        <span class="s3">return </span><span class="s1">orthogonal_mp_gram(</span>
            <span class="s1">G</span><span class="s3">,</span>
            <span class="s1">Xy</span><span class="s3">,</span>
            <span class="s1">n_nonzero_coefs=n_nonzero_coefs</span><span class="s3">,</span>
            <span class="s1">tol=tol</span><span class="s3">,</span>
            <span class="s1">norms_squared=norms_squared</span><span class="s3">,</span>
            <span class="s1">copy_Gram=copy_X</span><span class="s3">,</span>
            <span class="s1">copy_Xy=</span><span class="s3">False,</span>
            <span class="s1">return_path=return_path</span><span class="s3">,</span>
        <span class="s1">)</span>

    <span class="s3">if </span><span class="s1">return_path:</span>
        <span class="s1">coef = np.zeros((X.shape[</span><span class="s5">1</span><span class="s1">]</span><span class="s3">, </span><span class="s1">y.shape[</span><span class="s5">1</span><span class="s1">]</span><span class="s3">, </span><span class="s1">X.shape[</span><span class="s5">1</span><span class="s1">]))</span>
    <span class="s3">else</span><span class="s1">:</span>
        <span class="s1">coef = np.zeros((X.shape[</span><span class="s5">1</span><span class="s1">]</span><span class="s3">, </span><span class="s1">y.shape[</span><span class="s5">1</span><span class="s1">]))</span>
    <span class="s1">n_iters = []</span>

    <span class="s3">for </span><span class="s1">k </span><span class="s3">in </span><span class="s1">range(y.shape[</span><span class="s5">1</span><span class="s1">]):</span>
        <span class="s1">out = _cholesky_omp(</span>
            <span class="s1">X</span><span class="s3">, </span><span class="s1">y[:</span><span class="s3">, </span><span class="s1">k]</span><span class="s3">, </span><span class="s1">n_nonzero_coefs</span><span class="s3">, </span><span class="s1">tol</span><span class="s3">, </span><span class="s1">copy_X=copy_X</span><span class="s3">, </span><span class="s1">return_path=return_path</span>
        <span class="s1">)</span>
        <span class="s3">if </span><span class="s1">return_path:</span>
            <span class="s1">_</span><span class="s3">, </span><span class="s1">idx</span><span class="s3">, </span><span class="s1">coefs</span><span class="s3">, </span><span class="s1">n_iter = out</span>
            <span class="s1">coef = coef[:</span><span class="s3">, </span><span class="s1">:</span><span class="s3">, </span><span class="s1">: len(idx)]</span>
            <span class="s3">for </span><span class="s1">n_active</span><span class="s3">, </span><span class="s1">x </span><span class="s3">in </span><span class="s1">enumerate(coefs.T):</span>
                <span class="s1">coef[idx[: n_active + </span><span class="s5">1</span><span class="s1">]</span><span class="s3">, </span><span class="s1">k</span><span class="s3">, </span><span class="s1">n_active] = x[: n_active + </span><span class="s5">1</span><span class="s1">]</span>
        <span class="s3">else</span><span class="s1">:</span>
            <span class="s1">x</span><span class="s3">, </span><span class="s1">idx</span><span class="s3">, </span><span class="s1">n_iter = out</span>
            <span class="s1">coef[idx</span><span class="s3">, </span><span class="s1">k] = x</span>
        <span class="s1">n_iters.append(n_iter)</span>

    <span class="s3">if </span><span class="s1">y.shape[</span><span class="s5">1</span><span class="s1">] == </span><span class="s5">1</span><span class="s1">:</span>
        <span class="s1">n_iters = n_iters[</span><span class="s5">0</span><span class="s1">]</span>

    <span class="s3">if </span><span class="s1">return_n_iter:</span>
        <span class="s3">return </span><span class="s1">np.squeeze(coef)</span><span class="s3">, </span><span class="s1">n_iters</span>
    <span class="s3">else</span><span class="s1">:</span>
        <span class="s3">return </span><span class="s1">np.squeeze(coef)</span>


<span class="s3">def </span><span class="s1">orthogonal_mp_gram(</span>
    <span class="s1">Gram</span><span class="s3">,</span>
    <span class="s1">Xy</span><span class="s3">,</span>
    <span class="s1">*</span><span class="s3">,</span>
    <span class="s1">n_nonzero_coefs=</span><span class="s3">None,</span>
    <span class="s1">tol=</span><span class="s3">None,</span>
    <span class="s1">norms_squared=</span><span class="s3">None,</span>
    <span class="s1">copy_Gram=</span><span class="s3">True,</span>
    <span class="s1">copy_Xy=</span><span class="s3">True,</span>
    <span class="s1">return_path=</span><span class="s3">False,</span>
    <span class="s1">return_n_iter=</span><span class="s3">False,</span>
<span class="s1">):</span>
    <span class="s0">&quot;&quot;&quot;Gram Orthogonal Matching Pursuit (OMP). 
 
    Solves n_targets Orthogonal Matching Pursuit problems using only 
    the Gram matrix X.T * X and the product X.T * y. 
 
    Read more in the :ref:`User Guide &lt;omp&gt;`. 
 
    Parameters 
    ---------- 
    Gram : ndarray of shape (n_features, n_features) 
        Gram matrix of the input data: X.T * X. 
 
    Xy : ndarray of shape (n_features,) or (n_features, n_targets) 
        Input targets multiplied by X: X.T * y. 
 
    n_nonzero_coefs : int, default=None 
        Desired number of non-zero entries in the solution. If None (by 
        default) this value is set to 10% of n_features. 
 
    tol : float, default=None 
        Maximum squared norm of the residual. If not `None`, 
        overrides `n_nonzero_coefs`. 
 
    norms_squared : array-like of shape (n_targets,), default=None 
        Squared L2 norms of the lines of y. Required if tol is not None. 
 
    copy_Gram : bool, default=True 
        Whether the gram matrix must be copied by the algorithm. A false 
        value is only helpful if it is already Fortran-ordered, otherwise a 
        copy is made anyway. 
 
    copy_Xy : bool, default=True 
        Whether the covariance vector Xy must be copied by the algorithm. 
        If False, it may be overwritten. 
 
    return_path : bool, default=False 
        Whether to return every value of the nonzero coefficients along the 
        forward path. Useful for cross-validation. 
 
    return_n_iter : bool, default=False 
        Whether or not to return the number of iterations. 
 
    Returns 
    ------- 
    coef : ndarray of shape (n_features,) or (n_features, n_targets) 
        Coefficients of the OMP solution. If `return_path=True`, this contains 
        the whole coefficient path. In this case its shape is 
        (n_features, n_features) or (n_features, n_targets, n_features) and 
        iterating over the last axis yields coefficients in increasing order 
        of active features. 
 
    n_iters : array-like or int 
        Number of active features across every target. Returned only if 
        `return_n_iter` is set to True. 
 
    See Also 
    -------- 
    OrthogonalMatchingPursuit : Orthogonal Matching Pursuit model (OMP). 
    orthogonal_mp : Solves n_targets Orthogonal Matching Pursuit problems. 
    lars_path : Compute Least Angle Regression or Lasso path using 
        LARS algorithm. 
    sklearn.decomposition.sparse_encode : Generic sparse coding. 
        Each column of the result is the solution to a Lasso problem. 
 
    Notes 
    ----- 
    Orthogonal matching pursuit was introduced in G. Mallat, Z. Zhang, 
    Matching pursuits with time-frequency dictionaries, IEEE Transactions on 
    Signal Processing, Vol. 41, No. 12. (December 1993), pp. 3397-3415. 
    (https://www.di.ens.fr/~mallat/papiers/MallatPursuit93.pdf) 
 
    This implementation is based on Rubinstein, R., Zibulevsky, M. and Elad, 
    M., Efficient Implementation of the K-SVD Algorithm using Batch Orthogonal 
    Matching Pursuit Technical Report - CS Technion, April 2008. 
    https://www.cs.technion.ac.il/~ronrubin/Publications/KSVD-OMP-v2.pdf 
    &quot;&quot;&quot;</span>
    <span class="s1">Gram = check_array(Gram</span><span class="s3">, </span><span class="s1">order=</span><span class="s4">&quot;F&quot;</span><span class="s3">, </span><span class="s1">copy=copy_Gram)</span>
    <span class="s1">Xy = np.asarray(Xy)</span>
    <span class="s3">if </span><span class="s1">Xy.ndim &gt; </span><span class="s5">1 </span><span class="s3">and </span><span class="s1">Xy.shape[</span><span class="s5">1</span><span class="s1">] &gt; </span><span class="s5">1</span><span class="s1">:</span>
        <span class="s2"># or subsequent target will be affected</span>
        <span class="s1">copy_Gram = </span><span class="s3">True</span>
    <span class="s3">if </span><span class="s1">Xy.ndim == </span><span class="s5">1</span><span class="s1">:</span>
        <span class="s1">Xy = Xy[:</span><span class="s3">, </span><span class="s1">np.newaxis]</span>
        <span class="s3">if </span><span class="s1">tol </span><span class="s3">is not None</span><span class="s1">:</span>
            <span class="s1">norms_squared = [norms_squared]</span>
    <span class="s3">if </span><span class="s1">copy_Xy </span><span class="s3">or not </span><span class="s1">Xy.flags.writeable:</span>
        <span class="s2"># Make the copy once instead of many times in _gram_omp itself.</span>
        <span class="s1">Xy = Xy.copy()</span>

    <span class="s3">if </span><span class="s1">n_nonzero_coefs </span><span class="s3">is None and </span><span class="s1">tol </span><span class="s3">is None</span><span class="s1">:</span>
        <span class="s1">n_nonzero_coefs = int(</span><span class="s5">0.1 </span><span class="s1">* len(Gram))</span>
    <span class="s3">if </span><span class="s1">tol </span><span class="s3">is not None and </span><span class="s1">norms_squared </span><span class="s3">is None</span><span class="s1">:</span>
        <span class="s3">raise </span><span class="s1">ValueError(</span>
            <span class="s4">&quot;Gram OMP needs the precomputed norms in order &quot;</span>
            <span class="s4">&quot;to evaluate the error sum of squares.&quot;</span>
        <span class="s1">)</span>
    <span class="s3">if </span><span class="s1">tol </span><span class="s3">is not None and </span><span class="s1">tol &lt; </span><span class="s5">0</span><span class="s1">:</span>
        <span class="s3">raise </span><span class="s1">ValueError(</span><span class="s4">&quot;Epsilon cannot be negative&quot;</span><span class="s1">)</span>
    <span class="s3">if </span><span class="s1">tol </span><span class="s3">is None and </span><span class="s1">n_nonzero_coefs &lt;= </span><span class="s5">0</span><span class="s1">:</span>
        <span class="s3">raise </span><span class="s1">ValueError(</span><span class="s4">&quot;The number of atoms must be positive&quot;</span><span class="s1">)</span>
    <span class="s3">if </span><span class="s1">tol </span><span class="s3">is None and </span><span class="s1">n_nonzero_coefs &gt; len(Gram):</span>
        <span class="s3">raise </span><span class="s1">ValueError(</span>
            <span class="s4">&quot;The number of atoms cannot be more than the number of features&quot;</span>
        <span class="s1">)</span>

    <span class="s3">if </span><span class="s1">return_path:</span>
        <span class="s1">coef = np.zeros((len(Gram)</span><span class="s3">, </span><span class="s1">Xy.shape[</span><span class="s5">1</span><span class="s1">]</span><span class="s3">, </span><span class="s1">len(Gram))</span><span class="s3">, </span><span class="s1">dtype=Gram.dtype)</span>
    <span class="s3">else</span><span class="s1">:</span>
        <span class="s1">coef = np.zeros((len(Gram)</span><span class="s3">, </span><span class="s1">Xy.shape[</span><span class="s5">1</span><span class="s1">])</span><span class="s3">, </span><span class="s1">dtype=Gram.dtype)</span>

    <span class="s1">n_iters = []</span>
    <span class="s3">for </span><span class="s1">k </span><span class="s3">in </span><span class="s1">range(Xy.shape[</span><span class="s5">1</span><span class="s1">]):</span>
        <span class="s1">out = _gram_omp(</span>
            <span class="s1">Gram</span><span class="s3">,</span>
            <span class="s1">Xy[:</span><span class="s3">, </span><span class="s1">k]</span><span class="s3">,</span>
            <span class="s1">n_nonzero_coefs</span><span class="s3">,</span>
            <span class="s1">norms_squared[k] </span><span class="s3">if </span><span class="s1">tol </span><span class="s3">is not None else None,</span>
            <span class="s1">tol</span><span class="s3">,</span>
            <span class="s1">copy_Gram=copy_Gram</span><span class="s3">,</span>
            <span class="s1">copy_Xy=</span><span class="s3">False,</span>
            <span class="s1">return_path=return_path</span><span class="s3">,</span>
        <span class="s1">)</span>
        <span class="s3">if </span><span class="s1">return_path:</span>
            <span class="s1">_</span><span class="s3">, </span><span class="s1">idx</span><span class="s3">, </span><span class="s1">coefs</span><span class="s3">, </span><span class="s1">n_iter = out</span>
            <span class="s1">coef = coef[:</span><span class="s3">, </span><span class="s1">:</span><span class="s3">, </span><span class="s1">: len(idx)]</span>
            <span class="s3">for </span><span class="s1">n_active</span><span class="s3">, </span><span class="s1">x </span><span class="s3">in </span><span class="s1">enumerate(coefs.T):</span>
                <span class="s1">coef[idx[: n_active + </span><span class="s5">1</span><span class="s1">]</span><span class="s3">, </span><span class="s1">k</span><span class="s3">, </span><span class="s1">n_active] = x[: n_active + </span><span class="s5">1</span><span class="s1">]</span>
        <span class="s3">else</span><span class="s1">:</span>
            <span class="s1">x</span><span class="s3">, </span><span class="s1">idx</span><span class="s3">, </span><span class="s1">n_iter = out</span>
            <span class="s1">coef[idx</span><span class="s3">, </span><span class="s1">k] = x</span>
        <span class="s1">n_iters.append(n_iter)</span>

    <span class="s3">if </span><span class="s1">Xy.shape[</span><span class="s5">1</span><span class="s1">] == </span><span class="s5">1</span><span class="s1">:</span>
        <span class="s1">n_iters = n_iters[</span><span class="s5">0</span><span class="s1">]</span>

    <span class="s3">if </span><span class="s1">return_n_iter:</span>
        <span class="s3">return </span><span class="s1">np.squeeze(coef)</span><span class="s3">, </span><span class="s1">n_iters</span>
    <span class="s3">else</span><span class="s1">:</span>
        <span class="s3">return </span><span class="s1">np.squeeze(coef)</span>


<span class="s3">class </span><span class="s1">OrthogonalMatchingPursuit(MultiOutputMixin</span><span class="s3">, </span><span class="s1">RegressorMixin</span><span class="s3">, </span><span class="s1">LinearModel):</span>
    <span class="s0">&quot;&quot;&quot;Orthogonal Matching Pursuit model (OMP). 
 
    Read more in the :ref:`User Guide &lt;omp&gt;`. 
 
    Parameters 
    ---------- 
    n_nonzero_coefs : int, default=None 
        Desired number of non-zero entries in the solution. If None (by 
        default) this value is set to 10% of n_features. 
 
    tol : float, default=None 
        Maximum squared norm of the residual. If not None, overrides n_nonzero_coefs. 
 
    fit_intercept : bool, default=True 
        Whether to calculate the intercept for this model. If set 
        to false, no intercept will be used in calculations 
        (i.e. data is expected to be centered). 
 
    normalize : bool, default=False 
        This parameter is ignored when ``fit_intercept`` is set to False. 
        If True, the regressors X will be normalized before regression by 
        subtracting the mean and dividing by the l2-norm. 
        If you wish to standardize, please use 
        :class:`~sklearn.preprocessing.StandardScaler` before calling ``fit`` 
        on an estimator with ``normalize=False``. 
 
        .. versionchanged:: 1.2 
           default changed from True to False in 1.2. 
 
        .. deprecated:: 1.2 
            ``normalize`` was deprecated in version 1.2 and will be removed in 1.4. 
 
    precompute : 'auto' or bool, default='auto' 
        Whether to use a precomputed Gram and Xy matrix to speed up 
        calculations. Improves performance when :term:`n_targets` or 
        :term:`n_samples` is very large. Note that if you already have such 
        matrices, you can pass them directly to the fit method. 
 
    Attributes 
    ---------- 
    coef_ : ndarray of shape (n_features,) or (n_targets, n_features) 
        Parameter vector (w in the formula). 
 
    intercept_ : float or ndarray of shape (n_targets,) 
        Independent term in decision function. 
 
    n_iter_ : int or array-like 
        Number of active features across every target. 
 
    n_nonzero_coefs_ : int 
        The number of non-zero coefficients in the solution. If 
        `n_nonzero_coefs` is None and `tol` is None this value is either set 
        to 10% of `n_features` or 1, whichever is greater. 
 
    n_features_in_ : int 
        Number of features seen during :term:`fit`. 
 
        .. versionadded:: 0.24 
 
    feature_names_in_ : ndarray of shape (`n_features_in_`,) 
        Names of features seen during :term:`fit`. Defined only when `X` 
        has feature names that are all strings. 
 
        .. versionadded:: 1.0 
 
    See Also 
    -------- 
    orthogonal_mp : Solves n_targets Orthogonal Matching Pursuit problems. 
    orthogonal_mp_gram :  Solves n_targets Orthogonal Matching Pursuit 
        problems using only the Gram matrix X.T * X and the product X.T * y. 
    lars_path : Compute Least Angle Regression or Lasso path using LARS algorithm. 
    Lars : Least Angle Regression model a.k.a. LAR. 
    LassoLars : Lasso model fit with Least Angle Regression a.k.a. Lars. 
    sklearn.decomposition.sparse_encode : Generic sparse coding. 
        Each column of the result is the solution to a Lasso problem. 
    OrthogonalMatchingPursuitCV : Cross-validated 
        Orthogonal Matching Pursuit model (OMP). 
 
    Notes 
    ----- 
    Orthogonal matching pursuit was introduced in G. Mallat, Z. Zhang, 
    Matching pursuits with time-frequency dictionaries, IEEE Transactions on 
    Signal Processing, Vol. 41, No. 12. (December 1993), pp. 3397-3415. 
    (https://www.di.ens.fr/~mallat/papiers/MallatPursuit93.pdf) 
 
    This implementation is based on Rubinstein, R., Zibulevsky, M. and Elad, 
    M., Efficient Implementation of the K-SVD Algorithm using Batch Orthogonal 
    Matching Pursuit Technical Report - CS Technion, April 2008. 
    https://www.cs.technion.ac.il/~ronrubin/Publications/KSVD-OMP-v2.pdf 
 
    Examples 
    -------- 
    &gt;&gt;&gt; from sklearn.linear_model import OrthogonalMatchingPursuit 
    &gt;&gt;&gt; from sklearn.datasets import make_regression 
    &gt;&gt;&gt; X, y = make_regression(noise=4, random_state=0) 
    &gt;&gt;&gt; reg = OrthogonalMatchingPursuit().fit(X, y) 
    &gt;&gt;&gt; reg.score(X, y) 
    0.9991... 
    &gt;&gt;&gt; reg.predict(X[:1,]) 
    array([-78.3854...]) 
    &quot;&quot;&quot;</span>

    <span class="s1">_parameter_constraints: dict = {</span>
        <span class="s4">&quot;n_nonzero_coefs&quot;</span><span class="s1">: [Interval(Integral</span><span class="s3">, </span><span class="s5">1</span><span class="s3">, None, </span><span class="s1">closed=</span><span class="s4">&quot;left&quot;</span><span class="s1">)</span><span class="s3">, None</span><span class="s1">]</span><span class="s3">,</span>
        <span class="s4">&quot;tol&quot;</span><span class="s1">: [Interval(Real</span><span class="s3">, </span><span class="s5">0</span><span class="s3">, None, </span><span class="s1">closed=</span><span class="s4">&quot;left&quot;</span><span class="s1">)</span><span class="s3">, None</span><span class="s1">]</span><span class="s3">,</span>
        <span class="s4">&quot;fit_intercept&quot;</span><span class="s1">: [</span><span class="s4">&quot;boolean&quot;</span><span class="s1">]</span><span class="s3">,</span>
        <span class="s4">&quot;normalize&quot;</span><span class="s1">: [</span><span class="s4">&quot;boolean&quot;</span><span class="s3">, </span><span class="s1">Hidden(StrOptions({</span><span class="s4">&quot;deprecated&quot;</span><span class="s1">}))]</span><span class="s3">,</span>
        <span class="s4">&quot;precompute&quot;</span><span class="s1">: [StrOptions({</span><span class="s4">&quot;auto&quot;</span><span class="s1">})</span><span class="s3">, </span><span class="s4">&quot;boolean&quot;</span><span class="s1">]</span><span class="s3">,</span>
    <span class="s1">}</span>

    <span class="s3">def </span><span class="s1">__init__(</span>
        <span class="s1">self</span><span class="s3">,</span>
        <span class="s1">*</span><span class="s3">,</span>
        <span class="s1">n_nonzero_coefs=</span><span class="s3">None,</span>
        <span class="s1">tol=</span><span class="s3">None,</span>
        <span class="s1">fit_intercept=</span><span class="s3">True,</span>
        <span class="s1">normalize=</span><span class="s4">&quot;deprecated&quot;</span><span class="s3">,</span>
        <span class="s1">precompute=</span><span class="s4">&quot;auto&quot;</span><span class="s3">,</span>
    <span class="s1">):</span>
        <span class="s1">self.n_nonzero_coefs = n_nonzero_coefs</span>
        <span class="s1">self.tol = tol</span>
        <span class="s1">self.fit_intercept = fit_intercept</span>
        <span class="s1">self.normalize = normalize</span>
        <span class="s1">self.precompute = precompute</span>

    <span class="s1">@_fit_context(prefer_skip_nested_validation=</span><span class="s3">True</span><span class="s1">)</span>
    <span class="s3">def </span><span class="s1">fit(self</span><span class="s3">, </span><span class="s1">X</span><span class="s3">, </span><span class="s1">y):</span>
        <span class="s0">&quot;&quot;&quot;Fit the model using X, y as training data. 
 
        Parameters 
        ---------- 
        X : array-like of shape (n_samples, n_features) 
            Training data. 
 
        y : array-like of shape (n_samples,) or (n_samples, n_targets) 
            Target values. Will be cast to X's dtype if necessary. 
 
        Returns 
        ------- 
        self : object 
            Returns an instance of self. 
        &quot;&quot;&quot;</span>
        <span class="s1">_normalize = _deprecate_normalize(</span>
            <span class="s1">self.normalize</span><span class="s3">, </span><span class="s1">estimator_name=self.__class__.__name__</span>
        <span class="s1">)</span>

        <span class="s1">X</span><span class="s3">, </span><span class="s1">y = self._validate_data(X</span><span class="s3">, </span><span class="s1">y</span><span class="s3">, </span><span class="s1">multi_output=</span><span class="s3">True, </span><span class="s1">y_numeric=</span><span class="s3">True</span><span class="s1">)</span>
        <span class="s1">n_features = X.shape[</span><span class="s5">1</span><span class="s1">]</span>

        <span class="s1">X</span><span class="s3">, </span><span class="s1">y</span><span class="s3">, </span><span class="s1">X_offset</span><span class="s3">, </span><span class="s1">y_offset</span><span class="s3">, </span><span class="s1">X_scale</span><span class="s3">, </span><span class="s1">Gram</span><span class="s3">, </span><span class="s1">Xy = _pre_fit(</span>
            <span class="s1">X</span><span class="s3">, </span><span class="s1">y</span><span class="s3">, None, </span><span class="s1">self.precompute</span><span class="s3">, </span><span class="s1">_normalize</span><span class="s3">, </span><span class="s1">self.fit_intercept</span><span class="s3">, </span><span class="s1">copy=</span><span class="s3">True</span>
        <span class="s1">)</span>

        <span class="s3">if </span><span class="s1">y.ndim == </span><span class="s5">1</span><span class="s1">:</span>
            <span class="s1">y = y[:</span><span class="s3">, </span><span class="s1">np.newaxis]</span>

        <span class="s3">if </span><span class="s1">self.n_nonzero_coefs </span><span class="s3">is None and </span><span class="s1">self.tol </span><span class="s3">is None</span><span class="s1">:</span>
            <span class="s2"># default for n_nonzero_coefs is 0.1 * n_features</span>
            <span class="s2"># but at least one.</span>
            <span class="s1">self.n_nonzero_coefs_ = max(int(</span><span class="s5">0.1 </span><span class="s1">* n_features)</span><span class="s3">, </span><span class="s5">1</span><span class="s1">)</span>
        <span class="s3">else</span><span class="s1">:</span>
            <span class="s1">self.n_nonzero_coefs_ = self.n_nonzero_coefs</span>

        <span class="s3">if </span><span class="s1">Gram </span><span class="s3">is False</span><span class="s1">:</span>
            <span class="s1">coef_</span><span class="s3">, </span><span class="s1">self.n_iter_ = orthogonal_mp(</span>
                <span class="s1">X</span><span class="s3">,</span>
                <span class="s1">y</span><span class="s3">,</span>
                <span class="s1">n_nonzero_coefs=self.n_nonzero_coefs_</span><span class="s3">,</span>
                <span class="s1">tol=self.tol</span><span class="s3">,</span>
                <span class="s1">precompute=</span><span class="s3">False,</span>
                <span class="s1">copy_X=</span><span class="s3">True,</span>
                <span class="s1">return_n_iter=</span><span class="s3">True,</span>
            <span class="s1">)</span>
        <span class="s3">else</span><span class="s1">:</span>
            <span class="s1">norms_sq = np.sum(y**</span><span class="s5">2</span><span class="s3">, </span><span class="s1">axis=</span><span class="s5">0</span><span class="s1">) </span><span class="s3">if </span><span class="s1">self.tol </span><span class="s3">is not None else None</span>

            <span class="s1">coef_</span><span class="s3">, </span><span class="s1">self.n_iter_ = orthogonal_mp_gram(</span>
                <span class="s1">Gram</span><span class="s3">,</span>
                <span class="s1">Xy=Xy</span><span class="s3">,</span>
                <span class="s1">n_nonzero_coefs=self.n_nonzero_coefs_</span><span class="s3">,</span>
                <span class="s1">tol=self.tol</span><span class="s3">,</span>
                <span class="s1">norms_squared=norms_sq</span><span class="s3">,</span>
                <span class="s1">copy_Gram=</span><span class="s3">True,</span>
                <span class="s1">copy_Xy=</span><span class="s3">True,</span>
                <span class="s1">return_n_iter=</span><span class="s3">True,</span>
            <span class="s1">)</span>
        <span class="s1">self.coef_ = coef_.T</span>
        <span class="s1">self._set_intercept(X_offset</span><span class="s3">, </span><span class="s1">y_offset</span><span class="s3">, </span><span class="s1">X_scale)</span>
        <span class="s3">return </span><span class="s1">self</span>


<span class="s3">def </span><span class="s1">_omp_path_residues(</span>
    <span class="s1">X_train</span><span class="s3">,</span>
    <span class="s1">y_train</span><span class="s3">,</span>
    <span class="s1">X_test</span><span class="s3">,</span>
    <span class="s1">y_test</span><span class="s3">,</span>
    <span class="s1">copy=</span><span class="s3">True,</span>
    <span class="s1">fit_intercept=</span><span class="s3">True,</span>
    <span class="s1">normalize=</span><span class="s3">False,</span>
    <span class="s1">max_iter=</span><span class="s5">100</span><span class="s3">,</span>
<span class="s1">):</span>
    <span class="s0">&quot;&quot;&quot;Compute the residues on left-out data for a full LARS path. 
 
    Parameters 
    ---------- 
    X_train : ndarray of shape (n_samples, n_features) 
        The data to fit the LARS on. 
 
    y_train : ndarray of shape (n_samples) 
        The target variable to fit LARS on. 
 
    X_test : ndarray of shape (n_samples, n_features) 
        The data to compute the residues on. 
 
    y_test : ndarray of shape (n_samples) 
        The target variable to compute the residues on. 
 
    copy : bool, default=True 
        Whether X_train, X_test, y_train and y_test should be copied.  If 
        False, they may be overwritten. 
 
    fit_intercept : bool, default=True 
        Whether to calculate the intercept for this model. If set 
        to false, no intercept will be used in calculations 
        (i.e. data is expected to be centered). 
 
    normalize : bool, default=False 
        This parameter is ignored when ``fit_intercept`` is set to False. 
        If True, the regressors X will be normalized before regression by 
        subtracting the mean and dividing by the l2-norm. 
        If you wish to standardize, please use 
        :class:`~sklearn.preprocessing.StandardScaler` before calling ``fit`` 
        on an estimator with ``normalize=False``. 
 
        .. versionchanged:: 1.2 
           default changed from True to False in 1.2. 
 
        .. deprecated:: 1.2 
            ``normalize`` was deprecated in version 1.2 and will be removed in 1.4. 
 
    max_iter : int, default=100 
        Maximum numbers of iterations to perform, therefore maximum features 
        to include. 100 by default. 
 
    Returns 
    ------- 
    residues : ndarray of shape (n_samples, max_features) 
        Residues of the prediction on the test data. 
    &quot;&quot;&quot;</span>

    <span class="s3">if </span><span class="s1">copy:</span>
        <span class="s1">X_train = X_train.copy()</span>
        <span class="s1">y_train = y_train.copy()</span>
        <span class="s1">X_test = X_test.copy()</span>
        <span class="s1">y_test = y_test.copy()</span>

    <span class="s3">if </span><span class="s1">fit_intercept:</span>
        <span class="s1">X_mean = X_train.mean(axis=</span><span class="s5">0</span><span class="s1">)</span>
        <span class="s1">X_train -= X_mean</span>
        <span class="s1">X_test -= X_mean</span>
        <span class="s1">y_mean = y_train.mean(axis=</span><span class="s5">0</span><span class="s1">)</span>
        <span class="s1">y_train = as_float_array(y_train</span><span class="s3">, </span><span class="s1">copy=</span><span class="s3">False</span><span class="s1">)</span>
        <span class="s1">y_train -= y_mean</span>
        <span class="s1">y_test = as_float_array(y_test</span><span class="s3">, </span><span class="s1">copy=</span><span class="s3">False</span><span class="s1">)</span>
        <span class="s1">y_test -= y_mean</span>

    <span class="s3">if </span><span class="s1">normalize:</span>
        <span class="s1">norms = np.sqrt(np.sum(X_train**</span><span class="s5">2</span><span class="s3">, </span><span class="s1">axis=</span><span class="s5">0</span><span class="s1">))</span>
        <span class="s1">nonzeros = np.flatnonzero(norms)</span>
        <span class="s1">X_train[:</span><span class="s3">, </span><span class="s1">nonzeros] /= norms[nonzeros]</span>

    <span class="s1">coefs = orthogonal_mp(</span>
        <span class="s1">X_train</span><span class="s3">,</span>
        <span class="s1">y_train</span><span class="s3">,</span>
        <span class="s1">n_nonzero_coefs=max_iter</span><span class="s3">,</span>
        <span class="s1">tol=</span><span class="s3">None,</span>
        <span class="s1">precompute=</span><span class="s3">False,</span>
        <span class="s1">copy_X=</span><span class="s3">False,</span>
        <span class="s1">return_path=</span><span class="s3">True,</span>
    <span class="s1">)</span>
    <span class="s3">if </span><span class="s1">coefs.ndim == </span><span class="s5">1</span><span class="s1">:</span>
        <span class="s1">coefs = coefs[:</span><span class="s3">, </span><span class="s1">np.newaxis]</span>
    <span class="s3">if </span><span class="s1">normalize:</span>
        <span class="s1">coefs[nonzeros] /= norms[nonzeros][:</span><span class="s3">, </span><span class="s1">np.newaxis]</span>

    <span class="s3">return </span><span class="s1">np.dot(coefs.T</span><span class="s3">, </span><span class="s1">X_test.T) - y_test</span>


<span class="s3">class </span><span class="s1">OrthogonalMatchingPursuitCV(RegressorMixin</span><span class="s3">, </span><span class="s1">LinearModel):</span>
    <span class="s0">&quot;&quot;&quot;Cross-validated Orthogonal Matching Pursuit model (OMP). 
 
    See glossary entry for :term:`cross-validation estimator`. 
 
    Read more in the :ref:`User Guide &lt;omp&gt;`. 
 
    Parameters 
    ---------- 
    copy : bool, default=True 
        Whether the design matrix X must be copied by the algorithm. A false 
        value is only helpful if X is already Fortran-ordered, otherwise a 
        copy is made anyway. 
 
    fit_intercept : bool, default=True 
        Whether to calculate the intercept for this model. If set 
        to false, no intercept will be used in calculations 
        (i.e. data is expected to be centered). 
 
    normalize : bool, default=False 
        This parameter is ignored when ``fit_intercept`` is set to False. 
        If True, the regressors X will be normalized before regression by 
        subtracting the mean and dividing by the l2-norm. 
        If you wish to standardize, please use 
        :class:`~sklearn.preprocessing.StandardScaler` before calling ``fit`` 
        on an estimator with ``normalize=False``. 
 
        .. versionchanged:: 1.2 
           default changed from True to False in 1.2. 
 
        .. deprecated:: 1.2 
            ``normalize`` was deprecated in version 1.2 and will be removed in 1.4. 
 
    max_iter : int, default=None 
        Maximum numbers of iterations to perform, therefore maximum features 
        to include. 10% of ``n_features`` but at least 5 if available. 
 
    cv : int, cross-validation generator or iterable, default=None 
        Determines the cross-validation splitting strategy. 
        Possible inputs for cv are: 
 
        - None, to use the default 5-fold cross-validation, 
        - integer, to specify the number of folds. 
        - :term:`CV splitter`, 
        - An iterable yielding (train, test) splits as arrays of indices. 
 
        For integer/None inputs, :class:`~sklearn.model_selection.KFold` is used. 
 
        Refer :ref:`User Guide &lt;cross_validation&gt;` for the various 
        cross-validation strategies that can be used here. 
 
        .. versionchanged:: 0.22 
            ``cv`` default value if None changed from 3-fold to 5-fold. 
 
    n_jobs : int, default=None 
        Number of CPUs to use during the cross validation. 
        ``None`` means 1 unless in a :obj:`joblib.parallel_backend` context. 
        ``-1`` means using all processors. See :term:`Glossary &lt;n_jobs&gt;` 
        for more details. 
 
    verbose : bool or int, default=False 
        Sets the verbosity amount. 
 
    Attributes 
    ---------- 
    intercept_ : float or ndarray of shape (n_targets,) 
        Independent term in decision function. 
 
    coef_ : ndarray of shape (n_features,) or (n_targets, n_features) 
        Parameter vector (w in the problem formulation). 
 
    n_nonzero_coefs_ : int 
        Estimated number of non-zero coefficients giving the best mean squared 
        error over the cross-validation folds. 
 
    n_iter_ : int or array-like 
        Number of active features across every target for the model refit with 
        the best hyperparameters got by cross-validating across all folds. 
 
    n_features_in_ : int 
        Number of features seen during :term:`fit`. 
 
        .. versionadded:: 0.24 
 
    feature_names_in_ : ndarray of shape (`n_features_in_`,) 
        Names of features seen during :term:`fit`. Defined only when `X` 
        has feature names that are all strings. 
 
        .. versionadded:: 1.0 
 
    See Also 
    -------- 
    orthogonal_mp : Solves n_targets Orthogonal Matching Pursuit problems. 
    orthogonal_mp_gram : Solves n_targets Orthogonal Matching Pursuit 
        problems using only the Gram matrix X.T * X and the product X.T * y. 
    lars_path : Compute Least Angle Regression or Lasso path using LARS algorithm. 
    Lars : Least Angle Regression model a.k.a. LAR. 
    LassoLars : Lasso model fit with Least Angle Regression a.k.a. Lars. 
    OrthogonalMatchingPursuit : Orthogonal Matching Pursuit model (OMP). 
    LarsCV : Cross-validated Least Angle Regression model. 
    LassoLarsCV : Cross-validated Lasso model fit with Least Angle Regression. 
    sklearn.decomposition.sparse_encode : Generic sparse coding. 
        Each column of the result is the solution to a Lasso problem. 
 
    Notes 
    ----- 
    In `fit`, once the optimal number of non-zero coefficients is found through 
    cross-validation, the model is fit again using the entire training set. 
 
    Examples 
    -------- 
    &gt;&gt;&gt; from sklearn.linear_model import OrthogonalMatchingPursuitCV 
    &gt;&gt;&gt; from sklearn.datasets import make_regression 
    &gt;&gt;&gt; X, y = make_regression(n_features=100, n_informative=10, 
    ...                        noise=4, random_state=0) 
    &gt;&gt;&gt; reg = OrthogonalMatchingPursuitCV(cv=5).fit(X, y) 
    &gt;&gt;&gt; reg.score(X, y) 
    0.9991... 
    &gt;&gt;&gt; reg.n_nonzero_coefs_ 
    10 
    &gt;&gt;&gt; reg.predict(X[:1,]) 
    array([-78.3854...]) 
    &quot;&quot;&quot;</span>

    <span class="s1">_parameter_constraints: dict = {</span>
        <span class="s4">&quot;copy&quot;</span><span class="s1">: [</span><span class="s4">&quot;boolean&quot;</span><span class="s1">]</span><span class="s3">,</span>
        <span class="s4">&quot;fit_intercept&quot;</span><span class="s1">: [</span><span class="s4">&quot;boolean&quot;</span><span class="s1">]</span><span class="s3">,</span>
        <span class="s4">&quot;normalize&quot;</span><span class="s1">: [</span><span class="s4">&quot;boolean&quot;</span><span class="s3">, </span><span class="s1">Hidden(StrOptions({</span><span class="s4">&quot;deprecated&quot;</span><span class="s1">}))]</span><span class="s3">,</span>
        <span class="s4">&quot;max_iter&quot;</span><span class="s1">: [Interval(Integral</span><span class="s3">, </span><span class="s5">0</span><span class="s3">, None, </span><span class="s1">closed=</span><span class="s4">&quot;left&quot;</span><span class="s1">)</span><span class="s3">, None</span><span class="s1">]</span><span class="s3">,</span>
        <span class="s4">&quot;cv&quot;</span><span class="s1">: [</span><span class="s4">&quot;cv_object&quot;</span><span class="s1">]</span><span class="s3">,</span>
        <span class="s4">&quot;n_jobs&quot;</span><span class="s1">: [Integral</span><span class="s3">, None</span><span class="s1">]</span><span class="s3">,</span>
        <span class="s4">&quot;verbose&quot;</span><span class="s1">: [</span><span class="s4">&quot;verbose&quot;</span><span class="s1">]</span><span class="s3">,</span>
    <span class="s1">}</span>

    <span class="s3">def </span><span class="s1">__init__(</span>
        <span class="s1">self</span><span class="s3">,</span>
        <span class="s1">*</span><span class="s3">,</span>
        <span class="s1">copy=</span><span class="s3">True,</span>
        <span class="s1">fit_intercept=</span><span class="s3">True,</span>
        <span class="s1">normalize=</span><span class="s4">&quot;deprecated&quot;</span><span class="s3">,</span>
        <span class="s1">max_iter=</span><span class="s3">None,</span>
        <span class="s1">cv=</span><span class="s3">None,</span>
        <span class="s1">n_jobs=</span><span class="s3">None,</span>
        <span class="s1">verbose=</span><span class="s3">False,</span>
    <span class="s1">):</span>
        <span class="s1">self.copy = copy</span>
        <span class="s1">self.fit_intercept = fit_intercept</span>
        <span class="s1">self.normalize = normalize</span>
        <span class="s1">self.max_iter = max_iter</span>
        <span class="s1">self.cv = cv</span>
        <span class="s1">self.n_jobs = n_jobs</span>
        <span class="s1">self.verbose = verbose</span>

    <span class="s1">@_fit_context(prefer_skip_nested_validation=</span><span class="s3">True</span><span class="s1">)</span>
    <span class="s3">def </span><span class="s1">fit(self</span><span class="s3">, </span><span class="s1">X</span><span class="s3">, </span><span class="s1">y):</span>
        <span class="s0">&quot;&quot;&quot;Fit the model using X, y as training data. 
 
        Parameters 
        ---------- 
        X : array-like of shape (n_samples, n_features) 
            Training data. 
 
        y : array-like of shape (n_samples,) 
            Target values. Will be cast to X's dtype if necessary. 
 
        Returns 
        ------- 
        self : object 
            Returns an instance of self. 
        &quot;&quot;&quot;</span>
        <span class="s1">_normalize = _deprecate_normalize(</span>
            <span class="s1">self.normalize</span><span class="s3">, </span><span class="s1">estimator_name=self.__class__.__name__</span>
        <span class="s1">)</span>

        <span class="s1">X</span><span class="s3">, </span><span class="s1">y = self._validate_data(X</span><span class="s3">, </span><span class="s1">y</span><span class="s3">, </span><span class="s1">y_numeric=</span><span class="s3">True, </span><span class="s1">ensure_min_features=</span><span class="s5">2</span><span class="s1">)</span>
        <span class="s1">X = as_float_array(X</span><span class="s3">, </span><span class="s1">copy=</span><span class="s3">False, </span><span class="s1">force_all_finite=</span><span class="s3">False</span><span class="s1">)</span>
        <span class="s1">cv = check_cv(self.cv</span><span class="s3">, </span><span class="s1">classifier=</span><span class="s3">False</span><span class="s1">)</span>
        <span class="s1">max_iter = (</span>
            <span class="s1">min(max(int(</span><span class="s5">0.1 </span><span class="s1">* X.shape[</span><span class="s5">1</span><span class="s1">])</span><span class="s3">, </span><span class="s5">5</span><span class="s1">)</span><span class="s3">, </span><span class="s1">X.shape[</span><span class="s5">1</span><span class="s1">])</span>
            <span class="s3">if not </span><span class="s1">self.max_iter</span>
            <span class="s3">else </span><span class="s1">self.max_iter</span>
        <span class="s1">)</span>
        <span class="s1">cv_paths = Parallel(n_jobs=self.n_jobs</span><span class="s3">, </span><span class="s1">verbose=self.verbose)(</span>
            <span class="s1">delayed(_omp_path_residues)(</span>
                <span class="s1">X[train]</span><span class="s3">,</span>
                <span class="s1">y[train]</span><span class="s3">,</span>
                <span class="s1">X[test]</span><span class="s3">,</span>
                <span class="s1">y[test]</span><span class="s3">,</span>
                <span class="s1">self.copy</span><span class="s3">,</span>
                <span class="s1">self.fit_intercept</span><span class="s3">,</span>
                <span class="s1">_normalize</span><span class="s3">,</span>
                <span class="s1">max_iter</span><span class="s3">,</span>
            <span class="s1">)</span>
            <span class="s3">for </span><span class="s1">train</span><span class="s3">, </span><span class="s1">test </span><span class="s3">in </span><span class="s1">cv.split(X)</span>
        <span class="s1">)</span>

        <span class="s1">min_early_stop = min(fold.shape[</span><span class="s5">0</span><span class="s1">] </span><span class="s3">for </span><span class="s1">fold </span><span class="s3">in </span><span class="s1">cv_paths)</span>
        <span class="s1">mse_folds = np.array(</span>
            <span class="s1">[(fold[:min_early_stop] ** </span><span class="s5">2</span><span class="s1">).mean(axis=</span><span class="s5">1</span><span class="s1">) </span><span class="s3">for </span><span class="s1">fold </span><span class="s3">in </span><span class="s1">cv_paths]</span>
        <span class="s1">)</span>
        <span class="s1">best_n_nonzero_coefs = np.argmin(mse_folds.mean(axis=</span><span class="s5">0</span><span class="s1">)) + </span><span class="s5">1</span>
        <span class="s1">self.n_nonzero_coefs_ = best_n_nonzero_coefs</span>
        <span class="s1">omp = OrthogonalMatchingPursuit(</span>
            <span class="s1">n_nonzero_coefs=best_n_nonzero_coefs</span><span class="s3">,</span>
            <span class="s1">fit_intercept=self.fit_intercept</span><span class="s3">,</span>
            <span class="s1">normalize=_normalize</span><span class="s3">,</span>
        <span class="s1">)</span>

        <span class="s2"># avoid duplicating warning for deprecated normalize</span>
        <span class="s3">with </span><span class="s1">warnings.catch_warnings():</span>
            <span class="s1">warnings.filterwarnings(</span><span class="s4">&quot;ignore&quot;</span><span class="s3">, </span><span class="s1">category=FutureWarning)</span>
            <span class="s1">omp.fit(X</span><span class="s3">, </span><span class="s1">y)</span>

        <span class="s1">self.coef_ = omp.coef_</span>
        <span class="s1">self.intercept_ = omp.intercept_</span>
        <span class="s1">self.n_iter_ = omp.n_iter_</span>
        <span class="s3">return </span><span class="s1">self</span>
</pre>
</body>
</html>