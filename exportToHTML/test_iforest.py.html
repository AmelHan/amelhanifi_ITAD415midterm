<html>
<head>
<title>test_iforest.py</title>
<meta http-equiv="Content-Type" content="text/html; charset=utf-8">
<style type="text/css">
.s0 { color: #629755; font-style: italic;}
.s1 { color: #a9b7c6;}
.s2 { color: #808080;}
.s3 { color: #cc7832;}
.s4 { color: #6897bb;}
.s5 { color: #6a8759;}
</style>
</head>
<body bgcolor="#2b2b2b">
<table CELLSPACING=0 CELLPADDING=5 COLS=1 WIDTH="100%" BGCOLOR="#606060" >
<tr><td><center>
<font face="Arial, Helvetica" color="#000000">
test_iforest.py</font>
</center></td></tr></table>
<pre><span class="s0">&quot;&quot;&quot; 
Testing for Isolation Forest algorithm (sklearn.ensemble.iforest). 
&quot;&quot;&quot;</span>

<span class="s2"># Authors: Nicolas Goix &lt;nicolas.goix@telecom-paristech.fr&gt;</span>
<span class="s2">#          Alexandre Gramfort &lt;alexandre.gramfort@telecom-paristech.fr&gt;</span>
<span class="s2"># License: BSD 3 clause</span>

<span class="s3">import </span><span class="s1">warnings</span>
<span class="s3">from </span><span class="s1">unittest.mock </span><span class="s3">import </span><span class="s1">Mock</span><span class="s3">, </span><span class="s1">patch</span>

<span class="s3">import </span><span class="s1">numpy </span><span class="s3">as </span><span class="s1">np</span>
<span class="s3">import </span><span class="s1">pytest</span>
<span class="s3">from </span><span class="s1">scipy.sparse </span><span class="s3">import </span><span class="s1">csc_matrix</span><span class="s3">, </span><span class="s1">csr_matrix</span>

<span class="s3">from </span><span class="s1">sklearn.datasets </span><span class="s3">import </span><span class="s1">load_diabetes</span><span class="s3">, </span><span class="s1">load_iris</span><span class="s3">, </span><span class="s1">make_classification</span>
<span class="s3">from </span><span class="s1">sklearn.ensemble </span><span class="s3">import </span><span class="s1">IsolationForest</span>
<span class="s3">from </span><span class="s1">sklearn.ensemble._iforest </span><span class="s3">import </span><span class="s1">_average_path_length</span>
<span class="s3">from </span><span class="s1">sklearn.metrics </span><span class="s3">import </span><span class="s1">roc_auc_score</span>
<span class="s3">from </span><span class="s1">sklearn.model_selection </span><span class="s3">import </span><span class="s1">ParameterGrid</span><span class="s3">, </span><span class="s1">train_test_split</span>
<span class="s3">from </span><span class="s1">sklearn.utils </span><span class="s3">import </span><span class="s1">check_random_state</span>
<span class="s3">from </span><span class="s1">sklearn.utils._testing </span><span class="s3">import </span><span class="s1">(</span>
    <span class="s1">assert_allclose</span><span class="s3">,</span>
    <span class="s1">assert_array_almost_equal</span><span class="s3">,</span>
    <span class="s1">assert_array_equal</span><span class="s3">,</span>
    <span class="s1">ignore_warnings</span><span class="s3">,</span>
<span class="s1">)</span>

<span class="s2"># load iris &amp; diabetes dataset</span>
<span class="s1">iris = load_iris()</span>
<span class="s1">diabetes = load_diabetes()</span>


<span class="s3">def </span><span class="s1">test_iforest(global_random_seed):</span>
    <span class="s0">&quot;&quot;&quot;Check Isolation Forest for various parameter settings.&quot;&quot;&quot;</span>
    <span class="s1">X_train = np.array([[</span><span class="s4">0</span><span class="s3">, </span><span class="s4">1</span><span class="s1">]</span><span class="s3">, </span><span class="s1">[</span><span class="s4">1</span><span class="s3">, </span><span class="s4">2</span><span class="s1">]])</span>
    <span class="s1">X_test = np.array([[</span><span class="s4">2</span><span class="s3">, </span><span class="s4">1</span><span class="s1">]</span><span class="s3">, </span><span class="s1">[</span><span class="s4">1</span><span class="s3">, </span><span class="s4">1</span><span class="s1">]])</span>

    <span class="s1">grid = ParameterGrid(</span>
        <span class="s1">{</span><span class="s5">&quot;n_estimators&quot;</span><span class="s1">: [</span><span class="s4">3</span><span class="s1">]</span><span class="s3">, </span><span class="s5">&quot;max_samples&quot;</span><span class="s1">: [</span><span class="s4">0.5</span><span class="s3">, </span><span class="s4">1.0</span><span class="s3">, </span><span class="s4">3</span><span class="s1">]</span><span class="s3">, </span><span class="s5">&quot;bootstrap&quot;</span><span class="s1">: [</span><span class="s3">True, False</span><span class="s1">]}</span>
    <span class="s1">)</span>

    <span class="s3">with </span><span class="s1">ignore_warnings():</span>
        <span class="s3">for </span><span class="s1">params </span><span class="s3">in </span><span class="s1">grid:</span>
            <span class="s1">IsolationForest(random_state=global_random_seed</span><span class="s3">, </span><span class="s1">**params).fit(</span>
                <span class="s1">X_train</span>
            <span class="s1">).predict(X_test)</span>


<span class="s3">def </span><span class="s1">test_iforest_sparse(global_random_seed):</span>
    <span class="s0">&quot;&quot;&quot;Check IForest for various parameter settings on sparse input.&quot;&quot;&quot;</span>
    <span class="s1">rng = check_random_state(global_random_seed)</span>
    <span class="s1">X_train</span><span class="s3">, </span><span class="s1">X_test = train_test_split(diabetes.data[:</span><span class="s4">50</span><span class="s1">]</span><span class="s3">, </span><span class="s1">random_state=rng)</span>
    <span class="s1">grid = ParameterGrid({</span><span class="s5">&quot;max_samples&quot;</span><span class="s1">: [</span><span class="s4">0.5</span><span class="s3">, </span><span class="s4">1.0</span><span class="s1">]</span><span class="s3">, </span><span class="s5">&quot;bootstrap&quot;</span><span class="s1">: [</span><span class="s3">True, False</span><span class="s1">]})</span>

    <span class="s3">for </span><span class="s1">sparse_format </span><span class="s3">in </span><span class="s1">[csc_matrix</span><span class="s3">, </span><span class="s1">csr_matrix]:</span>
        <span class="s1">X_train_sparse = sparse_format(X_train)</span>
        <span class="s1">X_test_sparse = sparse_format(X_test)</span>

        <span class="s3">for </span><span class="s1">params </span><span class="s3">in </span><span class="s1">grid:</span>
            <span class="s2"># Trained on sparse format</span>
            <span class="s1">sparse_classifier = IsolationForest(</span>
                <span class="s1">n_estimators=</span><span class="s4">10</span><span class="s3">, </span><span class="s1">random_state=global_random_seed</span><span class="s3">, </span><span class="s1">**params</span>
            <span class="s1">).fit(X_train_sparse)</span>
            <span class="s1">sparse_results = sparse_classifier.predict(X_test_sparse)</span>

            <span class="s2"># Trained on dense format</span>
            <span class="s1">dense_classifier = IsolationForest(</span>
                <span class="s1">n_estimators=</span><span class="s4">10</span><span class="s3">, </span><span class="s1">random_state=global_random_seed</span><span class="s3">, </span><span class="s1">**params</span>
            <span class="s1">).fit(X_train)</span>
            <span class="s1">dense_results = dense_classifier.predict(X_test)</span>

            <span class="s1">assert_array_equal(sparse_results</span><span class="s3">, </span><span class="s1">dense_results)</span>


<span class="s3">def </span><span class="s1">test_iforest_error():</span>
    <span class="s0">&quot;&quot;&quot;Test that it gives proper exception on deficient input.&quot;&quot;&quot;</span>
    <span class="s1">X = iris.data</span>

    <span class="s2"># The dataset has less than 256 samples, explicitly setting</span>
    <span class="s2"># max_samples &gt; n_samples should result in a warning. If not set</span>
    <span class="s2"># explicitly there should be no warning</span>
    <span class="s1">warn_msg = </span><span class="s5">&quot;max_samples will be set to n_samples for estimation&quot;</span>
    <span class="s3">with </span><span class="s1">pytest.warns(UserWarning</span><span class="s3">, </span><span class="s1">match=warn_msg):</span>
        <span class="s1">IsolationForest(max_samples=</span><span class="s4">1000</span><span class="s1">).fit(X)</span>
    <span class="s3">with </span><span class="s1">warnings.catch_warnings():</span>
        <span class="s1">warnings.simplefilter(</span><span class="s5">&quot;error&quot;</span><span class="s3">, </span><span class="s1">UserWarning)</span>
        <span class="s1">IsolationForest(max_samples=</span><span class="s5">&quot;auto&quot;</span><span class="s1">).fit(X)</span>
    <span class="s3">with </span><span class="s1">warnings.catch_warnings():</span>
        <span class="s1">warnings.simplefilter(</span><span class="s5">&quot;error&quot;</span><span class="s3">, </span><span class="s1">UserWarning)</span>
        <span class="s1">IsolationForest(max_samples=np.int64(</span><span class="s4">2</span><span class="s1">)).fit(X)</span>

    <span class="s2"># test X_test n_features match X_train one:</span>
    <span class="s3">with </span><span class="s1">pytest.raises(ValueError):</span>
        <span class="s1">IsolationForest().fit(X).predict(X[:</span><span class="s3">, </span><span class="s4">1</span><span class="s1">:])</span>


<span class="s3">def </span><span class="s1">test_recalculate_max_depth():</span>
    <span class="s0">&quot;&quot;&quot;Check max_depth recalculation when max_samples is reset to n_samples&quot;&quot;&quot;</span>
    <span class="s1">X = iris.data</span>
    <span class="s1">clf = IsolationForest().fit(X)</span>
    <span class="s3">for </span><span class="s1">est </span><span class="s3">in </span><span class="s1">clf.estimators_:</span>
        <span class="s3">assert </span><span class="s1">est.max_depth == int(np.ceil(np.log2(X.shape[</span><span class="s4">0</span><span class="s1">])))</span>


<span class="s3">def </span><span class="s1">test_max_samples_attribute():</span>
    <span class="s1">X = iris.data</span>
    <span class="s1">clf = IsolationForest().fit(X)</span>
    <span class="s3">assert </span><span class="s1">clf.max_samples_ == X.shape[</span><span class="s4">0</span><span class="s1">]</span>

    <span class="s1">clf = IsolationForest(max_samples=</span><span class="s4">500</span><span class="s1">)</span>
    <span class="s1">warn_msg = </span><span class="s5">&quot;max_samples will be set to n_samples for estimation&quot;</span>
    <span class="s3">with </span><span class="s1">pytest.warns(UserWarning</span><span class="s3">, </span><span class="s1">match=warn_msg):</span>
        <span class="s1">clf.fit(X)</span>
    <span class="s3">assert </span><span class="s1">clf.max_samples_ == X.shape[</span><span class="s4">0</span><span class="s1">]</span>

    <span class="s1">clf = IsolationForest(max_samples=</span><span class="s4">0.4</span><span class="s1">).fit(X)</span>
    <span class="s3">assert </span><span class="s1">clf.max_samples_ == </span><span class="s4">0.4 </span><span class="s1">* X.shape[</span><span class="s4">0</span><span class="s1">]</span>


<span class="s3">def </span><span class="s1">test_iforest_parallel_regression(global_random_seed):</span>
    <span class="s0">&quot;&quot;&quot;Check parallel regression.&quot;&quot;&quot;</span>
    <span class="s1">rng = check_random_state(global_random_seed)</span>

    <span class="s1">X_train</span><span class="s3">, </span><span class="s1">X_test = train_test_split(diabetes.data</span><span class="s3">, </span><span class="s1">random_state=rng)</span>

    <span class="s1">ensemble = IsolationForest(n_jobs=</span><span class="s4">3</span><span class="s3">, </span><span class="s1">random_state=global_random_seed).fit(X_train)</span>

    <span class="s1">ensemble.set_params(n_jobs=</span><span class="s4">1</span><span class="s1">)</span>
    <span class="s1">y1 = ensemble.predict(X_test)</span>
    <span class="s1">ensemble.set_params(n_jobs=</span><span class="s4">2</span><span class="s1">)</span>
    <span class="s1">y2 = ensemble.predict(X_test)</span>
    <span class="s1">assert_array_almost_equal(y1</span><span class="s3">, </span><span class="s1">y2)</span>

    <span class="s1">ensemble = IsolationForest(n_jobs=</span><span class="s4">1</span><span class="s3">, </span><span class="s1">random_state=global_random_seed).fit(X_train)</span>

    <span class="s1">y3 = ensemble.predict(X_test)</span>
    <span class="s1">assert_array_almost_equal(y1</span><span class="s3">, </span><span class="s1">y3)</span>


<span class="s3">def </span><span class="s1">test_iforest_performance(global_random_seed):</span>
    <span class="s0">&quot;&quot;&quot;Test Isolation Forest performs well&quot;&quot;&quot;</span>

    <span class="s2"># Generate train/test data</span>
    <span class="s1">rng = check_random_state(global_random_seed)</span>
    <span class="s1">X = </span><span class="s4">0.3 </span><span class="s1">* rng.randn(</span><span class="s4">600</span><span class="s3">, </span><span class="s4">2</span><span class="s1">)</span>
    <span class="s1">X = rng.permutation(np.vstack((X + </span><span class="s4">2</span><span class="s3">, </span><span class="s1">X - </span><span class="s4">2</span><span class="s1">)))</span>
    <span class="s1">X_train = X[:</span><span class="s4">1000</span><span class="s1">]</span>

    <span class="s2"># Generate some abnormal novel observations</span>
    <span class="s1">X_outliers = rng.uniform(low=-</span><span class="s4">1</span><span class="s3">, </span><span class="s1">high=</span><span class="s4">1</span><span class="s3">, </span><span class="s1">size=(</span><span class="s4">200</span><span class="s3">, </span><span class="s4">2</span><span class="s1">))</span>
    <span class="s1">X_test = np.vstack((X[</span><span class="s4">1000</span><span class="s1">:]</span><span class="s3">, </span><span class="s1">X_outliers))</span>
    <span class="s1">y_test = np.array([</span><span class="s4">0</span><span class="s1">] * </span><span class="s4">200 </span><span class="s1">+ [</span><span class="s4">1</span><span class="s1">] * </span><span class="s4">200</span><span class="s1">)</span>

    <span class="s2"># fit the model</span>
    <span class="s1">clf = IsolationForest(max_samples=</span><span class="s4">100</span><span class="s3">, </span><span class="s1">random_state=rng).fit(X_train)</span>

    <span class="s2"># predict scores (the lower, the more normal)</span>
    <span class="s1">y_pred = -clf.decision_function(X_test)</span>

    <span class="s2"># check that there is at most 6 errors (false positive or false negative)</span>
    <span class="s3">assert </span><span class="s1">roc_auc_score(y_test</span><span class="s3">, </span><span class="s1">y_pred) &gt; </span><span class="s4">0.98</span>


<span class="s1">@pytest.mark.parametrize(</span><span class="s5">&quot;contamination&quot;</span><span class="s3">, </span><span class="s1">[</span><span class="s4">0.25</span><span class="s3">, </span><span class="s5">&quot;auto&quot;</span><span class="s1">])</span>
<span class="s3">def </span><span class="s1">test_iforest_works(contamination</span><span class="s3">, </span><span class="s1">global_random_seed):</span>
    <span class="s2"># toy sample (the last two samples are outliers)</span>
    <span class="s1">X = [[-</span><span class="s4">2</span><span class="s3">, </span><span class="s1">-</span><span class="s4">1</span><span class="s1">]</span><span class="s3">, </span><span class="s1">[-</span><span class="s4">1</span><span class="s3">, </span><span class="s1">-</span><span class="s4">1</span><span class="s1">]</span><span class="s3">, </span><span class="s1">[-</span><span class="s4">1</span><span class="s3">, </span><span class="s1">-</span><span class="s4">2</span><span class="s1">]</span><span class="s3">, </span><span class="s1">[</span><span class="s4">1</span><span class="s3">, </span><span class="s4">1</span><span class="s1">]</span><span class="s3">, </span><span class="s1">[</span><span class="s4">1</span><span class="s3">, </span><span class="s4">2</span><span class="s1">]</span><span class="s3">, </span><span class="s1">[</span><span class="s4">2</span><span class="s3">, </span><span class="s4">1</span><span class="s1">]</span><span class="s3">, </span><span class="s1">[</span><span class="s4">7</span><span class="s3">, </span><span class="s4">4</span><span class="s1">]</span><span class="s3">, </span><span class="s1">[-</span><span class="s4">5</span><span class="s3">, </span><span class="s4">9</span><span class="s1">]]</span>

    <span class="s2"># Test IsolationForest</span>
    <span class="s1">clf = IsolationForest(random_state=global_random_seed</span><span class="s3">, </span><span class="s1">contamination=contamination)</span>
    <span class="s1">clf.fit(X)</span>
    <span class="s1">decision_func = -clf.decision_function(X)</span>
    <span class="s1">pred = clf.predict(X)</span>
    <span class="s2"># assert detect outliers:</span>
    <span class="s3">assert </span><span class="s1">np.min(decision_func[-</span><span class="s4">2</span><span class="s1">:]) &gt; np.max(decision_func[:-</span><span class="s4">2</span><span class="s1">])</span>
    <span class="s1">assert_array_equal(pred</span><span class="s3">, </span><span class="s4">6 </span><span class="s1">* [</span><span class="s4">1</span><span class="s1">] + </span><span class="s4">2 </span><span class="s1">* [-</span><span class="s4">1</span><span class="s1">])</span>


<span class="s3">def </span><span class="s1">test_max_samples_consistency():</span>
    <span class="s2"># Make sure validated max_samples in iforest and BaseBagging are identical</span>
    <span class="s1">X = iris.data</span>
    <span class="s1">clf = IsolationForest().fit(X)</span>
    <span class="s3">assert </span><span class="s1">clf.max_samples_ == clf._max_samples</span>


<span class="s3">def </span><span class="s1">test_iforest_subsampled_features():</span>
    <span class="s2"># It tests non-regression for #5732 which failed at predict.</span>
    <span class="s1">rng = check_random_state(</span><span class="s4">0</span><span class="s1">)</span>
    <span class="s1">X_train</span><span class="s3">, </span><span class="s1">X_test</span><span class="s3">, </span><span class="s1">y_train</span><span class="s3">, </span><span class="s1">y_test = train_test_split(</span>
        <span class="s1">diabetes.data[:</span><span class="s4">50</span><span class="s1">]</span><span class="s3">, </span><span class="s1">diabetes.target[:</span><span class="s4">50</span><span class="s1">]</span><span class="s3">, </span><span class="s1">random_state=rng</span>
    <span class="s1">)</span>
    <span class="s1">clf = IsolationForest(max_features=</span><span class="s4">0.8</span><span class="s1">)</span>
    <span class="s1">clf.fit(X_train</span><span class="s3">, </span><span class="s1">y_train)</span>
    <span class="s1">clf.predict(X_test)</span>


<span class="s3">def </span><span class="s1">test_iforest_average_path_length():</span>
    <span class="s2"># It tests non-regression for #8549 which used the wrong formula</span>
    <span class="s2"># for average path length, strictly for the integer case</span>
    <span class="s2"># Updated to check average path length when input is &lt;= 2 (issue #11839)</span>
    <span class="s1">result_one = </span><span class="s4">2.0 </span><span class="s1">* (np.log(</span><span class="s4">4.0</span><span class="s1">) + np.euler_gamma) - </span><span class="s4">2.0 </span><span class="s1">* </span><span class="s4">4.0 </span><span class="s1">/ </span><span class="s4">5.0</span>
    <span class="s1">result_two = </span><span class="s4">2.0 </span><span class="s1">* (np.log(</span><span class="s4">998.0</span><span class="s1">) + np.euler_gamma) - </span><span class="s4">2.0 </span><span class="s1">* </span><span class="s4">998.0 </span><span class="s1">/ </span><span class="s4">999.0</span>
    <span class="s1">assert_allclose(_average_path_length([</span><span class="s4">0</span><span class="s1">])</span><span class="s3">, </span><span class="s1">[</span><span class="s4">0.0</span><span class="s1">])</span>
    <span class="s1">assert_allclose(_average_path_length([</span><span class="s4">1</span><span class="s1">])</span><span class="s3">, </span><span class="s1">[</span><span class="s4">0.0</span><span class="s1">])</span>
    <span class="s1">assert_allclose(_average_path_length([</span><span class="s4">2</span><span class="s1">])</span><span class="s3">, </span><span class="s1">[</span><span class="s4">1.0</span><span class="s1">])</span>
    <span class="s1">assert_allclose(_average_path_length([</span><span class="s4">5</span><span class="s1">])</span><span class="s3">, </span><span class="s1">[result_one])</span>
    <span class="s1">assert_allclose(_average_path_length([</span><span class="s4">999</span><span class="s1">])</span><span class="s3">, </span><span class="s1">[result_two])</span>
    <span class="s1">assert_allclose(</span>
        <span class="s1">_average_path_length(np.array([</span><span class="s4">1</span><span class="s3">, </span><span class="s4">2</span><span class="s3">, </span><span class="s4">5</span><span class="s3">, </span><span class="s4">999</span><span class="s1">]))</span><span class="s3">,</span>
        <span class="s1">[</span><span class="s4">0.0</span><span class="s3">, </span><span class="s4">1.0</span><span class="s3">, </span><span class="s1">result_one</span><span class="s3">, </span><span class="s1">result_two]</span><span class="s3">,</span>
    <span class="s1">)</span>
    <span class="s2"># _average_path_length is increasing</span>
    <span class="s1">avg_path_length = _average_path_length(np.arange(</span><span class="s4">5</span><span class="s1">))</span>
    <span class="s1">assert_array_equal(avg_path_length</span><span class="s3">, </span><span class="s1">np.sort(avg_path_length))</span>


<span class="s3">def </span><span class="s1">test_score_samples():</span>
    <span class="s1">X_train = [[</span><span class="s4">1</span><span class="s3">, </span><span class="s4">1</span><span class="s1">]</span><span class="s3">, </span><span class="s1">[</span><span class="s4">1</span><span class="s3">, </span><span class="s4">2</span><span class="s1">]</span><span class="s3">, </span><span class="s1">[</span><span class="s4">2</span><span class="s3">, </span><span class="s4">1</span><span class="s1">]]</span>
    <span class="s1">clf1 = IsolationForest(contamination=</span><span class="s4">0.1</span><span class="s1">).fit(X_train)</span>
    <span class="s1">clf2 = IsolationForest().fit(X_train)</span>
    <span class="s1">assert_array_equal(</span>
        <span class="s1">clf1.score_samples([[</span><span class="s4">2.0</span><span class="s3">, </span><span class="s4">2.0</span><span class="s1">]])</span><span class="s3">,</span>
        <span class="s1">clf1.decision_function([[</span><span class="s4">2.0</span><span class="s3">, </span><span class="s4">2.0</span><span class="s1">]]) + clf1.offset_</span><span class="s3">,</span>
    <span class="s1">)</span>
    <span class="s1">assert_array_equal(</span>
        <span class="s1">clf2.score_samples([[</span><span class="s4">2.0</span><span class="s3">, </span><span class="s4">2.0</span><span class="s1">]])</span><span class="s3">,</span>
        <span class="s1">clf2.decision_function([[</span><span class="s4">2.0</span><span class="s3">, </span><span class="s4">2.0</span><span class="s1">]]) + clf2.offset_</span><span class="s3">,</span>
    <span class="s1">)</span>
    <span class="s1">assert_array_equal(</span>
        <span class="s1">clf1.score_samples([[</span><span class="s4">2.0</span><span class="s3">, </span><span class="s4">2.0</span><span class="s1">]])</span><span class="s3">, </span><span class="s1">clf2.score_samples([[</span><span class="s4">2.0</span><span class="s3">, </span><span class="s4">2.0</span><span class="s1">]])</span>
    <span class="s1">)</span>


<span class="s3">def </span><span class="s1">test_iforest_warm_start():</span>
    <span class="s0">&quot;&quot;&quot;Test iterative addition of iTrees to an iForest&quot;&quot;&quot;</span>

    <span class="s1">rng = check_random_state(</span><span class="s4">0</span><span class="s1">)</span>
    <span class="s1">X = rng.randn(</span><span class="s4">20</span><span class="s3">, </span><span class="s4">2</span><span class="s1">)</span>

    <span class="s2"># fit first 10 trees</span>
    <span class="s1">clf = IsolationForest(</span>
        <span class="s1">n_estimators=</span><span class="s4">10</span><span class="s3">, </span><span class="s1">max_samples=</span><span class="s4">20</span><span class="s3">, </span><span class="s1">random_state=rng</span><span class="s3">, </span><span class="s1">warm_start=</span><span class="s3">True</span>
    <span class="s1">)</span>
    <span class="s1">clf.fit(X)</span>
    <span class="s2"># remember the 1st tree</span>
    <span class="s1">tree_1 = clf.estimators_[</span><span class="s4">0</span><span class="s1">]</span>
    <span class="s2"># fit another 10 trees</span>
    <span class="s1">clf.set_params(n_estimators=</span><span class="s4">20</span><span class="s1">)</span>
    <span class="s1">clf.fit(X)</span>
    <span class="s2"># expecting 20 fitted trees and no overwritten trees</span>
    <span class="s3">assert </span><span class="s1">len(clf.estimators_) == </span><span class="s4">20</span>
    <span class="s3">assert </span><span class="s1">clf.estimators_[</span><span class="s4">0</span><span class="s1">] </span><span class="s3">is </span><span class="s1">tree_1</span>


<span class="s2"># mock get_chunk_n_rows to actually test more than one chunk (here one</span>
<span class="s2"># chunk has 3 rows):</span>
<span class="s1">@patch(</span>
    <span class="s5">&quot;sklearn.ensemble._iforest.get_chunk_n_rows&quot;</span><span class="s3">,</span>
    <span class="s1">side_effect=Mock(**{</span><span class="s5">&quot;return_value&quot;</span><span class="s1">: </span><span class="s4">3</span><span class="s1">})</span><span class="s3">,</span>
<span class="s1">)</span>
<span class="s1">@pytest.mark.parametrize(</span><span class="s5">&quot;contamination, n_predict_calls&quot;</span><span class="s3">, </span><span class="s1">[(</span><span class="s4">0.25</span><span class="s3">, </span><span class="s4">3</span><span class="s1">)</span><span class="s3">, </span><span class="s1">(</span><span class="s5">&quot;auto&quot;</span><span class="s3">, </span><span class="s4">2</span><span class="s1">)])</span>
<span class="s3">def </span><span class="s1">test_iforest_chunks_works1(</span>
    <span class="s1">mocked_get_chunk</span><span class="s3">, </span><span class="s1">contamination</span><span class="s3">, </span><span class="s1">n_predict_calls</span><span class="s3">, </span><span class="s1">global_random_seed</span>
<span class="s1">):</span>
    <span class="s1">test_iforest_works(contamination</span><span class="s3">, </span><span class="s1">global_random_seed)</span>
    <span class="s3">assert </span><span class="s1">mocked_get_chunk.call_count == n_predict_calls</span>


<span class="s2"># idem with chunk_size = 10 rows</span>
<span class="s1">@patch(</span>
    <span class="s5">&quot;sklearn.ensemble._iforest.get_chunk_n_rows&quot;</span><span class="s3">,</span>
    <span class="s1">side_effect=Mock(**{</span><span class="s5">&quot;return_value&quot;</span><span class="s1">: </span><span class="s4">10</span><span class="s1">})</span><span class="s3">,</span>
<span class="s1">)</span>
<span class="s1">@pytest.mark.parametrize(</span><span class="s5">&quot;contamination, n_predict_calls&quot;</span><span class="s3">, </span><span class="s1">[(</span><span class="s4">0.25</span><span class="s3">, </span><span class="s4">3</span><span class="s1">)</span><span class="s3">, </span><span class="s1">(</span><span class="s5">&quot;auto&quot;</span><span class="s3">, </span><span class="s4">2</span><span class="s1">)])</span>
<span class="s3">def </span><span class="s1">test_iforest_chunks_works2(</span>
    <span class="s1">mocked_get_chunk</span><span class="s3">, </span><span class="s1">contamination</span><span class="s3">, </span><span class="s1">n_predict_calls</span><span class="s3">, </span><span class="s1">global_random_seed</span>
<span class="s1">):</span>
    <span class="s1">test_iforest_works(contamination</span><span class="s3">, </span><span class="s1">global_random_seed)</span>
    <span class="s3">assert </span><span class="s1">mocked_get_chunk.call_count == n_predict_calls</span>


<span class="s3">def </span><span class="s1">test_iforest_with_uniform_data():</span>
    <span class="s0">&quot;&quot;&quot;Test whether iforest predicts inliers when using uniform data&quot;&quot;&quot;</span>

    <span class="s2"># 2-d array of all 1s</span>
    <span class="s1">X = np.ones((</span><span class="s4">100</span><span class="s3">, </span><span class="s4">10</span><span class="s1">))</span>
    <span class="s1">iforest = IsolationForest()</span>
    <span class="s1">iforest.fit(X)</span>

    <span class="s1">rng = np.random.RandomState(</span><span class="s4">0</span><span class="s1">)</span>

    <span class="s3">assert </span><span class="s1">all(iforest.predict(X) == </span><span class="s4">1</span><span class="s1">)</span>
    <span class="s3">assert </span><span class="s1">all(iforest.predict(rng.randn(</span><span class="s4">100</span><span class="s3">, </span><span class="s4">10</span><span class="s1">)) == </span><span class="s4">1</span><span class="s1">)</span>
    <span class="s3">assert </span><span class="s1">all(iforest.predict(X + </span><span class="s4">1</span><span class="s1">) == </span><span class="s4">1</span><span class="s1">)</span>
    <span class="s3">assert </span><span class="s1">all(iforest.predict(X - </span><span class="s4">1</span><span class="s1">) == </span><span class="s4">1</span><span class="s1">)</span>

    <span class="s2"># 2-d array where columns contain the same value across rows</span>
    <span class="s1">X = np.repeat(rng.randn(</span><span class="s4">1</span><span class="s3">, </span><span class="s4">10</span><span class="s1">)</span><span class="s3">, </span><span class="s4">100</span><span class="s3">, </span><span class="s4">0</span><span class="s1">)</span>
    <span class="s1">iforest = IsolationForest()</span>
    <span class="s1">iforest.fit(X)</span>

    <span class="s3">assert </span><span class="s1">all(iforest.predict(X) == </span><span class="s4">1</span><span class="s1">)</span>
    <span class="s3">assert </span><span class="s1">all(iforest.predict(rng.randn(</span><span class="s4">100</span><span class="s3">, </span><span class="s4">10</span><span class="s1">)) == </span><span class="s4">1</span><span class="s1">)</span>
    <span class="s3">assert </span><span class="s1">all(iforest.predict(np.ones((</span><span class="s4">100</span><span class="s3">, </span><span class="s4">10</span><span class="s1">))) == </span><span class="s4">1</span><span class="s1">)</span>

    <span class="s2"># Single row</span>
    <span class="s1">X = rng.randn(</span><span class="s4">1</span><span class="s3">, </span><span class="s4">10</span><span class="s1">)</span>
    <span class="s1">iforest = IsolationForest()</span>
    <span class="s1">iforest.fit(X)</span>

    <span class="s3">assert </span><span class="s1">all(iforest.predict(X) == </span><span class="s4">1</span><span class="s1">)</span>
    <span class="s3">assert </span><span class="s1">all(iforest.predict(rng.randn(</span><span class="s4">100</span><span class="s3">, </span><span class="s4">10</span><span class="s1">)) == </span><span class="s4">1</span><span class="s1">)</span>
    <span class="s3">assert </span><span class="s1">all(iforest.predict(np.ones((</span><span class="s4">100</span><span class="s3">, </span><span class="s4">10</span><span class="s1">))) == </span><span class="s4">1</span><span class="s1">)</span>


<span class="s3">def </span><span class="s1">test_iforest_with_n_jobs_does_not_segfault():</span>
    <span class="s0">&quot;&quot;&quot;Check that Isolation Forest does not segfault with n_jobs=2 
 
    Non-regression test for #23252 
    &quot;&quot;&quot;</span>
    <span class="s1">X</span><span class="s3">, </span><span class="s1">_ = make_classification(n_samples=</span><span class="s4">85_000</span><span class="s3">, </span><span class="s1">n_features=</span><span class="s4">100</span><span class="s3">, </span><span class="s1">random_state=</span><span class="s4">0</span><span class="s1">)</span>
    <span class="s1">X = csc_matrix(X)</span>
    <span class="s1">IsolationForest(n_estimators=</span><span class="s4">10</span><span class="s3">, </span><span class="s1">max_samples=</span><span class="s4">256</span><span class="s3">, </span><span class="s1">n_jobs=</span><span class="s4">2</span><span class="s1">).fit(X)</span>


<span class="s2"># TODO(1.4): remove in 1.4</span>
<span class="s3">def </span><span class="s1">test_base_estimator_property_deprecated():</span>
    <span class="s1">X = np.array([[</span><span class="s4">1</span><span class="s3">, </span><span class="s4">2</span><span class="s1">]</span><span class="s3">, </span><span class="s1">[</span><span class="s4">3</span><span class="s3">, </span><span class="s4">4</span><span class="s1">]])</span>
    <span class="s1">y = np.array([</span><span class="s4">1</span><span class="s3">, </span><span class="s4">0</span><span class="s1">])</span>
    <span class="s1">model = IsolationForest()</span>
    <span class="s1">model.fit(X</span><span class="s3">, </span><span class="s1">y)</span>

    <span class="s1">warn_msg = (</span>
        <span class="s5">&quot;Attribute `base_estimator_` was deprecated in version 1.2 and &quot;</span>
        <span class="s5">&quot;will be removed in 1.4. Use `estimator_` instead.&quot;</span>
    <span class="s1">)</span>
    <span class="s3">with </span><span class="s1">pytest.warns(FutureWarning</span><span class="s3">, </span><span class="s1">match=warn_msg):</span>
        <span class="s1">model.base_estimator_</span>


<span class="s3">def </span><span class="s1">test_iforest_preserve_feature_names():</span>
    <span class="s0">&quot;&quot;&quot;Check that feature names are preserved when contamination is not &quot;auto&quot;. 
 
    Feature names are required for consistency checks during scoring. 
 
    Non-regression test for Issue #25844 
    &quot;&quot;&quot;</span>
    <span class="s1">pd = pytest.importorskip(</span><span class="s5">&quot;pandas&quot;</span><span class="s1">)</span>
    <span class="s1">rng = np.random.RandomState(</span><span class="s4">0</span><span class="s1">)</span>

    <span class="s1">X = pd.DataFrame(data=rng.randn(</span><span class="s4">4</span><span class="s1">)</span><span class="s3">, </span><span class="s1">columns=[</span><span class="s5">&quot;a&quot;</span><span class="s1">])</span>
    <span class="s1">model = IsolationForest(random_state=</span><span class="s4">0</span><span class="s3">, </span><span class="s1">contamination=</span><span class="s4">0.05</span><span class="s1">)</span>

    <span class="s3">with </span><span class="s1">warnings.catch_warnings():</span>
        <span class="s1">warnings.simplefilter(</span><span class="s5">&quot;error&quot;</span><span class="s3">, </span><span class="s1">UserWarning)</span>
        <span class="s1">model.fit(X)</span>
</pre>
</body>
</html>