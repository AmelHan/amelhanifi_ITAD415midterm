<html>
<head>
<title>test_successive_halving.py</title>
<meta http-equiv="Content-Type" content="text/html; charset=utf-8">
<style type="text/css">
.s0 { color: #cc7832;}
.s1 { color: #a9b7c6;}
.s2 { color: #808080;}
.s3 { color: #629755; font-style: italic;}
.s4 { color: #6a8759;}
.s5 { color: #6897bb;}
</style>
</head>
<body bgcolor="#2b2b2b">
<table CELLSPACING=0 CELLPADDING=5 COLS=1 WIDTH="100%" BGCOLOR="#606060" >
<tr><td><center>
<font face="Arial, Helvetica" color="#000000">
test_successive_halving.py</font>
</center></td></tr></table>
<pre><span class="s0">from </span><span class="s1">math </span><span class="s0">import </span><span class="s1">ceil</span>

<span class="s0">import </span><span class="s1">numpy </span><span class="s0">as </span><span class="s1">np</span>
<span class="s0">import </span><span class="s1">pytest</span>
<span class="s0">from </span><span class="s1">scipy.stats </span><span class="s0">import </span><span class="s1">expon</span><span class="s0">, </span><span class="s1">norm</span><span class="s0">, </span><span class="s1">randint</span>

<span class="s0">from </span><span class="s1">sklearn.datasets </span><span class="s0">import </span><span class="s1">make_classification</span>
<span class="s0">from </span><span class="s1">sklearn.dummy </span><span class="s0">import </span><span class="s1">DummyClassifier</span>
<span class="s0">from </span><span class="s1">sklearn.experimental </span><span class="s0">import </span><span class="s1">enable_halving_search_cv  </span><span class="s2"># noqa</span>
<span class="s0">from </span><span class="s1">sklearn.model_selection </span><span class="s0">import </span><span class="s1">(</span>
    <span class="s1">GroupKFold</span><span class="s0">,</span>
    <span class="s1">GroupShuffleSplit</span><span class="s0">,</span>
    <span class="s1">HalvingGridSearchCV</span><span class="s0">,</span>
    <span class="s1">HalvingRandomSearchCV</span><span class="s0">,</span>
    <span class="s1">KFold</span><span class="s0">,</span>
    <span class="s1">LeaveOneGroupOut</span><span class="s0">,</span>
    <span class="s1">LeavePGroupsOut</span><span class="s0">,</span>
    <span class="s1">ShuffleSplit</span><span class="s0">,</span>
    <span class="s1">StratifiedKFold</span><span class="s0">,</span>
    <span class="s1">StratifiedShuffleSplit</span><span class="s0">,</span>
<span class="s1">)</span>
<span class="s0">from </span><span class="s1">sklearn.model_selection._search_successive_halving </span><span class="s0">import </span><span class="s1">(</span>
    <span class="s1">_SubsampleMetaSplitter</span><span class="s0">,</span>
    <span class="s1">_top_k</span><span class="s0">,</span>
<span class="s1">)</span>
<span class="s0">from </span><span class="s1">sklearn.model_selection.tests.test_search </span><span class="s0">import </span><span class="s1">(</span>
    <span class="s1">check_cv_results_array_types</span><span class="s0">,</span>
    <span class="s1">check_cv_results_keys</span><span class="s0">,</span>
<span class="s1">)</span>
<span class="s0">from </span><span class="s1">sklearn.svm </span><span class="s0">import </span><span class="s1">SVC</span><span class="s0">, </span><span class="s1">LinearSVC</span>


<span class="s0">class </span><span class="s1">FastClassifier(DummyClassifier):</span>
    <span class="s3">&quot;&quot;&quot;Dummy classifier that accepts parameters a, b, ... z. 
 
    These parameter don't affect the predictions and are useful for fast 
    grid searching.&quot;&quot;&quot;</span>

    <span class="s2"># update the constraints such that we accept all parameters from a to z</span>
    <span class="s1">_parameter_constraints: dict = {</span>
        <span class="s1">**DummyClassifier._parameter_constraints</span><span class="s0">,</span>
        <span class="s1">**{</span>
            <span class="s1">chr(key): </span><span class="s4">&quot;no_validation&quot;  </span><span class="s2"># type: ignore</span>
            <span class="s0">for </span><span class="s1">key </span><span class="s0">in </span><span class="s1">range(ord(</span><span class="s4">&quot;a&quot;</span><span class="s1">)</span><span class="s0">, </span><span class="s1">ord(</span><span class="s4">&quot;z&quot;</span><span class="s1">) + </span><span class="s5">1</span><span class="s1">)</span>
        <span class="s1">}</span><span class="s0">,</span>
    <span class="s1">}</span>

    <span class="s0">def </span><span class="s1">__init__(</span>
        <span class="s1">self</span><span class="s0">, </span><span class="s1">strategy=</span><span class="s4">&quot;stratified&quot;</span><span class="s0">, </span><span class="s1">random_state=</span><span class="s0">None, </span><span class="s1">constant=</span><span class="s0">None, </span><span class="s1">**kwargs</span>
    <span class="s1">):</span>
        <span class="s1">super().__init__(</span>
            <span class="s1">strategy=strategy</span><span class="s0">, </span><span class="s1">random_state=random_state</span><span class="s0">, </span><span class="s1">constant=constant</span>
        <span class="s1">)</span>

    <span class="s0">def </span><span class="s1">get_params(self</span><span class="s0">, </span><span class="s1">deep=</span><span class="s0">False</span><span class="s1">):</span>
        <span class="s1">params = super().get_params(deep=deep)</span>
        <span class="s0">for </span><span class="s1">char </span><span class="s0">in </span><span class="s1">range(ord(</span><span class="s4">&quot;a&quot;</span><span class="s1">)</span><span class="s0">, </span><span class="s1">ord(</span><span class="s4">&quot;z&quot;</span><span class="s1">) + </span><span class="s5">1</span><span class="s1">):</span>
            <span class="s1">params[chr(char)] = </span><span class="s4">&quot;whatever&quot;</span>
        <span class="s0">return </span><span class="s1">params</span>


<span class="s0">class </span><span class="s1">SometimesFailClassifier(DummyClassifier):</span>
    <span class="s0">def </span><span class="s1">__init__(</span>
        <span class="s1">self</span><span class="s0">,</span>
        <span class="s1">strategy=</span><span class="s4">&quot;stratified&quot;</span><span class="s0">,</span>
        <span class="s1">random_state=</span><span class="s0">None,</span>
        <span class="s1">constant=</span><span class="s0">None,</span>
        <span class="s1">n_estimators=</span><span class="s5">10</span><span class="s0">,</span>
        <span class="s1">fail_fit=</span><span class="s0">False,</span>
        <span class="s1">fail_predict=</span><span class="s0">False,</span>
        <span class="s1">a=</span><span class="s5">0</span><span class="s0">,</span>
    <span class="s1">):</span>
        <span class="s1">self.fail_fit = fail_fit</span>
        <span class="s1">self.fail_predict = fail_predict</span>
        <span class="s1">self.n_estimators = n_estimators</span>
        <span class="s1">self.a = a</span>

        <span class="s1">super().__init__(</span>
            <span class="s1">strategy=strategy</span><span class="s0">, </span><span class="s1">random_state=random_state</span><span class="s0">, </span><span class="s1">constant=constant</span>
        <span class="s1">)</span>

    <span class="s0">def </span><span class="s1">fit(self</span><span class="s0">, </span><span class="s1">X</span><span class="s0">, </span><span class="s1">y):</span>
        <span class="s0">if </span><span class="s1">self.fail_fit:</span>
            <span class="s0">raise </span><span class="s1">Exception(</span><span class="s4">&quot;fitting failed&quot;</span><span class="s1">)</span>
        <span class="s0">return </span><span class="s1">super().fit(X</span><span class="s0">, </span><span class="s1">y)</span>

    <span class="s0">def </span><span class="s1">predict(self</span><span class="s0">, </span><span class="s1">X):</span>
        <span class="s0">if </span><span class="s1">self.fail_predict:</span>
            <span class="s0">raise </span><span class="s1">Exception(</span><span class="s4">&quot;predict failed&quot;</span><span class="s1">)</span>
        <span class="s0">return </span><span class="s1">super().predict(X)</span>


<span class="s1">@pytest.mark.filterwarnings(</span><span class="s4">&quot;ignore::sklearn.exceptions.FitFailedWarning&quot;</span><span class="s1">)</span>
<span class="s1">@pytest.mark.filterwarnings(</span><span class="s4">&quot;ignore:Scoring failed:UserWarning&quot;</span><span class="s1">)</span>
<span class="s1">@pytest.mark.filterwarnings(</span><span class="s4">&quot;ignore:One or more of the:UserWarning&quot;</span><span class="s1">)</span>
<span class="s1">@pytest.mark.parametrize(</span><span class="s4">&quot;HalvingSearch&quot;</span><span class="s0">, </span><span class="s1">(HalvingGridSearchCV</span><span class="s0">, </span><span class="s1">HalvingRandomSearchCV))</span>
<span class="s1">@pytest.mark.parametrize(</span><span class="s4">&quot;fail_at&quot;</span><span class="s0">, </span><span class="s1">(</span><span class="s4">&quot;fit&quot;</span><span class="s0">, </span><span class="s4">&quot;predict&quot;</span><span class="s1">))</span>
<span class="s0">def </span><span class="s1">test_nan_handling(HalvingSearch</span><span class="s0">, </span><span class="s1">fail_at):</span>
    <span class="s3">&quot;&quot;&quot;Check the selection of the best scores in presence of failure represented by 
    NaN values.&quot;&quot;&quot;</span>
    <span class="s1">n_samples = </span><span class="s5">1_000</span>
    <span class="s1">X</span><span class="s0">, </span><span class="s1">y = make_classification(n_samples=n_samples</span><span class="s0">, </span><span class="s1">random_state=</span><span class="s5">0</span><span class="s1">)</span>

    <span class="s1">search = HalvingSearch(</span>
        <span class="s1">SometimesFailClassifier()</span><span class="s0">,</span>
        <span class="s1">{</span><span class="s4">f&quot;fail_</span><span class="s0">{</span><span class="s1">fail_at</span><span class="s0">}</span><span class="s4">&quot;</span><span class="s1">: [</span><span class="s0">False, True</span><span class="s1">]</span><span class="s0">, </span><span class="s4">&quot;a&quot;</span><span class="s1">: range(</span><span class="s5">3</span><span class="s1">)}</span><span class="s0">,</span>
        <span class="s1">resource=</span><span class="s4">&quot;n_estimators&quot;</span><span class="s0">,</span>
        <span class="s1">max_resources=</span><span class="s5">6</span><span class="s0">,</span>
        <span class="s1">min_resources=</span><span class="s5">1</span><span class="s0">,</span>
        <span class="s1">factor=</span><span class="s5">2</span><span class="s0">,</span>
    <span class="s1">)</span>

    <span class="s1">search.fit(X</span><span class="s0">, </span><span class="s1">y)</span>

    <span class="s2"># estimators that failed during fit/predict should always rank lower</span>
    <span class="s2"># than ones where the fit/predict succeeded</span>
    <span class="s0">assert not </span><span class="s1">search.best_params_[</span><span class="s4">f&quot;fail_</span><span class="s0">{</span><span class="s1">fail_at</span><span class="s0">}</span><span class="s4">&quot;</span><span class="s1">]</span>
    <span class="s1">scores = search.cv_results_[</span><span class="s4">&quot;mean_test_score&quot;</span><span class="s1">]</span>
    <span class="s1">ranks = search.cv_results_[</span><span class="s4">&quot;rank_test_score&quot;</span><span class="s1">]</span>

    <span class="s2"># some scores should be NaN</span>
    <span class="s0">assert </span><span class="s1">np.isnan(scores).any()</span>

    <span class="s1">unique_nan_ranks = np.unique(ranks[np.isnan(scores)])</span>
    <span class="s2"># all NaN scores should have the same rank</span>
    <span class="s0">assert </span><span class="s1">unique_nan_ranks.shape[</span><span class="s5">0</span><span class="s1">] == </span><span class="s5">1</span>
    <span class="s2"># NaNs should have the lowest rank</span>
    <span class="s0">assert </span><span class="s1">(unique_nan_ranks[</span><span class="s5">0</span><span class="s1">] &gt;= ranks).all()</span>


<span class="s1">@pytest.mark.parametrize(</span><span class="s4">&quot;Est&quot;</span><span class="s0">, </span><span class="s1">(HalvingGridSearchCV</span><span class="s0">, </span><span class="s1">HalvingRandomSearchCV))</span>
<span class="s1">@pytest.mark.parametrize(</span>
    <span class="s1">(</span>
        <span class="s4">&quot;aggressive_elimination,&quot;</span>
        <span class="s4">&quot;max_resources,&quot;</span>
        <span class="s4">&quot;expected_n_iterations,&quot;</span>
        <span class="s4">&quot;expected_n_required_iterations,&quot;</span>
        <span class="s4">&quot;expected_n_possible_iterations,&quot;</span>
        <span class="s4">&quot;expected_n_remaining_candidates,&quot;</span>
        <span class="s4">&quot;expected_n_candidates,&quot;</span>
        <span class="s4">&quot;expected_n_resources,&quot;</span>
    <span class="s1">)</span><span class="s0">,</span>
    <span class="s1">[</span>
        <span class="s2"># notice how it loops at the beginning</span>
        <span class="s2"># also, the number of candidates evaluated at the last iteration is</span>
        <span class="s2"># &lt;= factor</span>
        <span class="s1">(</span><span class="s0">True, </span><span class="s4">&quot;limited&quot;</span><span class="s0">, </span><span class="s5">4</span><span class="s0">, </span><span class="s5">4</span><span class="s0">, </span><span class="s5">3</span><span class="s0">, </span><span class="s5">1</span><span class="s0">, </span><span class="s1">[</span><span class="s5">60</span><span class="s0">, </span><span class="s5">20</span><span class="s0">, </span><span class="s5">7</span><span class="s0">, </span><span class="s5">3</span><span class="s1">]</span><span class="s0">, </span><span class="s1">[</span><span class="s5">20</span><span class="s0">, </span><span class="s5">20</span><span class="s0">, </span><span class="s5">60</span><span class="s0">, </span><span class="s5">180</span><span class="s1">])</span><span class="s0">,</span>
        <span class="s2"># no aggressive elimination: we end up with less iterations, and</span>
        <span class="s2"># the number of candidates at the last iter is &gt; factor, which isn't</span>
        <span class="s2"># ideal</span>
        <span class="s1">(</span><span class="s0">False, </span><span class="s4">&quot;limited&quot;</span><span class="s0">, </span><span class="s5">3</span><span class="s0">, </span><span class="s5">4</span><span class="s0">, </span><span class="s5">3</span><span class="s0">, </span><span class="s5">3</span><span class="s0">, </span><span class="s1">[</span><span class="s5">60</span><span class="s0">, </span><span class="s5">20</span><span class="s0">, </span><span class="s5">7</span><span class="s1">]</span><span class="s0">, </span><span class="s1">[</span><span class="s5">20</span><span class="s0">, </span><span class="s5">60</span><span class="s0">, </span><span class="s5">180</span><span class="s1">])</span><span class="s0">,</span>
        <span class="s2">#  # When the amount of resource isn't limited, aggressive_elimination</span>
        <span class="s2">#  # has no effect. Here the default min_resources='exhaust' will take</span>
        <span class="s2">#  # over.</span>
        <span class="s1">(</span><span class="s0">True, </span><span class="s4">&quot;unlimited&quot;</span><span class="s0">, </span><span class="s5">4</span><span class="s0">, </span><span class="s5">4</span><span class="s0">, </span><span class="s5">4</span><span class="s0">, </span><span class="s5">1</span><span class="s0">, </span><span class="s1">[</span><span class="s5">60</span><span class="s0">, </span><span class="s5">20</span><span class="s0">, </span><span class="s5">7</span><span class="s0">, </span><span class="s5">3</span><span class="s1">]</span><span class="s0">, </span><span class="s1">[</span><span class="s5">37</span><span class="s0">, </span><span class="s5">111</span><span class="s0">, </span><span class="s5">333</span><span class="s0">, </span><span class="s5">999</span><span class="s1">])</span><span class="s0">,</span>
        <span class="s1">(</span><span class="s0">False, </span><span class="s4">&quot;unlimited&quot;</span><span class="s0">, </span><span class="s5">4</span><span class="s0">, </span><span class="s5">4</span><span class="s0">, </span><span class="s5">4</span><span class="s0">, </span><span class="s5">1</span><span class="s0">, </span><span class="s1">[</span><span class="s5">60</span><span class="s0">, </span><span class="s5">20</span><span class="s0">, </span><span class="s5">7</span><span class="s0">, </span><span class="s5">3</span><span class="s1">]</span><span class="s0">, </span><span class="s1">[</span><span class="s5">37</span><span class="s0">, </span><span class="s5">111</span><span class="s0">, </span><span class="s5">333</span><span class="s0">, </span><span class="s5">999</span><span class="s1">])</span><span class="s0">,</span>
    <span class="s1">]</span><span class="s0">,</span>
<span class="s1">)</span>
<span class="s0">def </span><span class="s1">test_aggressive_elimination(</span>
    <span class="s1">Est</span><span class="s0">,</span>
    <span class="s1">aggressive_elimination</span><span class="s0">,</span>
    <span class="s1">max_resources</span><span class="s0">,</span>
    <span class="s1">expected_n_iterations</span><span class="s0">,</span>
    <span class="s1">expected_n_required_iterations</span><span class="s0">,</span>
    <span class="s1">expected_n_possible_iterations</span><span class="s0">,</span>
    <span class="s1">expected_n_remaining_candidates</span><span class="s0">,</span>
    <span class="s1">expected_n_candidates</span><span class="s0">,</span>
    <span class="s1">expected_n_resources</span><span class="s0">,</span>
<span class="s1">):</span>
    <span class="s2"># Test the aggressive_elimination parameter.</span>

    <span class="s1">n_samples = </span><span class="s5">1000</span>
    <span class="s1">X</span><span class="s0">, </span><span class="s1">y = make_classification(n_samples=n_samples</span><span class="s0">, </span><span class="s1">random_state=</span><span class="s5">0</span><span class="s1">)</span>
    <span class="s1">param_grid = {</span><span class="s4">&quot;a&quot;</span><span class="s1">: (</span><span class="s4">&quot;l1&quot;</span><span class="s0">, </span><span class="s4">&quot;l2&quot;</span><span class="s1">)</span><span class="s0">, </span><span class="s4">&quot;b&quot;</span><span class="s1">: list(range(</span><span class="s5">30</span><span class="s1">))}</span>
    <span class="s1">base_estimator = FastClassifier()</span>

    <span class="s0">if </span><span class="s1">max_resources == </span><span class="s4">&quot;limited&quot;</span><span class="s1">:</span>
        <span class="s1">max_resources = </span><span class="s5">180</span>
    <span class="s0">else</span><span class="s1">:</span>
        <span class="s1">max_resources = n_samples</span>

    <span class="s1">sh = Est(</span>
        <span class="s1">base_estimator</span><span class="s0">,</span>
        <span class="s1">param_grid</span><span class="s0">,</span>
        <span class="s1">aggressive_elimination=aggressive_elimination</span><span class="s0">,</span>
        <span class="s1">max_resources=max_resources</span><span class="s0">,</span>
        <span class="s1">factor=</span><span class="s5">3</span><span class="s0">,</span>
    <span class="s1">)</span>
    <span class="s1">sh.set_params(verbose=</span><span class="s0">True</span><span class="s1">)  </span><span class="s2"># just for test coverage</span>

    <span class="s0">if </span><span class="s1">Est </span><span class="s0">is </span><span class="s1">HalvingRandomSearchCV:</span>
        <span class="s2"># same number of candidates as with the grid</span>
        <span class="s1">sh.set_params(n_candidates=</span><span class="s5">2 </span><span class="s1">* </span><span class="s5">30</span><span class="s0">, </span><span class="s1">min_resources=</span><span class="s4">&quot;exhaust&quot;</span><span class="s1">)</span>

    <span class="s1">sh.fit(X</span><span class="s0">, </span><span class="s1">y)</span>

    <span class="s0">assert </span><span class="s1">sh.n_iterations_ == expected_n_iterations</span>
    <span class="s0">assert </span><span class="s1">sh.n_required_iterations_ == expected_n_required_iterations</span>
    <span class="s0">assert </span><span class="s1">sh.n_possible_iterations_ == expected_n_possible_iterations</span>
    <span class="s0">assert </span><span class="s1">sh.n_resources_ == expected_n_resources</span>
    <span class="s0">assert </span><span class="s1">sh.n_candidates_ == expected_n_candidates</span>
    <span class="s0">assert </span><span class="s1">sh.n_remaining_candidates_ == expected_n_remaining_candidates</span>
    <span class="s0">assert </span><span class="s1">ceil(sh.n_candidates_[-</span><span class="s5">1</span><span class="s1">] / sh.factor) == sh.n_remaining_candidates_</span>


<span class="s1">@pytest.mark.parametrize(</span><span class="s4">&quot;Est&quot;</span><span class="s0">, </span><span class="s1">(HalvingGridSearchCV</span><span class="s0">, </span><span class="s1">HalvingRandomSearchCV))</span>
<span class="s1">@pytest.mark.parametrize(</span>
    <span class="s1">(</span>
        <span class="s4">&quot;min_resources,&quot;</span>
        <span class="s4">&quot;max_resources,&quot;</span>
        <span class="s4">&quot;expected_n_iterations,&quot;</span>
        <span class="s4">&quot;expected_n_possible_iterations,&quot;</span>
        <span class="s4">&quot;expected_n_resources,&quot;</span>
    <span class="s1">)</span><span class="s0">,</span>
    <span class="s1">[</span>
        <span class="s2"># with enough resources</span>
        <span class="s1">(</span><span class="s4">&quot;smallest&quot;</span><span class="s0">, </span><span class="s4">&quot;auto&quot;</span><span class="s0">, </span><span class="s5">2</span><span class="s0">, </span><span class="s5">4</span><span class="s0">, </span><span class="s1">[</span><span class="s5">20</span><span class="s0">, </span><span class="s5">60</span><span class="s1">])</span><span class="s0">,</span>
        <span class="s2"># with enough resources but min_resources set manually</span>
        <span class="s1">(</span><span class="s5">50</span><span class="s0">, </span><span class="s4">&quot;auto&quot;</span><span class="s0">, </span><span class="s5">2</span><span class="s0">, </span><span class="s5">3</span><span class="s0">, </span><span class="s1">[</span><span class="s5">50</span><span class="s0">, </span><span class="s5">150</span><span class="s1">])</span><span class="s0">,</span>
        <span class="s2"># without enough resources, only one iteration can be done</span>
        <span class="s1">(</span><span class="s4">&quot;smallest&quot;</span><span class="s0">, </span><span class="s5">30</span><span class="s0">, </span><span class="s5">1</span><span class="s0">, </span><span class="s5">1</span><span class="s0">, </span><span class="s1">[</span><span class="s5">20</span><span class="s1">])</span><span class="s0">,</span>
        <span class="s2"># with exhaust: use as much resources as possible at the last iter</span>
        <span class="s1">(</span><span class="s4">&quot;exhaust&quot;</span><span class="s0">, </span><span class="s4">&quot;auto&quot;</span><span class="s0">, </span><span class="s5">2</span><span class="s0">, </span><span class="s5">2</span><span class="s0">, </span><span class="s1">[</span><span class="s5">333</span><span class="s0">, </span><span class="s5">999</span><span class="s1">])</span><span class="s0">,</span>
        <span class="s1">(</span><span class="s4">&quot;exhaust&quot;</span><span class="s0">, </span><span class="s5">1000</span><span class="s0">, </span><span class="s5">2</span><span class="s0">, </span><span class="s5">2</span><span class="s0">, </span><span class="s1">[</span><span class="s5">333</span><span class="s0">, </span><span class="s5">999</span><span class="s1">])</span><span class="s0">,</span>
        <span class="s1">(</span><span class="s4">&quot;exhaust&quot;</span><span class="s0">, </span><span class="s5">999</span><span class="s0">, </span><span class="s5">2</span><span class="s0">, </span><span class="s5">2</span><span class="s0">, </span><span class="s1">[</span><span class="s5">333</span><span class="s0">, </span><span class="s5">999</span><span class="s1">])</span><span class="s0">,</span>
        <span class="s1">(</span><span class="s4">&quot;exhaust&quot;</span><span class="s0">, </span><span class="s5">600</span><span class="s0">, </span><span class="s5">2</span><span class="s0">, </span><span class="s5">2</span><span class="s0">, </span><span class="s1">[</span><span class="s5">200</span><span class="s0">, </span><span class="s5">600</span><span class="s1">])</span><span class="s0">,</span>
        <span class="s1">(</span><span class="s4">&quot;exhaust&quot;</span><span class="s0">, </span><span class="s5">599</span><span class="s0">, </span><span class="s5">2</span><span class="s0">, </span><span class="s5">2</span><span class="s0">, </span><span class="s1">[</span><span class="s5">199</span><span class="s0">, </span><span class="s5">597</span><span class="s1">])</span><span class="s0">,</span>
        <span class="s1">(</span><span class="s4">&quot;exhaust&quot;</span><span class="s0">, </span><span class="s5">300</span><span class="s0">, </span><span class="s5">2</span><span class="s0">, </span><span class="s5">2</span><span class="s0">, </span><span class="s1">[</span><span class="s5">100</span><span class="s0">, </span><span class="s5">300</span><span class="s1">])</span><span class="s0">,</span>
        <span class="s1">(</span><span class="s4">&quot;exhaust&quot;</span><span class="s0">, </span><span class="s5">60</span><span class="s0">, </span><span class="s5">2</span><span class="s0">, </span><span class="s5">2</span><span class="s0">, </span><span class="s1">[</span><span class="s5">20</span><span class="s0">, </span><span class="s5">60</span><span class="s1">])</span><span class="s0">,</span>
        <span class="s1">(</span><span class="s4">&quot;exhaust&quot;</span><span class="s0">, </span><span class="s5">50</span><span class="s0">, </span><span class="s5">1</span><span class="s0">, </span><span class="s5">1</span><span class="s0">, </span><span class="s1">[</span><span class="s5">20</span><span class="s1">])</span><span class="s0">,</span>
        <span class="s1">(</span><span class="s4">&quot;exhaust&quot;</span><span class="s0">, </span><span class="s5">20</span><span class="s0">, </span><span class="s5">1</span><span class="s0">, </span><span class="s5">1</span><span class="s0">, </span><span class="s1">[</span><span class="s5">20</span><span class="s1">])</span><span class="s0">,</span>
    <span class="s1">]</span><span class="s0">,</span>
<span class="s1">)</span>
<span class="s0">def </span><span class="s1">test_min_max_resources(</span>
    <span class="s1">Est</span><span class="s0">,</span>
    <span class="s1">min_resources</span><span class="s0">,</span>
    <span class="s1">max_resources</span><span class="s0">,</span>
    <span class="s1">expected_n_iterations</span><span class="s0">,</span>
    <span class="s1">expected_n_possible_iterations</span><span class="s0">,</span>
    <span class="s1">expected_n_resources</span><span class="s0">,</span>
<span class="s1">):</span>
    <span class="s2"># Test the min_resources and max_resources parameters, and how they affect</span>
    <span class="s2"># the number of resources used at each iteration</span>
    <span class="s1">n_samples = </span><span class="s5">1000</span>
    <span class="s1">X</span><span class="s0">, </span><span class="s1">y = make_classification(n_samples=n_samples</span><span class="s0">, </span><span class="s1">random_state=</span><span class="s5">0</span><span class="s1">)</span>
    <span class="s1">param_grid = {</span><span class="s4">&quot;a&quot;</span><span class="s1">: [</span><span class="s5">1</span><span class="s0">, </span><span class="s5">2</span><span class="s1">]</span><span class="s0">, </span><span class="s4">&quot;b&quot;</span><span class="s1">: [</span><span class="s5">1</span><span class="s0">, </span><span class="s5">2</span><span class="s0">, </span><span class="s5">3</span><span class="s1">]}</span>
    <span class="s1">base_estimator = FastClassifier()</span>

    <span class="s1">sh = Est(</span>
        <span class="s1">base_estimator</span><span class="s0">,</span>
        <span class="s1">param_grid</span><span class="s0">,</span>
        <span class="s1">factor=</span><span class="s5">3</span><span class="s0">,</span>
        <span class="s1">min_resources=min_resources</span><span class="s0">,</span>
        <span class="s1">max_resources=max_resources</span><span class="s0">,</span>
    <span class="s1">)</span>
    <span class="s0">if </span><span class="s1">Est </span><span class="s0">is </span><span class="s1">HalvingRandomSearchCV:</span>
        <span class="s1">sh.set_params(n_candidates=</span><span class="s5">6</span><span class="s1">)  </span><span class="s2"># same number as with the grid</span>

    <span class="s1">sh.fit(X</span><span class="s0">, </span><span class="s1">y)</span>

    <span class="s1">expected_n_required_iterations = </span><span class="s5">2  </span><span class="s2"># given 6 combinations and factor = 3</span>
    <span class="s0">assert </span><span class="s1">sh.n_iterations_ == expected_n_iterations</span>
    <span class="s0">assert </span><span class="s1">sh.n_required_iterations_ == expected_n_required_iterations</span>
    <span class="s0">assert </span><span class="s1">sh.n_possible_iterations_ == expected_n_possible_iterations</span>
    <span class="s0">assert </span><span class="s1">sh.n_resources_ == expected_n_resources</span>
    <span class="s0">if </span><span class="s1">min_resources == </span><span class="s4">&quot;exhaust&quot;</span><span class="s1">:</span>
        <span class="s0">assert </span><span class="s1">sh.n_possible_iterations_ == sh.n_iterations_ == len(sh.n_resources_)</span>


<span class="s1">@pytest.mark.parametrize(</span><span class="s4">&quot;Est&quot;</span><span class="s0">, </span><span class="s1">(HalvingRandomSearchCV</span><span class="s0">, </span><span class="s1">HalvingGridSearchCV))</span>
<span class="s1">@pytest.mark.parametrize(</span>
    <span class="s4">&quot;max_resources, n_iterations, n_possible_iterations&quot;</span><span class="s0">,</span>
    <span class="s1">[</span>
        <span class="s1">(</span><span class="s4">&quot;auto&quot;</span><span class="s0">, </span><span class="s5">5</span><span class="s0">, </span><span class="s5">9</span><span class="s1">)</span><span class="s0">,  </span><span class="s2"># all resources are used</span>
        <span class="s1">(</span><span class="s5">1024</span><span class="s0">, </span><span class="s5">5</span><span class="s0">, </span><span class="s5">9</span><span class="s1">)</span><span class="s0">,</span>
        <span class="s1">(</span><span class="s5">700</span><span class="s0">, </span><span class="s5">5</span><span class="s0">, </span><span class="s5">8</span><span class="s1">)</span><span class="s0">,</span>
        <span class="s1">(</span><span class="s5">512</span><span class="s0">, </span><span class="s5">5</span><span class="s0">, </span><span class="s5">8</span><span class="s1">)</span><span class="s0">,</span>
        <span class="s1">(</span><span class="s5">511</span><span class="s0">, </span><span class="s5">5</span><span class="s0">, </span><span class="s5">7</span><span class="s1">)</span><span class="s0">,</span>
        <span class="s1">(</span><span class="s5">32</span><span class="s0">, </span><span class="s5">4</span><span class="s0">, </span><span class="s5">4</span><span class="s1">)</span><span class="s0">,</span>
        <span class="s1">(</span><span class="s5">31</span><span class="s0">, </span><span class="s5">3</span><span class="s0">, </span><span class="s5">3</span><span class="s1">)</span><span class="s0">,</span>
        <span class="s1">(</span><span class="s5">16</span><span class="s0">, </span><span class="s5">3</span><span class="s0">, </span><span class="s5">3</span><span class="s1">)</span><span class="s0">,</span>
        <span class="s1">(</span><span class="s5">4</span><span class="s0">, </span><span class="s5">1</span><span class="s0">, </span><span class="s5">1</span><span class="s1">)</span><span class="s0">,  </span><span class="s2"># max_resources == min_resources, only one iteration is</span>
        <span class="s2"># possible</span>
    <span class="s1">]</span><span class="s0">,</span>
<span class="s1">)</span>
<span class="s0">def </span><span class="s1">test_n_iterations(Est</span><span class="s0">, </span><span class="s1">max_resources</span><span class="s0">, </span><span class="s1">n_iterations</span><span class="s0">, </span><span class="s1">n_possible_iterations):</span>
    <span class="s2"># test the number of actual iterations that were run depending on</span>
    <span class="s2"># max_resources</span>

    <span class="s1">n_samples = </span><span class="s5">1024</span>
    <span class="s1">X</span><span class="s0">, </span><span class="s1">y = make_classification(n_samples=n_samples</span><span class="s0">, </span><span class="s1">random_state=</span><span class="s5">1</span><span class="s1">)</span>
    <span class="s1">param_grid = {</span><span class="s4">&quot;a&quot;</span><span class="s1">: [</span><span class="s5">1</span><span class="s0">, </span><span class="s5">2</span><span class="s1">]</span><span class="s0">, </span><span class="s4">&quot;b&quot;</span><span class="s1">: list(range(</span><span class="s5">10</span><span class="s1">))}</span>
    <span class="s1">base_estimator = FastClassifier()</span>
    <span class="s1">factor = </span><span class="s5">2</span>

    <span class="s1">sh = Est(</span>
        <span class="s1">base_estimator</span><span class="s0">,</span>
        <span class="s1">param_grid</span><span class="s0">,</span>
        <span class="s1">cv=</span><span class="s5">2</span><span class="s0">,</span>
        <span class="s1">factor=factor</span><span class="s0">,</span>
        <span class="s1">max_resources=max_resources</span><span class="s0">,</span>
        <span class="s1">min_resources=</span><span class="s5">4</span><span class="s0">,</span>
    <span class="s1">)</span>
    <span class="s0">if </span><span class="s1">Est </span><span class="s0">is </span><span class="s1">HalvingRandomSearchCV:</span>
        <span class="s1">sh.set_params(n_candidates=</span><span class="s5">20</span><span class="s1">)  </span><span class="s2"># same as for HalvingGridSearchCV</span>
    <span class="s1">sh.fit(X</span><span class="s0">, </span><span class="s1">y)</span>
    <span class="s0">assert </span><span class="s1">sh.n_required_iterations_ == </span><span class="s5">5</span>
    <span class="s0">assert </span><span class="s1">sh.n_iterations_ == n_iterations</span>
    <span class="s0">assert </span><span class="s1">sh.n_possible_iterations_ == n_possible_iterations</span>


<span class="s1">@pytest.mark.parametrize(</span><span class="s4">&quot;Est&quot;</span><span class="s0">, </span><span class="s1">(HalvingRandomSearchCV</span><span class="s0">, </span><span class="s1">HalvingGridSearchCV))</span>
<span class="s0">def </span><span class="s1">test_resource_parameter(Est):</span>
    <span class="s2"># Test the resource parameter</span>

    <span class="s1">n_samples = </span><span class="s5">1000</span>
    <span class="s1">X</span><span class="s0">, </span><span class="s1">y = make_classification(n_samples=n_samples</span><span class="s0">, </span><span class="s1">random_state=</span><span class="s5">0</span><span class="s1">)</span>
    <span class="s1">param_grid = {</span><span class="s4">&quot;a&quot;</span><span class="s1">: [</span><span class="s5">1</span><span class="s0">, </span><span class="s5">2</span><span class="s1">]</span><span class="s0">, </span><span class="s4">&quot;b&quot;</span><span class="s1">: list(range(</span><span class="s5">10</span><span class="s1">))}</span>
    <span class="s1">base_estimator = FastClassifier()</span>
    <span class="s1">sh = Est(base_estimator</span><span class="s0">, </span><span class="s1">param_grid</span><span class="s0">, </span><span class="s1">cv=</span><span class="s5">2</span><span class="s0">, </span><span class="s1">resource=</span><span class="s4">&quot;c&quot;</span><span class="s0">, </span><span class="s1">max_resources=</span><span class="s5">10</span><span class="s0">, </span><span class="s1">factor=</span><span class="s5">3</span><span class="s1">)</span>
    <span class="s1">sh.fit(X</span><span class="s0">, </span><span class="s1">y)</span>
    <span class="s0">assert </span><span class="s1">set(sh.n_resources_) == set([</span><span class="s5">1</span><span class="s0">, </span><span class="s5">3</span><span class="s0">, </span><span class="s5">9</span><span class="s1">])</span>
    <span class="s0">for </span><span class="s1">r_i</span><span class="s0">, </span><span class="s1">params</span><span class="s0">, </span><span class="s1">param_c </span><span class="s0">in </span><span class="s1">zip(</span>
        <span class="s1">sh.cv_results_[</span><span class="s4">&quot;n_resources&quot;</span><span class="s1">]</span><span class="s0">,</span>
        <span class="s1">sh.cv_results_[</span><span class="s4">&quot;params&quot;</span><span class="s1">]</span><span class="s0">,</span>
        <span class="s1">sh.cv_results_[</span><span class="s4">&quot;param_c&quot;</span><span class="s1">]</span><span class="s0">,</span>
    <span class="s1">):</span>
        <span class="s0">assert </span><span class="s1">r_i == params[</span><span class="s4">&quot;c&quot;</span><span class="s1">] == param_c</span>

    <span class="s0">with </span><span class="s1">pytest.raises(</span>
        <span class="s1">ValueError</span><span class="s0">, </span><span class="s1">match=</span><span class="s4">&quot;Cannot use resource=1234 which is not supported &quot;</span>
    <span class="s1">):</span>
        <span class="s1">sh = HalvingGridSearchCV(</span>
            <span class="s1">base_estimator</span><span class="s0">, </span><span class="s1">param_grid</span><span class="s0">, </span><span class="s1">cv=</span><span class="s5">2</span><span class="s0">, </span><span class="s1">resource=</span><span class="s4">&quot;1234&quot;</span><span class="s0">, </span><span class="s1">max_resources=</span><span class="s5">10</span>
        <span class="s1">)</span>
        <span class="s1">sh.fit(X</span><span class="s0">, </span><span class="s1">y)</span>

    <span class="s0">with </span><span class="s1">pytest.raises(</span>
        <span class="s1">ValueError</span><span class="s0">,</span>
        <span class="s1">match=(</span>
            <span class="s4">&quot;Cannot use parameter c as the resource since it is part &quot;</span>
            <span class="s4">&quot;of the searched parameters.&quot;</span>
        <span class="s1">)</span><span class="s0">,</span>
    <span class="s1">):</span>
        <span class="s1">param_grid = {</span><span class="s4">&quot;a&quot;</span><span class="s1">: [</span><span class="s5">1</span><span class="s0">, </span><span class="s5">2</span><span class="s1">]</span><span class="s0">, </span><span class="s4">&quot;b&quot;</span><span class="s1">: [</span><span class="s5">1</span><span class="s0">, </span><span class="s5">2</span><span class="s1">]</span><span class="s0">, </span><span class="s4">&quot;c&quot;</span><span class="s1">: [</span><span class="s5">1</span><span class="s0">, </span><span class="s5">3</span><span class="s1">]}</span>
        <span class="s1">sh = HalvingGridSearchCV(</span>
            <span class="s1">base_estimator</span><span class="s0">, </span><span class="s1">param_grid</span><span class="s0">, </span><span class="s1">cv=</span><span class="s5">2</span><span class="s0">, </span><span class="s1">resource=</span><span class="s4">&quot;c&quot;</span><span class="s0">, </span><span class="s1">max_resources=</span><span class="s5">10</span>
        <span class="s1">)</span>
        <span class="s1">sh.fit(X</span><span class="s0">, </span><span class="s1">y)</span>


<span class="s1">@pytest.mark.parametrize(</span>
    <span class="s4">&quot;max_resources, n_candidates, expected_n_candidates&quot;</span><span class="s0">,</span>
    <span class="s1">[</span>
        <span class="s1">(</span><span class="s5">512</span><span class="s0">, </span><span class="s4">&quot;exhaust&quot;</span><span class="s0">, </span><span class="s5">128</span><span class="s1">)</span><span class="s0">,  </span><span class="s2"># generate exactly as much as needed</span>
        <span class="s1">(</span><span class="s5">32</span><span class="s0">, </span><span class="s4">&quot;exhaust&quot;</span><span class="s0">, </span><span class="s5">8</span><span class="s1">)</span><span class="s0">,</span>
        <span class="s1">(</span><span class="s5">32</span><span class="s0">, </span><span class="s5">8</span><span class="s0">, </span><span class="s5">8</span><span class="s1">)</span><span class="s0">,</span>
        <span class="s1">(</span><span class="s5">32</span><span class="s0">, </span><span class="s5">7</span><span class="s0">, </span><span class="s5">7</span><span class="s1">)</span><span class="s0">,  </span><span class="s2"># ask for less than what we could</span>
        <span class="s1">(</span><span class="s5">32</span><span class="s0">, </span><span class="s5">9</span><span class="s0">, </span><span class="s5">9</span><span class="s1">)</span><span class="s0">,  </span><span class="s2"># ask for more than 'reasonable'</span>
    <span class="s1">]</span><span class="s0">,</span>
<span class="s1">)</span>
<span class="s0">def </span><span class="s1">test_random_search(max_resources</span><span class="s0">, </span><span class="s1">n_candidates</span><span class="s0">, </span><span class="s1">expected_n_candidates):</span>
    <span class="s2"># Test random search and make sure the number of generated candidates is</span>
    <span class="s2"># as expected</span>

    <span class="s1">n_samples = </span><span class="s5">1024</span>
    <span class="s1">X</span><span class="s0">, </span><span class="s1">y = make_classification(n_samples=n_samples</span><span class="s0">, </span><span class="s1">random_state=</span><span class="s5">0</span><span class="s1">)</span>
    <span class="s1">param_grid = {</span><span class="s4">&quot;a&quot;</span><span class="s1">: norm</span><span class="s0">, </span><span class="s4">&quot;b&quot;</span><span class="s1">: norm}</span>
    <span class="s1">base_estimator = FastClassifier()</span>
    <span class="s1">sh = HalvingRandomSearchCV(</span>
        <span class="s1">base_estimator</span><span class="s0">,</span>
        <span class="s1">param_grid</span><span class="s0">,</span>
        <span class="s1">n_candidates=n_candidates</span><span class="s0">,</span>
        <span class="s1">cv=</span><span class="s5">2</span><span class="s0">,</span>
        <span class="s1">max_resources=max_resources</span><span class="s0">,</span>
        <span class="s1">factor=</span><span class="s5">2</span><span class="s0">,</span>
        <span class="s1">min_resources=</span><span class="s5">4</span><span class="s0">,</span>
    <span class="s1">)</span>
    <span class="s1">sh.fit(X</span><span class="s0">, </span><span class="s1">y)</span>
    <span class="s0">assert </span><span class="s1">sh.n_candidates_[</span><span class="s5">0</span><span class="s1">] == expected_n_candidates</span>
    <span class="s0">if </span><span class="s1">n_candidates == </span><span class="s4">&quot;exhaust&quot;</span><span class="s1">:</span>
        <span class="s2"># Make sure 'exhaust' makes the last iteration use as much resources as</span>
        <span class="s2"># we can</span>
        <span class="s0">assert </span><span class="s1">sh.n_resources_[-</span><span class="s5">1</span><span class="s1">] == max_resources</span>


<span class="s1">@pytest.mark.parametrize(</span>
    <span class="s4">&quot;param_distributions, expected_n_candidates&quot;</span><span class="s0">,</span>
    <span class="s1">[</span>
        <span class="s1">({</span><span class="s4">&quot;a&quot;</span><span class="s1">: [</span><span class="s5">1</span><span class="s0">, </span><span class="s5">2</span><span class="s1">]}</span><span class="s0">, </span><span class="s5">2</span><span class="s1">)</span><span class="s0">,  </span><span class="s2"># all lists, sample less than n_candidates</span>
        <span class="s1">({</span><span class="s4">&quot;a&quot;</span><span class="s1">: randint(</span><span class="s5">1</span><span class="s0">, </span><span class="s5">3</span><span class="s1">)}</span><span class="s0">, </span><span class="s5">10</span><span class="s1">)</span><span class="s0">,  </span><span class="s2"># not all list, respect n_candidates</span>
    <span class="s1">]</span><span class="s0">,</span>
<span class="s1">)</span>
<span class="s0">def </span><span class="s1">test_random_search_discrete_distributions(</span>
    <span class="s1">param_distributions</span><span class="s0">, </span><span class="s1">expected_n_candidates</span>
<span class="s1">):</span>
    <span class="s2"># Make sure random search samples the appropriate number of candidates when</span>
    <span class="s2"># we ask for more than what's possible. How many parameters are sampled</span>
    <span class="s2"># depends whether the distributions are 'all lists' or not (see</span>
    <span class="s2"># ParameterSampler for details). This is somewhat redundant with the checks</span>
    <span class="s2"># in ParameterSampler but interaction bugs were discovered during</span>
    <span class="s2"># development of SH</span>

    <span class="s1">n_samples = </span><span class="s5">1024</span>
    <span class="s1">X</span><span class="s0">, </span><span class="s1">y = make_classification(n_samples=n_samples</span><span class="s0">, </span><span class="s1">random_state=</span><span class="s5">0</span><span class="s1">)</span>
    <span class="s1">base_estimator = FastClassifier()</span>
    <span class="s1">sh = HalvingRandomSearchCV(base_estimator</span><span class="s0">, </span><span class="s1">param_distributions</span><span class="s0">, </span><span class="s1">n_candidates=</span><span class="s5">10</span><span class="s1">)</span>
    <span class="s1">sh.fit(X</span><span class="s0">, </span><span class="s1">y)</span>
    <span class="s0">assert </span><span class="s1">sh.n_candidates_[</span><span class="s5">0</span><span class="s1">] == expected_n_candidates</span>


<span class="s1">@pytest.mark.parametrize(</span><span class="s4">&quot;Est&quot;</span><span class="s0">, </span><span class="s1">(HalvingGridSearchCV</span><span class="s0">, </span><span class="s1">HalvingRandomSearchCV))</span>
<span class="s1">@pytest.mark.parametrize(</span>
    <span class="s4">&quot;params, expected_error_message&quot;</span><span class="s0">,</span>
    <span class="s1">[</span>
        <span class="s1">(</span>
            <span class="s1">{</span><span class="s4">&quot;resource&quot;</span><span class="s1">: </span><span class="s4">&quot;not_a_parameter&quot;</span><span class="s1">}</span><span class="s0">,</span>
            <span class="s4">&quot;Cannot use resource=not_a_parameter which is not supported&quot;</span><span class="s0">,</span>
        <span class="s1">)</span><span class="s0">,</span>
        <span class="s1">(</span>
            <span class="s1">{</span><span class="s4">&quot;resource&quot;</span><span class="s1">: </span><span class="s4">&quot;a&quot;</span><span class="s0">, </span><span class="s4">&quot;max_resources&quot;</span><span class="s1">: </span><span class="s5">100</span><span class="s1">}</span><span class="s0">,</span>
            <span class="s4">&quot;Cannot use parameter a as the resource since it is part of&quot;</span><span class="s0">,</span>
        <span class="s1">)</span><span class="s0">,</span>
        <span class="s1">(</span>
            <span class="s1">{</span><span class="s4">&quot;max_resources&quot;</span><span class="s1">: </span><span class="s4">&quot;auto&quot;</span><span class="s0">, </span><span class="s4">&quot;resource&quot;</span><span class="s1">: </span><span class="s4">&quot;b&quot;</span><span class="s1">}</span><span class="s0">,</span>
            <span class="s4">&quot;resource can only be 'n_samples' when max_resources='auto'&quot;</span><span class="s0">,</span>
        <span class="s1">)</span><span class="s0">,</span>
        <span class="s1">(</span>
            <span class="s1">{</span><span class="s4">&quot;min_resources&quot;</span><span class="s1">: </span><span class="s5">15</span><span class="s0">, </span><span class="s4">&quot;max_resources&quot;</span><span class="s1">: </span><span class="s5">14</span><span class="s1">}</span><span class="s0">,</span>
            <span class="s4">&quot;min_resources_=15 is greater than max_resources_=14&quot;</span><span class="s0">,</span>
        <span class="s1">)</span><span class="s0">,</span>
        <span class="s1">({</span><span class="s4">&quot;cv&quot;</span><span class="s1">: KFold(shuffle=</span><span class="s0">True</span><span class="s1">)}</span><span class="s0">, </span><span class="s4">&quot;must yield consistent folds&quot;</span><span class="s1">)</span><span class="s0">,</span>
        <span class="s1">({</span><span class="s4">&quot;cv&quot;</span><span class="s1">: ShuffleSplit()}</span><span class="s0">, </span><span class="s4">&quot;must yield consistent folds&quot;</span><span class="s1">)</span><span class="s0">,</span>
    <span class="s1">]</span><span class="s0">,</span>
<span class="s1">)</span>
<span class="s0">def </span><span class="s1">test_input_errors(Est</span><span class="s0">, </span><span class="s1">params</span><span class="s0">, </span><span class="s1">expected_error_message):</span>
    <span class="s1">base_estimator = FastClassifier()</span>
    <span class="s1">param_grid = {</span><span class="s4">&quot;a&quot;</span><span class="s1">: [</span><span class="s5">1</span><span class="s1">]}</span>
    <span class="s1">X</span><span class="s0">, </span><span class="s1">y = make_classification(</span><span class="s5">100</span><span class="s1">)</span>

    <span class="s1">sh = Est(base_estimator</span><span class="s0">, </span><span class="s1">param_grid</span><span class="s0">, </span><span class="s1">**params)</span>

    <span class="s0">with </span><span class="s1">pytest.raises(ValueError</span><span class="s0">, </span><span class="s1">match=expected_error_message):</span>
        <span class="s1">sh.fit(X</span><span class="s0">, </span><span class="s1">y)</span>


<span class="s1">@pytest.mark.parametrize(</span>
    <span class="s4">&quot;params, expected_error_message&quot;</span><span class="s0">,</span>
    <span class="s1">[</span>
        <span class="s1">(</span>
            <span class="s1">{</span><span class="s4">&quot;n_candidates&quot;</span><span class="s1">: </span><span class="s4">&quot;exhaust&quot;</span><span class="s0">, </span><span class="s4">&quot;min_resources&quot;</span><span class="s1">: </span><span class="s4">&quot;exhaust&quot;</span><span class="s1">}</span><span class="s0">,</span>
            <span class="s4">&quot;cannot be both set to 'exhaust'&quot;</span><span class="s0">,</span>
        <span class="s1">)</span><span class="s0">,</span>
    <span class="s1">]</span><span class="s0">,</span>
<span class="s1">)</span>
<span class="s0">def </span><span class="s1">test_input_errors_randomized(params</span><span class="s0">, </span><span class="s1">expected_error_message):</span>
    <span class="s2"># tests specific to HalvingRandomSearchCV</span>

    <span class="s1">base_estimator = FastClassifier()</span>
    <span class="s1">param_grid = {</span><span class="s4">&quot;a&quot;</span><span class="s1">: [</span><span class="s5">1</span><span class="s1">]}</span>
    <span class="s1">X</span><span class="s0">, </span><span class="s1">y = make_classification(</span><span class="s5">100</span><span class="s1">)</span>

    <span class="s1">sh = HalvingRandomSearchCV(base_estimator</span><span class="s0">, </span><span class="s1">param_grid</span><span class="s0">, </span><span class="s1">**params)</span>

    <span class="s0">with </span><span class="s1">pytest.raises(ValueError</span><span class="s0">, </span><span class="s1">match=expected_error_message):</span>
        <span class="s1">sh.fit(X</span><span class="s0">, </span><span class="s1">y)</span>


<span class="s1">@pytest.mark.parametrize(</span>
    <span class="s4">&quot;fraction, subsample_test, expected_train_size, expected_test_size&quot;</span><span class="s0">,</span>
    <span class="s1">[</span>
        <span class="s1">(</span><span class="s5">0.5</span><span class="s0">, True, </span><span class="s5">40</span><span class="s0">, </span><span class="s5">10</span><span class="s1">)</span><span class="s0">,</span>
        <span class="s1">(</span><span class="s5">0.5</span><span class="s0">, False, </span><span class="s5">40</span><span class="s0">, </span><span class="s5">20</span><span class="s1">)</span><span class="s0">,</span>
        <span class="s1">(</span><span class="s5">0.2</span><span class="s0">, True, </span><span class="s5">16</span><span class="s0">, </span><span class="s5">4</span><span class="s1">)</span><span class="s0">,</span>
        <span class="s1">(</span><span class="s5">0.2</span><span class="s0">, False, </span><span class="s5">16</span><span class="s0">, </span><span class="s5">20</span><span class="s1">)</span><span class="s0">,</span>
    <span class="s1">]</span><span class="s0">,</span>
<span class="s1">)</span>
<span class="s0">def </span><span class="s1">test_subsample_splitter_shapes(</span>
    <span class="s1">fraction</span><span class="s0">, </span><span class="s1">subsample_test</span><span class="s0">, </span><span class="s1">expected_train_size</span><span class="s0">, </span><span class="s1">expected_test_size</span>
<span class="s1">):</span>
    <span class="s2"># Make sure splits returned by SubsampleMetaSplitter are of appropriate</span>
    <span class="s2"># size</span>

    <span class="s1">n_samples = </span><span class="s5">100</span>
    <span class="s1">X</span><span class="s0">, </span><span class="s1">y = make_classification(n_samples)</span>
    <span class="s1">cv = _SubsampleMetaSplitter(</span>
        <span class="s1">base_cv=KFold(</span><span class="s5">5</span><span class="s1">)</span><span class="s0">,</span>
        <span class="s1">fraction=fraction</span><span class="s0">,</span>
        <span class="s1">subsample_test=subsample_test</span><span class="s0">,</span>
        <span class="s1">random_state=</span><span class="s0">None,</span>
    <span class="s1">)</span>

    <span class="s0">for </span><span class="s1">train</span><span class="s0">, </span><span class="s1">test </span><span class="s0">in </span><span class="s1">cv.split(X</span><span class="s0">, </span><span class="s1">y):</span>
        <span class="s0">assert </span><span class="s1">train.shape[</span><span class="s5">0</span><span class="s1">] == expected_train_size</span>
        <span class="s0">assert </span><span class="s1">test.shape[</span><span class="s5">0</span><span class="s1">] == expected_test_size</span>
        <span class="s0">if </span><span class="s1">subsample_test:</span>
            <span class="s0">assert </span><span class="s1">train.shape[</span><span class="s5">0</span><span class="s1">] + test.shape[</span><span class="s5">0</span><span class="s1">] == int(n_samples * fraction)</span>
        <span class="s0">else</span><span class="s1">:</span>
            <span class="s0">assert </span><span class="s1">test.shape[</span><span class="s5">0</span><span class="s1">] == n_samples // cv.base_cv.get_n_splits()</span>


<span class="s1">@pytest.mark.parametrize(</span><span class="s4">&quot;subsample_test&quot;</span><span class="s0">, </span><span class="s1">(</span><span class="s0">True, False</span><span class="s1">))</span>
<span class="s0">def </span><span class="s1">test_subsample_splitter_determinism(subsample_test):</span>
    <span class="s2"># Make sure _SubsampleMetaSplitter is consistent across calls to split():</span>
    <span class="s2"># - we're OK having training sets differ (they're always sampled with a</span>
    <span class="s2">#   different fraction anyway)</span>
    <span class="s2"># - when we don't subsample the test set, we want it to be always the same.</span>
    <span class="s2">#   This check is the most important. This is ensured by the determinism</span>
    <span class="s2">#   of the base_cv.</span>

    <span class="s2"># Note: we could force both train and test splits to be always the same if</span>
    <span class="s2"># we drew an int seed in _SubsampleMetaSplitter.__init__</span>

    <span class="s1">n_samples = </span><span class="s5">100</span>
    <span class="s1">X</span><span class="s0">, </span><span class="s1">y = make_classification(n_samples)</span>
    <span class="s1">cv = _SubsampleMetaSplitter(</span>
        <span class="s1">base_cv=KFold(</span><span class="s5">5</span><span class="s1">)</span><span class="s0">, </span><span class="s1">fraction=</span><span class="s5">0.5</span><span class="s0">, </span><span class="s1">subsample_test=subsample_test</span><span class="s0">, </span><span class="s1">random_state=</span><span class="s0">None</span>
    <span class="s1">)</span>

    <span class="s1">folds_a = list(cv.split(X</span><span class="s0">, </span><span class="s1">y</span><span class="s0">, </span><span class="s1">groups=</span><span class="s0">None</span><span class="s1">))</span>
    <span class="s1">folds_b = list(cv.split(X</span><span class="s0">, </span><span class="s1">y</span><span class="s0">, </span><span class="s1">groups=</span><span class="s0">None</span><span class="s1">))</span>

    <span class="s0">for </span><span class="s1">(train_a</span><span class="s0">, </span><span class="s1">test_a)</span><span class="s0">, </span><span class="s1">(train_b</span><span class="s0">, </span><span class="s1">test_b) </span><span class="s0">in </span><span class="s1">zip(folds_a</span><span class="s0">, </span><span class="s1">folds_b):</span>
        <span class="s0">assert not </span><span class="s1">np.all(train_a == train_b)</span>

        <span class="s0">if </span><span class="s1">subsample_test:</span>
            <span class="s0">assert not </span><span class="s1">np.all(test_a == test_b)</span>
        <span class="s0">else</span><span class="s1">:</span>
            <span class="s0">assert </span><span class="s1">np.all(test_a == test_b)</span>
            <span class="s0">assert </span><span class="s1">np.all(X[test_a] == X[test_b])</span>


<span class="s1">@pytest.mark.parametrize(</span>
    <span class="s4">&quot;k, itr, expected&quot;</span><span class="s0">,</span>
    <span class="s1">[</span>
        <span class="s1">(</span><span class="s5">1</span><span class="s0">, </span><span class="s5">0</span><span class="s0">, </span><span class="s1">[</span><span class="s4">&quot;c&quot;</span><span class="s1">])</span><span class="s0">,</span>
        <span class="s1">(</span><span class="s5">2</span><span class="s0">, </span><span class="s5">0</span><span class="s0">, </span><span class="s1">[</span><span class="s4">&quot;a&quot;</span><span class="s0">, </span><span class="s4">&quot;c&quot;</span><span class="s1">])</span><span class="s0">,</span>
        <span class="s1">(</span><span class="s5">4</span><span class="s0">, </span><span class="s5">0</span><span class="s0">, </span><span class="s1">[</span><span class="s4">&quot;d&quot;</span><span class="s0">, </span><span class="s4">&quot;b&quot;</span><span class="s0">, </span><span class="s4">&quot;a&quot;</span><span class="s0">, </span><span class="s4">&quot;c&quot;</span><span class="s1">])</span><span class="s0">,</span>
        <span class="s1">(</span><span class="s5">10</span><span class="s0">, </span><span class="s5">0</span><span class="s0">, </span><span class="s1">[</span><span class="s4">&quot;d&quot;</span><span class="s0">, </span><span class="s4">&quot;b&quot;</span><span class="s0">, </span><span class="s4">&quot;a&quot;</span><span class="s0">, </span><span class="s4">&quot;c&quot;</span><span class="s1">])</span><span class="s0">,</span>
        <span class="s1">(</span><span class="s5">1</span><span class="s0">, </span><span class="s5">1</span><span class="s0">, </span><span class="s1">[</span><span class="s4">&quot;e&quot;</span><span class="s1">])</span><span class="s0">,</span>
        <span class="s1">(</span><span class="s5">2</span><span class="s0">, </span><span class="s5">1</span><span class="s0">, </span><span class="s1">[</span><span class="s4">&quot;f&quot;</span><span class="s0">, </span><span class="s4">&quot;e&quot;</span><span class="s1">])</span><span class="s0">,</span>
        <span class="s1">(</span><span class="s5">10</span><span class="s0">, </span><span class="s5">1</span><span class="s0">, </span><span class="s1">[</span><span class="s4">&quot;f&quot;</span><span class="s0">, </span><span class="s4">&quot;e&quot;</span><span class="s1">])</span><span class="s0">,</span>
        <span class="s1">(</span><span class="s5">1</span><span class="s0">, </span><span class="s5">2</span><span class="s0">, </span><span class="s1">[</span><span class="s4">&quot;i&quot;</span><span class="s1">])</span><span class="s0">,</span>
        <span class="s1">(</span><span class="s5">10</span><span class="s0">, </span><span class="s5">2</span><span class="s0">, </span><span class="s1">[</span><span class="s4">&quot;g&quot;</span><span class="s0">, </span><span class="s4">&quot;h&quot;</span><span class="s0">, </span><span class="s4">&quot;i&quot;</span><span class="s1">])</span><span class="s0">,</span>
    <span class="s1">]</span><span class="s0">,</span>
<span class="s1">)</span>
<span class="s0">def </span><span class="s1">test_top_k(k</span><span class="s0">, </span><span class="s1">itr</span><span class="s0">, </span><span class="s1">expected):</span>
    <span class="s1">results = {  </span><span class="s2"># this isn't a 'real world' result dict</span>
        <span class="s4">&quot;iter&quot;</span><span class="s1">: [</span><span class="s5">0</span><span class="s0">, </span><span class="s5">0</span><span class="s0">, </span><span class="s5">0</span><span class="s0">, </span><span class="s5">0</span><span class="s0">, </span><span class="s5">1</span><span class="s0">, </span><span class="s5">1</span><span class="s0">, </span><span class="s5">2</span><span class="s0">, </span><span class="s5">2</span><span class="s0">, </span><span class="s5">2</span><span class="s1">]</span><span class="s0">,</span>
        <span class="s4">&quot;mean_test_score&quot;</span><span class="s1">: [</span><span class="s5">4</span><span class="s0">, </span><span class="s5">3</span><span class="s0">, </span><span class="s5">5</span><span class="s0">, </span><span class="s5">1</span><span class="s0">, </span><span class="s5">11</span><span class="s0">, </span><span class="s5">10</span><span class="s0">, </span><span class="s5">5</span><span class="s0">, </span><span class="s5">6</span><span class="s0">, </span><span class="s5">9</span><span class="s1">]</span><span class="s0">,</span>
        <span class="s4">&quot;params&quot;</span><span class="s1">: [</span><span class="s4">&quot;a&quot;</span><span class="s0">, </span><span class="s4">&quot;b&quot;</span><span class="s0">, </span><span class="s4">&quot;c&quot;</span><span class="s0">, </span><span class="s4">&quot;d&quot;</span><span class="s0">, </span><span class="s4">&quot;e&quot;</span><span class="s0">, </span><span class="s4">&quot;f&quot;</span><span class="s0">, </span><span class="s4">&quot;g&quot;</span><span class="s0">, </span><span class="s4">&quot;h&quot;</span><span class="s0">, </span><span class="s4">&quot;i&quot;</span><span class="s1">]</span><span class="s0">,</span>
    <span class="s1">}</span>
    <span class="s1">got = _top_k(results</span><span class="s0">, </span><span class="s1">k=k</span><span class="s0">, </span><span class="s1">itr=itr)</span>
    <span class="s0">assert </span><span class="s1">np.all(got == expected)</span>


<span class="s1">@pytest.mark.parametrize(</span><span class="s4">&quot;Est&quot;</span><span class="s0">, </span><span class="s1">(HalvingRandomSearchCV</span><span class="s0">, </span><span class="s1">HalvingGridSearchCV))</span>
<span class="s0">def </span><span class="s1">test_cv_results(Est):</span>
    <span class="s2"># test that the cv_results_ matches correctly the logic of the</span>
    <span class="s2"># tournament: in particular that the candidates continued in each</span>
    <span class="s2"># successive iteration are those that were best in the previous iteration</span>
    <span class="s1">pd = pytest.importorskip(</span><span class="s4">&quot;pandas&quot;</span><span class="s1">)</span>

    <span class="s1">rng = np.random.RandomState(</span><span class="s5">0</span><span class="s1">)</span>

    <span class="s1">n_samples = </span><span class="s5">1000</span>
    <span class="s1">X</span><span class="s0">, </span><span class="s1">y = make_classification(n_samples=n_samples</span><span class="s0">, </span><span class="s1">random_state=</span><span class="s5">0</span><span class="s1">)</span>
    <span class="s1">param_grid = {</span><span class="s4">&quot;a&quot;</span><span class="s1">: (</span><span class="s4">&quot;l1&quot;</span><span class="s0">, </span><span class="s4">&quot;l2&quot;</span><span class="s1">)</span><span class="s0">, </span><span class="s4">&quot;b&quot;</span><span class="s1">: list(range(</span><span class="s5">30</span><span class="s1">))}</span>
    <span class="s1">base_estimator = FastClassifier()</span>

    <span class="s2"># generate random scores: we want to avoid ties, which would otherwise</span>
    <span class="s2"># mess with the ordering and make testing harder</span>
    <span class="s0">def </span><span class="s1">scorer(est</span><span class="s0">, </span><span class="s1">X</span><span class="s0">, </span><span class="s1">y):</span>
        <span class="s0">return </span><span class="s1">rng.rand()</span>

    <span class="s1">sh = Est(base_estimator</span><span class="s0">, </span><span class="s1">param_grid</span><span class="s0">, </span><span class="s1">factor=</span><span class="s5">2</span><span class="s0">, </span><span class="s1">scoring=scorer)</span>
    <span class="s0">if </span><span class="s1">Est </span><span class="s0">is </span><span class="s1">HalvingRandomSearchCV:</span>
        <span class="s2"># same number of candidates as with the grid</span>
        <span class="s1">sh.set_params(n_candidates=</span><span class="s5">2 </span><span class="s1">* </span><span class="s5">30</span><span class="s0">, </span><span class="s1">min_resources=</span><span class="s4">&quot;exhaust&quot;</span><span class="s1">)</span>

    <span class="s1">sh.fit(X</span><span class="s0">, </span><span class="s1">y)</span>

    <span class="s2"># non-regression check for</span>
    <span class="s2"># https://github.com/scikit-learn/scikit-learn/issues/19203</span>
    <span class="s0">assert </span><span class="s1">isinstance(sh.cv_results_[</span><span class="s4">&quot;iter&quot;</span><span class="s1">]</span><span class="s0">, </span><span class="s1">np.ndarray)</span>
    <span class="s0">assert </span><span class="s1">isinstance(sh.cv_results_[</span><span class="s4">&quot;n_resources&quot;</span><span class="s1">]</span><span class="s0">, </span><span class="s1">np.ndarray)</span>

    <span class="s1">cv_results_df = pd.DataFrame(sh.cv_results_)</span>

    <span class="s2"># just make sure we don't have ties</span>
    <span class="s0">assert </span><span class="s1">len(cv_results_df[</span><span class="s4">&quot;mean_test_score&quot;</span><span class="s1">].unique()) == len(cv_results_df)</span>

    <span class="s1">cv_results_df[</span><span class="s4">&quot;params_str&quot;</span><span class="s1">] = cv_results_df[</span><span class="s4">&quot;params&quot;</span><span class="s1">].apply(str)</span>
    <span class="s1">table = cv_results_df.pivot(</span>
        <span class="s1">index=</span><span class="s4">&quot;params_str&quot;</span><span class="s0">, </span><span class="s1">columns=</span><span class="s4">&quot;iter&quot;</span><span class="s0">, </span><span class="s1">values=</span><span class="s4">&quot;mean_test_score&quot;</span>
    <span class="s1">)</span>

    <span class="s2"># table looks like something like this:</span>
    <span class="s2"># iter                    0      1       2        3   4   5</span>
    <span class="s2"># params_str</span>
    <span class="s2"># {'a': 'l2', 'b': 23} 0.75    NaN     NaN      NaN NaN NaN</span>
    <span class="s2"># {'a': 'l1', 'b': 30} 0.90  0.875     NaN      NaN NaN NaN</span>
    <span class="s2"># {'a': 'l1', 'b': 0}  0.75    NaN     NaN      NaN NaN NaN</span>
    <span class="s2"># {'a': 'l2', 'b': 3}  0.85  0.925  0.9125  0.90625 NaN NaN</span>
    <span class="s2"># {'a': 'l1', 'b': 5}  0.80    NaN     NaN      NaN NaN NaN</span>
    <span class="s2"># ...</span>

    <span class="s2"># where a NaN indicates that the candidate wasn't evaluated at a given</span>
    <span class="s2"># iteration, because it wasn't part of the top-K at some previous</span>
    <span class="s2"># iteration. We here make sure that candidates that aren't in the top-k at</span>
    <span class="s2"># any given iteration are indeed not evaluated at the subsequent</span>
    <span class="s2"># iterations.</span>
    <span class="s1">nan_mask = pd.isna(table)</span>
    <span class="s1">n_iter = sh.n_iterations_</span>
    <span class="s0">for </span><span class="s1">it </span><span class="s0">in </span><span class="s1">range(n_iter - </span><span class="s5">1</span><span class="s1">):</span>
        <span class="s1">already_discarded_mask = nan_mask[it]</span>

        <span class="s2"># make sure that if a candidate is already discarded, we don't evaluate</span>
        <span class="s2"># it later</span>
        <span class="s0">assert </span><span class="s1">(</span>
            <span class="s1">already_discarded_mask &amp; nan_mask[it + </span><span class="s5">1</span><span class="s1">] == already_discarded_mask</span>
        <span class="s1">).all()</span>

        <span class="s2"># make sure that the number of discarded candidate is correct</span>
        <span class="s1">discarded_now_mask = ~already_discarded_mask &amp; nan_mask[it + </span><span class="s5">1</span><span class="s1">]</span>
        <span class="s1">kept_mask = ~already_discarded_mask &amp; ~discarded_now_mask</span>
        <span class="s0">assert </span><span class="s1">kept_mask.sum() == sh.n_candidates_[it + </span><span class="s5">1</span><span class="s1">]</span>

        <span class="s2"># make sure that all discarded candidates have a lower score than the</span>
        <span class="s2"># kept candidates</span>
        <span class="s1">discarded_max_score = table[it].where(discarded_now_mask).max()</span>
        <span class="s1">kept_min_score = table[it].where(kept_mask).min()</span>
        <span class="s0">assert </span><span class="s1">discarded_max_score &lt; kept_min_score</span>

    <span class="s2"># We now make sure that the best candidate is chosen only from the last</span>
    <span class="s2"># iteration.</span>
    <span class="s2"># We also make sure this is true even if there were higher scores in</span>
    <span class="s2"># earlier rounds (this isn't generally the case, but worth ensuring it's</span>
    <span class="s2"># possible).</span>

    <span class="s1">last_iter = cv_results_df[</span><span class="s4">&quot;iter&quot;</span><span class="s1">].max()</span>
    <span class="s1">idx_best_last_iter = cv_results_df[cv_results_df[</span><span class="s4">&quot;iter&quot;</span><span class="s1">] == last_iter][</span>
        <span class="s4">&quot;mean_test_score&quot;</span>
    <span class="s1">].idxmax()</span>
    <span class="s1">idx_best_all_iters = cv_results_df[</span><span class="s4">&quot;mean_test_score&quot;</span><span class="s1">].idxmax()</span>

    <span class="s0">assert </span><span class="s1">sh.best_params_ == cv_results_df.iloc[idx_best_last_iter][</span><span class="s4">&quot;params&quot;</span><span class="s1">]</span>
    <span class="s0">assert </span><span class="s1">(</span>
        <span class="s1">cv_results_df.iloc[idx_best_last_iter][</span><span class="s4">&quot;mean_test_score&quot;</span><span class="s1">]</span>
        <span class="s1">&lt; cv_results_df.iloc[idx_best_all_iters][</span><span class="s4">&quot;mean_test_score&quot;</span><span class="s1">]</span>
    <span class="s1">)</span>
    <span class="s0">assert </span><span class="s1">(</span>
        <span class="s1">cv_results_df.iloc[idx_best_last_iter][</span><span class="s4">&quot;params&quot;</span><span class="s1">]</span>
        <span class="s1">!= cv_results_df.iloc[idx_best_all_iters][</span><span class="s4">&quot;params&quot;</span><span class="s1">]</span>
    <span class="s1">)</span>


<span class="s1">@pytest.mark.parametrize(</span><span class="s4">&quot;Est&quot;</span><span class="s0">, </span><span class="s1">(HalvingGridSearchCV</span><span class="s0">, </span><span class="s1">HalvingRandomSearchCV))</span>
<span class="s0">def </span><span class="s1">test_base_estimator_inputs(Est):</span>
    <span class="s2"># make sure that the base estimators are passed the correct parameters and</span>
    <span class="s2"># number of samples at each iteration.</span>
    <span class="s1">pd = pytest.importorskip(</span><span class="s4">&quot;pandas&quot;</span><span class="s1">)</span>

    <span class="s1">passed_n_samples_fit = []</span>
    <span class="s1">passed_n_samples_predict = []</span>
    <span class="s1">passed_params = []</span>

    <span class="s0">class </span><span class="s1">FastClassifierBookKeeping(FastClassifier):</span>
        <span class="s0">def </span><span class="s1">fit(self</span><span class="s0">, </span><span class="s1">X</span><span class="s0">, </span><span class="s1">y):</span>
            <span class="s1">passed_n_samples_fit.append(X.shape[</span><span class="s5">0</span><span class="s1">])</span>
            <span class="s0">return </span><span class="s1">super().fit(X</span><span class="s0">, </span><span class="s1">y)</span>

        <span class="s0">def </span><span class="s1">predict(self</span><span class="s0">, </span><span class="s1">X):</span>
            <span class="s1">passed_n_samples_predict.append(X.shape[</span><span class="s5">0</span><span class="s1">])</span>
            <span class="s0">return </span><span class="s1">super().predict(X)</span>

        <span class="s0">def </span><span class="s1">set_params(self</span><span class="s0">, </span><span class="s1">**params):</span>
            <span class="s1">passed_params.append(params)</span>
            <span class="s0">return </span><span class="s1">super().set_params(**params)</span>

    <span class="s1">n_samples = </span><span class="s5">1024</span>
    <span class="s1">n_splits = </span><span class="s5">2</span>
    <span class="s1">X</span><span class="s0">, </span><span class="s1">y = make_classification(n_samples=n_samples</span><span class="s0">, </span><span class="s1">random_state=</span><span class="s5">0</span><span class="s1">)</span>
    <span class="s1">param_grid = {</span><span class="s4">&quot;a&quot;</span><span class="s1">: (</span><span class="s4">&quot;l1&quot;</span><span class="s0">, </span><span class="s4">&quot;l2&quot;</span><span class="s1">)</span><span class="s0">, </span><span class="s4">&quot;b&quot;</span><span class="s1">: list(range(</span><span class="s5">30</span><span class="s1">))}</span>
    <span class="s1">base_estimator = FastClassifierBookKeeping()</span>

    <span class="s1">sh = Est(</span>
        <span class="s1">base_estimator</span><span class="s0">,</span>
        <span class="s1">param_grid</span><span class="s0">,</span>
        <span class="s1">factor=</span><span class="s5">2</span><span class="s0">,</span>
        <span class="s1">cv=n_splits</span><span class="s0">,</span>
        <span class="s1">return_train_score=</span><span class="s0">False,</span>
        <span class="s1">refit=</span><span class="s0">False,</span>
    <span class="s1">)</span>
    <span class="s0">if </span><span class="s1">Est </span><span class="s0">is </span><span class="s1">HalvingRandomSearchCV:</span>
        <span class="s2"># same number of candidates as with the grid</span>
        <span class="s1">sh.set_params(n_candidates=</span><span class="s5">2 </span><span class="s1">* </span><span class="s5">30</span><span class="s0">, </span><span class="s1">min_resources=</span><span class="s4">&quot;exhaust&quot;</span><span class="s1">)</span>

    <span class="s1">sh.fit(X</span><span class="s0">, </span><span class="s1">y)</span>

    <span class="s0">assert </span><span class="s1">len(passed_n_samples_fit) == len(passed_n_samples_predict)</span>
    <span class="s1">passed_n_samples = [</span>
        <span class="s1">x + y </span><span class="s0">for </span><span class="s1">(x</span><span class="s0">, </span><span class="s1">y) </span><span class="s0">in </span><span class="s1">zip(passed_n_samples_fit</span><span class="s0">, </span><span class="s1">passed_n_samples_predict)</span>
    <span class="s1">]</span>

    <span class="s2"># Lists are of length n_splits * n_iter * n_candidates_at_i.</span>
    <span class="s2"># Each chunk of size n_splits corresponds to the n_splits folds for the</span>
    <span class="s2"># same candidate at the same iteration, so they contain equal values. We</span>
    <span class="s2"># subsample such that the lists are of length n_iter * n_candidates_at_it</span>
    <span class="s1">passed_n_samples = passed_n_samples[::n_splits]</span>
    <span class="s1">passed_params = passed_params[::n_splits]</span>

    <span class="s1">cv_results_df = pd.DataFrame(sh.cv_results_)</span>

    <span class="s0">assert </span><span class="s1">len(passed_params) == len(passed_n_samples) == len(cv_results_df)</span>

    <span class="s1">uniques</span><span class="s0">, </span><span class="s1">counts = np.unique(passed_n_samples</span><span class="s0">, </span><span class="s1">return_counts=</span><span class="s0">True</span><span class="s1">)</span>
    <span class="s0">assert </span><span class="s1">(sh.n_resources_ == uniques).all()</span>
    <span class="s0">assert </span><span class="s1">(sh.n_candidates_ == counts).all()</span>

    <span class="s0">assert </span><span class="s1">(cv_results_df[</span><span class="s4">&quot;params&quot;</span><span class="s1">] == passed_params).all()</span>
    <span class="s0">assert </span><span class="s1">(cv_results_df[</span><span class="s4">&quot;n_resources&quot;</span><span class="s1">] == passed_n_samples).all()</span>


<span class="s1">@pytest.mark.parametrize(</span><span class="s4">&quot;Est&quot;</span><span class="s0">, </span><span class="s1">(HalvingGridSearchCV</span><span class="s0">, </span><span class="s1">HalvingRandomSearchCV))</span>
<span class="s0">def </span><span class="s1">test_groups_support(Est):</span>
    <span class="s2"># Check if ValueError (when groups is None) propagates to</span>
    <span class="s2"># HalvingGridSearchCV and HalvingRandomSearchCV</span>
    <span class="s2"># And also check if groups is correctly passed to the cv object</span>
    <span class="s1">rng = np.random.RandomState(</span><span class="s5">0</span><span class="s1">)</span>

    <span class="s1">X</span><span class="s0">, </span><span class="s1">y = make_classification(n_samples=</span><span class="s5">50</span><span class="s0">, </span><span class="s1">n_classes=</span><span class="s5">2</span><span class="s0">, </span><span class="s1">random_state=</span><span class="s5">0</span><span class="s1">)</span>
    <span class="s1">groups = rng.randint(</span><span class="s5">0</span><span class="s0">, </span><span class="s5">3</span><span class="s0">, </span><span class="s5">50</span><span class="s1">)</span>

    <span class="s1">clf = LinearSVC(dual=</span><span class="s4">&quot;auto&quot;</span><span class="s0">, </span><span class="s1">random_state=</span><span class="s5">0</span><span class="s1">)</span>
    <span class="s1">grid = {</span><span class="s4">&quot;C&quot;</span><span class="s1">: [</span><span class="s5">1</span><span class="s1">]}</span>

    <span class="s1">group_cvs = [</span>
        <span class="s1">LeaveOneGroupOut()</span><span class="s0">,</span>
        <span class="s1">LeavePGroupsOut(</span><span class="s5">2</span><span class="s1">)</span><span class="s0">,</span>
        <span class="s1">GroupKFold(n_splits=</span><span class="s5">3</span><span class="s1">)</span><span class="s0">,</span>
        <span class="s1">GroupShuffleSplit(random_state=</span><span class="s5">0</span><span class="s1">)</span><span class="s0">,</span>
    <span class="s1">]</span>
    <span class="s1">error_msg = </span><span class="s4">&quot;The 'groups' parameter should not be None.&quot;</span>
    <span class="s0">for </span><span class="s1">cv </span><span class="s0">in </span><span class="s1">group_cvs:</span>
        <span class="s1">gs = Est(clf</span><span class="s0">, </span><span class="s1">grid</span><span class="s0">, </span><span class="s1">cv=cv</span><span class="s0">, </span><span class="s1">random_state=</span><span class="s5">0</span><span class="s1">)</span>
        <span class="s0">with </span><span class="s1">pytest.raises(ValueError</span><span class="s0">, </span><span class="s1">match=error_msg):</span>
            <span class="s1">gs.fit(X</span><span class="s0">, </span><span class="s1">y)</span>
        <span class="s1">gs.fit(X</span><span class="s0">, </span><span class="s1">y</span><span class="s0">, </span><span class="s1">groups=groups)</span>

    <span class="s1">non_group_cvs = [StratifiedKFold()</span><span class="s0">, </span><span class="s1">StratifiedShuffleSplit(random_state=</span><span class="s5">0</span><span class="s1">)]</span>
    <span class="s0">for </span><span class="s1">cv </span><span class="s0">in </span><span class="s1">non_group_cvs:</span>
        <span class="s1">gs = Est(clf</span><span class="s0">, </span><span class="s1">grid</span><span class="s0">, </span><span class="s1">cv=cv)</span>
        <span class="s2"># Should not raise an error</span>
        <span class="s1">gs.fit(X</span><span class="s0">, </span><span class="s1">y)</span>


<span class="s1">@pytest.mark.parametrize(</span><span class="s4">&quot;SearchCV&quot;</span><span class="s0">, </span><span class="s1">[HalvingRandomSearchCV</span><span class="s0">, </span><span class="s1">HalvingGridSearchCV])</span>
<span class="s0">def </span><span class="s1">test_min_resources_null(SearchCV):</span>
    <span class="s3">&quot;&quot;&quot;Check that we raise an error if the minimum resources is set to 0.&quot;&quot;&quot;</span>
    <span class="s1">base_estimator = FastClassifier()</span>
    <span class="s1">param_grid = {</span><span class="s4">&quot;a&quot;</span><span class="s1">: [</span><span class="s5">1</span><span class="s1">]}</span>
    <span class="s1">X = np.empty(</span><span class="s5">0</span><span class="s1">).reshape(</span><span class="s5">0</span><span class="s0">, </span><span class="s5">3</span><span class="s1">)</span>

    <span class="s1">search = SearchCV(base_estimator</span><span class="s0">, </span><span class="s1">param_grid</span><span class="s0">, </span><span class="s1">min_resources=</span><span class="s4">&quot;smallest&quot;</span><span class="s1">)</span>

    <span class="s1">err_msg = </span><span class="s4">&quot;min_resources_=0: you might have passed an empty dataset X.&quot;</span>
    <span class="s0">with </span><span class="s1">pytest.raises(ValueError</span><span class="s0">, </span><span class="s1">match=err_msg):</span>
        <span class="s1">search.fit(X</span><span class="s0">, </span><span class="s1">[])</span>


<span class="s1">@pytest.mark.parametrize(</span><span class="s4">&quot;SearchCV&quot;</span><span class="s0">, </span><span class="s1">[HalvingGridSearchCV</span><span class="s0">, </span><span class="s1">HalvingRandomSearchCV])</span>
<span class="s0">def </span><span class="s1">test_select_best_index(SearchCV):</span>
    <span class="s3">&quot;&quot;&quot;Check the selection strategy of the halving search.&quot;&quot;&quot;</span>
    <span class="s1">results = {  </span><span class="s2"># this isn't a 'real world' result dict</span>
        <span class="s4">&quot;iter&quot;</span><span class="s1">: np.array([</span><span class="s5">0</span><span class="s0">, </span><span class="s5">0</span><span class="s0">, </span><span class="s5">0</span><span class="s0">, </span><span class="s5">0</span><span class="s0">, </span><span class="s5">1</span><span class="s0">, </span><span class="s5">1</span><span class="s0">, </span><span class="s5">2</span><span class="s0">, </span><span class="s5">2</span><span class="s0">, </span><span class="s5">2</span><span class="s1">])</span><span class="s0">,</span>
        <span class="s4">&quot;mean_test_score&quot;</span><span class="s1">: np.array([</span><span class="s5">4</span><span class="s0">, </span><span class="s5">3</span><span class="s0">, </span><span class="s5">5</span><span class="s0">, </span><span class="s5">1</span><span class="s0">, </span><span class="s5">11</span><span class="s0">, </span><span class="s5">10</span><span class="s0">, </span><span class="s5">5</span><span class="s0">, </span><span class="s5">6</span><span class="s0">, </span><span class="s5">9</span><span class="s1">])</span><span class="s0">,</span>
        <span class="s4">&quot;params&quot;</span><span class="s1">: np.array([</span><span class="s4">&quot;a&quot;</span><span class="s0">, </span><span class="s4">&quot;b&quot;</span><span class="s0">, </span><span class="s4">&quot;c&quot;</span><span class="s0">, </span><span class="s4">&quot;d&quot;</span><span class="s0">, </span><span class="s4">&quot;e&quot;</span><span class="s0">, </span><span class="s4">&quot;f&quot;</span><span class="s0">, </span><span class="s4">&quot;g&quot;</span><span class="s0">, </span><span class="s4">&quot;h&quot;</span><span class="s0">, </span><span class="s4">&quot;i&quot;</span><span class="s1">])</span><span class="s0">,</span>
    <span class="s1">}</span>

    <span class="s2"># we expect the index of 'i'</span>
    <span class="s1">best_index = SearchCV._select_best_index(</span><span class="s0">None, None, </span><span class="s1">results)</span>
    <span class="s0">assert </span><span class="s1">best_index == </span><span class="s5">8</span>


<span class="s0">def </span><span class="s1">test_halving_random_search_list_of_dicts():</span>
    <span class="s3">&quot;&quot;&quot;Check the behaviour of the `HalvingRandomSearchCV` with `param_distribution` 
    being a list of dictionary. 
    &quot;&quot;&quot;</span>
    <span class="s1">X</span><span class="s0">, </span><span class="s1">y = make_classification(n_samples=</span><span class="s5">150</span><span class="s0">, </span><span class="s1">n_features=</span><span class="s5">4</span><span class="s0">, </span><span class="s1">random_state=</span><span class="s5">42</span><span class="s1">)</span>

    <span class="s1">params = [</span>
        <span class="s1">{</span><span class="s4">&quot;kernel&quot;</span><span class="s1">: [</span><span class="s4">&quot;rbf&quot;</span><span class="s1">]</span><span class="s0">, </span><span class="s4">&quot;C&quot;</span><span class="s1">: expon(scale=</span><span class="s5">10</span><span class="s1">)</span><span class="s0">, </span><span class="s4">&quot;gamma&quot;</span><span class="s1">: expon(scale=</span><span class="s5">0.1</span><span class="s1">)}</span><span class="s0">,</span>
        <span class="s1">{</span><span class="s4">&quot;kernel&quot;</span><span class="s1">: [</span><span class="s4">&quot;poly&quot;</span><span class="s1">]</span><span class="s0">, </span><span class="s4">&quot;degree&quot;</span><span class="s1">: [</span><span class="s5">2</span><span class="s0">, </span><span class="s5">3</span><span class="s1">]}</span><span class="s0">,</span>
    <span class="s1">]</span>
    <span class="s1">param_keys = (</span>
        <span class="s4">&quot;param_C&quot;</span><span class="s0">,</span>
        <span class="s4">&quot;param_degree&quot;</span><span class="s0">,</span>
        <span class="s4">&quot;param_gamma&quot;</span><span class="s0">,</span>
        <span class="s4">&quot;param_kernel&quot;</span><span class="s0">,</span>
    <span class="s1">)</span>
    <span class="s1">score_keys = (</span>
        <span class="s4">&quot;mean_test_score&quot;</span><span class="s0">,</span>
        <span class="s4">&quot;mean_train_score&quot;</span><span class="s0">,</span>
        <span class="s4">&quot;rank_test_score&quot;</span><span class="s0">,</span>
        <span class="s4">&quot;split0_test_score&quot;</span><span class="s0">,</span>
        <span class="s4">&quot;split1_test_score&quot;</span><span class="s0">,</span>
        <span class="s4">&quot;split2_test_score&quot;</span><span class="s0">,</span>
        <span class="s4">&quot;split0_train_score&quot;</span><span class="s0">,</span>
        <span class="s4">&quot;split1_train_score&quot;</span><span class="s0">,</span>
        <span class="s4">&quot;split2_train_score&quot;</span><span class="s0">,</span>
        <span class="s4">&quot;std_test_score&quot;</span><span class="s0">,</span>
        <span class="s4">&quot;std_train_score&quot;</span><span class="s0">,</span>
        <span class="s4">&quot;mean_fit_time&quot;</span><span class="s0">,</span>
        <span class="s4">&quot;std_fit_time&quot;</span><span class="s0">,</span>
        <span class="s4">&quot;mean_score_time&quot;</span><span class="s0">,</span>
        <span class="s4">&quot;std_score_time&quot;</span><span class="s0">,</span>
    <span class="s1">)</span>
    <span class="s1">extra_keys = (</span><span class="s4">&quot;n_resources&quot;</span><span class="s0">, </span><span class="s4">&quot;iter&quot;</span><span class="s1">)</span>

    <span class="s1">search = HalvingRandomSearchCV(</span>
        <span class="s1">SVC()</span><span class="s0">, </span><span class="s1">cv=</span><span class="s5">3</span><span class="s0">, </span><span class="s1">param_distributions=params</span><span class="s0">, </span><span class="s1">return_train_score=</span><span class="s0">True, </span><span class="s1">random_state=</span><span class="s5">0</span>
    <span class="s1">)</span>
    <span class="s1">search.fit(X</span><span class="s0">, </span><span class="s1">y)</span>
    <span class="s1">n_candidates = sum(search.n_candidates_)</span>
    <span class="s1">cv_results = search.cv_results_</span>
    <span class="s2"># Check results structure</span>
    <span class="s1">check_cv_results_keys(cv_results</span><span class="s0">, </span><span class="s1">param_keys</span><span class="s0">, </span><span class="s1">score_keys</span><span class="s0">, </span><span class="s1">n_candidates</span><span class="s0">, </span><span class="s1">extra_keys)</span>
    <span class="s1">check_cv_results_array_types(search</span><span class="s0">, </span><span class="s1">param_keys</span><span class="s0">, </span><span class="s1">score_keys)</span>

    <span class="s0">assert </span><span class="s1">all(</span>
        <span class="s1">(</span>
            <span class="s1">cv_results[</span><span class="s4">&quot;param_C&quot;</span><span class="s1">].mask[i]</span>
            <span class="s0">and </span><span class="s1">cv_results[</span><span class="s4">&quot;param_gamma&quot;</span><span class="s1">].mask[i]</span>
            <span class="s0">and not </span><span class="s1">cv_results[</span><span class="s4">&quot;param_degree&quot;</span><span class="s1">].mask[i]</span>
        <span class="s1">)</span>
        <span class="s0">for </span><span class="s1">i </span><span class="s0">in </span><span class="s1">range(n_candidates)</span>
        <span class="s0">if </span><span class="s1">cv_results[</span><span class="s4">&quot;param_kernel&quot;</span><span class="s1">][i] == </span><span class="s4">&quot;poly&quot;</span>
    <span class="s1">)</span>
    <span class="s0">assert </span><span class="s1">all(</span>
        <span class="s1">(</span>
            <span class="s0">not </span><span class="s1">cv_results[</span><span class="s4">&quot;param_C&quot;</span><span class="s1">].mask[i]</span>
            <span class="s0">and not </span><span class="s1">cv_results[</span><span class="s4">&quot;param_gamma&quot;</span><span class="s1">].mask[i]</span>
            <span class="s0">and </span><span class="s1">cv_results[</span><span class="s4">&quot;param_degree&quot;</span><span class="s1">].mask[i]</span>
        <span class="s1">)</span>
        <span class="s0">for </span><span class="s1">i </span><span class="s0">in </span><span class="s1">range(n_candidates)</span>
        <span class="s0">if </span><span class="s1">cv_results[</span><span class="s4">&quot;param_kernel&quot;</span><span class="s1">][i] == </span><span class="s4">&quot;rbf&quot;</span>
    <span class="s1">)</span>
</pre>
</body>
</html>