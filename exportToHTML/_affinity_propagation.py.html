<html>
<head>
<title>_affinity_propagation.py</title>
<meta http-equiv="Content-Type" content="text/html; charset=utf-8">
<style type="text/css">
.s0 { color: #629755; font-style: italic;}
.s1 { color: #a9b7c6;}
.s2 { color: #808080;}
.s3 { color: #cc7832;}
.s4 { color: #6897bb;}
.s5 { color: #6a8759;}
</style>
</head>
<body bgcolor="#2b2b2b">
<table CELLSPACING=0 CELLPADDING=5 COLS=1 WIDTH="100%" BGCOLOR="#606060" >
<tr><td><center>
<font face="Arial, Helvetica" color="#000000">
_affinity_propagation.py</font>
</center></td></tr></table>
<pre><span class="s0">&quot;&quot;&quot;Affinity Propagation clustering algorithm.&quot;&quot;&quot;</span>

<span class="s2"># Author: Alexandre Gramfort alexandre.gramfort@inria.fr</span>
<span class="s2">#        Gael Varoquaux gael.varoquaux@normalesup.org</span>

<span class="s2"># License: BSD 3 clause</span>

<span class="s3">import </span><span class="s1">warnings</span>
<span class="s3">from </span><span class="s1">numbers </span><span class="s3">import </span><span class="s1">Integral</span><span class="s3">, </span><span class="s1">Real</span>

<span class="s3">import </span><span class="s1">numpy </span><span class="s3">as </span><span class="s1">np</span>

<span class="s3">from </span><span class="s1">.._config </span><span class="s3">import </span><span class="s1">config_context</span>
<span class="s3">from </span><span class="s1">..base </span><span class="s3">import </span><span class="s1">BaseEstimator</span><span class="s3">, </span><span class="s1">ClusterMixin</span><span class="s3">, </span><span class="s1">_fit_context</span>
<span class="s3">from </span><span class="s1">..exceptions </span><span class="s3">import </span><span class="s1">ConvergenceWarning</span>
<span class="s3">from </span><span class="s1">..metrics </span><span class="s3">import </span><span class="s1">euclidean_distances</span><span class="s3">, </span><span class="s1">pairwise_distances_argmin</span>
<span class="s3">from </span><span class="s1">..utils </span><span class="s3">import </span><span class="s1">check_random_state</span>
<span class="s3">from </span><span class="s1">..utils._param_validation </span><span class="s3">import </span><span class="s1">Interval</span><span class="s3">, </span><span class="s1">StrOptions</span><span class="s3">, </span><span class="s1">validate_params</span>
<span class="s3">from </span><span class="s1">..utils.validation </span><span class="s3">import </span><span class="s1">check_is_fitted</span>


<span class="s3">def </span><span class="s1">_equal_similarities_and_preferences(S</span><span class="s3">, </span><span class="s1">preference):</span>
    <span class="s3">def </span><span class="s1">all_equal_preferences():</span>
        <span class="s3">return </span><span class="s1">np.all(preference == preference.flat[</span><span class="s4">0</span><span class="s1">])</span>

    <span class="s3">def </span><span class="s1">all_equal_similarities():</span>
        <span class="s2"># Create mask to ignore diagonal of S</span>
        <span class="s1">mask = np.ones(S.shape</span><span class="s3">, </span><span class="s1">dtype=bool)</span>
        <span class="s1">np.fill_diagonal(mask</span><span class="s3">, </span><span class="s4">0</span><span class="s1">)</span>

        <span class="s3">return </span><span class="s1">np.all(S[mask].flat == S[mask].flat[</span><span class="s4">0</span><span class="s1">])</span>

    <span class="s3">return </span><span class="s1">all_equal_preferences() </span><span class="s3">and </span><span class="s1">all_equal_similarities()</span>


<span class="s3">def </span><span class="s1">_affinity_propagation(</span>
    <span class="s1">S</span><span class="s3">,</span>
    <span class="s1">*</span><span class="s3">,</span>
    <span class="s1">preference</span><span class="s3">,</span>
    <span class="s1">convergence_iter</span><span class="s3">,</span>
    <span class="s1">max_iter</span><span class="s3">,</span>
    <span class="s1">damping</span><span class="s3">,</span>
    <span class="s1">verbose</span><span class="s3">,</span>
    <span class="s1">return_n_iter</span><span class="s3">,</span>
    <span class="s1">random_state</span><span class="s3">,</span>
<span class="s1">):</span>
    <span class="s0">&quot;&quot;&quot;Main affinity propagation algorithm.&quot;&quot;&quot;</span>
    <span class="s1">n_samples = S.shape[</span><span class="s4">0</span><span class="s1">]</span>
    <span class="s3">if </span><span class="s1">n_samples == </span><span class="s4">1 </span><span class="s3">or </span><span class="s1">_equal_similarities_and_preferences(S</span><span class="s3">, </span><span class="s1">preference):</span>
        <span class="s2"># It makes no sense to run the algorithm in this case, so return 1 or</span>
        <span class="s2"># n_samples clusters, depending on preferences</span>
        <span class="s1">warnings.warn(</span>
            <span class="s5">&quot;All samples have mutually equal similarities. &quot;</span>
            <span class="s5">&quot;Returning arbitrary cluster center(s).&quot;</span>
        <span class="s1">)</span>
        <span class="s3">if </span><span class="s1">preference.flat[</span><span class="s4">0</span><span class="s1">] &gt;= S.flat[n_samples - </span><span class="s4">1</span><span class="s1">]:</span>
            <span class="s3">return </span><span class="s1">(</span>
                <span class="s1">(np.arange(n_samples)</span><span class="s3">, </span><span class="s1">np.arange(n_samples)</span><span class="s3">, </span><span class="s4">0</span><span class="s1">)</span>
                <span class="s3">if </span><span class="s1">return_n_iter</span>
                <span class="s3">else </span><span class="s1">(np.arange(n_samples)</span><span class="s3">, </span><span class="s1">np.arange(n_samples))</span>
            <span class="s1">)</span>
        <span class="s3">else</span><span class="s1">:</span>
            <span class="s3">return </span><span class="s1">(</span>
                <span class="s1">(np.array([</span><span class="s4">0</span><span class="s1">])</span><span class="s3">, </span><span class="s1">np.array([</span><span class="s4">0</span><span class="s1">] * n_samples)</span><span class="s3">, </span><span class="s4">0</span><span class="s1">)</span>
                <span class="s3">if </span><span class="s1">return_n_iter</span>
                <span class="s3">else </span><span class="s1">(np.array([</span><span class="s4">0</span><span class="s1">])</span><span class="s3">, </span><span class="s1">np.array([</span><span class="s4">0</span><span class="s1">] * n_samples))</span>
            <span class="s1">)</span>

    <span class="s2"># Place preference on the diagonal of S</span>
    <span class="s1">S.flat[:: (n_samples + </span><span class="s4">1</span><span class="s1">)] = preference</span>

    <span class="s1">A = np.zeros((n_samples</span><span class="s3">, </span><span class="s1">n_samples))</span>
    <span class="s1">R = np.zeros((n_samples</span><span class="s3">, </span><span class="s1">n_samples))  </span><span class="s2"># Initialize messages</span>
    <span class="s2"># Intermediate results</span>
    <span class="s1">tmp = np.zeros((n_samples</span><span class="s3">, </span><span class="s1">n_samples))</span>

    <span class="s2"># Remove degeneracies</span>
    <span class="s1">S += (</span>
        <span class="s1">np.finfo(S.dtype).eps * S + np.finfo(S.dtype).tiny * </span><span class="s4">100</span>
    <span class="s1">) * random_state.standard_normal(size=(n_samples</span><span class="s3">, </span><span class="s1">n_samples))</span>

    <span class="s2"># Execute parallel affinity propagation updates</span>
    <span class="s1">e = np.zeros((n_samples</span><span class="s3">, </span><span class="s1">convergence_iter))</span>

    <span class="s1">ind = np.arange(n_samples)</span>

    <span class="s3">for </span><span class="s1">it </span><span class="s3">in </span><span class="s1">range(max_iter):</span>
        <span class="s2"># tmp = A + S; compute responsibilities</span>
        <span class="s1">np.add(A</span><span class="s3">, </span><span class="s1">S</span><span class="s3">, </span><span class="s1">tmp)</span>
        <span class="s1">I = np.argmax(tmp</span><span class="s3">, </span><span class="s1">axis=</span><span class="s4">1</span><span class="s1">)</span>
        <span class="s1">Y = tmp[ind</span><span class="s3">, </span><span class="s1">I]  </span><span class="s2"># np.max(A + S, axis=1)</span>
        <span class="s1">tmp[ind</span><span class="s3">, </span><span class="s1">I] = -np.inf</span>
        <span class="s1">Y2 = np.max(tmp</span><span class="s3">, </span><span class="s1">axis=</span><span class="s4">1</span><span class="s1">)</span>

        <span class="s2"># tmp = Rnew</span>
        <span class="s1">np.subtract(S</span><span class="s3">, </span><span class="s1">Y[:</span><span class="s3">, None</span><span class="s1">]</span><span class="s3">, </span><span class="s1">tmp)</span>
        <span class="s1">tmp[ind</span><span class="s3">, </span><span class="s1">I] = S[ind</span><span class="s3">, </span><span class="s1">I] - Y2</span>

        <span class="s2"># Damping</span>
        <span class="s1">tmp *= </span><span class="s4">1 </span><span class="s1">- damping</span>
        <span class="s1">R *= damping</span>
        <span class="s1">R += tmp</span>

        <span class="s2"># tmp = Rp; compute availabilities</span>
        <span class="s1">np.maximum(R</span><span class="s3">, </span><span class="s4">0</span><span class="s3">, </span><span class="s1">tmp)</span>
        <span class="s1">tmp.flat[:: n_samples + </span><span class="s4">1</span><span class="s1">] = R.flat[:: n_samples + </span><span class="s4">1</span><span class="s1">]</span>

        <span class="s2"># tmp = -Anew</span>
        <span class="s1">tmp -= np.sum(tmp</span><span class="s3">, </span><span class="s1">axis=</span><span class="s4">0</span><span class="s1">)</span>
        <span class="s1">dA = np.diag(tmp).copy()</span>
        <span class="s1">tmp.clip(</span><span class="s4">0</span><span class="s3">, </span><span class="s1">np.inf</span><span class="s3">, </span><span class="s1">tmp)</span>
        <span class="s1">tmp.flat[:: n_samples + </span><span class="s4">1</span><span class="s1">] = dA</span>

        <span class="s2"># Damping</span>
        <span class="s1">tmp *= </span><span class="s4">1 </span><span class="s1">- damping</span>
        <span class="s1">A *= damping</span>
        <span class="s1">A -= tmp</span>

        <span class="s2"># Check for convergence</span>
        <span class="s1">E = (np.diag(A) + np.diag(R)) &gt; </span><span class="s4">0</span>
        <span class="s1">e[:</span><span class="s3">, </span><span class="s1">it % convergence_iter] = E</span>
        <span class="s1">K = np.sum(E</span><span class="s3">, </span><span class="s1">axis=</span><span class="s4">0</span><span class="s1">)</span>

        <span class="s3">if </span><span class="s1">it &gt;= convergence_iter:</span>
            <span class="s1">se = np.sum(e</span><span class="s3">, </span><span class="s1">axis=</span><span class="s4">1</span><span class="s1">)</span>
            <span class="s1">unconverged = np.sum((se == convergence_iter) + (se == </span><span class="s4">0</span><span class="s1">)) != n_samples</span>
            <span class="s3">if </span><span class="s1">(</span><span class="s3">not </span><span class="s1">unconverged </span><span class="s3">and </span><span class="s1">(K &gt; </span><span class="s4">0</span><span class="s1">)) </span><span class="s3">or </span><span class="s1">(it == max_iter):</span>
                <span class="s1">never_converged = </span><span class="s3">False</span>
                <span class="s3">if </span><span class="s1">verbose:</span>
                    <span class="s1">print(</span><span class="s5">&quot;Converged after %d iterations.&quot; </span><span class="s1">% it)</span>
                <span class="s3">break</span>
    <span class="s3">else</span><span class="s1">:</span>
        <span class="s1">never_converged = </span><span class="s3">True</span>
        <span class="s3">if </span><span class="s1">verbose:</span>
            <span class="s1">print(</span><span class="s5">&quot;Did not converge&quot;</span><span class="s1">)</span>

    <span class="s1">I = np.flatnonzero(E)</span>
    <span class="s1">K = I.size  </span><span class="s2"># Identify exemplars</span>

    <span class="s3">if </span><span class="s1">K &gt; </span><span class="s4">0</span><span class="s1">:</span>
        <span class="s3">if </span><span class="s1">never_converged:</span>
            <span class="s1">warnings.warn(</span>
                <span class="s1">(</span>
                    <span class="s5">&quot;Affinity propagation did not converge, this model &quot;</span>
                    <span class="s5">&quot;may return degenerate cluster centers and labels.&quot;</span>
                <span class="s1">)</span><span class="s3">,</span>
                <span class="s1">ConvergenceWarning</span><span class="s3">,</span>
            <span class="s1">)</span>
        <span class="s1">c = np.argmax(S[:</span><span class="s3">, </span><span class="s1">I]</span><span class="s3">, </span><span class="s1">axis=</span><span class="s4">1</span><span class="s1">)</span>
        <span class="s1">c[I] = np.arange(K)  </span><span class="s2"># Identify clusters</span>
        <span class="s2"># Refine the final set of exemplars and clusters and return results</span>
        <span class="s3">for </span><span class="s1">k </span><span class="s3">in </span><span class="s1">range(K):</span>
            <span class="s1">ii = np.where(c == k)[</span><span class="s4">0</span><span class="s1">]</span>
            <span class="s1">j = np.argmax(np.sum(S[ii[:</span><span class="s3">, </span><span class="s1">np.newaxis]</span><span class="s3">, </span><span class="s1">ii]</span><span class="s3">, </span><span class="s1">axis=</span><span class="s4">0</span><span class="s1">))</span>
            <span class="s1">I[k] = ii[j]</span>

        <span class="s1">c = np.argmax(S[:</span><span class="s3">, </span><span class="s1">I]</span><span class="s3">, </span><span class="s1">axis=</span><span class="s4">1</span><span class="s1">)</span>
        <span class="s1">c[I] = np.arange(K)</span>
        <span class="s1">labels = I[c]</span>
        <span class="s2"># Reduce labels to a sorted, gapless, list</span>
        <span class="s1">cluster_centers_indices = np.unique(labels)</span>
        <span class="s1">labels = np.searchsorted(cluster_centers_indices</span><span class="s3">, </span><span class="s1">labels)</span>
    <span class="s3">else</span><span class="s1">:</span>
        <span class="s1">warnings.warn(</span>
            <span class="s1">(</span>
                <span class="s5">&quot;Affinity propagation did not converge and this model &quot;</span>
                <span class="s5">&quot;will not have any cluster centers.&quot;</span>
            <span class="s1">)</span><span class="s3">,</span>
            <span class="s1">ConvergenceWarning</span><span class="s3">,</span>
        <span class="s1">)</span>
        <span class="s1">labels = np.array([-</span><span class="s4">1</span><span class="s1">] * n_samples)</span>
        <span class="s1">cluster_centers_indices = []</span>

    <span class="s3">if </span><span class="s1">return_n_iter:</span>
        <span class="s3">return </span><span class="s1">cluster_centers_indices</span><span class="s3">, </span><span class="s1">labels</span><span class="s3">, </span><span class="s1">it + </span><span class="s4">1</span>
    <span class="s3">else</span><span class="s1">:</span>
        <span class="s3">return </span><span class="s1">cluster_centers_indices</span><span class="s3">, </span><span class="s1">labels</span>


<span class="s2">###############################################################################</span>
<span class="s2"># Public API</span>


<span class="s1">@validate_params(</span>
    <span class="s1">{</span>
        <span class="s5">&quot;S&quot;</span><span class="s1">: [</span><span class="s5">&quot;array-like&quot;</span><span class="s1">]</span><span class="s3">,</span>
        <span class="s5">&quot;return_n_iter&quot;</span><span class="s1">: [</span><span class="s5">&quot;boolean&quot;</span><span class="s1">]</span><span class="s3">,</span>
    <span class="s1">}</span><span class="s3">,</span>
    <span class="s1">prefer_skip_nested_validation=</span><span class="s3">False,</span>
<span class="s1">)</span>
<span class="s3">def </span><span class="s1">affinity_propagation(</span>
    <span class="s1">S</span><span class="s3">,</span>
    <span class="s1">*</span><span class="s3">,</span>
    <span class="s1">preference=</span><span class="s3">None,</span>
    <span class="s1">convergence_iter=</span><span class="s4">15</span><span class="s3">,</span>
    <span class="s1">max_iter=</span><span class="s4">200</span><span class="s3">,</span>
    <span class="s1">damping=</span><span class="s4">0.5</span><span class="s3">,</span>
    <span class="s1">copy=</span><span class="s3">True,</span>
    <span class="s1">verbose=</span><span class="s3">False,</span>
    <span class="s1">return_n_iter=</span><span class="s3">False,</span>
    <span class="s1">random_state=</span><span class="s3">None,</span>
<span class="s1">):</span>
    <span class="s0">&quot;&quot;&quot;Perform Affinity Propagation Clustering of data. 
 
    Read more in the :ref:`User Guide &lt;affinity_propagation&gt;`. 
 
    Parameters 
    ---------- 
    S : array-like of shape (n_samples, n_samples) 
        Matrix of similarities between points. 
 
    preference : array-like of shape (n_samples,) or float, default=None 
        Preferences for each point - points with larger values of 
        preferences are more likely to be chosen as exemplars. The number of 
        exemplars, i.e. of clusters, is influenced by the input preferences 
        value. If the preferences are not passed as arguments, they will be 
        set to the median of the input similarities (resulting in a moderate 
        number of clusters). For a smaller amount of clusters, this can be set 
        to the minimum value of the similarities. 
 
    convergence_iter : int, default=15 
        Number of iterations with no change in the number 
        of estimated clusters that stops the convergence. 
 
    max_iter : int, default=200 
        Maximum number of iterations. 
 
    damping : float, default=0.5 
        Damping factor between 0.5 and 1. 
 
    copy : bool, default=True 
        If copy is False, the affinity matrix is modified inplace by the 
        algorithm, for memory efficiency. 
 
    verbose : bool, default=False 
        The verbosity level. 
 
    return_n_iter : bool, default=False 
        Whether or not to return the number of iterations. 
 
    random_state : int, RandomState instance or None, default=None 
        Pseudo-random number generator to control the starting state. 
        Use an int for reproducible results across function calls. 
        See the :term:`Glossary &lt;random_state&gt;`. 
 
        .. versionadded:: 0.23 
            this parameter was previously hardcoded as 0. 
 
    Returns 
    ------- 
    cluster_centers_indices : ndarray of shape (n_clusters,) 
        Index of clusters centers. 
 
    labels : ndarray of shape (n_samples,) 
        Cluster labels for each point. 
 
    n_iter : int 
        Number of iterations run. Returned only if `return_n_iter` is 
        set to True. 
 
    Notes 
    ----- 
    For an example, see :ref:`examples/cluster/plot_affinity_propagation.py 
    &lt;sphx_glr_auto_examples_cluster_plot_affinity_propagation.py&gt;`. 
 
    When the algorithm does not converge, it will still return a arrays of 
    ``cluster_center_indices`` and labels if there are any exemplars/clusters, 
    however they may be degenerate and should be used with caution. 
 
    When all training samples have equal similarities and equal preferences, 
    the assignment of cluster centers and labels depends on the preference. 
    If the preference is smaller than the similarities, a single cluster center 
    and label ``0`` for every sample will be returned. Otherwise, every 
    training sample becomes its own cluster center and is assigned a unique 
    label. 
 
    References 
    ---------- 
    Brendan J. Frey and Delbert Dueck, &quot;Clustering by Passing Messages 
    Between Data Points&quot;, Science Feb. 2007 
    &quot;&quot;&quot;</span>
    <span class="s1">estimator = AffinityPropagation(</span>
        <span class="s1">damping=damping</span><span class="s3">,</span>
        <span class="s1">max_iter=max_iter</span><span class="s3">,</span>
        <span class="s1">convergence_iter=convergence_iter</span><span class="s3">,</span>
        <span class="s1">copy=copy</span><span class="s3">,</span>
        <span class="s1">preference=preference</span><span class="s3">,</span>
        <span class="s1">affinity=</span><span class="s5">&quot;precomputed&quot;</span><span class="s3">,</span>
        <span class="s1">verbose=verbose</span><span class="s3">,</span>
        <span class="s1">random_state=random_state</span><span class="s3">,</span>
    <span class="s1">).fit(S)</span>

    <span class="s3">if </span><span class="s1">return_n_iter:</span>
        <span class="s3">return </span><span class="s1">estimator.cluster_centers_indices_</span><span class="s3">, </span><span class="s1">estimator.labels_</span><span class="s3">, </span><span class="s1">estimator.n_iter_</span>
    <span class="s3">return </span><span class="s1">estimator.cluster_centers_indices_</span><span class="s3">, </span><span class="s1">estimator.labels_</span>


<span class="s3">class </span><span class="s1">AffinityPropagation(ClusterMixin</span><span class="s3">, </span><span class="s1">BaseEstimator):</span>
    <span class="s0">&quot;&quot;&quot;Perform Affinity Propagation Clustering of data. 
 
    Read more in the :ref:`User Guide &lt;affinity_propagation&gt;`. 
 
    Parameters 
    ---------- 
    damping : float, default=0.5 
        Damping factor in the range `[0.5, 1.0)` is the extent to 
        which the current value is maintained relative to 
        incoming values (weighted 1 - damping). This in order 
        to avoid numerical oscillations when updating these 
        values (messages). 
 
    max_iter : int, default=200 
        Maximum number of iterations. 
 
    convergence_iter : int, default=15 
        Number of iterations with no change in the number 
        of estimated clusters that stops the convergence. 
 
    copy : bool, default=True 
        Make a copy of input data. 
 
    preference : array-like of shape (n_samples,) or float, default=None 
        Preferences for each point - points with larger values of 
        preferences are more likely to be chosen as exemplars. The number 
        of exemplars, ie of clusters, is influenced by the input 
        preferences value. If the preferences are not passed as arguments, 
        they will be set to the median of the input similarities. 
 
    affinity : {'euclidean', 'precomputed'}, default='euclidean' 
        Which affinity to use. At the moment 'precomputed' and 
        ``euclidean`` are supported. 'euclidean' uses the 
        negative squared euclidean distance between points. 
 
    verbose : bool, default=False 
        Whether to be verbose. 
 
    random_state : int, RandomState instance or None, default=None 
        Pseudo-random number generator to control the starting state. 
        Use an int for reproducible results across function calls. 
        See the :term:`Glossary &lt;random_state&gt;`. 
 
        .. versionadded:: 0.23 
            this parameter was previously hardcoded as 0. 
 
    Attributes 
    ---------- 
    cluster_centers_indices_ : ndarray of shape (n_clusters,) 
        Indices of cluster centers. 
 
    cluster_centers_ : ndarray of shape (n_clusters, n_features) 
        Cluster centers (if affinity != ``precomputed``). 
 
    labels_ : ndarray of shape (n_samples,) 
        Labels of each point. 
 
    affinity_matrix_ : ndarray of shape (n_samples, n_samples) 
        Stores the affinity matrix used in ``fit``. 
 
    n_iter_ : int 
        Number of iterations taken to converge. 
 
    n_features_in_ : int 
        Number of features seen during :term:`fit`. 
 
        .. versionadded:: 0.24 
 
    feature_names_in_ : ndarray of shape (`n_features_in_`,) 
        Names of features seen during :term:`fit`. Defined only when `X` 
        has feature names that are all strings. 
 
        .. versionadded:: 1.0 
 
    See Also 
    -------- 
    AgglomerativeClustering : Recursively merges the pair of 
        clusters that minimally increases a given linkage distance. 
    FeatureAgglomeration : Similar to AgglomerativeClustering, 
        but recursively merges features instead of samples. 
    KMeans : K-Means clustering. 
    MiniBatchKMeans : Mini-Batch K-Means clustering. 
    MeanShift : Mean shift clustering using a flat kernel. 
    SpectralClustering : Apply clustering to a projection 
        of the normalized Laplacian. 
 
    Notes 
    ----- 
    For an example, see :ref:`examples/cluster/plot_affinity_propagation.py 
    &lt;sphx_glr_auto_examples_cluster_plot_affinity_propagation.py&gt;`. 
 
    The algorithmic complexity of affinity propagation is quadratic 
    in the number of points. 
 
    When the algorithm does not converge, it will still return a arrays of 
    ``cluster_center_indices`` and labels if there are any exemplars/clusters, 
    however they may be degenerate and should be used with caution. 
 
    When ``fit`` does not converge, ``cluster_centers_`` is still populated 
    however it may be degenerate. In such a case, proceed with caution. 
    If ``fit`` does not converge and fails to produce any ``cluster_centers_`` 
    then ``predict`` will label every sample as ``-1``. 
 
    When all training samples have equal similarities and equal preferences, 
    the assignment of cluster centers and labels depends on the preference. 
    If the preference is smaller than the similarities, ``fit`` will result in 
    a single cluster center and label ``0`` for every sample. Otherwise, every 
    training sample becomes its own cluster center and is assigned a unique 
    label. 
 
    References 
    ---------- 
 
    Brendan J. Frey and Delbert Dueck, &quot;Clustering by Passing Messages 
    Between Data Points&quot;, Science Feb. 2007 
 
    Examples 
    -------- 
    &gt;&gt;&gt; from sklearn.cluster import AffinityPropagation 
    &gt;&gt;&gt; import numpy as np 
    &gt;&gt;&gt; X = np.array([[1, 2], [1, 4], [1, 0], 
    ...               [4, 2], [4, 4], [4, 0]]) 
    &gt;&gt;&gt; clustering = AffinityPropagation(random_state=5).fit(X) 
    &gt;&gt;&gt; clustering 
    AffinityPropagation(random_state=5) 
    &gt;&gt;&gt; clustering.labels_ 
    array([0, 0, 0, 1, 1, 1]) 
    &gt;&gt;&gt; clustering.predict([[0, 0], [4, 4]]) 
    array([0, 1]) 
    &gt;&gt;&gt; clustering.cluster_centers_ 
    array([[1, 2], 
           [4, 2]]) 
    &quot;&quot;&quot;</span>

    <span class="s1">_parameter_constraints: dict = {</span>
        <span class="s5">&quot;damping&quot;</span><span class="s1">: [Interval(Real</span><span class="s3">, </span><span class="s4">0.5</span><span class="s3">, </span><span class="s4">1.0</span><span class="s3">, </span><span class="s1">closed=</span><span class="s5">&quot;left&quot;</span><span class="s1">)]</span><span class="s3">,</span>
        <span class="s5">&quot;max_iter&quot;</span><span class="s1">: [Interval(Integral</span><span class="s3">, </span><span class="s4">1</span><span class="s3">, None, </span><span class="s1">closed=</span><span class="s5">&quot;left&quot;</span><span class="s1">)]</span><span class="s3">,</span>
        <span class="s5">&quot;convergence_iter&quot;</span><span class="s1">: [Interval(Integral</span><span class="s3">, </span><span class="s4">1</span><span class="s3">, None, </span><span class="s1">closed=</span><span class="s5">&quot;left&quot;</span><span class="s1">)]</span><span class="s3">,</span>
        <span class="s5">&quot;copy&quot;</span><span class="s1">: [</span><span class="s5">&quot;boolean&quot;</span><span class="s1">]</span><span class="s3">,</span>
        <span class="s5">&quot;preference&quot;</span><span class="s1">: [</span>
            <span class="s5">&quot;array-like&quot;</span><span class="s3">,</span>
            <span class="s1">Interval(Real</span><span class="s3">, None, None, </span><span class="s1">closed=</span><span class="s5">&quot;neither&quot;</span><span class="s1">)</span><span class="s3">,</span>
            <span class="s3">None,</span>
        <span class="s1">]</span><span class="s3">,</span>
        <span class="s5">&quot;affinity&quot;</span><span class="s1">: [StrOptions({</span><span class="s5">&quot;euclidean&quot;</span><span class="s3">, </span><span class="s5">&quot;precomputed&quot;</span><span class="s1">})]</span><span class="s3">,</span>
        <span class="s5">&quot;verbose&quot;</span><span class="s1">: [</span><span class="s5">&quot;verbose&quot;</span><span class="s1">]</span><span class="s3">,</span>
        <span class="s5">&quot;random_state&quot;</span><span class="s1">: [</span><span class="s5">&quot;random_state&quot;</span><span class="s1">]</span><span class="s3">,</span>
    <span class="s1">}</span>

    <span class="s3">def </span><span class="s1">__init__(</span>
        <span class="s1">self</span><span class="s3">,</span>
        <span class="s1">*</span><span class="s3">,</span>
        <span class="s1">damping=</span><span class="s4">0.5</span><span class="s3">,</span>
        <span class="s1">max_iter=</span><span class="s4">200</span><span class="s3">,</span>
        <span class="s1">convergence_iter=</span><span class="s4">15</span><span class="s3">,</span>
        <span class="s1">copy=</span><span class="s3">True,</span>
        <span class="s1">preference=</span><span class="s3">None,</span>
        <span class="s1">affinity=</span><span class="s5">&quot;euclidean&quot;</span><span class="s3">,</span>
        <span class="s1">verbose=</span><span class="s3">False,</span>
        <span class="s1">random_state=</span><span class="s3">None,</span>
    <span class="s1">):</span>
        <span class="s1">self.damping = damping</span>
        <span class="s1">self.max_iter = max_iter</span>
        <span class="s1">self.convergence_iter = convergence_iter</span>
        <span class="s1">self.copy = copy</span>
        <span class="s1">self.verbose = verbose</span>
        <span class="s1">self.preference = preference</span>
        <span class="s1">self.affinity = affinity</span>
        <span class="s1">self.random_state = random_state</span>

    <span class="s3">def </span><span class="s1">_more_tags(self):</span>
        <span class="s3">return </span><span class="s1">{</span><span class="s5">&quot;pairwise&quot;</span><span class="s1">: self.affinity == </span><span class="s5">&quot;precomputed&quot;</span><span class="s1">}</span>

    <span class="s1">@_fit_context(prefer_skip_nested_validation=</span><span class="s3">True</span><span class="s1">)</span>
    <span class="s3">def </span><span class="s1">fit(self</span><span class="s3">, </span><span class="s1">X</span><span class="s3">, </span><span class="s1">y=</span><span class="s3">None</span><span class="s1">):</span>
        <span class="s0">&quot;&quot;&quot;Fit the clustering from features, or affinity matrix. 
 
        Parameters 
        ---------- 
        X : {array-like, sparse matrix} of shape (n_samples, n_features), or \ 
                array-like of shape (n_samples, n_samples) 
            Training instances to cluster, or similarities / affinities between 
            instances if ``affinity='precomputed'``. If a sparse feature matrix 
            is provided, it will be converted into a sparse ``csr_matrix``. 
 
        y : Ignored 
            Not used, present here for API consistency by convention. 
 
        Returns 
        ------- 
        self 
            Returns the instance itself. 
        &quot;&quot;&quot;</span>
        <span class="s3">if </span><span class="s1">self.affinity == </span><span class="s5">&quot;precomputed&quot;</span><span class="s1">:</span>
            <span class="s1">accept_sparse = </span><span class="s3">False</span>
        <span class="s3">else</span><span class="s1">:</span>
            <span class="s1">accept_sparse = </span><span class="s5">&quot;csr&quot;</span>
        <span class="s1">X = self._validate_data(X</span><span class="s3">, </span><span class="s1">accept_sparse=accept_sparse)</span>
        <span class="s3">if </span><span class="s1">self.affinity == </span><span class="s5">&quot;precomputed&quot;</span><span class="s1">:</span>
            <span class="s1">self.affinity_matrix_ = X.copy() </span><span class="s3">if </span><span class="s1">self.copy </span><span class="s3">else </span><span class="s1">X</span>
        <span class="s3">else</span><span class="s1">:  </span><span class="s2"># self.affinity == &quot;euclidean&quot;</span>
            <span class="s1">self.affinity_matrix_ = -euclidean_distances(X</span><span class="s3">, </span><span class="s1">squared=</span><span class="s3">True</span><span class="s1">)</span>

        <span class="s3">if </span><span class="s1">self.affinity_matrix_.shape[</span><span class="s4">0</span><span class="s1">] != self.affinity_matrix_.shape[</span><span class="s4">1</span><span class="s1">]:</span>
            <span class="s3">raise </span><span class="s1">ValueError(</span>
                <span class="s5">&quot;The matrix of similarities must be a square array. &quot;</span>
                <span class="s5">f&quot;Got </span><span class="s3">{</span><span class="s1">self.affinity_matrix_.shape</span><span class="s3">} </span><span class="s5">instead.&quot;</span>
            <span class="s1">)</span>

        <span class="s3">if </span><span class="s1">self.preference </span><span class="s3">is None</span><span class="s1">:</span>
            <span class="s1">preference = np.median(self.affinity_matrix_)</span>
        <span class="s3">else</span><span class="s1">:</span>
            <span class="s1">preference = self.preference</span>
        <span class="s1">preference = np.array(preference</span><span class="s3">, </span><span class="s1">copy=</span><span class="s3">False</span><span class="s1">)</span>

        <span class="s1">random_state = check_random_state(self.random_state)</span>

        <span class="s1">(</span>
            <span class="s1">self.cluster_centers_indices_</span><span class="s3">,</span>
            <span class="s1">self.labels_</span><span class="s3">,</span>
            <span class="s1">self.n_iter_</span><span class="s3">,</span>
        <span class="s1">) = _affinity_propagation(</span>
            <span class="s1">self.affinity_matrix_</span><span class="s3">,</span>
            <span class="s1">max_iter=self.max_iter</span><span class="s3">,</span>
            <span class="s1">convergence_iter=self.convergence_iter</span><span class="s3">,</span>
            <span class="s1">preference=preference</span><span class="s3">,</span>
            <span class="s1">damping=self.damping</span><span class="s3">,</span>
            <span class="s1">verbose=self.verbose</span><span class="s3">,</span>
            <span class="s1">return_n_iter=</span><span class="s3">True,</span>
            <span class="s1">random_state=random_state</span><span class="s3">,</span>
        <span class="s1">)</span>

        <span class="s3">if </span><span class="s1">self.affinity != </span><span class="s5">&quot;precomputed&quot;</span><span class="s1">:</span>
            <span class="s1">self.cluster_centers_ = X[self.cluster_centers_indices_].copy()</span>

        <span class="s3">return </span><span class="s1">self</span>

    <span class="s3">def </span><span class="s1">predict(self</span><span class="s3">, </span><span class="s1">X):</span>
        <span class="s0">&quot;&quot;&quot;Predict the closest cluster each sample in X belongs to. 
 
        Parameters 
        ---------- 
        X : {array-like, sparse matrix} of shape (n_samples, n_features) 
            New data to predict. If a sparse matrix is provided, it will be 
            converted into a sparse ``csr_matrix``. 
 
        Returns 
        ------- 
        labels : ndarray of shape (n_samples,) 
            Cluster labels. 
        &quot;&quot;&quot;</span>
        <span class="s1">check_is_fitted(self)</span>
        <span class="s1">X = self._validate_data(X</span><span class="s3">, </span><span class="s1">reset=</span><span class="s3">False, </span><span class="s1">accept_sparse=</span><span class="s5">&quot;csr&quot;</span><span class="s1">)</span>
        <span class="s3">if not </span><span class="s1">hasattr(self</span><span class="s3">, </span><span class="s5">&quot;cluster_centers_&quot;</span><span class="s1">):</span>
            <span class="s3">raise </span><span class="s1">ValueError(</span>
                <span class="s5">&quot;Predict method is not supported when affinity='precomputed'.&quot;</span>
            <span class="s1">)</span>

        <span class="s3">if </span><span class="s1">self.cluster_centers_.shape[</span><span class="s4">0</span><span class="s1">] &gt; </span><span class="s4">0</span><span class="s1">:</span>
            <span class="s3">with </span><span class="s1">config_context(assume_finite=</span><span class="s3">True</span><span class="s1">):</span>
                <span class="s3">return </span><span class="s1">pairwise_distances_argmin(X</span><span class="s3">, </span><span class="s1">self.cluster_centers_)</span>
        <span class="s3">else</span><span class="s1">:</span>
            <span class="s1">warnings.warn(</span>
                <span class="s1">(</span>
                    <span class="s5">&quot;This model does not have any cluster centers &quot;</span>
                    <span class="s5">&quot;because affinity propagation did not converge. &quot;</span>
                    <span class="s5">&quot;Labeling every sample as '-1'.&quot;</span>
                <span class="s1">)</span><span class="s3">,</span>
                <span class="s1">ConvergenceWarning</span><span class="s3">,</span>
            <span class="s1">)</span>
            <span class="s3">return </span><span class="s1">np.array([-</span><span class="s4">1</span><span class="s1">] * X.shape[</span><span class="s4">0</span><span class="s1">])</span>

    <span class="s3">def </span><span class="s1">fit_predict(self</span><span class="s3">, </span><span class="s1">X</span><span class="s3">, </span><span class="s1">y=</span><span class="s3">None</span><span class="s1">):</span>
        <span class="s0">&quot;&quot;&quot;Fit clustering from features/affinity matrix; return cluster labels. 
 
        Parameters 
        ---------- 
        X : {array-like, sparse matrix} of shape (n_samples, n_features), or \ 
                array-like of shape (n_samples, n_samples) 
            Training instances to cluster, or similarities / affinities between 
            instances if ``affinity='precomputed'``. If a sparse feature matrix 
            is provided, it will be converted into a sparse ``csr_matrix``. 
 
        y : Ignored 
            Not used, present here for API consistency by convention. 
 
        Returns 
        ------- 
        labels : ndarray of shape (n_samples,) 
            Cluster labels. 
        &quot;&quot;&quot;</span>
        <span class="s3">return </span><span class="s1">super().fit_predict(X</span><span class="s3">, </span><span class="s1">y)</span>
</pre>
</body>
</html>