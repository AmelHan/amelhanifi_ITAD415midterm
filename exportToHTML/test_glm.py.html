<html>
<head>
<title>test_glm.py</title>
<meta http-equiv="Content-Type" content="text/html; charset=utf-8">
<style type="text/css">
.s0 { color: #629755; font-style: italic;}
.s1 { color: #a9b7c6;}
.s2 { color: #cc7832;}
.s3 { color: #808080;}
.s4 { color: #6897bb;}
.s5 { color: #6a8759;}
</style>
</head>
<body bgcolor="#2b2b2b">
<table CELLSPACING=0 CELLPADDING=5 COLS=1 WIDTH="100%" BGCOLOR="#606060" >
<tr><td><center>
<font face="Arial, Helvetica" color="#000000">
test_glm.py</font>
</center></td></tr></table>
<pre><span class="s0">&quot;&quot;&quot; 
Test functions for models.GLM 
&quot;&quot;&quot;</span>
<span class="s2">import </span><span class="s1">os</span>
<span class="s2">import </span><span class="s1">warnings</span>

<span class="s2">import </span><span class="s1">numpy </span><span class="s2">as </span><span class="s1">np</span>
<span class="s2">from </span><span class="s1">numpy.testing </span><span class="s2">import </span><span class="s1">(</span>
    <span class="s1">assert_</span><span class="s2">,</span>
    <span class="s1">assert_allclose</span><span class="s2">,</span>
    <span class="s1">assert_almost_equal</span><span class="s2">,</span>
    <span class="s1">assert_array_less</span><span class="s2">,</span>
    <span class="s1">assert_equal</span><span class="s2">,</span>
    <span class="s1">assert_raises</span><span class="s2">,</span>
<span class="s1">)</span>
<span class="s2">import </span><span class="s1">pandas </span><span class="s2">as </span><span class="s1">pd</span>
<span class="s2">from </span><span class="s1">pandas.testing </span><span class="s2">import </span><span class="s1">assert_series_equal</span>
<span class="s2">import </span><span class="s1">pytest</span>
<span class="s2">from </span><span class="s1">scipy </span><span class="s2">import </span><span class="s1">stats</span>

<span class="s2">import </span><span class="s1">statsmodels.api </span><span class="s2">as </span><span class="s1">sm</span>
<span class="s2">from </span><span class="s1">statsmodels.compat.scipy </span><span class="s2">import </span><span class="s1">SP_LT_17</span>
<span class="s2">from </span><span class="s1">statsmodels.datasets </span><span class="s2">import </span><span class="s1">cpunish</span><span class="s2">, </span><span class="s1">longley</span>
<span class="s2">from </span><span class="s1">statsmodels.discrete </span><span class="s2">import </span><span class="s1">discrete_model </span><span class="s2">as </span><span class="s1">discrete</span>
<span class="s2">from </span><span class="s1">statsmodels.genmod.generalized_linear_model </span><span class="s2">import </span><span class="s1">GLM</span><span class="s2">, </span><span class="s1">SET_USE_BIC_LLF</span>
<span class="s2">from </span><span class="s1">statsmodels.tools.numdiff </span><span class="s2">import </span><span class="s1">(</span>
    <span class="s1">approx_fprime</span><span class="s2">,</span>
    <span class="s1">approx_fprime_cs</span><span class="s2">,</span>
    <span class="s1">approx_hess</span><span class="s2">,</span>
    <span class="s1">approx_hess_cs</span><span class="s2">,</span>
<span class="s1">)</span>
<span class="s2">from </span><span class="s1">statsmodels.tools.sm_exceptions </span><span class="s2">import </span><span class="s1">(</span>
    <span class="s1">DomainWarning</span><span class="s2">,</span>
    <span class="s1">PerfectSeparationWarning</span><span class="s2">,</span>
    <span class="s1">ValueWarning</span><span class="s2">,</span>
<span class="s1">)</span>
<span class="s2">from </span><span class="s1">statsmodels.tools.tools </span><span class="s2">import </span><span class="s1">add_constant</span>

<span class="s3"># Test Precisions</span>
<span class="s1">DECIMAL_4 = </span><span class="s4">4</span>
<span class="s1">DECIMAL_3 = </span><span class="s4">3</span>
<span class="s1">DECIMAL_2 = </span><span class="s4">2</span>
<span class="s1">DECIMAL_1 = </span><span class="s4">1</span>
<span class="s1">DECIMAL_0 = </span><span class="s4">0</span>

<span class="s1">pdf_output = </span><span class="s2">False</span>

<span class="s2">if </span><span class="s1">pdf_output:</span>
    <span class="s2">from </span><span class="s1">matplotlib.backends.backend_pdf </span><span class="s2">import </span><span class="s1">PdfPages</span>
    <span class="s1">pdf = PdfPages(</span><span class="s5">&quot;test_glm.pdf&quot;</span><span class="s1">)</span>
<span class="s2">else</span><span class="s1">:</span>
    <span class="s1">pdf = </span><span class="s2">None</span>


<span class="s2">def </span><span class="s1">close_or_save(pdf</span><span class="s2">, </span><span class="s1">fig):</span>
    <span class="s2">if </span><span class="s1">pdf_output:</span>
        <span class="s1">pdf.savefig(fig)</span>


<span class="s2">def </span><span class="s1">teardown_module():</span>
    <span class="s2">if </span><span class="s1">pdf_output:</span>
        <span class="s1">pdf.close()</span>


<span class="s1">@pytest.fixture(scope=</span><span class="s5">&quot;module&quot;</span><span class="s1">)</span>
<span class="s2">def </span><span class="s1">iris():</span>
    <span class="s1">cur_dir = os.path.dirname(os.path.abspath(__file__))</span>
    <span class="s2">return </span><span class="s1">np.genfromtxt(os.path.join(cur_dir</span><span class="s2">, </span><span class="s5">'results'</span><span class="s2">, </span><span class="s5">'iris.csv'</span><span class="s1">)</span><span class="s2">,</span>
                         <span class="s1">delimiter=</span><span class="s5">&quot;,&quot;</span><span class="s2">, </span><span class="s1">skip_header=</span><span class="s4">1</span><span class="s1">)</span>


<span class="s2">class </span><span class="s1">CheckModelResultsMixin:</span>
    <span class="s0">''' 
    res2 should be either the results from RModelWrap 
    or the results as defined in model_results_data 
    '''</span>

    <span class="s1">decimal_params = DECIMAL_4</span>
    <span class="s2">def </span><span class="s1">test_params(self):</span>
        <span class="s1">assert_almost_equal(self.res1.params</span><span class="s2">, </span><span class="s1">self.res2.params</span><span class="s2">,</span>
                <span class="s1">self.decimal_params)</span>

    <span class="s1">decimal_bse = DECIMAL_4</span>
    <span class="s2">def </span><span class="s1">test_standard_errors(self):</span>
        <span class="s1">assert_allclose(self.res1.bse</span><span class="s2">, </span><span class="s1">self.res2.bse</span><span class="s2">,</span>
                        <span class="s1">atol=</span><span class="s4">10</span><span class="s1">**(-self.decimal_bse)</span><span class="s2">, </span><span class="s1">rtol=</span><span class="s4">1e-5</span><span class="s1">)</span>

    <span class="s1">decimal_resids = DECIMAL_4</span>
    <span class="s2">def </span><span class="s1">test_residuals(self):</span>
        <span class="s3"># fix incorrect numbers in resid_working results</span>
        <span class="s3"># residuals for Poisson are also tested in test_glm_weights.py</span>
        <span class="s2">import </span><span class="s1">copy</span>

        <span class="s3"># new numpy would have copy method</span>
        <span class="s1">resid2 = copy.copy(self.res2.resids)</span>
        <span class="s1">resid2[:</span><span class="s2">, </span><span class="s4">2</span><span class="s1">] *= self.res1.family.link.deriv(self.res1.mu)**</span><span class="s4">2</span>

        <span class="s1">atol = </span><span class="s4">10</span><span class="s1">**(-self.decimal_resids)</span>
        <span class="s1">resid_a = self.res1.resid_anscombe_unscaled</span>
        <span class="s1">resids = np.column_stack((self.res1.resid_pearson</span><span class="s2">,</span>
                <span class="s1">self.res1.resid_deviance</span><span class="s2">, </span><span class="s1">self.res1.resid_working</span><span class="s2">,</span>
                <span class="s1">resid_a</span><span class="s2">, </span><span class="s1">self.res1.resid_response))</span>
        <span class="s1">assert_allclose(resids</span><span class="s2">, </span><span class="s1">resid2</span><span class="s2">, </span><span class="s1">rtol=</span><span class="s4">1e-6</span><span class="s2">, </span><span class="s1">atol=atol)</span>

    <span class="s1">decimal_aic_R = DECIMAL_4</span>

    <span class="s2">def </span><span class="s1">test_aic_R(self):</span>
        <span class="s3"># R includes the estimation of the scale as a lost dof</span>
        <span class="s3"># Does not with Gamma though</span>
        <span class="s2">if </span><span class="s1">self.res1.scale != </span><span class="s4">1</span><span class="s1">:</span>
            <span class="s1">dof = </span><span class="s4">2</span>
        <span class="s2">else</span><span class="s1">:</span>
            <span class="s1">dof = </span><span class="s4">0</span>
        <span class="s2">if </span><span class="s1">isinstance(self.res1.model.family</span><span class="s2">, </span><span class="s1">(sm.families.NegativeBinomial)):</span>
            <span class="s1">llf = self.res1.model.family.loglike(self.res1.model.endog</span><span class="s2">,</span>
                                                 <span class="s1">self.res1.mu</span><span class="s2">,</span>
                                                 <span class="s1">self.res1.model.var_weights</span><span class="s2">,</span>
                                                 <span class="s1">self.res1.model.freq_weights</span><span class="s2">,</span>
                                                 <span class="s1">scale=</span><span class="s4">1</span><span class="s1">)</span>
            <span class="s1">aic = (-</span><span class="s4">2</span><span class="s1">*llf+</span><span class="s4">2</span><span class="s1">*(self.res1.df_model+</span><span class="s4">1</span><span class="s1">))</span>
        <span class="s2">else</span><span class="s1">:</span>
            <span class="s1">aic = self.res1.aic</span>
        <span class="s1">assert_almost_equal(aic+dof</span><span class="s2">, </span><span class="s1">self.res2.aic_R</span><span class="s2">,</span>
                <span class="s1">self.decimal_aic_R)</span>

    <span class="s1">decimal_aic_Stata = DECIMAL_4</span>
    <span class="s2">def </span><span class="s1">test_aic_Stata(self):</span>
        <span class="s3"># Stata uses the below llf for aic definition for these families</span>
        <span class="s2">if </span><span class="s1">isinstance(self.res1.model.family</span><span class="s2">, </span><span class="s1">(sm.families.Gamma</span><span class="s2">,</span>
                                               <span class="s1">sm.families.InverseGaussian</span><span class="s2">,</span>
                                               <span class="s1">sm.families.NegativeBinomial)):</span>
            <span class="s1">llf = self.res1.model.family.loglike(self.res1.model.endog</span><span class="s2">,</span>
                                                 <span class="s1">self.res1.mu</span><span class="s2">,</span>
                                                 <span class="s1">self.res1.model.var_weights</span><span class="s2">,</span>
                                                 <span class="s1">self.res1.model.freq_weights</span><span class="s2">,</span>
                                                 <span class="s1">scale=</span><span class="s4">1</span><span class="s1">)</span>
            <span class="s1">aic = (-</span><span class="s4">2</span><span class="s1">*llf+</span><span class="s4">2</span><span class="s1">*(self.res1.df_model+</span><span class="s4">1</span><span class="s1">))/self.res1.nobs</span>
        <span class="s2">else</span><span class="s1">:</span>
            <span class="s1">aic = self.res1.aic/self.res1.nobs</span>
        <span class="s1">assert_almost_equal(aic</span><span class="s2">, </span><span class="s1">self.res2.aic_Stata</span><span class="s2">, </span><span class="s1">self.decimal_aic_Stata)</span>

    <span class="s1">decimal_deviance = DECIMAL_4</span>
    <span class="s2">def </span><span class="s1">test_deviance(self):</span>
        <span class="s1">assert_almost_equal(self.res1.deviance</span><span class="s2">, </span><span class="s1">self.res2.deviance</span><span class="s2">,</span>
                <span class="s1">self.decimal_deviance)</span>

    <span class="s1">decimal_scale = DECIMAL_4</span>
    <span class="s2">def </span><span class="s1">test_scale(self):</span>
        <span class="s1">assert_almost_equal(self.res1.scale</span><span class="s2">, </span><span class="s1">self.res2.scale</span><span class="s2">,</span>
                <span class="s1">self.decimal_scale)</span>

    <span class="s1">decimal_loglike = DECIMAL_4</span>
    <span class="s2">def </span><span class="s1">test_loglike(self):</span>
        <span class="s3"># Stata uses the below llf for these families</span>
        <span class="s3"># We differ with R for them</span>
        <span class="s2">if </span><span class="s1">isinstance(self.res1.model.family</span><span class="s2">, </span><span class="s1">(sm.families.Gamma</span><span class="s2">,</span>
                                               <span class="s1">sm.families.InverseGaussian</span><span class="s2">,</span>
                                               <span class="s1">sm.families.NegativeBinomial)):</span>
            <span class="s1">llf = self.res1.model.family.loglike(self.res1.model.endog</span><span class="s2">,</span>
                                                 <span class="s1">self.res1.mu</span><span class="s2">,</span>
                                                 <span class="s1">self.res1.model.var_weights</span><span class="s2">,</span>
                                                 <span class="s1">self.res1.model.freq_weights</span><span class="s2">,</span>
                                                 <span class="s1">scale=</span><span class="s4">1</span><span class="s1">)</span>
        <span class="s2">else</span><span class="s1">:</span>
            <span class="s1">llf = self.res1.llf</span>
        <span class="s1">assert_almost_equal(llf</span><span class="s2">, </span><span class="s1">self.res2.llf</span><span class="s2">, </span><span class="s1">self.decimal_loglike)</span>

    <span class="s1">decimal_null_deviance = DECIMAL_4</span>
    <span class="s2">def </span><span class="s1">test_null_deviance(self):</span>
        <span class="s2">with </span><span class="s1">warnings.catch_warnings():</span>
            <span class="s1">warnings.simplefilter(</span><span class="s5">&quot;ignore&quot;</span><span class="s2">, </span><span class="s1">DomainWarning)</span>

            <span class="s1">assert_almost_equal(self.res1.null_deviance</span><span class="s2">,</span>
                                <span class="s1">self.res2.null_deviance</span><span class="s2">,</span>
                                <span class="s1">self.decimal_null_deviance)</span>

    <span class="s1">decimal_bic = DECIMAL_4</span>
    <span class="s2">def </span><span class="s1">test_bic(self):</span>
        <span class="s2">with </span><span class="s1">warnings.catch_warnings():</span>
            <span class="s1">warnings.simplefilter(</span><span class="s5">&quot;ignore&quot;</span><span class="s1">)</span>
            <span class="s1">assert_almost_equal(self.res1.bic</span><span class="s2">,</span>
                                <span class="s1">self.res2.bic_Stata</span><span class="s2">,</span>
                                <span class="s1">self.decimal_bic)</span>

    <span class="s2">def </span><span class="s1">test_degrees(self):</span>
        <span class="s1">assert_equal(self.res1.model.df_resid</span><span class="s2">,</span><span class="s1">self.res2.df_resid)</span>

    <span class="s1">decimal_fittedvalues = DECIMAL_4</span>
    <span class="s2">def </span><span class="s1">test_fittedvalues(self):</span>
        <span class="s1">assert_almost_equal(self.res1.fittedvalues</span><span class="s2">, </span><span class="s1">self.res2.fittedvalues</span><span class="s2">,</span>
                <span class="s1">self.decimal_fittedvalues)</span>

    <span class="s2">def </span><span class="s1">test_tpvalues(self):</span>
        <span class="s3"># test comparing tvalues and pvalues with normal implementation</span>
        <span class="s3"># make sure they use normal distribution (inherited in results class)</span>
        <span class="s1">params = self.res1.params</span>
        <span class="s1">tvalues = params / self.res1.bse</span>
        <span class="s1">pvalues = stats.norm.sf(np.abs(tvalues)) * </span><span class="s4">2</span>
        <span class="s1">half_width = stats.norm.isf(</span><span class="s4">0.025</span><span class="s1">) * self.res1.bse</span>
        <span class="s1">conf_int = np.column_stack((params - half_width</span><span class="s2">, </span><span class="s1">params + half_width))</span>
        <span class="s2">if </span><span class="s1">isinstance(tvalues</span><span class="s2">, </span><span class="s1">pd.Series):</span>
            <span class="s1">assert_series_equal(self.res1.tvalues</span><span class="s2">, </span><span class="s1">tvalues)</span>
        <span class="s2">else</span><span class="s1">:</span>
            <span class="s1">assert_almost_equal(self.res1.tvalues</span><span class="s2">, </span><span class="s1">tvalues)</span>
        <span class="s1">assert_almost_equal(self.res1.pvalues</span><span class="s2">, </span><span class="s1">pvalues)</span>
        <span class="s1">assert_almost_equal(self.res1.conf_int()</span><span class="s2">, </span><span class="s1">conf_int)</span>

    <span class="s2">def </span><span class="s1">test_pearson_chi2(self):</span>
        <span class="s2">if </span><span class="s1">hasattr(self.res2</span><span class="s2">, </span><span class="s5">'pearson_chi2'</span><span class="s1">):</span>
            <span class="s1">assert_allclose(self.res1.pearson_chi2</span><span class="s2">, </span><span class="s1">self.res2.pearson_chi2</span><span class="s2">,</span>
                            <span class="s1">atol=</span><span class="s4">1e-6</span><span class="s2">, </span><span class="s1">rtol=</span><span class="s4">1e-6</span><span class="s1">)</span>

    <span class="s2">def </span><span class="s1">test_prsquared(self):</span>
        <span class="s2">if </span><span class="s1">hasattr(self.res2</span><span class="s2">, </span><span class="s5">'prsquared'</span><span class="s1">):</span>
            <span class="s1">assert_allclose(self.res1.pseudo_rsquared(kind=</span><span class="s5">&quot;mcf&quot;</span><span class="s1">)</span><span class="s2">,</span>
                            <span class="s1">self.res2.prsquared</span><span class="s2">, </span><span class="s1">rtol=</span><span class="s4">0.05</span><span class="s1">)</span>

        <span class="s2">if </span><span class="s1">hasattr(self.res2</span><span class="s2">, </span><span class="s5">'prsquared_cox_snell'</span><span class="s1">):</span>
            <span class="s1">assert_allclose(float(self.res1.pseudo_rsquared(kind=</span><span class="s5">&quot;cs&quot;</span><span class="s1">))</span><span class="s2">,</span>
                            <span class="s1">self.res2.prsquared_cox_snell</span><span class="s2">, </span><span class="s1">rtol=</span><span class="s4">0.05</span><span class="s1">)</span>

    <span class="s1">@pytest.mark.smoke</span>
    <span class="s2">def </span><span class="s1">test_summary(self):</span>
        <span class="s1">self.res1.summary()</span>

    <span class="s1">@pytest.mark.smoke</span>
    <span class="s2">def </span><span class="s1">test_summary2(self):</span>
        <span class="s2">with </span><span class="s1">warnings.catch_warnings():</span>
            <span class="s1">warnings.simplefilter(</span><span class="s5">&quot;ignore&quot;</span><span class="s2">, </span><span class="s1">DomainWarning)</span>
            <span class="s1">self.res1.summary2()</span>

    <span class="s2">def </span><span class="s1">test_get_distribution(self):</span>
        <span class="s1">res1 = self.res1</span>
        <span class="s2">if not </span><span class="s1">hasattr(res1.model.family</span><span class="s2">, </span><span class="s5">&quot;get_distribution&quot;</span><span class="s1">):</span>
            <span class="s3"># only Tweedie has not get_distribution</span>
            <span class="s1">pytest.skip(</span><span class="s5">&quot;get_distribution not available&quot;</span><span class="s1">)</span>

        <span class="s2">if </span><span class="s1">isinstance(res1.model.family</span><span class="s2">, </span><span class="s1">sm.families.NegativeBinomial):</span>
            <span class="s1">res_scale = </span><span class="s4">1  </span><span class="s3"># QMLE scale can differ from 1</span>
        <span class="s2">else</span><span class="s1">:</span>
            <span class="s1">res_scale = res1.scale</span>

        <span class="s1">distr = res1.model.family.get_distribution(res1.fittedvalues</span><span class="s2">,</span>
                                                   <span class="s1">res_scale)</span>
        <span class="s1">var_endog = res1.model.family.variance(res1.fittedvalues) * res_scale</span>
        <span class="s1">m</span><span class="s2">, </span><span class="s1">v = distr.stats()</span>
        <span class="s1">assert_allclose(res1.fittedvalues</span><span class="s2">, </span><span class="s1">m</span><span class="s2">, </span><span class="s1">rtol=</span><span class="s4">1e-13</span><span class="s1">)</span>
        <span class="s1">assert_allclose(var_endog</span><span class="s2">, </span><span class="s1">v</span><span class="s2">, </span><span class="s1">rtol=</span><span class="s4">1e-13</span><span class="s1">)</span>
        <span class="s3"># check model method</span>
        <span class="s1">distr2 = res1.model.get_distribution(res1.params</span><span class="s2">, </span><span class="s1">res_scale)</span>
        <span class="s2">for </span><span class="s1">k </span><span class="s2">in </span><span class="s1">distr2.kwds:</span>
            <span class="s1">assert_allclose(distr.kwds[k]</span><span class="s2">, </span><span class="s1">distr2.kwds[k]</span><span class="s2">, </span><span class="s1">rtol=</span><span class="s4">1e-13</span><span class="s1">)</span>

        <span class="s3"># compare var with predict</span>
        <span class="s1">var_ = res1.predict(which=</span><span class="s5">&quot;var_unscaled&quot;</span><span class="s1">)</span>
        <span class="s1">assert_allclose(var_ * res_scale</span><span class="s2">, </span><span class="s1">var_endog</span><span class="s2">, </span><span class="s1">rtol=</span><span class="s4">1e-13</span><span class="s1">)</span>

        <span class="s3"># check get_distribution of results instance</span>
        <span class="s2">if </span><span class="s1">getattr(self</span><span class="s2">, </span><span class="s5">&quot;has_edispersion&quot;</span><span class="s2">, False</span><span class="s1">):</span>
            <span class="s2">with </span><span class="s1">pytest.warns(UserWarning</span><span class="s2">, </span><span class="s1">match=</span><span class="s5">&quot;using scale=1&quot;</span><span class="s1">):</span>
                <span class="s1">distr3 = res1.get_distribution()</span>
        <span class="s2">else</span><span class="s1">:</span>
            <span class="s1">distr3 = res1.get_distribution()</span>
        <span class="s2">for </span><span class="s1">k </span><span class="s2">in </span><span class="s1">distr2.kwds:</span>
            <span class="s1">assert_allclose(distr3.kwds[k]</span><span class="s2">, </span><span class="s1">distr2.kwds[k]</span><span class="s2">, </span><span class="s1">rtol=</span><span class="s4">1e-13</span><span class="s1">)</span>


<span class="s2">class </span><span class="s1">CheckComparisonMixin:</span>

    <span class="s2">def </span><span class="s1">test_compare_discrete(self):</span>
        <span class="s1">res1 = self.res1</span>
        <span class="s1">resd = self.resd</span>

        <span class="s1">assert_allclose(res1.llf</span><span class="s2">, </span><span class="s1">resd.llf</span><span class="s2">, </span><span class="s1">rtol=</span><span class="s4">1e-10</span><span class="s1">)</span>
        <span class="s1">score_obs1 = res1.model.score_obs(res1.params * </span><span class="s4">0.98</span><span class="s1">)</span>
        <span class="s1">score_obsd = resd.model.score_obs(resd.params * </span><span class="s4">0.98</span><span class="s1">)</span>
        <span class="s1">assert_allclose(score_obs1</span><span class="s2">, </span><span class="s1">score_obsd</span><span class="s2">, </span><span class="s1">rtol=</span><span class="s4">1e-10</span><span class="s1">)</span>

        <span class="s3"># score</span>
        <span class="s1">score1 = res1.model.score(res1.params * </span><span class="s4">0.98</span><span class="s1">)</span>
        <span class="s1">assert_allclose(score1</span><span class="s2">, </span><span class="s1">score_obs1.sum(</span><span class="s4">0</span><span class="s1">)</span><span class="s2">, </span><span class="s1">atol=</span><span class="s4">1e-20</span><span class="s1">)</span>
        <span class="s1">score0 = res1.model.score(res1.params)</span>
        <span class="s1">assert_allclose(score0</span><span class="s2">, </span><span class="s1">np.zeros(score_obs1.shape[</span><span class="s4">1</span><span class="s1">])</span><span class="s2">, </span><span class="s1">atol=</span><span class="s4">5e-7</span><span class="s1">)</span>

        <span class="s1">hessian1 = res1.model.hessian(res1.params * </span><span class="s4">0.98</span><span class="s2">, </span><span class="s1">observed=</span><span class="s2">False</span><span class="s1">)</span>
        <span class="s1">hessiand = resd.model.hessian(resd.params * </span><span class="s4">0.98</span><span class="s1">)</span>
        <span class="s1">assert_allclose(hessian1</span><span class="s2">, </span><span class="s1">hessiand</span><span class="s2">, </span><span class="s1">rtol=</span><span class="s4">1e-10</span><span class="s1">)</span>

        <span class="s1">hessian1 = res1.model.hessian(res1.params * </span><span class="s4">0.98</span><span class="s2">, </span><span class="s1">observed=</span><span class="s2">True</span><span class="s1">)</span>
        <span class="s1">hessiand = resd.model.hessian(resd.params * </span><span class="s4">0.98</span><span class="s1">)</span>
        <span class="s1">assert_allclose(hessian1</span><span class="s2">, </span><span class="s1">hessiand</span><span class="s2">, </span><span class="s1">rtol=</span><span class="s4">1e-9</span><span class="s1">)</span>

    <span class="s2">def </span><span class="s1">test_score_test(self):</span>
        <span class="s1">res1 = self.res1</span>
        <span class="s3"># fake example, should be zero, k_constraint should be 0</span>
        <span class="s1">st</span><span class="s2">, </span><span class="s1">pv</span><span class="s2">, </span><span class="s1">df = res1.model.score_test(res1.params</span><span class="s2">, </span><span class="s1">k_constraints=</span><span class="s4">1</span><span class="s1">)</span>
        <span class="s1">assert_allclose(st</span><span class="s2">, </span><span class="s4">0</span><span class="s2">, </span><span class="s1">atol=</span><span class="s4">1e-20</span><span class="s1">)</span>
        <span class="s1">assert_allclose(pv</span><span class="s2">, </span><span class="s4">1</span><span class="s2">, </span><span class="s1">atol=</span><span class="s4">1e-10</span><span class="s1">)</span>
        <span class="s1">assert_equal(df</span><span class="s2">, </span><span class="s4">1</span><span class="s1">)</span>

        <span class="s1">st</span><span class="s2">, </span><span class="s1">pv</span><span class="s2">, </span><span class="s1">df = res1.model.score_test(res1.params</span><span class="s2">, </span><span class="s1">k_constraints=</span><span class="s4">0</span><span class="s1">)</span>
        <span class="s1">assert_allclose(st</span><span class="s2">, </span><span class="s4">0</span><span class="s2">, </span><span class="s1">atol=</span><span class="s4">1e-20</span><span class="s1">)</span>
        <span class="s1">assert_(np.isnan(pv)</span><span class="s2">, </span><span class="s1">msg=repr(pv))</span>
        <span class="s1">assert_equal(df</span><span class="s2">, </span><span class="s4">0</span><span class="s1">)</span>

        <span class="s3"># TODO: no verified numbers largely SMOKE test</span>
        <span class="s1">exog_extra = res1.model.exog[:</span><span class="s2">,</span><span class="s4">1</span><span class="s1">]**</span><span class="s4">2</span>
        <span class="s1">st</span><span class="s2">, </span><span class="s1">pv</span><span class="s2">, </span><span class="s1">df = res1.model.score_test(res1.params</span><span class="s2">, </span><span class="s1">exog_extra=exog_extra)</span>
        <span class="s1">assert_array_less(</span><span class="s4">0.1</span><span class="s2">, </span><span class="s1">st)</span>
        <span class="s1">assert_array_less(</span><span class="s4">0.1</span><span class="s2">, </span><span class="s1">pv)</span>
        <span class="s1">assert_equal(df</span><span class="s2">, </span><span class="s4">1</span><span class="s1">)</span>

    <span class="s2">def </span><span class="s1">test_get_prediction(self):</span>
        <span class="s1">pred1 = self.res1.get_prediction()  </span><span class="s3"># GLM</span>
        <span class="s1">predd = self.resd.get_prediction()  </span><span class="s3"># discrete class</span>
        <span class="s1">assert_allclose(predd.predicted</span><span class="s2">, </span><span class="s1">pred1.predicted_mean</span><span class="s2">, </span><span class="s1">rtol=</span><span class="s4">1e-11</span><span class="s1">)</span>
        <span class="s1">assert_allclose(predd.se</span><span class="s2">, </span><span class="s1">pred1.se_mean</span><span class="s2">, </span><span class="s1">rtol=</span><span class="s4">1e-6</span><span class="s1">)</span>
        <span class="s1">assert_allclose(predd.summary_frame().values</span><span class="s2">,</span>
                        <span class="s1">pred1.summary_frame().values</span><span class="s2">, </span><span class="s1">rtol=</span><span class="s4">1e-6</span><span class="s1">)</span>

        <span class="s1">pred1 = self.res1.get_prediction(which=</span><span class="s5">&quot;mean&quot;</span><span class="s1">)  </span><span class="s3"># GLM</span>
        <span class="s1">predd = self.resd.get_prediction()  </span><span class="s3"># discrete class</span>
        <span class="s1">assert_allclose(predd.predicted</span><span class="s2">, </span><span class="s1">pred1.predicted</span><span class="s2">, </span><span class="s1">rtol=</span><span class="s4">1e-11</span><span class="s1">)</span>
        <span class="s1">assert_allclose(predd.se</span><span class="s2">, </span><span class="s1">pred1.se</span><span class="s2">, </span><span class="s1">rtol=</span><span class="s4">1e-6</span><span class="s1">)</span>
        <span class="s1">assert_allclose(predd.summary_frame().values</span><span class="s2">,</span>
                        <span class="s1">pred1.summary_frame().values</span><span class="s2">, </span><span class="s1">rtol=</span><span class="s4">1e-6</span><span class="s1">)</span>


<span class="s2">class </span><span class="s1">TestGlmGaussian(CheckModelResultsMixin):</span>
    <span class="s1">@classmethod</span>
    <span class="s2">def </span><span class="s1">setup_class(cls):</span>
        <span class="s0">''' 
        Test Gaussian family with canonical identity link 
        '''</span>
        <span class="s3"># Test Precisions</span>
        <span class="s1">cls.decimal_resids = DECIMAL_3</span>
        <span class="s1">cls.decimal_params = DECIMAL_2</span>
        <span class="s1">cls.decimal_bic = DECIMAL_0</span>
        <span class="s1">cls.decimal_bse = DECIMAL_3</span>

        <span class="s2">from </span><span class="s1">statsmodels.datasets.longley </span><span class="s2">import </span><span class="s1">load</span>
        <span class="s1">cls.data = load()</span>
        <span class="s1">cls.data.endog = np.require(cls.data.endog</span><span class="s2">, </span><span class="s1">requirements=</span><span class="s5">&quot;W&quot;</span><span class="s1">)</span>
        <span class="s1">cls.data.exog = np.require(cls.data.exog</span><span class="s2">, </span><span class="s1">requirements=</span><span class="s5">&quot;W&quot;</span><span class="s1">)</span>
        <span class="s1">cls.data.exog = add_constant(cls.data.exog</span><span class="s2">, </span><span class="s1">prepend=</span><span class="s2">False</span><span class="s1">)</span>
        <span class="s1">cls.res1 = GLM(cls.data.endog</span><span class="s2">, </span><span class="s1">cls.data.exog</span><span class="s2">,</span>
                        <span class="s1">family=sm.families.Gaussian()).fit()</span>
        <span class="s2">from </span><span class="s1">.results.results_glm </span><span class="s2">import </span><span class="s1">Longley</span>
        <span class="s1">cls.res2 = Longley()</span>


    <span class="s2">def </span><span class="s1">test_compare_OLS(self):</span>
        <span class="s1">res1 = self.res1</span>
        <span class="s3"># OLS does not define score_obs</span>
        <span class="s2">from </span><span class="s1">statsmodels.regression.linear_model </span><span class="s2">import </span><span class="s1">OLS</span>
        <span class="s1">resd = OLS(self.data.endog</span><span class="s2">, </span><span class="s1">self.data.exog).fit(use_t=</span><span class="s2">False</span><span class="s1">)</span>
        <span class="s1">self.resd = resd  </span><span class="s3"># attach to access from the outside</span>

        <span class="s1">assert_allclose(res1.llf</span><span class="s2">, </span><span class="s1">resd.llf</span><span class="s2">, </span><span class="s1">rtol=</span><span class="s4">1e-10</span><span class="s1">)</span>
        <span class="s1">score_obs1 = res1.model.score_obs(res1.params</span><span class="s2">, </span><span class="s1">scale=</span><span class="s2">None</span><span class="s1">)</span>
        <span class="s1">score_obsd = resd.resid[:</span><span class="s2">, None</span><span class="s1">] / resd.scale * resd.model.exog</span>
        <span class="s3"># low precision because of badly scaled exog</span>
        <span class="s1">assert_allclose(score_obs1</span><span class="s2">, </span><span class="s1">score_obsd</span><span class="s2">, </span><span class="s1">rtol=</span><span class="s4">1e-8</span><span class="s1">)</span>

        <span class="s1">score_obs1 = res1.model.score_obs(res1.params</span><span class="s2">, </span><span class="s1">scale=</span><span class="s4">1</span><span class="s1">)</span>
        <span class="s1">score_obsd = resd.resid[:</span><span class="s2">, None</span><span class="s1">] * resd.model.exog</span>
        <span class="s1">assert_allclose(score_obs1</span><span class="s2">, </span><span class="s1">score_obsd</span><span class="s2">, </span><span class="s1">rtol=</span><span class="s4">1e-8</span><span class="s1">)</span>

        <span class="s1">hess_obs1 = res1.model.hessian(res1.params</span><span class="s2">, </span><span class="s1">scale=</span><span class="s2">None</span><span class="s1">)</span>
        <span class="s1">hess_obsd = -</span><span class="s4">1. </span><span class="s1">/ resd.scale * resd.model.exog.T.dot(resd.model.exog)</span>
        <span class="s3"># low precision because of badly scaled exog</span>
        <span class="s1">assert_allclose(hess_obs1</span><span class="s2">, </span><span class="s1">hess_obsd</span><span class="s2">, </span><span class="s1">rtol=</span><span class="s4">1e-8</span><span class="s1">)</span>

        <span class="s1">pred1 = res1.get_prediction()  </span><span class="s3"># GLM</span>
        <span class="s1">predd = resd.get_prediction()  </span><span class="s3"># discrete class</span>
        <span class="s1">assert_allclose(predd.predicted</span><span class="s2">, </span><span class="s1">pred1.predicted_mean</span><span class="s2">, </span><span class="s1">rtol=</span><span class="s4">1e-11</span><span class="s1">)</span>
        <span class="s1">assert_allclose(predd.se</span><span class="s2">, </span><span class="s1">pred1.se_mean</span><span class="s2">, </span><span class="s1">rtol=</span><span class="s4">1e-6</span><span class="s1">)</span>
        <span class="s1">assert_allclose(predd.summary_frame().values[:</span><span class="s2">, </span><span class="s1">:</span><span class="s4">4</span><span class="s1">]</span><span class="s2">,</span>
                        <span class="s1">pred1.summary_frame().values</span><span class="s2">, </span><span class="s1">rtol=</span><span class="s4">1e-6</span><span class="s1">)</span>

        <span class="s1">pred1 = self.res1.get_prediction(which=</span><span class="s5">&quot;mean&quot;</span><span class="s1">)  </span><span class="s3"># GLM</span>
        <span class="s1">predd = self.resd.get_prediction()  </span><span class="s3"># discrete class</span>
        <span class="s1">assert_allclose(predd.predicted</span><span class="s2">, </span><span class="s1">pred1.predicted</span><span class="s2">, </span><span class="s1">rtol=</span><span class="s4">1e-11</span><span class="s1">)</span>
        <span class="s1">assert_allclose(predd.se</span><span class="s2">, </span><span class="s1">pred1.se</span><span class="s2">, </span><span class="s1">rtol=</span><span class="s4">1e-6</span><span class="s1">)</span>
        <span class="s1">assert_allclose(predd.summary_frame().values[:</span><span class="s2">, </span><span class="s1">:</span><span class="s4">4</span><span class="s1">]</span><span class="s2">,</span>
                        <span class="s1">pred1.summary_frame().values</span><span class="s2">, </span><span class="s1">rtol=</span><span class="s4">1e-6</span><span class="s1">)</span>

<span class="s3"># FIXME: enable or delete</span>
<span class="s3">#    def setup_method(self):</span>
<span class="s3">#        if skipR:</span>
<span class="s3">#            raise SkipTest, &quot;Rpy not installed.&quot;</span>
<span class="s3">#        Gauss = r.gaussian</span>
<span class="s3">#        self.res2 = RModel(self.data.endog, self.data.exog, r.glm, family=Gauss)</span>
<span class="s3">#        self.res2.resids = np.array(self.res2.resid)[:,None]*np.ones((1,5))</span>
<span class="s3">#        self.res2.null_deviance = 185008826 # taken from R. Rpy bug?</span>


<span class="s2">class </span><span class="s1">TestGlmGaussianGradient(TestGlmGaussian):</span>
    <span class="s1">@classmethod</span>
    <span class="s2">def </span><span class="s1">setup_class(cls):</span>
        <span class="s0">''' 
        Test Gaussian family with canonical identity link 
        '''</span>
        <span class="s3"># Test Precisions</span>
        <span class="s1">cls.decimal_resids = DECIMAL_3</span>
        <span class="s1">cls.decimal_params = DECIMAL_2</span>
        <span class="s1">cls.decimal_bic = DECIMAL_0</span>
        <span class="s1">cls.decimal_bse = DECIMAL_2</span>

        <span class="s2">from </span><span class="s1">statsmodels.datasets.longley </span><span class="s2">import </span><span class="s1">load</span>
        <span class="s1">cls.data = load()</span>
        <span class="s1">cls.data.endog = np.require(cls.data.endog</span><span class="s2">, </span><span class="s1">requirements=</span><span class="s5">&quot;W&quot;</span><span class="s1">)</span>
        <span class="s1">cls.data.exog = np.require(cls.data.exog</span><span class="s2">, </span><span class="s1">requirements=</span><span class="s5">&quot;W&quot;</span><span class="s1">)</span>
        <span class="s1">cls.data.exog = add_constant(cls.data.exog</span><span class="s2">, </span><span class="s1">prepend=</span><span class="s2">False</span><span class="s1">)</span>
        <span class="s1">cls.res1 = GLM(cls.data.endog</span><span class="s2">, </span><span class="s1">cls.data.exog</span><span class="s2">,</span>
                       <span class="s1">family=sm.families.Gaussian()).fit(method=</span><span class="s5">'newton'</span><span class="s1">)</span>
        <span class="s2">from </span><span class="s1">.results.results_glm </span><span class="s2">import </span><span class="s1">Longley</span>
        <span class="s1">cls.res2 = Longley()</span>


<span class="s2">class </span><span class="s1">TestGaussianLog(CheckModelResultsMixin):</span>
    <span class="s1">@classmethod</span>
    <span class="s2">def </span><span class="s1">setup_class(cls):</span>
        <span class="s3"># Test Precision</span>
        <span class="s1">cls.decimal_aic_R = DECIMAL_0</span>
        <span class="s1">cls.decimal_aic_Stata = DECIMAL_2</span>
        <span class="s1">cls.decimal_loglike = DECIMAL_0</span>
        <span class="s1">cls.decimal_null_deviance = DECIMAL_1</span>

        <span class="s1">nobs = </span><span class="s4">100</span>
        <span class="s1">x = np.arange(nobs)</span>
        <span class="s1">np.random.seed(</span><span class="s4">54321</span><span class="s1">)</span>
<span class="s3">#        y = 1.0 - .02*x - .001*x**2 + 0.001 * np.random.randn(nobs)</span>
        <span class="s1">cls.X = np.c_[np.ones((nobs</span><span class="s2">,</span><span class="s4">1</span><span class="s1">))</span><span class="s2">,</span><span class="s1">x</span><span class="s2">,</span><span class="s1">x**</span><span class="s4">2</span><span class="s1">]</span>
        <span class="s1">cls.lny = np.exp(-(-</span><span class="s4">1.0 </span><span class="s1">+ </span><span class="s4">0.02</span><span class="s1">*x + </span><span class="s4">0.0001</span><span class="s1">*x**</span><span class="s4">2</span><span class="s1">)) +\</span>
                        <span class="s4">0.001 </span><span class="s1">* np.random.randn(nobs)</span>

        <span class="s1">GaussLog_Model = GLM(cls.lny</span><span class="s2">, </span><span class="s1">cls.X</span><span class="s2">,</span>
                             <span class="s1">family=sm.families.Gaussian(sm.families.links.Log()))</span>
        <span class="s1">cls.res1 = GaussLog_Model.fit()</span>
        <span class="s2">from </span><span class="s1">.results.results_glm </span><span class="s2">import </span><span class="s1">GaussianLog</span>
        <span class="s1">cls.res2 = GaussianLog()</span>

<span class="s3"># FIXME: enable or delete</span>
<span class="s3">#    def setup(cls):</span>
<span class="s3">#        if skipR:</span>
<span class="s3">#            raise SkipTest, &quot;Rpy not installed&quot;</span>
<span class="s3">#        GaussLogLink = r.gaussian(link = &quot;log&quot;)</span>
<span class="s3">#        GaussLog_Res_R = RModel(cls.lny, cls.X, r.glm, family=GaussLogLink)</span>
<span class="s3">#        cls.res2 = GaussLog_Res_R</span>

<span class="s2">class </span><span class="s1">TestGaussianInverse(CheckModelResultsMixin):</span>
    <span class="s1">@classmethod</span>
    <span class="s2">def </span><span class="s1">setup_class(cls):</span>
        <span class="s3"># Test Precisions</span>
        <span class="s1">cls.decimal_bic = DECIMAL_1</span>
        <span class="s1">cls.decimal_aic_R = DECIMAL_1</span>
        <span class="s1">cls.decimal_aic_Stata = DECIMAL_3</span>
        <span class="s1">cls.decimal_loglike = DECIMAL_1</span>
        <span class="s1">cls.decimal_resids = DECIMAL_3</span>

        <span class="s1">nobs = </span><span class="s4">100</span>
        <span class="s1">x = np.arange(nobs)</span>
        <span class="s1">np.random.seed(</span><span class="s4">54321</span><span class="s1">)</span>
        <span class="s1">y = </span><span class="s4">1.0 </span><span class="s1">+ </span><span class="s4">2.0 </span><span class="s1">* x + x**</span><span class="s4">2 </span><span class="s1">+ </span><span class="s4">0.1 </span><span class="s1">* np.random.randn(nobs)</span>
        <span class="s1">cls.X = np.c_[np.ones((nobs</span><span class="s2">,</span><span class="s4">1</span><span class="s1">))</span><span class="s2">,</span><span class="s1">x</span><span class="s2">,</span><span class="s1">x**</span><span class="s4">2</span><span class="s1">]</span>
        <span class="s1">cls.y_inv = (</span><span class="s4">1. </span><span class="s1">+ </span><span class="s4">.02</span><span class="s1">*x + </span><span class="s4">.001</span><span class="s1">*x**</span><span class="s4">2</span><span class="s1">)**-</span><span class="s4">1 </span><span class="s1">+ </span><span class="s4">.001 </span><span class="s1">* np.random.randn(nobs)</span>
        <span class="s1">InverseLink_Model = GLM(cls.y_inv</span><span class="s2">, </span><span class="s1">cls.X</span><span class="s2">,</span>
                <span class="s1">family=sm.families.Gaussian(sm.families.links.InversePower()))</span>
        <span class="s1">InverseLink_Res = InverseLink_Model.fit()</span>
        <span class="s1">cls.res1 = InverseLink_Res</span>
        <span class="s2">from </span><span class="s1">.results.results_glm </span><span class="s2">import </span><span class="s1">GaussianInverse</span>
        <span class="s1">cls.res2 = GaussianInverse()</span>

<span class="s3"># FIXME: enable or delete</span>
<span class="s3">#    def setup(cls):</span>
<span class="s3">#        if skipR:</span>
<span class="s3">#            raise SkipTest, &quot;Rpy not installed.&quot;</span>
<span class="s3">#        InverseLink = r.gaussian(link = &quot;inverse&quot;)</span>
<span class="s3">#        InverseLink_Res_R = RModel(cls.y_inv, cls.X, r.glm, family=InverseLink)</span>
<span class="s3">#        cls.res2 = InverseLink_Res_R</span>

<span class="s2">class </span><span class="s1">TestGlmBinomial(CheckModelResultsMixin):</span>
    <span class="s1">@classmethod</span>
    <span class="s2">def </span><span class="s1">setup_class(cls):</span>
        <span class="s0">''' 
        Test Binomial family with canonical logit link using star98 dataset. 
        '''</span>
        <span class="s1">cls.decimal_resids = DECIMAL_1</span>
        <span class="s1">cls.decimal_bic = DECIMAL_2</span>

        <span class="s2">from </span><span class="s1">statsmodels.datasets.star98 </span><span class="s2">import </span><span class="s1">load</span>

        <span class="s2">from </span><span class="s1">.results.results_glm </span><span class="s2">import </span><span class="s1">Star98</span>
        <span class="s1">data = load()</span>
        <span class="s1">data.endog = np.require(data.endog</span><span class="s2">, </span><span class="s1">requirements=</span><span class="s5">&quot;W&quot;</span><span class="s1">)</span>
        <span class="s1">data.exog = np.require(data.exog</span><span class="s2">, </span><span class="s1">requirements=</span><span class="s5">&quot;W&quot;</span><span class="s1">)</span>
        <span class="s1">data.exog = add_constant(data.exog</span><span class="s2">, </span><span class="s1">prepend=</span><span class="s2">False</span><span class="s1">)</span>
        <span class="s1">cls.res1 = GLM(data.endog</span><span class="s2">, </span><span class="s1">data.exog</span><span class="s2">,</span>
                       <span class="s1">family=sm.families.Binomial()).fit()</span>
        <span class="s3"># NOTE: if you want to replicate with RModel</span>
        <span class="s3"># res2 = RModel(data.endog[:,0]/trials, data.exog, r.glm,</span>
        <span class="s3">#        family=r.binomial, weights=trials)</span>

        <span class="s1">cls.res2 = Star98()</span>

    <span class="s2">def </span><span class="s1">test_endog_dtype(self):</span>
        <span class="s2">from </span><span class="s1">statsmodels.datasets.star98 </span><span class="s2">import </span><span class="s1">load</span>
        <span class="s1">data = load()</span>
        <span class="s1">data.exog = add_constant(data.exog</span><span class="s2">, </span><span class="s1">prepend=</span><span class="s2">False</span><span class="s1">)</span>
        <span class="s1">endog = data.endog.astype(int)</span>
        <span class="s1">res2 = GLM(endog</span><span class="s2">, </span><span class="s1">data.exog</span><span class="s2">, </span><span class="s1">family=sm.families.Binomial()).fit()</span>
        <span class="s1">assert_allclose(res2.params</span><span class="s2">, </span><span class="s1">self.res1.params)</span>
        <span class="s1">endog = data.endog.astype(np.double)</span>
        <span class="s1">res3 = GLM(endog</span><span class="s2">, </span><span class="s1">data.exog</span><span class="s2">, </span><span class="s1">family=sm.families.Binomial()).fit()</span>
        <span class="s1">assert_allclose(res3.params</span><span class="s2">, </span><span class="s1">self.res1.params)</span>

    <span class="s2">def </span><span class="s1">test_invalid_endog(self</span><span class="s2">, </span><span class="s1">reset_randomstate):</span>
        <span class="s3"># GH2733 inspired check</span>
        <span class="s1">endog = np.random.randint(</span><span class="s4">0</span><span class="s2">, </span><span class="s4">100</span><span class="s2">, </span><span class="s1">size=(</span><span class="s4">1000</span><span class="s2">, </span><span class="s4">3</span><span class="s1">))</span>
        <span class="s1">exog = np.random.standard_normal((</span><span class="s4">1000</span><span class="s2">, </span><span class="s4">2</span><span class="s1">))</span>
        <span class="s2">with </span><span class="s1">pytest.raises(ValueError</span><span class="s2">, </span><span class="s1">match=</span><span class="s5">'endog has more than 2 columns'</span><span class="s1">):</span>
            <span class="s1">GLM(endog</span><span class="s2">, </span><span class="s1">exog</span><span class="s2">, </span><span class="s1">family=sm.families.Binomial())</span>

    <span class="s2">def </span><span class="s1">test_invalid_endog_formula(self</span><span class="s2">, </span><span class="s1">reset_randomstate):</span>
        <span class="s3"># GH2733</span>
        <span class="s1">n = </span><span class="s4">200</span>
        <span class="s1">exog = np.random.normal(size=(n</span><span class="s2">, </span><span class="s4">2</span><span class="s1">))</span>
        <span class="s1">endog = np.random.randint(</span><span class="s4">0</span><span class="s2">, </span><span class="s4">3</span><span class="s2">, </span><span class="s1">size=n).astype(str)</span>
        <span class="s3"># formula interface</span>
        <span class="s1">data = pd.DataFrame({</span><span class="s5">&quot;y&quot;</span><span class="s1">: endog</span><span class="s2">, </span><span class="s5">&quot;x1&quot;</span><span class="s1">: exog[:</span><span class="s2">, </span><span class="s4">0</span><span class="s1">]</span><span class="s2">, </span><span class="s5">&quot;x2&quot;</span><span class="s1">: exog[:</span><span class="s2">, </span><span class="s4">1</span><span class="s1">]})</span>
        <span class="s2">with </span><span class="s1">pytest.raises(ValueError</span><span class="s2">, </span><span class="s1">match=</span><span class="s5">'array with multiple columns'</span><span class="s1">):</span>
            <span class="s1">sm.GLM.from_formula(</span><span class="s5">&quot;y ~ x1 + x2&quot;</span><span class="s2">, </span><span class="s1">data</span><span class="s2">,</span>
                                <span class="s1">family=sm.families.Binomial())</span>

    <span class="s2">def </span><span class="s1">test_get_distribution_binom_count(self):</span>
        <span class="s3"># test for binomial counts with n_trials &gt; 1</span>
        <span class="s1">res1 = self.res1</span>
        <span class="s1">res_scale = </span><span class="s4">1  </span><span class="s3"># QMLE scale can differ from 1</span>

        <span class="s1">mu_prob = res1.fittedvalues</span>
        <span class="s1">n = res1.model.n_trials</span>
        <span class="s1">distr = res1.model.family.get_distribution(mu_prob</span><span class="s2">, </span><span class="s1">res_scale</span><span class="s2">,</span>
                                                   <span class="s1">n_trials=n)</span>
        <span class="s1">var_endog = res1.model.family.variance(mu_prob) * res_scale</span>
        <span class="s1">m</span><span class="s2">, </span><span class="s1">v = distr.stats()</span>
        <span class="s1">assert_allclose(mu_prob * n</span><span class="s2">, </span><span class="s1">m</span><span class="s2">, </span><span class="s1">rtol=</span><span class="s4">1e-13</span><span class="s1">)</span>
        <span class="s1">assert_allclose(var_endog * n</span><span class="s2">, </span><span class="s1">v</span><span class="s2">, </span><span class="s1">rtol=</span><span class="s4">1e-13</span><span class="s1">)</span>

        <span class="s3"># check model method</span>
        <span class="s1">distr2 = res1.model.get_distribution(res1.params</span><span class="s2">, </span><span class="s1">res_scale</span><span class="s2">,</span>
                                             <span class="s1">n_trials=n)</span>
        <span class="s2">for </span><span class="s1">k </span><span class="s2">in </span><span class="s1">distr2.kwds:</span>
            <span class="s1">assert_allclose(distr.kwds[k]</span><span class="s2">, </span><span class="s1">distr2.kwds[k]</span><span class="s2">, </span><span class="s1">rtol=</span><span class="s4">1e-13</span><span class="s1">)</span>


<span class="s3"># FIXME: enable/xfail/skip or delete</span>
<span class="s3"># TODO:</span>
<span class="s3"># Non-Canonical Links for the Binomial family require the algorithm to be</span>
<span class="s3"># slightly changed</span>
<span class="s3"># class TestGlmBinomialLog(CheckModelResultsMixin):</span>
<span class="s3">#    pass</span>

<span class="s3"># class TestGlmBinomialLogit(CheckModelResultsMixin):</span>
<span class="s3">#    pass</span>

<span class="s3"># class TestGlmBinomialProbit(CheckModelResultsMixin):</span>
<span class="s3">#    pass</span>

<span class="s3"># class TestGlmBinomialCloglog(CheckModelResultsMixin):</span>
<span class="s3">#    pass</span>

<span class="s3"># class TestGlmBinomialPower(CheckModelResultsMixin):</span>
<span class="s3">#    pass</span>

<span class="s3"># class TestGlmBinomialLoglog(CheckModelResultsMixin):</span>
<span class="s3">#    pass</span>

<span class="s3"># class TestGlmBinomialLogc(CheckModelResultsMixin):</span>
<span class="s3"># TODO: need include logc link</span>
<span class="s3">#    pass</span>


<span class="s2">class </span><span class="s1">TestGlmBernoulli(CheckModelResultsMixin</span><span class="s2">, </span><span class="s1">CheckComparisonMixin):</span>
    <span class="s1">@classmethod</span>
    <span class="s2">def </span><span class="s1">setup_class(cls):</span>
        <span class="s2">from </span><span class="s1">.results.results_glm </span><span class="s2">import </span><span class="s1">Lbw</span>
        <span class="s1">cls.res2 = Lbw()</span>
        <span class="s1">cls.res1 = GLM(cls.res2.endog</span><span class="s2">, </span><span class="s1">cls.res2.exog</span><span class="s2">,</span>
                       <span class="s1">family=sm.families.Binomial()).fit()</span>

        <span class="s1">modd = discrete.Logit(cls.res2.endog</span><span class="s2">, </span><span class="s1">cls.res2.exog)</span>
        <span class="s1">cls.resd = modd.fit(start_params=cls.res1.params * </span><span class="s4">0.9</span><span class="s2">, </span><span class="s1">disp=</span><span class="s2">False</span><span class="s1">)</span>

    <span class="s2">def </span><span class="s1">test_score_r(self):</span>
        <span class="s1">res1 = self.res1</span>
        <span class="s1">res2 = self.res2</span>
        <span class="s1">st</span><span class="s2">, </span><span class="s1">pv</span><span class="s2">, </span><span class="s1">df = res1.model.score_test(res1.params</span><span class="s2">,</span>
                                           <span class="s1">exog_extra=res1.model.exog[:</span><span class="s2">, </span><span class="s4">1</span><span class="s1">]**</span><span class="s4">2</span><span class="s1">)</span>
        <span class="s1">st_res = </span><span class="s4">0.2837680293459376  </span><span class="s3"># (-0.5326988167303712)**2</span>
        <span class="s1">assert_allclose(st</span><span class="s2">, </span><span class="s1">st_res</span><span class="s2">, </span><span class="s1">rtol=</span><span class="s4">1e-4</span><span class="s1">)</span>

        <span class="s1">st</span><span class="s2">, </span><span class="s1">pv</span><span class="s2">, </span><span class="s1">df = res1.model.score_test(res1.params</span><span class="s2">,</span>
                                          <span class="s1">exog_extra=res1.model.exog[:</span><span class="s2">, </span><span class="s4">0</span><span class="s1">]**</span><span class="s4">2</span><span class="s1">)</span>
        <span class="s1">st_res = </span><span class="s4">0.6713492821514992  </span><span class="s3"># (-0.8193590679009413)**2</span>
        <span class="s1">assert_allclose(st</span><span class="s2">, </span><span class="s1">st_res</span><span class="s2">, </span><span class="s1">rtol=</span><span class="s4">1e-4</span><span class="s1">)</span>

        <span class="s1">select = list(range(</span><span class="s4">9</span><span class="s1">))</span>
        <span class="s1">select.pop(</span><span class="s4">7</span><span class="s1">)</span>

        <span class="s1">res1b = GLM(res2.endog</span><span class="s2">, </span><span class="s1">res2.exog.iloc[:</span><span class="s2">, </span><span class="s1">select]</span><span class="s2">,</span>
                    <span class="s1">family=sm.families.Binomial()).fit()</span>
        <span class="s1">tres = res1b.model.score_test(res1b.params</span><span class="s2">,</span>
                                      <span class="s1">exog_extra=res1.model.exog[:</span><span class="s2">, </span><span class="s1">-</span><span class="s4">2</span><span class="s1">])</span>
        <span class="s1">tres = np.asarray(tres[:</span><span class="s4">2</span><span class="s1">]).ravel()</span>
        <span class="s1">tres_r = (</span><span class="s4">2.7864148487452</span><span class="s2">, </span><span class="s4">0.0950667</span><span class="s1">)</span>
        <span class="s1">assert_allclose(tres</span><span class="s2">, </span><span class="s1">tres_r</span><span class="s2">, </span><span class="s1">rtol=</span><span class="s4">1e-4</span><span class="s1">)</span>

        <span class="s1">cmd_r = </span><span class="s5">&quot;&quot;&quot;</span><span class="s2">\ 
        </span><span class="s5">data = read.csv(&quot;...statsmodels</span><span class="s2">\\</span><span class="s5">statsmodels</span><span class="s2">\\</span><span class="s5">genmod</span><span class="s2">\\</span><span class="s5">tests</span><span class="s2">\\</span><span class="s5">results</span><span class="s2">\\</span><span class="s5">stata_lbw_glm.csv&quot;) 
 
        data[&quot;race_black&quot;] = data[&quot;race&quot;] == &quot;black&quot; 
        data[&quot;race_other&quot;] = data[&quot;race&quot;] == &quot;other&quot; 
        mod = glm(low ~ age + lwt + race_black + race_other + smoke + ptl + ht + ui, family=binomial, data=data) 
        options(digits=16) 
        anova(mod, test=&quot;Rao&quot;) 
 
        library(statmod) 
        s = glm.scoretest(mod, data[&quot;age&quot;]**2) 
        s**2 
        s = glm.scoretest(mod, data[&quot;lwt&quot;]**2) 
        s**2 
        &quot;&quot;&quot;</span>

<span class="s3"># class TestGlmBernoulliIdentity(CheckModelResultsMixin):</span>
<span class="s3">#    pass</span>

<span class="s3"># class TestGlmBernoulliLog(CheckModelResultsMixin):</span>
<span class="s3">#    pass</span>

<span class="s3"># class TestGlmBernoulliProbit(CheckModelResultsMixin):</span>
<span class="s3">#    pass</span>

<span class="s3"># class TestGlmBernoulliCloglog(CheckModelResultsMixin):</span>
<span class="s3">#    pass</span>

<span class="s3"># class TestGlmBernoulliPower(CheckModelResultsMixin):</span>
<span class="s3">#    pass</span>

<span class="s3"># class TestGlmBernoulliLoglog(CheckModelResultsMixin):</span>
<span class="s3">#    pass</span>

<span class="s3"># class test_glm_bernoulli_logc(CheckModelResultsMixin):</span>
<span class="s3">#    pass</span>


<span class="s2">class </span><span class="s1">TestGlmGamma(CheckModelResultsMixin):</span>

    <span class="s1">@classmethod</span>
    <span class="s2">def </span><span class="s1">setup_class(cls):</span>
        <span class="s0">''' 
        Tests Gamma family with canonical inverse link (power -1) 
        '''</span>
        <span class="s3"># Test Precisions</span>
        <span class="s1">cls.decimal_aic_R = -</span><span class="s4">1 </span><span class="s3">#TODO: off by about 1, we are right with Stata</span>
        <span class="s1">cls.decimal_resids = DECIMAL_2</span>

        <span class="s2">from </span><span class="s1">statsmodels.datasets.scotland </span><span class="s2">import </span><span class="s1">load</span>

        <span class="s2">from </span><span class="s1">.results.results_glm </span><span class="s2">import </span><span class="s1">Scotvote</span>
        <span class="s1">data = load()</span>
        <span class="s1">data.exog = add_constant(data.exog</span><span class="s2">, </span><span class="s1">prepend=</span><span class="s2">False</span><span class="s1">)</span>
        <span class="s2">with </span><span class="s1">warnings.catch_warnings():</span>
            <span class="s1">warnings.simplefilter(</span><span class="s5">&quot;ignore&quot;</span><span class="s1">)</span>
            <span class="s1">res1 = GLM(data.endog</span><span class="s2">, </span><span class="s1">data.exog</span><span class="s2">,</span>
                       <span class="s1">family=sm.families.Gamma()).fit()</span>
        <span class="s1">cls.res1 = res1</span>
<span class="s3">#        res2 = RModel(data.endog, data.exog, r.glm, family=r.Gamma)</span>
        <span class="s1">res2 = Scotvote()</span>
        <span class="s1">res2.aic_R += </span><span class="s4">2 </span><span class="s3"># R does not count degree of freedom for scale with gamma</span>
        <span class="s1">cls.res2 = res2</span>


<span class="s2">class </span><span class="s1">TestGlmGammaLog(CheckModelResultsMixin):</span>
    <span class="s1">@classmethod</span>
    <span class="s2">def </span><span class="s1">setup_class(cls):</span>
        <span class="s3"># Test Precisions</span>
        <span class="s1">cls.decimal_resids = DECIMAL_3</span>
        <span class="s1">cls.decimal_aic_R = DECIMAL_0</span>
        <span class="s1">cls.decimal_fittedvalues = DECIMAL_3</span>

        <span class="s2">from </span><span class="s1">.results.results_glm </span><span class="s2">import </span><span class="s1">CancerLog</span>
        <span class="s1">res2 = CancerLog()</span>
        <span class="s1">cls.res1 = GLM(res2.endog</span><span class="s2">, </span><span class="s1">res2.exog</span><span class="s2">,</span>
            <span class="s1">family=sm.families.Gamma(link=sm.families.links.Log())).fit()</span>
        <span class="s1">cls.res2 = res2</span>

<span class="s3"># FIXME: enable or delete</span>
<span class="s3">#    def setup(cls):</span>
<span class="s3">#        if skipR:</span>
<span class="s3">#            raise SkipTest, &quot;Rpy not installed.&quot;</span>
<span class="s3">#        cls.res2 = RModel(cls.data.endog, cls.data.exog, r.glm,</span>
<span class="s3">#            family=r.Gamma(link=&quot;log&quot;))</span>
<span class="s3">#        cls.res2.null_deviance = 27.92207137420696 # From R (bug in rpy)</span>
<span class="s3">#        cls.res2.bic = -154.1582089453923 # from Stata</span>


<span class="s2">class </span><span class="s1">TestGlmGammaIdentity(CheckModelResultsMixin):</span>
    <span class="s1">@classmethod</span>
    <span class="s2">def </span><span class="s1">setup_class(cls):</span>
        <span class="s3"># Test Precisions</span>
        <span class="s1">cls.decimal_resids = -</span><span class="s4">100 </span><span class="s3">#TODO Very off from Stata?</span>
        <span class="s1">cls.decimal_params = DECIMAL_2</span>
        <span class="s1">cls.decimal_aic_R = DECIMAL_0</span>
        <span class="s1">cls.decimal_loglike = DECIMAL_1</span>

        <span class="s2">from </span><span class="s1">.results.results_glm </span><span class="s2">import </span><span class="s1">CancerIdentity</span>
        <span class="s1">res2 = CancerIdentity()</span>
        <span class="s2">with </span><span class="s1">warnings.catch_warnings():</span>
            <span class="s1">warnings.simplefilter(</span><span class="s5">&quot;ignore&quot;</span><span class="s1">)</span>
            <span class="s1">fam = sm.families.Gamma(link=sm.families.links.Identity())</span>
            <span class="s1">cls.res1 = GLM(res2.endog</span><span class="s2">, </span><span class="s1">res2.exog</span><span class="s2">, </span><span class="s1">family=fam).fit()</span>
        <span class="s1">cls.res2 = res2</span>

<span class="s3"># FIXME: enable or delete</span>
<span class="s3">#    def setup(cls):</span>
<span class="s3">#        if skipR:</span>
<span class="s3">#            raise SkipTest, &quot;Rpy not installed.&quot;</span>
<span class="s3">#        cls.res2 = RModel(cls.data.endog, cls.data.exog, r.glm,</span>
<span class="s3">#            family=r.Gamma(link=&quot;identity&quot;))</span>
<span class="s3">#        cls.res2.null_deviance = 27.92207137420696 # from R, Rpy bug</span>

<span class="s2">class </span><span class="s1">TestGlmPoisson(CheckModelResultsMixin</span><span class="s2">, </span><span class="s1">CheckComparisonMixin):</span>
    <span class="s1">@classmethod</span>
    <span class="s2">def </span><span class="s1">setup_class(cls):</span>
        <span class="s0">''' 
        Tests Poisson family with canonical log link. 
 
        Test results were obtained by R. 
        '''</span>
        <span class="s2">from </span><span class="s1">.results.results_glm </span><span class="s2">import </span><span class="s1">Cpunish</span>
        <span class="s1">cls.data = cpunish.load()</span>
        <span class="s1">cls.data.endog = np.require(cls.data.endog</span><span class="s2">, </span><span class="s1">requirements=</span><span class="s5">&quot;W&quot;</span><span class="s1">)</span>
        <span class="s1">cls.data.exog = np.require(cls.data.exog</span><span class="s2">, </span><span class="s1">requirements=</span><span class="s5">&quot;W&quot;</span><span class="s1">)</span>
        <span class="s1">cls.data.exog[:</span><span class="s2">, </span><span class="s4">3</span><span class="s1">] = np.log(cls.data.exog[:</span><span class="s2">, </span><span class="s4">3</span><span class="s1">])</span>
        <span class="s1">cls.data.exog = add_constant(cls.data.exog</span><span class="s2">, </span><span class="s1">prepend=</span><span class="s2">False</span><span class="s1">)</span>
        <span class="s1">cls.res1 = GLM(cls.data.endog</span><span class="s2">, </span><span class="s1">cls.data.exog</span><span class="s2">,</span>
                       <span class="s1">family=sm.families.Poisson()).fit()</span>
        <span class="s1">cls.res2 = Cpunish()</span>
        <span class="s3"># compare with discrete, start close to save time</span>
        <span class="s1">modd = discrete.Poisson(cls.data.endog</span><span class="s2">, </span><span class="s1">cls.data.exog)</span>
        <span class="s1">cls.resd = modd.fit(start_params=cls.res1.params * </span><span class="s4">0.9</span><span class="s2">, </span><span class="s1">disp=</span><span class="s2">False</span><span class="s1">)</span>

<span class="s3">#class TestGlmPoissonIdentity(CheckModelResultsMixin):</span>
<span class="s3">#    pass</span>

<span class="s3">#class TestGlmPoissonPower(CheckModelResultsMixin):</span>
<span class="s3">#    pass</span>


<span class="s2">class </span><span class="s1">TestGlmInvgauss(CheckModelResultsMixin):</span>
    <span class="s1">@classmethod</span>
    <span class="s2">def </span><span class="s1">setup_class(cls):</span>
        <span class="s0">''' 
        Tests the Inverse Gaussian family in GLM. 
 
        Notes 
        ----- 
        Used the rndivgx.ado file provided by Hardin and Hilbe to 
        generate the data.  Results are read from model_results, which 
        were obtained by running R_ig.s 
        '''</span>
        <span class="s3"># Test Precisions</span>
        <span class="s1">cls.decimal_aic_R = DECIMAL_0</span>
        <span class="s1">cls.decimal_loglike = DECIMAL_0</span>

        <span class="s2">from </span><span class="s1">.results.results_glm </span><span class="s2">import </span><span class="s1">InvGauss</span>
        <span class="s1">res2 = InvGauss()</span>
        <span class="s1">res1 = GLM(res2.endog</span><span class="s2">, </span><span class="s1">res2.exog</span><span class="s2">,</span>
                   <span class="s1">family=sm.families.InverseGaussian()).fit()</span>
        <span class="s1">cls.res1 = res1</span>
        <span class="s1">cls.res2 = res2</span>

    <span class="s2">def </span><span class="s1">test_get_distribution(self):</span>
        <span class="s1">res1 = self.res1</span>
        <span class="s1">distr = res1.model.family.get_distribution(res1.fittedvalues</span><span class="s2">,</span>
                                                   <span class="s1">res1.scale)</span>
        <span class="s1">var_endog = res1.model.family.variance(res1.fittedvalues) * res1.scale</span>
        <span class="s1">m</span><span class="s2">, </span><span class="s1">v = distr.stats()</span>
        <span class="s1">assert_allclose(res1.fittedvalues</span><span class="s2">, </span><span class="s1">m</span><span class="s2">, </span><span class="s1">rtol=</span><span class="s4">1e-13</span><span class="s1">)</span>
        <span class="s1">assert_allclose(var_endog</span><span class="s2">, </span><span class="s1">v</span><span class="s2">, </span><span class="s1">rtol=</span><span class="s4">1e-13</span><span class="s1">)</span>


<span class="s2">class </span><span class="s1">TestGlmInvgaussLog(CheckModelResultsMixin):</span>
    <span class="s1">@classmethod</span>
    <span class="s2">def </span><span class="s1">setup_class(cls):</span>
        <span class="s3"># Test Precisions</span>
        <span class="s1">cls.decimal_aic_R = -</span><span class="s4">10 </span><span class="s3"># Big difference vs R.</span>
        <span class="s1">cls.decimal_resids = DECIMAL_3</span>

        <span class="s2">from </span><span class="s1">.results.results_glm </span><span class="s2">import </span><span class="s1">InvGaussLog</span>
        <span class="s1">res2 = InvGaussLog()</span>
        <span class="s1">cls.res1 = GLM(res2.endog</span><span class="s2">, </span><span class="s1">res2.exog</span><span class="s2">,</span>
            <span class="s1">family=sm.families.InverseGaussian(</span>
                <span class="s1">link=sm.families.links.Log())).fit()</span>
        <span class="s1">cls.res2 = res2</span>

<span class="s3"># FIXME: enable or delete</span>
<span class="s3">#    def setup(cls):</span>
<span class="s3">#        if skipR:</span>
<span class="s3">#            raise SkipTest, &quot;Rpy not installed.&quot;</span>
<span class="s3">#        cls.res2 = RModel(cls.data.endog, cls.data.exog, r.glm,</span>
<span class="s3">#            family=r.inverse_gaussian(link=&quot;log&quot;))</span>
<span class="s3">#        cls.res2.null_deviance = 335.1539777981053 # from R, Rpy bug</span>
<span class="s3">#        cls.res2.llf = -12162.72308 # from Stata, R's has big rounding diff</span>


<span class="s2">class </span><span class="s1">TestGlmInvgaussIdentity(CheckModelResultsMixin):</span>
    <span class="s1">@classmethod</span>
    <span class="s2">def </span><span class="s1">setup_class(cls):</span>
        <span class="s3"># Test Precisions</span>
        <span class="s1">cls.decimal_aic_R = -</span><span class="s4">10 </span><span class="s3">#TODO: Big difference vs R</span>
        <span class="s1">cls.decimal_fittedvalues = DECIMAL_3</span>
        <span class="s1">cls.decimal_params = DECIMAL_3</span>

        <span class="s2">from </span><span class="s1">.results.results_glm </span><span class="s2">import </span><span class="s1">Medpar1</span>
        <span class="s1">data = Medpar1()</span>
        <span class="s2">with </span><span class="s1">warnings.catch_warnings():</span>
            <span class="s1">warnings.simplefilter(</span><span class="s5">&quot;ignore&quot;</span><span class="s1">)</span>
            <span class="s1">cls.res1 = GLM(data.endog</span><span class="s2">, </span><span class="s1">data.exog</span><span class="s2">,</span>
                            <span class="s1">family=sm.families.InverseGaussian(</span>
                                <span class="s1">link=sm.families.links.Identity())).fit()</span>
        <span class="s2">from </span><span class="s1">.results.results_glm </span><span class="s2">import </span><span class="s1">InvGaussIdentity</span>
        <span class="s1">cls.res2 = InvGaussIdentity()</span>

<span class="s3"># FIXME: enable or delete</span>
<span class="s3">#    def setup(cls):</span>
<span class="s3">#        if skipR:</span>
<span class="s3">#            raise SkipTest, &quot;Rpy not installed.&quot;</span>
<span class="s3">#        cls.res2 = RModel(cls.data.endog, cls.data.exog, r.glm,</span>
<span class="s3">#            family=r.inverse_gaussian(link=&quot;identity&quot;))</span>
<span class="s3">#        cls.res2.null_deviance = 335.1539777981053 # from R, Rpy bug</span>
<span class="s3">#        cls.res2.llf = -12163.25545    # from Stata, big diff with R</span>


<span class="s2">class </span><span class="s1">TestGlmNegbinomial(CheckModelResultsMixin):</span>
    <span class="s1">@classmethod</span>
    <span class="s2">def </span><span class="s1">setup_class(cls):</span>
        <span class="s0">''' 
        Test Negative Binomial family with log link 
        '''</span>
        <span class="s3"># Test Precision</span>
        <span class="s1">cls.decimal_resid = DECIMAL_1</span>
        <span class="s1">cls.decimal_params = DECIMAL_3</span>
        <span class="s1">cls.decimal_resids = -</span><span class="s4">1 </span><span class="s3"># 1 % mismatch at 0</span>
        <span class="s1">cls.decimal_fittedvalues = DECIMAL_1</span>

        <span class="s2">from </span><span class="s1">statsmodels.datasets.committee </span><span class="s2">import </span><span class="s1">load</span>
        <span class="s1">cls.data = load()</span>
        <span class="s1">cls.data.endog = np.require(cls.data.endog</span><span class="s2">, </span><span class="s1">requirements=</span><span class="s5">&quot;W&quot;</span><span class="s1">)</span>
        <span class="s1">cls.data.exog = np.require(cls.data.exog</span><span class="s2">, </span><span class="s1">requirements=</span><span class="s5">&quot;W&quot;</span><span class="s1">)</span>
        <span class="s1">cls.data.exog[:</span><span class="s2">,</span><span class="s4">2</span><span class="s1">] = np.log(cls.data.exog[:</span><span class="s2">,</span><span class="s4">2</span><span class="s1">])</span>
        <span class="s1">interaction = cls.data.exog[:</span><span class="s2">,</span><span class="s4">2</span><span class="s1">]*cls.data.exog[:</span><span class="s2">,</span><span class="s4">1</span><span class="s1">]</span>
        <span class="s1">cls.data.exog = np.column_stack((cls.data.exog</span><span class="s2">,</span><span class="s1">interaction))</span>
        <span class="s1">cls.data.exog = add_constant(cls.data.exog</span><span class="s2">, </span><span class="s1">prepend=</span><span class="s2">False</span><span class="s1">)</span>
        <span class="s2">with </span><span class="s1">warnings.catch_warnings():</span>
            <span class="s1">warnings.simplefilter(</span><span class="s5">&quot;ignore&quot;</span><span class="s2">, </span><span class="s1">category=DomainWarning)</span>
            <span class="s2">with </span><span class="s1">pytest.warns(UserWarning):</span>
                <span class="s1">fam = sm.families.NegativeBinomial()</span>

        <span class="s1">cls.res1 = GLM(cls.data.endog</span><span class="s2">, </span><span class="s1">cls.data.exog</span><span class="s2">,</span>
                <span class="s1">family=fam).fit(scale=</span><span class="s5">'x2'</span><span class="s1">)</span>
        <span class="s2">from </span><span class="s1">.results.results_glm </span><span class="s2">import </span><span class="s1">Committee</span>
        <span class="s1">res2 = Committee()</span>
        <span class="s1">res2.aic_R += </span><span class="s4">2 </span><span class="s3"># They do not count a degree of freedom for the scale</span>
        <span class="s1">cls.res2 = res2</span>
        <span class="s1">cls.has_edispersion = </span><span class="s2">True</span>

<span class="s3"># FIXME: enable or delete</span>
<span class="s3">#    def setup_method(self):</span>
<span class="s3">#        if skipR:</span>
<span class="s3">#            raise SkipTest, &quot;Rpy not installed&quot;</span>
<span class="s3">#        r.library('MASS')  # this does not work when done in rmodelwrap?</span>
<span class="s3">#        self.res2 = RModel(self.data.endog, self.data.exog, r.glm,</span>
<span class="s3">#                family=r.negative_binomial(1))</span>
<span class="s3">#        self.res2.null_deviance = 27.8110469364343</span>

<span class="s3"># FIXME: enable/xfail/skip or delete</span>
<span class="s3">#class TestGlmNegbinomial_log(CheckModelResultsMixin):</span>
<span class="s3">#    pass</span>

<span class="s3"># FIXME: enable/xfail/skip or delete</span>
<span class="s3">#class TestGlmNegbinomial_power(CheckModelResultsMixin):</span>
<span class="s3">#    pass</span>

<span class="s3"># FIXME: enable/xfail/skip or delete</span>
<span class="s3">#class TestGlmNegbinomial_nbinom(CheckModelResultsMixin):</span>
<span class="s3">#    pass</span>


<span class="s2">class </span><span class="s1">TestGlmPoissonOffset(CheckModelResultsMixin):</span>
    <span class="s1">@classmethod</span>
    <span class="s2">def </span><span class="s1">setup_class(cls):</span>
        <span class="s2">from </span><span class="s1">.results.results_glm </span><span class="s2">import </span><span class="s1">Cpunish_offset</span>
        <span class="s1">cls.decimal_params = DECIMAL_4</span>
        <span class="s1">cls.decimal_bse = DECIMAL_4</span>
        <span class="s1">cls.decimal_aic_R = </span><span class="s4">3</span>
        <span class="s1">data = cpunish.load()</span>
        <span class="s1">data.endog = np.asarray(data.endog)</span>
        <span class="s1">data.exog = np.asarray(data.exog)</span>
        <span class="s1">data.exog[:</span><span class="s2">, </span><span class="s4">3</span><span class="s1">] = np.log(data.exog[:</span><span class="s2">, </span><span class="s4">3</span><span class="s1">])</span>
        <span class="s1">data.exog = add_constant(data.exog</span><span class="s2">, </span><span class="s1">prepend=</span><span class="s2">True</span><span class="s1">)</span>
        <span class="s1">exposure = [</span><span class="s4">100</span><span class="s1">] * len(data.endog)</span>
        <span class="s1">cls.data = data</span>
        <span class="s1">cls.exposure = exposure</span>
        <span class="s1">cls.res1 = GLM(data.endog</span><span class="s2">, </span><span class="s1">data.exog</span><span class="s2">, </span><span class="s1">family=sm.families.Poisson()</span><span class="s2">,</span>
                       <span class="s1">exposure=exposure).fit()</span>
        <span class="s1">cls.res2 = Cpunish_offset()</span>

    <span class="s2">def </span><span class="s1">test_missing(self):</span>
        <span class="s3"># make sure offset is dropped correctly</span>
        <span class="s1">endog = self.data.endog.copy()</span>
        <span class="s1">endog[[</span><span class="s4">2</span><span class="s2">,</span><span class="s4">4</span><span class="s2">,</span><span class="s4">6</span><span class="s2">,</span><span class="s4">8</span><span class="s1">]] = np.nan</span>
        <span class="s1">mod = GLM(endog</span><span class="s2">, </span><span class="s1">self.data.exog</span><span class="s2">, </span><span class="s1">family=sm.families.Poisson()</span><span class="s2">,</span>
                    <span class="s1">exposure=self.exposure</span><span class="s2">, </span><span class="s1">missing=</span><span class="s5">'drop'</span><span class="s1">)</span>
        <span class="s1">assert_equal(mod.exposure.shape[</span><span class="s4">0</span><span class="s1">]</span><span class="s2">, </span><span class="s4">13</span><span class="s1">)</span>

    <span class="s2">def </span><span class="s1">test_offset_exposure(self):</span>
        <span class="s3"># exposure=x and offset=log(x) should have the same effect</span>
        <span class="s1">np.random.seed(</span><span class="s4">382304</span><span class="s1">)</span>
        <span class="s1">endog = np.random.randint(</span><span class="s4">0</span><span class="s2">, </span><span class="s4">10</span><span class="s2">, </span><span class="s4">100</span><span class="s1">)</span>
        <span class="s1">exog = np.random.normal(size=(</span><span class="s4">100</span><span class="s2">,</span><span class="s4">3</span><span class="s1">))</span>
        <span class="s1">exposure = np.random.uniform(</span><span class="s4">1</span><span class="s2">, </span><span class="s4">2</span><span class="s2">, </span><span class="s4">100</span><span class="s1">)</span>
        <span class="s1">offset = np.random.uniform(</span><span class="s4">1</span><span class="s2">, </span><span class="s4">2</span><span class="s2">, </span><span class="s4">100</span><span class="s1">)</span>
        <span class="s1">mod1 = GLM(endog</span><span class="s2">, </span><span class="s1">exog</span><span class="s2">, </span><span class="s1">family=sm.families.Poisson()</span><span class="s2">,</span>
                   <span class="s1">offset=offset</span><span class="s2">, </span><span class="s1">exposure=exposure).fit()</span>
        <span class="s1">offset2 = offset + np.log(exposure)</span>
        <span class="s1">mod2 = GLM(endog</span><span class="s2">, </span><span class="s1">exog</span><span class="s2">, </span><span class="s1">family=sm.families.Poisson()</span><span class="s2">,</span>
                   <span class="s1">offset=offset2).fit()</span>
        <span class="s1">assert_almost_equal(mod1.params</span><span class="s2">, </span><span class="s1">mod2.params)</span>
        <span class="s1">assert_allclose(mod1.null</span><span class="s2">, </span><span class="s1">mod2.null</span><span class="s2">, </span><span class="s1">rtol=</span><span class="s4">1e-10</span><span class="s1">)</span>

        <span class="s3"># test recreating model</span>
        <span class="s1">mod1_ = mod1.model</span>
        <span class="s1">kwds = mod1_._get_init_kwds()</span>
        <span class="s1">assert_allclose(kwds[</span><span class="s5">'exposure'</span><span class="s1">]</span><span class="s2">, </span><span class="s1">exposure</span><span class="s2">, </span><span class="s1">rtol=</span><span class="s4">1e-14</span><span class="s1">)</span>
        <span class="s1">assert_allclose(kwds[</span><span class="s5">'offset'</span><span class="s1">]</span><span class="s2">, </span><span class="s1">mod1_.offset</span><span class="s2">, </span><span class="s1">rtol=</span><span class="s4">1e-14</span><span class="s1">)</span>
        <span class="s1">mod3 = mod1_.__class__(mod1_.endog</span><span class="s2">, </span><span class="s1">mod1_.exog</span><span class="s2">, </span><span class="s1">**kwds)</span>
        <span class="s1">assert_allclose(mod3.exposure</span><span class="s2">, </span><span class="s1">mod1_.exposure</span><span class="s2">, </span><span class="s1">rtol=</span><span class="s4">1e-14</span><span class="s1">)</span>
        <span class="s1">assert_allclose(mod3.offset</span><span class="s2">, </span><span class="s1">mod1_.offset</span><span class="s2">, </span><span class="s1">rtol=</span><span class="s4">1e-14</span><span class="s1">)</span>

        <span class="s3"># test fit_regularized exposure, see #4605</span>
        <span class="s1">resr1 = mod1.model.fit_regularized()</span>
        <span class="s1">resr2 = mod2.model.fit_regularized()</span>
        <span class="s1">assert_allclose(resr1.params</span><span class="s2">, </span><span class="s1">resr2.params</span><span class="s2">, </span><span class="s1">rtol=</span><span class="s4">1e-10</span><span class="s1">)</span>


    <span class="s2">def </span><span class="s1">test_predict(self):</span>
        <span class="s1">np.random.seed(</span><span class="s4">382304</span><span class="s1">)</span>
        <span class="s1">endog = np.random.randint(</span><span class="s4">0</span><span class="s2">, </span><span class="s4">10</span><span class="s2">, </span><span class="s4">100</span><span class="s1">)</span>
        <span class="s1">exog = np.random.normal(size=(</span><span class="s4">100</span><span class="s2">,</span><span class="s4">3</span><span class="s1">))</span>
        <span class="s1">exposure = np.random.uniform(</span><span class="s4">1</span><span class="s2">, </span><span class="s4">2</span><span class="s2">, </span><span class="s4">100</span><span class="s1">)</span>
        <span class="s1">mod1 = GLM(endog</span><span class="s2">, </span><span class="s1">exog</span><span class="s2">, </span><span class="s1">family=sm.families.Poisson()</span><span class="s2">,</span>
                   <span class="s1">exposure=exposure).fit()</span>
        <span class="s1">exog1 = np.random.normal(size=(</span><span class="s4">10</span><span class="s2">,</span><span class="s4">3</span><span class="s1">))</span>
        <span class="s1">exposure1 = np.random.uniform(</span><span class="s4">1</span><span class="s2">, </span><span class="s4">2</span><span class="s2">, </span><span class="s4">10</span><span class="s1">)</span>

        <span class="s3"># Doubling exposure time should double expected response</span>
        <span class="s1">pred1 = mod1.predict(exog=exog1</span><span class="s2">, </span><span class="s1">exposure=exposure1)</span>
        <span class="s1">pred2 = mod1.predict(exog=exog1</span><span class="s2">, </span><span class="s1">exposure=</span><span class="s4">2</span><span class="s1">*exposure1)</span>
        <span class="s1">assert_almost_equal(pred2</span><span class="s2">, </span><span class="s4">2</span><span class="s1">*pred1)</span>

        <span class="s3"># Check exposure defaults</span>
        <span class="s1">pred3 = mod1.predict()</span>
        <span class="s1">pred4 = mod1.predict(exposure=exposure)</span>
        <span class="s1">pred5 = mod1.predict(exog=exog</span><span class="s2">, </span><span class="s1">exposure=exposure)</span>
        <span class="s1">assert_almost_equal(pred3</span><span class="s2">, </span><span class="s1">pred4)</span>
        <span class="s1">assert_almost_equal(pred4</span><span class="s2">, </span><span class="s1">pred5)</span>

        <span class="s3"># Check offset defaults</span>
        <span class="s1">offset = np.random.uniform(</span><span class="s4">1</span><span class="s2">, </span><span class="s4">2</span><span class="s2">, </span><span class="s4">100</span><span class="s1">)</span>
        <span class="s1">mod2 = GLM(endog</span><span class="s2">, </span><span class="s1">exog</span><span class="s2">, </span><span class="s1">offset=offset</span><span class="s2">,</span>
                   <span class="s1">family=sm.families.Poisson()).fit()</span>
        <span class="s1">pred1 = mod2.predict()</span>
        <span class="s1">pred2 = mod2.predict(which=</span><span class="s5">&quot;mean&quot;</span><span class="s2">, </span><span class="s1">offset=offset)</span>
        <span class="s1">pred3 = mod2.predict(exog=exog</span><span class="s2">, </span><span class="s1">which=</span><span class="s5">&quot;mean&quot;</span><span class="s2">, </span><span class="s1">offset=offset)</span>
        <span class="s1">assert_almost_equal(pred1</span><span class="s2">, </span><span class="s1">pred2)</span>
        <span class="s1">assert_almost_equal(pred2</span><span class="s2">, </span><span class="s1">pred3)</span>

        <span class="s3"># Check that offset shifts the linear predictor</span>
        <span class="s1">mod3 = GLM(endog</span><span class="s2">, </span><span class="s1">exog</span><span class="s2">, </span><span class="s1">family=sm.families.Poisson()).fit()</span>
        <span class="s1">offset = np.random.uniform(</span><span class="s4">1</span><span class="s2">, </span><span class="s4">2</span><span class="s2">, </span><span class="s4">10</span><span class="s1">)</span>
        <span class="s2">with </span><span class="s1">pytest.warns(FutureWarning):</span>
            <span class="s3"># deprecation warning for linear keyword</span>
            <span class="s1">pred1 = mod3.predict(exog=exog1</span><span class="s2">, </span><span class="s1">offset=offset</span><span class="s2">, </span><span class="s1">linear=</span><span class="s2">True</span><span class="s1">)</span>
        <span class="s1">pred2 = mod3.predict(exog=exog1</span><span class="s2">, </span><span class="s1">offset=</span><span class="s4">2</span><span class="s1">*offset</span><span class="s2">, </span><span class="s1">which=</span><span class="s5">&quot;linear&quot;</span><span class="s1">)</span>
        <span class="s1">assert_almost_equal(pred2</span><span class="s2">, </span><span class="s1">pred1+offset)</span>

        <span class="s3"># Passing exposure as a pandas series should not effect output type</span>
        <span class="s2">assert </span><span class="s1">isinstance(</span>
            <span class="s1">mod1.predict(exog=exog1</span><span class="s2">, </span><span class="s1">exposure=pd.Series(exposure1))</span><span class="s2">,</span>
            <span class="s1">np.ndarray</span>
        <span class="s1">)</span>


<span class="s2">def </span><span class="s1">test_perfect_pred(iris):</span>
    <span class="s1">y = iris[:</span><span class="s2">, </span><span class="s1">-</span><span class="s4">1</span><span class="s1">]</span>
    <span class="s1">X = iris[:</span><span class="s2">, </span><span class="s1">:-</span><span class="s4">1</span><span class="s1">]</span>
    <span class="s1">X = X[y != </span><span class="s4">2</span><span class="s1">]</span>
    <span class="s1">y = y[y != </span><span class="s4">2</span><span class="s1">]</span>
    <span class="s1">X = add_constant(X</span><span class="s2">, </span><span class="s1">prepend=</span><span class="s2">True</span><span class="s1">)</span>
    <span class="s1">glm = GLM(y</span><span class="s2">, </span><span class="s1">X</span><span class="s2">, </span><span class="s1">family=sm.families.Binomial())</span>

    <span class="s2">with </span><span class="s1">pytest.warns(PerfectSeparationWarning):</span>
        <span class="s1">glm.fit()</span>


<span class="s2">def </span><span class="s1">test_score_test_ols():</span>
    <span class="s3"># nicer example than Longley</span>
    <span class="s2">from </span><span class="s1">statsmodels.regression.linear_model </span><span class="s2">import </span><span class="s1">OLS</span>
    <span class="s1">np.random.seed(</span><span class="s4">5</span><span class="s1">)</span>
    <span class="s1">nobs = </span><span class="s4">100</span>
    <span class="s1">sige = </span><span class="s4">0.5</span>
    <span class="s1">x = np.random.uniform(</span><span class="s4">0</span><span class="s2">, </span><span class="s4">1</span><span class="s2">, </span><span class="s1">size=(nobs</span><span class="s2">, </span><span class="s4">5</span><span class="s1">))</span>
    <span class="s1">x[:</span><span class="s2">, </span><span class="s4">0</span><span class="s1">] = </span><span class="s4">1</span>
    <span class="s1">beta = </span><span class="s4">1. </span><span class="s1">/ np.arange(</span><span class="s4">1.</span><span class="s2">, </span><span class="s1">x.shape[</span><span class="s4">1</span><span class="s1">] + </span><span class="s4">1</span><span class="s1">)</span>
    <span class="s1">y = x.dot(beta) + sige * np.random.randn(nobs)</span>

    <span class="s1">res_ols = OLS(y</span><span class="s2">, </span><span class="s1">x).fit()</span>
    <span class="s1">res_olsc = OLS(y</span><span class="s2">, </span><span class="s1">x[:</span><span class="s2">, </span><span class="s1">:-</span><span class="s4">2</span><span class="s1">]).fit()</span>
    <span class="s1">co = res_ols.compare_lm_test(res_olsc</span><span class="s2">, </span><span class="s1">demean=</span><span class="s2">False</span><span class="s1">)</span>

    <span class="s1">res_glm = GLM(y</span><span class="s2">, </span><span class="s1">x[:</span><span class="s2">, </span><span class="s1">:-</span><span class="s4">2</span><span class="s1">]</span><span class="s2">, </span><span class="s1">family=sm.families.Gaussian()).fit()</span>
    <span class="s1">co2 = res_glm.model.score_test(res_glm.params</span><span class="s2">, </span><span class="s1">exog_extra=x[:</span><span class="s2">, </span><span class="s1">-</span><span class="s4">2</span><span class="s1">:])</span>
    <span class="s3"># difference in df_resid versus nobs in scale see #1786</span>
    <span class="s1">assert_allclose(co[</span><span class="s4">0</span><span class="s1">] * </span><span class="s4">97 </span><span class="s1">/ </span><span class="s4">100.</span><span class="s2">, </span><span class="s1">co2[</span><span class="s4">0</span><span class="s1">]</span><span class="s2">, </span><span class="s1">rtol=</span><span class="s4">1e-13</span><span class="s1">)</span>


<span class="s2">def </span><span class="s1">test_attribute_writable_resettable():</span>
    <span class="s3"># Regression test for mutables and class constructors.</span>
    <span class="s1">data = sm.datasets.longley.load()</span>
    <span class="s1">endog</span><span class="s2">, </span><span class="s1">exog = data.endog</span><span class="s2">, </span><span class="s1">data.exog</span>
    <span class="s1">glm_model = sm.GLM(endog</span><span class="s2">, </span><span class="s1">exog)</span>
    <span class="s1">assert_equal(glm_model.family.link.power</span><span class="s2">, </span><span class="s4">1.0</span><span class="s1">)</span>
    <span class="s1">glm_model.family.link.power = </span><span class="s4">2.</span>
    <span class="s1">assert_equal(glm_model.family.link.power</span><span class="s2">, </span><span class="s4">2.0</span><span class="s1">)</span>
    <span class="s1">glm_model2 = sm.GLM(endog</span><span class="s2">, </span><span class="s1">exog)</span>
    <span class="s1">assert_equal(glm_model2.family.link.power</span><span class="s2">, </span><span class="s4">1.0</span><span class="s1">)</span>


<span class="s2">class </span><span class="s1">TestStartParams(CheckModelResultsMixin):</span>
    <span class="s1">@classmethod</span>
    <span class="s2">def </span><span class="s1">setup_class(cls):</span>
        <span class="s0">''' 
        Test Gaussian family with canonical identity link 
        '''</span>
        <span class="s3"># Test Precisions</span>
        <span class="s1">cls.decimal_resids = DECIMAL_3</span>
        <span class="s1">cls.decimal_params = DECIMAL_2</span>
        <span class="s1">cls.decimal_bic = DECIMAL_0</span>
        <span class="s1">cls.decimal_bse = DECIMAL_3</span>

        <span class="s2">from </span><span class="s1">statsmodels.datasets.longley </span><span class="s2">import </span><span class="s1">load</span>
        <span class="s1">cls.data = load()</span>
        <span class="s1">cls.data.exog = add_constant(cls.data.exog</span><span class="s2">, </span><span class="s1">prepend=</span><span class="s2">False</span><span class="s1">)</span>
        <span class="s1">params = sm.OLS(cls.data.endog</span><span class="s2">, </span><span class="s1">cls.data.exog).fit().params</span>
        <span class="s1">cls.res1 = GLM(cls.data.endog</span><span class="s2">, </span><span class="s1">cls.data.exog</span><span class="s2">,</span>
                        <span class="s1">family=sm.families.Gaussian()).fit(start_params=params)</span>
        <span class="s2">from </span><span class="s1">.results.results_glm </span><span class="s2">import </span><span class="s1">Longley</span>
        <span class="s1">cls.res2 = Longley()</span>


<span class="s2">def </span><span class="s1">test_glm_start_params():</span>
    <span class="s3"># see 1604</span>
    <span class="s1">y2 = np.array(</span><span class="s5">'0 1 0 0 0 1'</span><span class="s1">.split()</span><span class="s2">, </span><span class="s1">int)</span>
    <span class="s1">wt = np.array([</span><span class="s4">50</span><span class="s2">,</span><span class="s4">1</span><span class="s2">,</span><span class="s4">50</span><span class="s2">,</span><span class="s4">1</span><span class="s2">,</span><span class="s4">5</span><span class="s2">,</span><span class="s4">10</span><span class="s1">])</span>
    <span class="s1">y2 = np.repeat(y2</span><span class="s2">, </span><span class="s1">wt)</span>
    <span class="s1">x2 = np.repeat([</span><span class="s4">0</span><span class="s2">,</span><span class="s4">0</span><span class="s2">,</span><span class="s4">0.001</span><span class="s2">,</span><span class="s4">100</span><span class="s2">,</span><span class="s1">-</span><span class="s4">1</span><span class="s2">,</span><span class="s1">-</span><span class="s4">1</span><span class="s1">]</span><span class="s2">, </span><span class="s1">wt)</span>
    <span class="s1">mod = sm.GLM(y2</span><span class="s2">, </span><span class="s1">sm.add_constant(x2)</span><span class="s2">, </span><span class="s1">family=sm.families.Binomial())</span>
    <span class="s1">res = mod.fit(start_params=[-</span><span class="s4">4</span><span class="s2">, </span><span class="s1">-</span><span class="s4">5</span><span class="s1">])</span>
    <span class="s1">np.testing.assert_almost_equal(res.params</span><span class="s2">, </span><span class="s1">[-</span><span class="s4">4.60305022</span><span class="s2">, </span><span class="s1">-</span><span class="s4">5.29634545</span><span class="s1">]</span><span class="s2">, </span><span class="s4">6</span><span class="s1">)</span>


<span class="s2">def </span><span class="s1">test_loglike_no_opt():</span>
    <span class="s3"># see 1728</span>

    <span class="s1">y = np.asarray([</span><span class="s4">0</span><span class="s2">, </span><span class="s4">1</span><span class="s2">, </span><span class="s4">0</span><span class="s2">, </span><span class="s4">0</span><span class="s2">, </span><span class="s4">1</span><span class="s2">, </span><span class="s4">1</span><span class="s2">, </span><span class="s4">0</span><span class="s2">, </span><span class="s4">1</span><span class="s2">, </span><span class="s4">1</span><span class="s2">, </span><span class="s4">1</span><span class="s1">])</span>
    <span class="s1">x = np.arange(</span><span class="s4">10</span><span class="s2">, </span><span class="s1">dtype=np.float64)</span>

    <span class="s2">def </span><span class="s1">llf(params):</span>
        <span class="s1">lin_pred = params[</span><span class="s4">0</span><span class="s1">] + params[</span><span class="s4">1</span><span class="s1">]*x</span>
        <span class="s1">pr = </span><span class="s4">1 </span><span class="s1">/ (</span><span class="s4">1 </span><span class="s1">+ np.exp(-lin_pred))</span>
        <span class="s2">return </span><span class="s1">np.sum(y*np.log(pr) + (</span><span class="s4">1</span><span class="s1">-y)*np.log(</span><span class="s4">1</span><span class="s1">-pr))</span>

    <span class="s2">for </span><span class="s1">params </span><span class="s2">in </span><span class="s1">[</span><span class="s4">0</span><span class="s2">,</span><span class="s4">0</span><span class="s1">]</span><span class="s2">, </span><span class="s1">[</span><span class="s4">0</span><span class="s2">,</span><span class="s4">1</span><span class="s1">]</span><span class="s2">, </span><span class="s1">[</span><span class="s4">0.5</span><span class="s2">,</span><span class="s4">0.5</span><span class="s1">]:</span>
        <span class="s1">mod = sm.GLM(y</span><span class="s2">, </span><span class="s1">sm.add_constant(x)</span><span class="s2">, </span><span class="s1">family=sm.families.Binomial())</span>
        <span class="s1">res = mod.fit(start_params=params</span><span class="s2">, </span><span class="s1">maxiter=</span><span class="s4">0</span><span class="s1">)</span>
        <span class="s1">like = llf(params)</span>
        <span class="s1">assert_almost_equal(like</span><span class="s2">, </span><span class="s1">res.llf)</span>


<span class="s2">def </span><span class="s1">test_formula_missing_exposure():</span>
    <span class="s3"># see 2083</span>
    <span class="s2">import </span><span class="s1">statsmodels.formula.api </span><span class="s2">as </span><span class="s1">smf</span>

    <span class="s1">d = {</span><span class="s5">'Foo'</span><span class="s1">: [</span><span class="s4">1</span><span class="s2">, </span><span class="s4">2</span><span class="s2">, </span><span class="s4">10</span><span class="s2">, </span><span class="s4">149</span><span class="s1">]</span><span class="s2">, </span><span class="s5">'Bar'</span><span class="s1">: [</span><span class="s4">1</span><span class="s2">, </span><span class="s4">2</span><span class="s2">, </span><span class="s4">3</span><span class="s2">, </span><span class="s1">np.nan]</span><span class="s2">,</span>
         <span class="s5">'constant'</span><span class="s1">: [</span><span class="s4">1</span><span class="s1">] * </span><span class="s4">4</span><span class="s2">, </span><span class="s5">'exposure'</span><span class="s1">: np.random.uniform(size=</span><span class="s4">4</span><span class="s1">)</span><span class="s2">,</span>
         <span class="s5">'x'</span><span class="s1">: [</span><span class="s4">1</span><span class="s2">, </span><span class="s4">3</span><span class="s2">, </span><span class="s4">2</span><span class="s2">, </span><span class="s4">1.5</span><span class="s1">]}</span>
    <span class="s1">df = pd.DataFrame(d)</span>

    <span class="s1">family = sm.families.Gaussian(link=sm.families.links.Log())</span>

    <span class="s1">mod = smf.glm(</span><span class="s5">&quot;Foo ~ Bar&quot;</span><span class="s2">, </span><span class="s1">data=df</span><span class="s2">, </span><span class="s1">exposure=df.exposure</span><span class="s2">,</span>
                  <span class="s1">family=family)</span>
    <span class="s1">assert_(type(mod.exposure) </span><span class="s2">is </span><span class="s1">np.ndarray</span><span class="s2">, </span><span class="s1">msg=</span><span class="s5">'Exposure is not ndarray'</span><span class="s1">)</span>

    <span class="s1">exposure = pd.Series(np.random.uniform(size=</span><span class="s4">5</span><span class="s1">))</span>
    <span class="s1">df.loc[</span><span class="s4">3</span><span class="s2">, </span><span class="s5">'Bar'</span><span class="s1">] = </span><span class="s4">4   </span><span class="s3"># nan not relevant for Valueerror for shape mismatch</span>
    <span class="s1">assert_raises(ValueError</span><span class="s2">, </span><span class="s1">smf.glm</span><span class="s2">, </span><span class="s5">&quot;Foo ~ Bar&quot;</span><span class="s2">, </span><span class="s1">data=df</span><span class="s2">,</span>
                  <span class="s1">exposure=exposure</span><span class="s2">, </span><span class="s1">family=family)</span>
    <span class="s1">assert_raises(ValueError</span><span class="s2">, </span><span class="s1">GLM</span><span class="s2">, </span><span class="s1">df.Foo</span><span class="s2">, </span><span class="s1">df[[</span><span class="s5">'constant'</span><span class="s2">, </span><span class="s5">'Bar'</span><span class="s1">]]</span><span class="s2">,</span>
                  <span class="s1">exposure=exposure</span><span class="s2">, </span><span class="s1">family=family)</span>


<span class="s1">@pytest.mark.matplotlib</span>
<span class="s2">def </span><span class="s1">test_plots(close_figures):</span>

    <span class="s1">np.random.seed(</span><span class="s4">378</span><span class="s1">)</span>
    <span class="s1">n = </span><span class="s4">200</span>
    <span class="s1">exog = np.random.normal(size=(n</span><span class="s2">, </span><span class="s4">2</span><span class="s1">))</span>
    <span class="s1">lin_pred = exog[:</span><span class="s2">, </span><span class="s4">0</span><span class="s1">] + exog[:</span><span class="s2">, </span><span class="s4">1</span><span class="s1">]**</span><span class="s4">2</span>
    <span class="s1">prob = </span><span class="s4">1 </span><span class="s1">/ (</span><span class="s4">1 </span><span class="s1">+ np.exp(-lin_pred))</span>
    <span class="s1">endog = </span><span class="s4">1 </span><span class="s1">* (np.random.uniform(size=n) &lt; prob)</span>

    <span class="s1">model = sm.GLM(endog</span><span class="s2">, </span><span class="s1">exog</span><span class="s2">, </span><span class="s1">family=sm.families.Binomial())</span>
    <span class="s1">result = model.fit()</span>

    <span class="s2">import </span><span class="s1">pandas </span><span class="s2">as </span><span class="s1">pd</span>

    <span class="s2">from </span><span class="s1">statsmodels.graphics.regressionplots </span><span class="s2">import </span><span class="s1">add_lowess</span>

    <span class="s3"># array interface</span>
    <span class="s2">for </span><span class="s1">j </span><span class="s2">in </span><span class="s4">0</span><span class="s2">,</span><span class="s4">1</span><span class="s1">:</span>
        <span class="s1">fig = result.plot_added_variable(j)</span>
        <span class="s1">add_lowess(fig.axes[</span><span class="s4">0</span><span class="s1">]</span><span class="s2">, </span><span class="s1">frac=</span><span class="s4">0.5</span><span class="s1">)</span>
        <span class="s1">close_or_save(pdf</span><span class="s2">, </span><span class="s1">fig)</span>
        <span class="s1">fig = result.plot_partial_residuals(j)</span>
        <span class="s1">add_lowess(fig.axes[</span><span class="s4">0</span><span class="s1">]</span><span class="s2">, </span><span class="s1">frac=</span><span class="s4">0.5</span><span class="s1">)</span>
        <span class="s1">close_or_save(pdf</span><span class="s2">, </span><span class="s1">fig)</span>
        <span class="s1">fig = result.plot_ceres_residuals(j)</span>
        <span class="s1">add_lowess(fig.axes[</span><span class="s4">0</span><span class="s1">]</span><span class="s2">, </span><span class="s1">frac=</span><span class="s4">0.5</span><span class="s1">)</span>
        <span class="s1">close_or_save(pdf</span><span class="s2">, </span><span class="s1">fig)</span>

    <span class="s3"># formula interface</span>
    <span class="s1">data = pd.DataFrame({</span><span class="s5">&quot;y&quot;</span><span class="s1">: endog</span><span class="s2">, </span><span class="s5">&quot;x1&quot;</span><span class="s1">: exog[:</span><span class="s2">, </span><span class="s4">0</span><span class="s1">]</span><span class="s2">, </span><span class="s5">&quot;x2&quot;</span><span class="s1">: exog[:</span><span class="s2">, </span><span class="s4">1</span><span class="s1">]})</span>
    <span class="s1">model = sm.GLM.from_formula(</span><span class="s5">&quot;y ~ x1 + x2&quot;</span><span class="s2">, </span><span class="s1">data</span><span class="s2">, </span><span class="s1">family=sm.families.Binomial())</span>
    <span class="s1">result = model.fit()</span>
    <span class="s2">for </span><span class="s1">j </span><span class="s2">in </span><span class="s4">0</span><span class="s2">,</span><span class="s4">1</span><span class="s1">:</span>
        <span class="s1">xname = [</span><span class="s5">&quot;x1&quot;</span><span class="s2">, </span><span class="s5">&quot;x2&quot;</span><span class="s1">][j]</span>
        <span class="s1">fig = result.plot_added_variable(xname)</span>
        <span class="s1">add_lowess(fig.axes[</span><span class="s4">0</span><span class="s1">]</span><span class="s2">, </span><span class="s1">frac=</span><span class="s4">0.5</span><span class="s1">)</span>
        <span class="s1">close_or_save(pdf</span><span class="s2">, </span><span class="s1">fig)</span>
        <span class="s1">fig = result.plot_partial_residuals(xname)</span>
        <span class="s1">add_lowess(fig.axes[</span><span class="s4">0</span><span class="s1">]</span><span class="s2">, </span><span class="s1">frac=</span><span class="s4">0.5</span><span class="s1">)</span>
        <span class="s1">close_or_save(pdf</span><span class="s2">, </span><span class="s1">fig)</span>
        <span class="s1">fig = result.plot_ceres_residuals(xname)</span>
        <span class="s1">add_lowess(fig.axes[</span><span class="s4">0</span><span class="s1">]</span><span class="s2">, </span><span class="s1">frac=</span><span class="s4">0.5</span><span class="s1">)</span>
        <span class="s1">close_or_save(pdf</span><span class="s2">, </span><span class="s1">fig)</span>

<span class="s2">def </span><span class="s1">gen_endog(lin_pred</span><span class="s2">, </span><span class="s1">family_class</span><span class="s2">, </span><span class="s1">link</span><span class="s2">, </span><span class="s1">binom_version=</span><span class="s4">0</span><span class="s1">):</span>

    <span class="s1">np.random.seed(</span><span class="s4">872</span><span class="s1">)</span>

    <span class="s1">fam = sm.families</span>

    <span class="s1">mu = link().inverse(lin_pred)</span>

    <span class="s2">if </span><span class="s1">family_class == fam.Binomial:</span>
        <span class="s2">if </span><span class="s1">binom_version == </span><span class="s4">0</span><span class="s1">:</span>
            <span class="s1">endog = </span><span class="s4">1</span><span class="s1">*(np.random.uniform(size=len(lin_pred)) &lt; mu)</span>
        <span class="s2">else</span><span class="s1">:</span>
            <span class="s1">endog = np.empty((len(lin_pred)</span><span class="s2">, </span><span class="s4">2</span><span class="s1">))</span>
            <span class="s1">n = </span><span class="s4">10</span>
            <span class="s1">endog[:</span><span class="s2">, </span><span class="s4">0</span><span class="s1">] = (np.random.uniform(size=(len(lin_pred)</span><span class="s2">, </span><span class="s1">n)) &lt; mu[:</span><span class="s2">, None</span><span class="s1">]).sum(</span><span class="s4">1</span><span class="s1">)</span>
            <span class="s1">endog[:</span><span class="s2">, </span><span class="s4">1</span><span class="s1">] = n - endog[:</span><span class="s2">, </span><span class="s4">0</span><span class="s1">]</span>
    <span class="s2">elif </span><span class="s1">family_class == fam.Poisson:</span>
        <span class="s1">endog = np.random.poisson(mu)</span>
    <span class="s2">elif </span><span class="s1">family_class == fam.Gamma:</span>
        <span class="s1">endog = np.random.gamma(</span><span class="s4">2</span><span class="s2">, </span><span class="s1">mu)</span>
    <span class="s2">elif </span><span class="s1">family_class == fam.Gaussian:</span>
        <span class="s1">endog = mu + </span><span class="s4">2 </span><span class="s1">* np.random.normal(size=len(lin_pred))</span>
    <span class="s2">elif </span><span class="s1">family_class == fam.NegativeBinomial:</span>
        <span class="s2">from </span><span class="s1">scipy.stats.distributions </span><span class="s2">import </span><span class="s1">nbinom</span>
        <span class="s1">endog = nbinom.rvs(mu</span><span class="s2">, </span><span class="s4">0.5</span><span class="s1">)</span>
    <span class="s2">elif </span><span class="s1">family_class == fam.InverseGaussian:</span>
        <span class="s2">from </span><span class="s1">scipy.stats.distributions </span><span class="s2">import </span><span class="s1">invgauss</span>
        <span class="s1">endog = invgauss.rvs(mu</span><span class="s2">, </span><span class="s1">scale=</span><span class="s4">20</span><span class="s1">)</span>
    <span class="s2">else</span><span class="s1">:</span>
        <span class="s2">raise </span><span class="s1">ValueError</span>

    <span class="s2">return </span><span class="s1">endog</span>


<span class="s1">@pytest.mark.smoke</span>
<span class="s2">def </span><span class="s1">test_summary():</span>
    <span class="s1">np.random.seed(</span><span class="s4">4323</span><span class="s1">)</span>

    <span class="s1">n = </span><span class="s4">100</span>
    <span class="s1">exog = np.random.normal(size=(n</span><span class="s2">, </span><span class="s4">2</span><span class="s1">))</span>
    <span class="s1">exog[:</span><span class="s2">, </span><span class="s4">0</span><span class="s1">] = </span><span class="s4">1</span>
    <span class="s1">endog = np.random.normal(size=n)</span>

    <span class="s2">for </span><span class="s1">method </span><span class="s2">in </span><span class="s1">[</span><span class="s5">&quot;irls&quot;</span><span class="s2">, </span><span class="s5">&quot;cg&quot;</span><span class="s1">]:</span>
        <span class="s1">fa = sm.families.Gaussian()</span>
        <span class="s1">model = sm.GLM(endog</span><span class="s2">, </span><span class="s1">exog</span><span class="s2">, </span><span class="s1">family=fa)</span>
        <span class="s1">rslt = model.fit(method=method)</span>
        <span class="s1">s = rslt.summary()</span>


<span class="s2">def </span><span class="s1">check_score_hessian(results):</span>
    <span class="s3"># compare models core and hessian with numerical derivatives</span>

    <span class="s1">params = results.params</span>
    <span class="s3"># avoid checking score at MLE, score close to zero</span>
    <span class="s1">sc = results.model.score(params * </span><span class="s4">0.98</span><span class="s2">, </span><span class="s1">scale=</span><span class="s4">1</span><span class="s1">)</span>
    <span class="s3"># cs currently (0.9) does not work for all families</span>
    <span class="s1">llfunc = </span><span class="s2">lambda </span><span class="s1">x: results.model.loglike(x</span><span class="s2">, </span><span class="s1">scale=</span><span class="s4">1</span><span class="s1">)  </span><span class="s3"># noqa</span>
    <span class="s1">sc2 = approx_fprime(params * </span><span class="s4">0.98</span><span class="s2">, </span><span class="s1">llfunc)</span>
    <span class="s1">assert_allclose(sc</span><span class="s2">, </span><span class="s1">sc2</span><span class="s2">, </span><span class="s1">rtol=</span><span class="s4">1e-4</span><span class="s2">, </span><span class="s1">atol=</span><span class="s4">1e-4</span><span class="s1">)</span>

    <span class="s1">hess = results.model.hessian(params</span><span class="s2">, </span><span class="s1">scale=</span><span class="s4">1</span><span class="s1">)</span>
    <span class="s1">hess2 = approx_hess(params</span><span class="s2">, </span><span class="s1">llfunc)</span>
    <span class="s1">assert_allclose(hess</span><span class="s2">, </span><span class="s1">hess2</span><span class="s2">, </span><span class="s1">rtol=</span><span class="s4">1e-4</span><span class="s1">)</span>
    <span class="s1">scfunc = </span><span class="s2">lambda </span><span class="s1">x: results.model.score(x</span><span class="s2">, </span><span class="s1">scale=</span><span class="s4">1</span><span class="s1">)  </span><span class="s3"># noqa</span>
    <span class="s1">hess3 = approx_fprime(params</span><span class="s2">, </span><span class="s1">scfunc)</span>
    <span class="s1">assert_allclose(hess</span><span class="s2">, </span><span class="s1">hess3</span><span class="s2">, </span><span class="s1">rtol=</span><span class="s4">1e-4</span><span class="s1">)</span>


<span class="s2">def </span><span class="s1">test_gradient_irls():</span>
    <span class="s3"># Compare the results when using gradient optimization and IRLS.</span>

    <span class="s3"># TODO: Find working examples for inverse_squared link</span>

    <span class="s1">np.random.seed(</span><span class="s4">87342</span><span class="s1">)</span>

    <span class="s1">fam = sm.families</span>
    <span class="s1">lnk = sm.families.links</span>
    <span class="s1">families = [(fam.Binomial</span><span class="s2">, </span><span class="s1">[lnk.Logit</span><span class="s2">, </span><span class="s1">lnk.Probit</span><span class="s2">, </span><span class="s1">lnk.CLogLog</span><span class="s2">, </span><span class="s1">lnk.Log</span><span class="s2">, </span><span class="s1">lnk.Cauchy])</span><span class="s2">,</span>
                <span class="s1">(fam.Poisson</span><span class="s2">, </span><span class="s1">[lnk.Log</span><span class="s2">, </span><span class="s1">lnk.Identity</span><span class="s2">, </span><span class="s1">lnk.Sqrt])</span><span class="s2">,</span>
                <span class="s1">(fam.Gamma</span><span class="s2">, </span><span class="s1">[lnk.Log</span><span class="s2">, </span><span class="s1">lnk.Identity</span><span class="s2">, </span><span class="s1">lnk.InversePower])</span><span class="s2">,</span>
                <span class="s1">(fam.Gaussian</span><span class="s2">, </span><span class="s1">[lnk.Identity</span><span class="s2">, </span><span class="s1">lnk.Log</span><span class="s2">, </span><span class="s1">lnk.InversePower])</span><span class="s2">,</span>
                <span class="s1">(fam.InverseGaussian</span><span class="s2">, </span><span class="s1">[lnk.Log</span><span class="s2">, </span><span class="s1">lnk.Identity</span><span class="s2">, </span><span class="s1">lnk.InversePower</span><span class="s2">, </span><span class="s1">lnk.InverseSquared])</span><span class="s2">,</span>
                <span class="s1">(fam.NegativeBinomial</span><span class="s2">, </span><span class="s1">[lnk.Log</span><span class="s2">, </span><span class="s1">lnk.InversePower</span><span class="s2">, </span><span class="s1">lnk.InverseSquared</span><span class="s2">, </span><span class="s1">lnk.Identity])]</span>

    <span class="s1">n = </span><span class="s4">100</span>
    <span class="s1">p = </span><span class="s4">3</span>
    <span class="s1">exog = np.random.normal(size=(n</span><span class="s2">, </span><span class="s1">p))</span>
    <span class="s1">exog[:</span><span class="s2">, </span><span class="s4">0</span><span class="s1">] = </span><span class="s4">1</span>

    <span class="s1">skip_one = </span><span class="s2">False</span>
    <span class="s2">for </span><span class="s1">family_class</span><span class="s2">, </span><span class="s1">family_links </span><span class="s2">in </span><span class="s1">families:</span>
        <span class="s2">for </span><span class="s1">link </span><span class="s2">in </span><span class="s1">family_links:</span>
            <span class="s2">for </span><span class="s1">binom_version </span><span class="s2">in </span><span class="s4">0</span><span class="s2">,</span><span class="s4">1</span><span class="s1">:</span>

                <span class="s2">if </span><span class="s1">family_class != fam.Binomial </span><span class="s2">and </span><span class="s1">binom_version == </span><span class="s4">1</span><span class="s1">:</span>
                    <span class="s2">continue</span>

                <span class="s2">if </span><span class="s1">(family_class</span><span class="s2">, </span><span class="s1">link) == (fam.Poisson</span><span class="s2">, </span><span class="s1">lnk.Identity):</span>
                    <span class="s1">lin_pred = </span><span class="s4">20 </span><span class="s1">+ exog.sum(</span><span class="s4">1</span><span class="s1">)</span>
                <span class="s2">elif </span><span class="s1">(family_class</span><span class="s2">, </span><span class="s1">link) == (fam.Binomial</span><span class="s2">, </span><span class="s1">lnk.Log):</span>
                    <span class="s1">lin_pred = -</span><span class="s4">1 </span><span class="s1">+ exog.sum(</span><span class="s4">1</span><span class="s1">) / </span><span class="s4">8</span>
                <span class="s2">elif </span><span class="s1">(family_class</span><span class="s2">, </span><span class="s1">link) == (fam.Poisson</span><span class="s2">, </span><span class="s1">lnk.Sqrt):</span>
                    <span class="s1">lin_pred = </span><span class="s4">2 </span><span class="s1">+ exog.sum(</span><span class="s4">1</span><span class="s1">)</span>
                <span class="s2">elif </span><span class="s1">(family_class</span><span class="s2">, </span><span class="s1">link) == (fam.InverseGaussian</span><span class="s2">, </span><span class="s1">lnk.Log):</span>
                    <span class="s3">#skip_zero = True</span>
                    <span class="s1">lin_pred = -</span><span class="s4">1 </span><span class="s1">+ exog.sum(</span><span class="s4">1</span><span class="s1">)</span>
                <span class="s2">elif </span><span class="s1">(family_class</span><span class="s2">, </span><span class="s1">link) == (fam.InverseGaussian</span><span class="s2">, </span><span class="s1">lnk.Identity):</span>
                    <span class="s1">lin_pred = </span><span class="s4">20 </span><span class="s1">+ </span><span class="s4">5</span><span class="s1">*exog.sum(</span><span class="s4">1</span><span class="s1">)</span>
                    <span class="s1">lin_pred = np.clip(lin_pred</span><span class="s2">, </span><span class="s4">1e-4</span><span class="s2">, </span><span class="s1">np.inf)</span>
                <span class="s2">elif </span><span class="s1">(family_class</span><span class="s2">, </span><span class="s1">link) == (fam.InverseGaussian</span><span class="s2">, </span><span class="s1">lnk.InverseSquared):</span>
                    <span class="s1">lin_pred = </span><span class="s4">0.5 </span><span class="s1">+ exog.sum(</span><span class="s4">1</span><span class="s1">) / </span><span class="s4">5</span>
                    <span class="s2">continue </span><span class="s3"># skip due to non-convergence</span>
                <span class="s2">elif </span><span class="s1">(family_class</span><span class="s2">, </span><span class="s1">link) == (fam.InverseGaussian</span><span class="s2">, </span><span class="s1">lnk.InversePower):</span>
                    <span class="s1">lin_pred = </span><span class="s4">1 </span><span class="s1">+ exog.sum(</span><span class="s4">1</span><span class="s1">) / </span><span class="s4">5</span>
                <span class="s2">elif </span><span class="s1">(family_class</span><span class="s2">, </span><span class="s1">link) == (fam.NegativeBinomial</span><span class="s2">, </span><span class="s1">lnk.Identity):</span>
                    <span class="s1">lin_pred = </span><span class="s4">20 </span><span class="s1">+ </span><span class="s4">5</span><span class="s1">*exog.sum(</span><span class="s4">1</span><span class="s1">)</span>
                    <span class="s1">lin_pred = np.clip(lin_pred</span><span class="s2">, </span><span class="s4">1e-4</span><span class="s2">, </span><span class="s1">np.inf)</span>
                <span class="s2">elif </span><span class="s1">(family_class</span><span class="s2">, </span><span class="s1">link) == (fam.NegativeBinomial</span><span class="s2">, </span><span class="s1">lnk.InverseSquared):</span>
                    <span class="s1">lin_pred = </span><span class="s4">0.1 </span><span class="s1">+ np.random.uniform(size=exog.shape[</span><span class="s4">0</span><span class="s1">])</span>
                    <span class="s2">continue </span><span class="s3"># skip due to non-convergence</span>
                <span class="s2">elif </span><span class="s1">(family_class</span><span class="s2">, </span><span class="s1">link) == (fam.NegativeBinomial</span><span class="s2">, </span><span class="s1">lnk.InversePower):</span>
                    <span class="s1">lin_pred = </span><span class="s4">1 </span><span class="s1">+ exog.sum(</span><span class="s4">1</span><span class="s1">) / </span><span class="s4">5</span>

                <span class="s2">elif </span><span class="s1">(family_class</span><span class="s2">, </span><span class="s1">link) == (fam.Gaussian</span><span class="s2">, </span><span class="s1">lnk.InversePower):</span>
                    <span class="s3"># adding skip because of convergence failure</span>
                    <span class="s1">skip_one = </span><span class="s2">True</span>
                <span class="s3"># the following fails with Identity link, because endog &lt; 0</span>
                <span class="s3"># elif family_class == fam.Gamma:</span>
                <span class="s3">#     lin_pred = 0.5 * exog.sum(1) + np.random.uniform(size=exog.shape[0])</span>
                <span class="s2">else</span><span class="s1">:</span>
                    <span class="s1">lin_pred = np.random.uniform(size=exog.shape[</span><span class="s4">0</span><span class="s1">])</span>

                <span class="s1">endog = gen_endog(lin_pred</span><span class="s2">, </span><span class="s1">family_class</span><span class="s2">, </span><span class="s1">link</span><span class="s2">, </span><span class="s1">binom_version)</span>

                <span class="s2">with </span><span class="s1">warnings.catch_warnings():</span>
                    <span class="s1">warnings.simplefilter(</span><span class="s5">&quot;ignore&quot;</span><span class="s1">)</span>
                    <span class="s1">mod_irls = sm.GLM(endog</span><span class="s2">, </span><span class="s1">exog</span><span class="s2">, </span><span class="s1">family=family_class(link=link()))</span>
                <span class="s1">rslt_irls = mod_irls.fit(method=</span><span class="s5">&quot;IRLS&quot;</span><span class="s1">)</span>

                <span class="s2">if not </span><span class="s1">(family_class</span><span class="s2">, </span><span class="s1">link) </span><span class="s2">in </span><span class="s1">[(fam.Poisson</span><span class="s2">, </span><span class="s1">lnk.Sqrt)</span><span class="s2">,</span>
                                                <span class="s1">(fam.Gamma</span><span class="s2">, </span><span class="s1">lnk.InversePower)</span><span class="s2">,</span>
                                                <span class="s1">(fam.InverseGaussian</span><span class="s2">, </span><span class="s1">lnk.Identity)</span>
                                                <span class="s1">]:</span>
                    <span class="s1">check_score_hessian(rslt_irls)</span>

                <span class="s3"># Try with and without starting values.</span>
                <span class="s2">for </span><span class="s1">max_start_irls</span><span class="s2">, </span><span class="s1">start_params </span><span class="s2">in </span><span class="s1">(</span><span class="s4">0</span><span class="s2">, </span><span class="s1">rslt_irls.params)</span><span class="s2">, </span><span class="s1">(</span><span class="s4">3</span><span class="s2">, None</span><span class="s1">):</span>
                    <span class="s3"># TODO: skip convergence failures for now</span>
                    <span class="s2">if </span><span class="s1">max_start_irls &gt; </span><span class="s4">0 </span><span class="s2">and </span><span class="s1">skip_one:</span>
                        <span class="s2">continue</span>
                    <span class="s2">with </span><span class="s1">warnings.catch_warnings():</span>
                        <span class="s1">warnings.simplefilter(</span><span class="s5">&quot;ignore&quot;</span><span class="s1">)</span>
                        <span class="s1">mod_gradient = sm.GLM(endog</span><span class="s2">, </span><span class="s1">exog</span><span class="s2">, </span><span class="s1">family=family_class(link=link()))</span>
                    <span class="s1">rslt_gradient = mod_gradient.fit(max_start_irls=max_start_irls</span><span class="s2">,</span>
                                                     <span class="s1">start_params=start_params</span><span class="s2">,</span>
                                                     <span class="s1">method=</span><span class="s5">&quot;newton&quot;</span><span class="s2">, </span><span class="s1">maxiter=</span><span class="s4">300</span><span class="s1">)</span>

                    <span class="s1">assert_allclose(rslt_gradient.params</span><span class="s2">,</span>
                                    <span class="s1">rslt_irls.params</span><span class="s2">, </span><span class="s1">rtol=</span><span class="s4">1e-6</span><span class="s2">, </span><span class="s1">atol=</span><span class="s4">5e-5</span><span class="s1">)</span>

                    <span class="s1">assert_allclose(rslt_gradient.llf</span><span class="s2">, </span><span class="s1">rslt_irls.llf</span><span class="s2">,</span>
                                    <span class="s1">rtol=</span><span class="s4">1e-6</span><span class="s2">, </span><span class="s1">atol=</span><span class="s4">1e-6</span><span class="s1">)</span>

                    <span class="s1">assert_allclose(rslt_gradient.scale</span><span class="s2">, </span><span class="s1">rslt_irls.scale</span><span class="s2">,</span>
                                    <span class="s1">rtol=</span><span class="s4">1e-6</span><span class="s2">, </span><span class="s1">atol=</span><span class="s4">1e-6</span><span class="s1">)</span>

                    <span class="s3"># Get the standard errors using expected information.</span>
                    <span class="s1">gradient_bse = rslt_gradient.bse</span>
                    <span class="s1">ehess = mod_gradient.hessian(rslt_gradient.params</span><span class="s2">, </span><span class="s1">observed=</span><span class="s2">False</span><span class="s1">)</span>
                    <span class="s1">gradient_bse = np.sqrt(-np.diag(np.linalg.inv(ehess)))</span>
                    <span class="s1">assert_allclose(gradient_bse</span><span class="s2">, </span><span class="s1">rslt_irls.bse</span><span class="s2">, </span><span class="s1">rtol=</span><span class="s4">1e-6</span><span class="s2">, </span><span class="s1">atol=</span><span class="s4">5e-5</span><span class="s1">)</span>
                    <span class="s3"># rslt_irls.bse corresponds to observed=True</span>
                    <span class="s1">assert_allclose(rslt_gradient.bse</span><span class="s2">, </span><span class="s1">rslt_irls.bse</span><span class="s2">, </span><span class="s1">rtol=</span><span class="s4">0.2</span><span class="s2">, </span><span class="s1">atol=</span><span class="s4">5e-5</span><span class="s1">)</span>

                    <span class="s1">rslt_gradient_eim = mod_gradient.fit(max_start_irls=</span><span class="s4">0</span><span class="s2">,</span>
                                                         <span class="s1">cov_type=</span><span class="s5">'eim'</span><span class="s2">,</span>
                                                         <span class="s1">start_params=rslt_gradient.params</span><span class="s2">,</span>
                                                         <span class="s1">method=</span><span class="s5">&quot;newton&quot;</span><span class="s2">, </span><span class="s1">maxiter=</span><span class="s4">300</span><span class="s1">)</span>
                    <span class="s1">assert_allclose(rslt_gradient_eim.bse</span><span class="s2">, </span><span class="s1">rslt_irls.bse</span><span class="s2">, </span><span class="s1">rtol=</span><span class="s4">5e-5</span><span class="s2">, </span><span class="s1">atol=</span><span class="s4">0</span><span class="s1">)</span>


<span class="s2">def </span><span class="s1">test_gradient_irls_eim():</span>
    <span class="s3"># Compare the results when using eime gradient optimization and IRLS.</span>

    <span class="s3"># TODO: Find working examples for inverse_squared link</span>

    <span class="s1">np.random.seed(</span><span class="s4">87342</span><span class="s1">)</span>

    <span class="s1">fam = sm.families</span>
    <span class="s1">lnk = sm.families.links</span>
    <span class="s1">families = [(fam.Binomial</span><span class="s2">, </span><span class="s1">[lnk.Logit</span><span class="s2">, </span><span class="s1">lnk.Probit</span><span class="s2">, </span><span class="s1">lnk.CLogLog</span><span class="s2">, </span><span class="s1">lnk.Log</span><span class="s2">,</span>
                                <span class="s1">lnk.Cauchy])</span><span class="s2">,</span>
                <span class="s1">(fam.Poisson</span><span class="s2">, </span><span class="s1">[lnk.Log</span><span class="s2">, </span><span class="s1">lnk.Identity</span><span class="s2">, </span><span class="s1">lnk.Sqrt])</span><span class="s2">,</span>
                <span class="s1">(fam.Gamma</span><span class="s2">, </span><span class="s1">[lnk.Log</span><span class="s2">, </span><span class="s1">lnk.Identity</span><span class="s2">, </span><span class="s1">lnk.InversePower])</span><span class="s2">,</span>
                <span class="s1">(fam.Gaussian</span><span class="s2">, </span><span class="s1">[lnk.Identity</span><span class="s2">, </span><span class="s1">lnk.Log</span><span class="s2">, </span><span class="s1">lnk.InversePower])</span><span class="s2">,</span>
                <span class="s1">(fam.InverseGaussian</span><span class="s2">, </span><span class="s1">[lnk.Log</span><span class="s2">, </span><span class="s1">lnk.Identity</span><span class="s2">,</span>
                                       <span class="s1">lnk.InversePower</span><span class="s2">,</span>
                                       <span class="s1">lnk.InverseSquared])</span><span class="s2">,</span>
                <span class="s1">(fam.NegativeBinomial</span><span class="s2">, </span><span class="s1">[lnk.Log</span><span class="s2">, </span><span class="s1">lnk.InversePower</span><span class="s2">,</span>
                                        <span class="s1">lnk.InverseSquared</span><span class="s2">, </span><span class="s1">lnk.Identity])]</span>

    <span class="s1">n = </span><span class="s4">100</span>
    <span class="s1">p = </span><span class="s4">3</span>
    <span class="s1">exog = np.random.normal(size=(n</span><span class="s2">, </span><span class="s1">p))</span>
    <span class="s1">exog[:</span><span class="s2">, </span><span class="s4">0</span><span class="s1">] = </span><span class="s4">1</span>

    <span class="s1">skip_one = </span><span class="s2">False</span>
    <span class="s2">for </span><span class="s1">family_class</span><span class="s2">, </span><span class="s1">family_links </span><span class="s2">in </span><span class="s1">families:</span>
        <span class="s2">for </span><span class="s1">link </span><span class="s2">in </span><span class="s1">family_links:</span>
            <span class="s2">for </span><span class="s1">binom_version </span><span class="s2">in </span><span class="s4">0</span><span class="s2">, </span><span class="s4">1</span><span class="s1">:</span>

                <span class="s2">if </span><span class="s1">family_class != fam.Binomial </span><span class="s2">and </span><span class="s1">binom_version == </span><span class="s4">1</span><span class="s1">:</span>
                    <span class="s2">continue</span>

                <span class="s2">if </span><span class="s1">(family_class</span><span class="s2">, </span><span class="s1">link) == (fam.Poisson</span><span class="s2">, </span><span class="s1">lnk.Identity):</span>
                    <span class="s1">lin_pred = </span><span class="s4">20 </span><span class="s1">+ exog.sum(</span><span class="s4">1</span><span class="s1">)</span>
                <span class="s2">elif </span><span class="s1">(family_class</span><span class="s2">, </span><span class="s1">link) == (fam.Binomial</span><span class="s2">, </span><span class="s1">lnk.Log):</span>
                    <span class="s1">lin_pred = -</span><span class="s4">1 </span><span class="s1">+ exog.sum(</span><span class="s4">1</span><span class="s1">) / </span><span class="s4">8</span>
                <span class="s2">elif </span><span class="s1">(family_class</span><span class="s2">, </span><span class="s1">link) == (fam.Poisson</span><span class="s2">, </span><span class="s1">lnk.Sqrt):</span>
                    <span class="s1">lin_pred = </span><span class="s4">2 </span><span class="s1">+ exog.sum(</span><span class="s4">1</span><span class="s1">)</span>
                <span class="s2">elif </span><span class="s1">(family_class</span><span class="s2">, </span><span class="s1">link) == (fam.InverseGaussian</span><span class="s2">, </span><span class="s1">lnk.Log):</span>
                    <span class="s3"># skip_zero = True</span>
                    <span class="s1">lin_pred = -</span><span class="s4">1 </span><span class="s1">+ exog.sum(</span><span class="s4">1</span><span class="s1">)</span>
                <span class="s2">elif </span><span class="s1">(family_class</span><span class="s2">, </span><span class="s1">link) == (fam.InverseGaussian</span><span class="s2">,</span>
                                              <span class="s1">lnk.Identity):</span>
                    <span class="s1">lin_pred = </span><span class="s4">20 </span><span class="s1">+ </span><span class="s4">5</span><span class="s1">*exog.sum(</span><span class="s4">1</span><span class="s1">)</span>
                    <span class="s1">lin_pred = np.clip(lin_pred</span><span class="s2">, </span><span class="s4">1e-4</span><span class="s2">, </span><span class="s1">np.inf)</span>
                <span class="s2">elif </span><span class="s1">(family_class</span><span class="s2">, </span><span class="s1">link) == (fam.InverseGaussian</span><span class="s2">,</span>
                                              <span class="s1">lnk.InverseSquared):</span>
                    <span class="s1">lin_pred = </span><span class="s4">0.5 </span><span class="s1">+ exog.sum(</span><span class="s4">1</span><span class="s1">) / </span><span class="s4">5</span>
                    <span class="s2">continue  </span><span class="s3"># skip due to non-convergence</span>
                <span class="s2">elif </span><span class="s1">(family_class</span><span class="s2">, </span><span class="s1">link) == (fam.InverseGaussian</span><span class="s2">,</span>
                                              <span class="s1">lnk.InversePower):</span>
                    <span class="s1">lin_pred = </span><span class="s4">1 </span><span class="s1">+ exog.sum(</span><span class="s4">1</span><span class="s1">) / </span><span class="s4">5</span>
                <span class="s2">elif </span><span class="s1">(family_class</span><span class="s2">, </span><span class="s1">link) == (fam.NegativeBinomial</span><span class="s2">,</span>
                                              <span class="s1">lnk.Identity):</span>
                    <span class="s1">lin_pred = </span><span class="s4">20 </span><span class="s1">+ </span><span class="s4">5</span><span class="s1">*exog.sum(</span><span class="s4">1</span><span class="s1">)</span>
                    <span class="s1">lin_pred = np.clip(lin_pred</span><span class="s2">, </span><span class="s4">1e-4</span><span class="s2">, </span><span class="s1">np.inf)</span>
                <span class="s2">elif </span><span class="s1">(family_class</span><span class="s2">, </span><span class="s1">link) == (fam.NegativeBinomial</span><span class="s2">,</span>
                                              <span class="s1">lnk.InverseSquared):</span>
                    <span class="s1">lin_pred = </span><span class="s4">0.1 </span><span class="s1">+ np.random.uniform(size=exog.shape[</span><span class="s4">0</span><span class="s1">])</span>
                    <span class="s2">continue  </span><span class="s3"># skip due to non-convergence</span>
                <span class="s2">elif </span><span class="s1">(family_class</span><span class="s2">, </span><span class="s1">link) == (fam.NegativeBinomial</span><span class="s2">,</span>
                                              <span class="s1">lnk.InversePower):</span>
                    <span class="s1">lin_pred = </span><span class="s4">1 </span><span class="s1">+ exog.sum(</span><span class="s4">1</span><span class="s1">) / </span><span class="s4">5</span>

                <span class="s2">elif </span><span class="s1">(family_class</span><span class="s2">, </span><span class="s1">link) == (fam.Gaussian</span><span class="s2">, </span><span class="s1">lnk.InversePower):</span>
                    <span class="s3"># adding skip because of convergence failure</span>
                    <span class="s1">skip_one = </span><span class="s2">True</span>
                <span class="s2">else</span><span class="s1">:</span>
                    <span class="s1">lin_pred = np.random.uniform(size=exog.shape[</span><span class="s4">0</span><span class="s1">])</span>

                <span class="s1">endog = gen_endog(lin_pred</span><span class="s2">, </span><span class="s1">family_class</span><span class="s2">, </span><span class="s1">link</span><span class="s2">, </span><span class="s1">binom_version)</span>

                <span class="s2">with </span><span class="s1">warnings.catch_warnings():</span>
                    <span class="s1">warnings.simplefilter(</span><span class="s5">&quot;ignore&quot;</span><span class="s1">)</span>
                    <span class="s1">mod_irls = sm.GLM(endog</span><span class="s2">, </span><span class="s1">exog</span><span class="s2">,</span>
                                      <span class="s1">family=family_class(link=link()))</span>
                <span class="s1">rslt_irls = mod_irls.fit(method=</span><span class="s5">&quot;IRLS&quot;</span><span class="s1">)</span>

                <span class="s3"># Try with and without starting values.</span>
                <span class="s2">for </span><span class="s1">max_start_irls</span><span class="s2">, </span><span class="s1">start_params </span><span class="s2">in </span><span class="s1">((</span><span class="s4">0</span><span class="s2">, </span><span class="s1">rslt_irls.params)</span><span class="s2">,</span>
                                                     <span class="s1">(</span><span class="s4">3</span><span class="s2">, None</span><span class="s1">)):</span>
                    <span class="s3"># TODO: skip convergence failures for now</span>
                    <span class="s2">if </span><span class="s1">max_start_irls &gt; </span><span class="s4">0 </span><span class="s2">and </span><span class="s1">skip_one:</span>
                        <span class="s2">continue</span>
                    <span class="s2">with </span><span class="s1">warnings.catch_warnings():</span>
                        <span class="s1">warnings.simplefilter(</span><span class="s5">&quot;ignore&quot;</span><span class="s1">)</span>
                        <span class="s1">mod_gradient = sm.GLM(endog</span><span class="s2">, </span><span class="s1">exog</span><span class="s2">,</span>
                                              <span class="s1">family=family_class(link=link()))</span>
                    <span class="s1">rslt_gradient = mod_gradient.fit(</span>
                            <span class="s1">max_start_irls=max_start_irls</span><span class="s2">,</span>
                            <span class="s1">start_params=start_params</span><span class="s2">,</span>
                            <span class="s1">method=</span><span class="s5">&quot;newton&quot;</span><span class="s2">,</span>
                            <span class="s1">optim_hessian=</span><span class="s5">'eim'</span>
                    <span class="s1">)</span>

                    <span class="s1">assert_allclose(rslt_gradient.params</span><span class="s2">, </span><span class="s1">rslt_irls.params</span><span class="s2">,</span>
                                    <span class="s1">rtol=</span><span class="s4">1e-6</span><span class="s2">, </span><span class="s1">atol=</span><span class="s4">5e-5</span><span class="s1">)</span>

                    <span class="s1">assert_allclose(rslt_gradient.llf</span><span class="s2">, </span><span class="s1">rslt_irls.llf</span><span class="s2">,</span>
                                    <span class="s1">rtol=</span><span class="s4">1e-6</span><span class="s2">, </span><span class="s1">atol=</span><span class="s4">1e-6</span><span class="s1">)</span>

                    <span class="s1">assert_allclose(rslt_gradient.scale</span><span class="s2">, </span><span class="s1">rslt_irls.scale</span><span class="s2">,</span>
                                    <span class="s1">rtol=</span><span class="s4">1e-6</span><span class="s2">, </span><span class="s1">atol=</span><span class="s4">1e-6</span><span class="s1">)</span>

                    <span class="s3"># Get the standard errors using expected information.</span>
                    <span class="s1">ehess = mod_gradient.hessian(rslt_gradient.params</span><span class="s2">,</span>
                                                 <span class="s1">observed=</span><span class="s2">False</span><span class="s1">)</span>
                    <span class="s1">gradient_bse = np.sqrt(-np.diag(np.linalg.inv(ehess)))</span>

                    <span class="s1">assert_allclose(gradient_bse</span><span class="s2">, </span><span class="s1">rslt_irls.bse</span><span class="s2">, </span><span class="s1">rtol=</span><span class="s4">1e-6</span><span class="s2">,</span>
                                    <span class="s1">atol=</span><span class="s4">5e-5</span><span class="s1">)</span>


<span class="s2">def </span><span class="s1">test_glm_irls_method():</span>
    <span class="s1">nobs</span><span class="s2">, </span><span class="s1">k_vars = </span><span class="s4">50</span><span class="s2">, </span><span class="s4">4</span>
    <span class="s1">np.random.seed(</span><span class="s4">987126</span><span class="s1">)</span>
    <span class="s1">x = np.random.randn(nobs</span><span class="s2">, </span><span class="s1">k_vars - </span><span class="s4">1</span><span class="s1">)</span>
    <span class="s1">exog = add_constant(x</span><span class="s2">, </span><span class="s1">has_constant=</span><span class="s5">'add'</span><span class="s1">)</span>
    <span class="s1">y = exog.sum(</span><span class="s4">1</span><span class="s1">) + np.random.randn(nobs)</span>

    <span class="s1">mod = GLM(y</span><span class="s2">, </span><span class="s1">exog)</span>
    <span class="s1">res1 = mod.fit()</span>
    <span class="s1">res2 = mod.fit(wls_method=</span><span class="s5">'pinv'</span><span class="s2">, </span><span class="s1">attach_wls=</span><span class="s2">True</span><span class="s1">)</span>
    <span class="s1">res3 = mod.fit(wls_method=</span><span class="s5">'qr'</span><span class="s2">, </span><span class="s1">attach_wls=</span><span class="s2">True</span><span class="s1">)</span>
    <span class="s3"># fit_gradient does not attach mle_settings</span>
    <span class="s1">res_g1 = mod.fit(start_params=res1.params</span><span class="s2">, </span><span class="s1">method=</span><span class="s5">'bfgs'</span><span class="s1">)</span>

    <span class="s2">for </span><span class="s1">r </span><span class="s2">in </span><span class="s1">[res1</span><span class="s2">, </span><span class="s1">res2</span><span class="s2">, </span><span class="s1">res3]:</span>
        <span class="s1">assert_equal(r.mle_settings[</span><span class="s5">'optimizer'</span><span class="s1">]</span><span class="s2">, </span><span class="s5">'IRLS'</span><span class="s1">)</span>
        <span class="s1">assert_equal(r.method</span><span class="s2">, </span><span class="s5">'IRLS'</span><span class="s1">)</span>

    <span class="s1">assert_equal(res1.mle_settings[</span><span class="s5">'wls_method'</span><span class="s1">]</span><span class="s2">, </span><span class="s5">'lstsq'</span><span class="s1">)</span>
    <span class="s1">assert_equal(res2.mle_settings[</span><span class="s5">'wls_method'</span><span class="s1">]</span><span class="s2">, </span><span class="s5">'pinv'</span><span class="s1">)</span>
    <span class="s1">assert_equal(res3.mle_settings[</span><span class="s5">'wls_method'</span><span class="s1">]</span><span class="s2">, </span><span class="s5">'qr'</span><span class="s1">)</span>

    <span class="s1">assert_(hasattr(res2.results_wls.model</span><span class="s2">, </span><span class="s5">'pinv_wexog'</span><span class="s1">))</span>
    <span class="s1">assert_(hasattr(res3.results_wls.model</span><span class="s2">, </span><span class="s5">'exog_Q'</span><span class="s1">))</span>

    <span class="s3"># fit_gradient currently does not attach mle_settings</span>
    <span class="s1">assert_equal(res_g1.method</span><span class="s2">, </span><span class="s5">'bfgs'</span><span class="s1">)</span>


<span class="s2">class </span><span class="s1">CheckWtdDuplicationMixin:</span>
    <span class="s1">decimal_params = DECIMAL_4</span>

    <span class="s1">@classmethod</span>
    <span class="s2">def </span><span class="s1">setup_class(cls):</span>
        <span class="s1">cls.data = cpunish.load()</span>
        <span class="s1">cls.data.endog = np.asarray(cls.data.endog)</span>
        <span class="s1">cls.data.exog = np.asarray(cls.data.exog)</span>
        <span class="s1">cls.endog = cls.data.endog</span>
        <span class="s1">cls.exog = cls.data.exog</span>
        <span class="s1">np.random.seed(</span><span class="s4">1234</span><span class="s1">)</span>
        <span class="s1">cls.weight = np.random.randint(</span><span class="s4">5</span><span class="s2">, </span><span class="s4">100</span><span class="s2">, </span><span class="s1">len(cls.endog))</span>
        <span class="s1">cls.endog_big = np.repeat(cls.endog</span><span class="s2">, </span><span class="s1">cls.weight)</span>
        <span class="s1">cls.exog_big = np.repeat(cls.exog</span><span class="s2">, </span><span class="s1">cls.weight</span><span class="s2">, </span><span class="s1">axis=</span><span class="s4">0</span><span class="s1">)</span>

    <span class="s2">def </span><span class="s1">test_params(self):</span>
        <span class="s1">assert_allclose(self.res1.params</span><span class="s2">, </span><span class="s1">self.res2.params</span><span class="s2">,  </span><span class="s1">atol=</span><span class="s4">1e-6</span><span class="s2">,</span>
                        <span class="s1">rtol=</span><span class="s4">1e-6</span><span class="s1">)</span>

    <span class="s1">decimal_bse = DECIMAL_4</span>

    <span class="s2">def </span><span class="s1">test_standard_errors(self):</span>
        <span class="s1">assert_allclose(self.res1.bse</span><span class="s2">, </span><span class="s1">self.res2.bse</span><span class="s2">, </span><span class="s1">rtol=</span><span class="s4">1e-5</span><span class="s2">, </span><span class="s1">atol=</span><span class="s4">1e-6</span><span class="s1">)</span>

    <span class="s1">decimal_resids = DECIMAL_4</span>

    <span class="s3"># TODO: This does not work... Arrays are of different shape.</span>
    <span class="s3"># Perhaps we use self.res1.model.family.resid_XXX()?</span>
    <span class="s5">&quot;&quot;&quot; 
    def test_residuals(self): 
        resids1 = np.column_stack((self.res1.resid_pearson, 
                                   self.res1.resid_deviance, 
                                   self.res1.resid_working, 
                                   self.res1.resid_anscombe, 
                                   self.res1.resid_response)) 
        resids2 = np.column_stack((self.res1.resid_pearson, 
                                   self.res2.resid_deviance, 
                                   self.res2.resid_working, 
                                   self.res2.resid_anscombe, 
                                   self.res2.resid_response)) 
        assert_allclose(resids1, resids2, self.decimal_resids) 
    &quot;&quot;&quot;</span>

    <span class="s2">def </span><span class="s1">test_aic(self):</span>
        <span class="s3"># R includes the estimation of the scale as a lost dof</span>
        <span class="s3"># Does not with Gamma though</span>
        <span class="s1">assert_allclose(self.res1.aic</span><span class="s2">, </span><span class="s1">self.res2.aic</span><span class="s2">,  </span><span class="s1">atol=</span><span class="s4">1e-6</span><span class="s2">, </span><span class="s1">rtol=</span><span class="s4">1e-6</span><span class="s1">)</span>

    <span class="s2">def </span><span class="s1">test_deviance(self):</span>
        <span class="s1">assert_allclose(self.res1.deviance</span><span class="s2">, </span><span class="s1">self.res2.deviance</span><span class="s2">,  </span><span class="s1">atol=</span><span class="s4">1e-6</span><span class="s2">,</span>
                        <span class="s1">rtol=</span><span class="s4">1e-6</span><span class="s1">)</span>

    <span class="s2">def </span><span class="s1">test_scale(self):</span>
        <span class="s1">assert_allclose(self.res1.scale</span><span class="s2">, </span><span class="s1">self.res2.scale</span><span class="s2">, </span><span class="s1">atol=</span><span class="s4">1e-6</span><span class="s2">, </span><span class="s1">rtol=</span><span class="s4">1e-6</span><span class="s1">)</span>

    <span class="s2">def </span><span class="s1">test_loglike(self):</span>
        <span class="s3"># Stata uses the below llf for these families</span>
        <span class="s3"># We differ with R for them</span>
        <span class="s1">assert_allclose(self.res1.llf</span><span class="s2">, </span><span class="s1">self.res2.llf</span><span class="s2">, </span><span class="s4">1e-6</span><span class="s1">)</span>

    <span class="s1">decimal_null_deviance = DECIMAL_4</span>

    <span class="s2">def </span><span class="s1">test_null_deviance(self):</span>
        <span class="s2">with </span><span class="s1">warnings.catch_warnings():</span>
            <span class="s1">warnings.simplefilter(</span><span class="s5">&quot;ignore&quot;</span><span class="s2">, </span><span class="s1">DomainWarning)</span>

            <span class="s1">assert_allclose(self.res1.null_deviance</span><span class="s2">,</span>
                            <span class="s1">self.res2.null_deviance</span><span class="s2">,</span>
                            <span class="s1">atol=</span><span class="s4">1e-6</span><span class="s2">,</span>
                            <span class="s1">rtol=</span><span class="s4">1e-6</span><span class="s1">)</span>

    <span class="s1">decimal_bic = DECIMAL_4</span>

    <span class="s2">def </span><span class="s1">test_bic(self):</span>
        <span class="s2">with </span><span class="s1">warnings.catch_warnings():</span>
            <span class="s1">warnings.simplefilter(</span><span class="s5">&quot;ignore&quot;</span><span class="s1">)</span>
            <span class="s1">assert_allclose(self.res1.bic</span><span class="s2">, </span><span class="s1">self.res2.bic</span><span class="s2">,  </span><span class="s1">atol=</span><span class="s4">1e-6</span><span class="s2">, </span><span class="s1">rtol=</span><span class="s4">1e-6</span><span class="s1">)</span>

    <span class="s1">decimal_fittedvalues = DECIMAL_4</span>

    <span class="s2">def </span><span class="s1">test_fittedvalues(self):</span>
        <span class="s1">res2_fitted = self.res2.predict(self.res1.model.exog)</span>
        <span class="s1">assert_allclose(self.res1.fittedvalues</span><span class="s2">, </span><span class="s1">res2_fitted</span><span class="s2">, </span><span class="s1">atol=</span><span class="s4">1e-5</span><span class="s2">,</span>
                        <span class="s1">rtol=</span><span class="s4">1e-5</span><span class="s1">)</span>

    <span class="s1">decimal_tpvalues = DECIMAL_4</span>

    <span class="s2">def </span><span class="s1">test_tpvalues(self):</span>
        <span class="s3"># test comparing tvalues and pvalues with normal implementation</span>
        <span class="s3"># make sure they use normal distribution (inherited in results class)</span>
        <span class="s1">assert_allclose(self.res1.tvalues</span><span class="s2">, </span><span class="s1">self.res2.tvalues</span><span class="s2">, </span><span class="s1">atol=</span><span class="s4">1e-6</span><span class="s2">,</span>
                        <span class="s1">rtol=</span><span class="s4">2e-4</span><span class="s1">)</span>
        <span class="s1">assert_allclose(self.res1.pvalues</span><span class="s2">, </span><span class="s1">self.res2.pvalues</span><span class="s2">, </span><span class="s1">atol=</span><span class="s4">1e-6</span><span class="s2">,</span>
                        <span class="s1">rtol=</span><span class="s4">1e-6</span><span class="s1">)</span>
        <span class="s1">assert_allclose(self.res1.conf_int()</span><span class="s2">, </span><span class="s1">self.res2.conf_int()</span><span class="s2">, </span><span class="s1">atol=</span><span class="s4">1e-6</span><span class="s2">,</span>
                        <span class="s1">rtol=</span><span class="s4">1e-6</span><span class="s1">)</span>


<span class="s2">class </span><span class="s1">TestWtdGlmPoisson(CheckWtdDuplicationMixin):</span>

    <span class="s1">@classmethod</span>
    <span class="s2">def </span><span class="s1">setup_class(cls):</span>
        <span class="s0">''' 
        Tests Poisson family with canonical log link. 
        '''</span>
        <span class="s1">super(TestWtdGlmPoisson</span><span class="s2">, </span><span class="s1">cls).setup_class()</span>
        <span class="s1">cls.endog = np.asarray(cls.endog)</span>
        <span class="s1">cls.exog = np.asarray(cls.exog)</span>

        <span class="s1">cls.res1 = GLM(cls.endog</span><span class="s2">, </span><span class="s1">cls.exog</span><span class="s2">,</span>
                        <span class="s1">freq_weights=cls.weight</span><span class="s2">,</span>
                        <span class="s1">family=sm.families.Poisson()).fit()</span>
        <span class="s1">cls.res2 = GLM(cls.endog_big</span><span class="s2">, </span><span class="s1">cls.exog_big</span><span class="s2">,</span>
                        <span class="s1">family=sm.families.Poisson()).fit()</span>


<span class="s2">class </span><span class="s1">TestWtdGlmPoissonNewton(CheckWtdDuplicationMixin):</span>
    <span class="s1">@classmethod</span>
    <span class="s2">def </span><span class="s1">setup_class(cls):</span>
        <span class="s0">''' 
        Tests Poisson family with canonical log link. 
        '''</span>
        <span class="s1">super(TestWtdGlmPoissonNewton</span><span class="s2">, </span><span class="s1">cls).setup_class()</span>

        <span class="s1">start_params = np.array([</span><span class="s4">1.82794424e-04</span><span class="s2">, </span><span class="s1">-</span><span class="s4">4.76785037e-02</span><span class="s2">,</span>
                                 <span class="s1">-</span><span class="s4">9.48249717e-02</span><span class="s2">, </span><span class="s1">-</span><span class="s4">2.92293226e-04</span><span class="s2">,</span>
                                 <span class="s4">2.63728909e+00</span><span class="s2">, </span><span class="s1">-</span><span class="s4">2.05934384e+01</span><span class="s1">])</span>

        <span class="s1">fit_kwds = dict(method=</span><span class="s5">'newton'</span><span class="s1">)</span>
        <span class="s1">cls.res1 = GLM(cls.endog</span><span class="s2">, </span><span class="s1">cls.exog</span><span class="s2">,</span>
                        <span class="s1">freq_weights=cls.weight</span><span class="s2">,</span>
                        <span class="s1">family=sm.families.Poisson()).fit(**fit_kwds)</span>
        <span class="s1">fit_kwds = dict(method=</span><span class="s5">'newton'</span><span class="s2">, </span><span class="s1">start_params=start_params)</span>
        <span class="s1">cls.res2 = GLM(cls.endog_big</span><span class="s2">, </span><span class="s1">cls.exog_big</span><span class="s2">,</span>
                        <span class="s1">family=sm.families.Poisson()).fit(**fit_kwds)</span>


<span class="s2">class </span><span class="s1">TestWtdGlmPoissonHC0(CheckWtdDuplicationMixin):</span>
    <span class="s1">@classmethod</span>
    <span class="s2">def </span><span class="s1">setup_class(cls):</span>

        <span class="s0">''' 
        Tests Poisson family with canonical log link. 
        '''</span>
        <span class="s1">super(TestWtdGlmPoissonHC0</span><span class="s2">, </span><span class="s1">cls).setup_class()</span>

        <span class="s1">start_params = np.array([</span><span class="s4">1.82794424e-04</span><span class="s2">, </span><span class="s1">-</span><span class="s4">4.76785037e-02</span><span class="s2">,</span>
                                 <span class="s1">-</span><span class="s4">9.48249717e-02</span><span class="s2">, </span><span class="s1">-</span><span class="s4">2.92293226e-04</span><span class="s2">,</span>
                                 <span class="s4">2.63728909e+00</span><span class="s2">, </span><span class="s1">-</span><span class="s4">2.05934384e+01</span><span class="s1">])</span>

        <span class="s1">fit_kwds = dict(cov_type=</span><span class="s5">'HC0'</span><span class="s1">)</span>
        <span class="s1">cls.res1 = GLM(cls.endog</span><span class="s2">, </span><span class="s1">cls.exog</span><span class="s2">,</span>
                        <span class="s1">freq_weights=cls.weight</span><span class="s2">,</span>
                        <span class="s1">family=sm.families.Poisson()).fit(**fit_kwds)</span>
        <span class="s1">fit_kwds = dict(cov_type=</span><span class="s5">'HC0'</span><span class="s2">, </span><span class="s1">start_params=start_params)</span>
        <span class="s1">cls.res2 = GLM(cls.endog_big</span><span class="s2">, </span><span class="s1">cls.exog_big</span><span class="s2">,</span>
                        <span class="s1">family=sm.families.Poisson()).fit(**fit_kwds)</span>


<span class="s2">class </span><span class="s1">TestWtdGlmPoissonClu(CheckWtdDuplicationMixin):</span>
    <span class="s1">@classmethod</span>
    <span class="s2">def </span><span class="s1">setup_class(cls):</span>

        <span class="s0">''' 
        Tests Poisson family with canonical log link. 
        '''</span>
        <span class="s1">super(TestWtdGlmPoissonClu</span><span class="s2">, </span><span class="s1">cls).setup_class()</span>

        <span class="s1">start_params = np.array([</span><span class="s4">1.82794424e-04</span><span class="s2">, </span><span class="s1">-</span><span class="s4">4.76785037e-02</span><span class="s2">,</span>
                                 <span class="s1">-</span><span class="s4">9.48249717e-02</span><span class="s2">, </span><span class="s1">-</span><span class="s4">2.92293226e-04</span><span class="s2">,</span>
                                 <span class="s4">2.63728909e+00</span><span class="s2">, </span><span class="s1">-</span><span class="s4">2.05934384e+01</span><span class="s1">])</span>

        <span class="s1">gid = np.arange(</span><span class="s4">1</span><span class="s2">, </span><span class="s1">len(cls.endog) + </span><span class="s4">1</span><span class="s1">) // </span><span class="s4">2</span>
        <span class="s1">fit_kwds = dict(cov_type=</span><span class="s5">'cluster'</span><span class="s2">, </span><span class="s1">cov_kwds={</span><span class="s5">'groups'</span><span class="s1">: gid</span><span class="s2">, </span><span class="s5">'use_correction'</span><span class="s1">:</span><span class="s2">False</span><span class="s1">})</span>

        <span class="s2">import </span><span class="s1">warnings</span>
        <span class="s2">with </span><span class="s1">warnings.catch_warnings():</span>
            <span class="s1">warnings.simplefilter(</span><span class="s5">&quot;ignore&quot;</span><span class="s1">)</span>
            <span class="s1">cls.res1 = GLM(cls.endog</span><span class="s2">, </span><span class="s1">cls.exog</span><span class="s2">,</span>
                            <span class="s1">freq_weights=cls.weight</span><span class="s2">,</span>
                            <span class="s1">family=sm.families.Poisson()).fit(**fit_kwds)</span>
            <span class="s1">gidr = np.repeat(gid</span><span class="s2">, </span><span class="s1">cls.weight)</span>
            <span class="s1">fit_kwds = dict(cov_type=</span><span class="s5">'cluster'</span><span class="s2">, </span><span class="s1">cov_kwds={</span><span class="s5">'groups'</span><span class="s1">: gidr</span><span class="s2">, </span><span class="s5">'use_correction'</span><span class="s1">:</span><span class="s2">False</span><span class="s1">})</span>
            <span class="s1">cls.res2 = GLM(cls.endog_big</span><span class="s2">, </span><span class="s1">cls.exog_big</span><span class="s2">,</span>
                            <span class="s1">family=sm.families.Poisson()).fit(start_params=start_params</span><span class="s2">,</span>
                                                              <span class="s1">**fit_kwds)</span>


<span class="s2">class </span><span class="s1">TestWtdGlmBinomial(CheckWtdDuplicationMixin):</span>
    <span class="s1">@classmethod</span>
    <span class="s2">def </span><span class="s1">setup_class(cls):</span>

        <span class="s0">''' 
        Tests Binomial family with canonical logit link. 
        '''</span>
        <span class="s1">super(TestWtdGlmBinomial</span><span class="s2">, </span><span class="s1">cls).setup_class()</span>
        <span class="s1">cls.endog = cls.endog / </span><span class="s4">100</span>
        <span class="s1">cls.endog_big = cls.endog_big / </span><span class="s4">100</span>
        <span class="s1">cls.res1 = GLM(cls.endog</span><span class="s2">, </span><span class="s1">cls.exog</span><span class="s2">,</span>
                       <span class="s1">freq_weights=cls.weight</span><span class="s2">,</span>
                       <span class="s1">family=sm.families.Binomial()).fit()</span>
        <span class="s1">cls.res2 = GLM(cls.endog_big</span><span class="s2">, </span><span class="s1">cls.exog_big</span><span class="s2">,</span>
                       <span class="s1">family=sm.families.Binomial()).fit()</span>


<span class="s2">class </span><span class="s1">TestWtdGlmNegativeBinomial(CheckWtdDuplicationMixin):</span>
    <span class="s1">@classmethod</span>
    <span class="s2">def </span><span class="s1">setup_class(cls):</span>

        <span class="s0">''' 
        Tests Negative Binomial family with canonical link 
        g(p) = log(p/(p + 1/alpha)) 
        '''</span>
        <span class="s1">super(TestWtdGlmNegativeBinomial</span><span class="s2">, </span><span class="s1">cls).setup_class()</span>
        <span class="s1">alpha = </span><span class="s4">1.</span>

        <span class="s2">with </span><span class="s1">warnings.catch_warnings():</span>
            <span class="s1">warnings.simplefilter(</span><span class="s5">&quot;ignore&quot;</span><span class="s2">, </span><span class="s1">category=DomainWarning)</span>
            <span class="s1">family_link = sm.families.NegativeBinomial(</span>
                <span class="s1">link=sm.families.links.NegativeBinomial(alpha=alpha)</span><span class="s2">,</span>
                <span class="s1">alpha=alpha)</span>
            <span class="s1">cls.res1 = GLM(cls.endog</span><span class="s2">, </span><span class="s1">cls.exog</span><span class="s2">,</span>
                           <span class="s1">freq_weights=cls.weight</span><span class="s2">,</span>
                           <span class="s1">family=family_link).fit()</span>
            <span class="s1">cls.res2 = GLM(cls.endog_big</span><span class="s2">, </span><span class="s1">cls.exog_big</span><span class="s2">,</span>
                           <span class="s1">family=family_link).fit()</span>


<span class="s2">class </span><span class="s1">TestWtdGlmGamma(CheckWtdDuplicationMixin):</span>
    <span class="s1">@classmethod</span>
    <span class="s2">def </span><span class="s1">setup_class(cls):</span>

        <span class="s0">''' 
        Tests Gamma family with log link. 
        '''</span>
        <span class="s1">super(TestWtdGlmGamma</span><span class="s2">, </span><span class="s1">cls).setup_class()</span>
        <span class="s1">family_link = sm.families.Gamma(sm.families.links.Log())</span>
        <span class="s1">cls.res1 = GLM(cls.endog</span><span class="s2">, </span><span class="s1">cls.exog</span><span class="s2">,</span>
                       <span class="s1">freq_weights=cls.weight</span><span class="s2">,</span>
                       <span class="s1">family=family_link).fit()</span>
        <span class="s1">cls.res2 = GLM(cls.endog_big</span><span class="s2">, </span><span class="s1">cls.exog_big</span><span class="s2">,</span>
                       <span class="s1">family=family_link).fit()</span>


<span class="s2">class </span><span class="s1">TestWtdGlmGaussian(CheckWtdDuplicationMixin):</span>
    <span class="s1">@classmethod</span>
    <span class="s2">def </span><span class="s1">setup_class(cls):</span>
        <span class="s0">''' 
        Tests Gaussian family with log link. 
        '''</span>
        <span class="s1">super(TestWtdGlmGaussian</span><span class="s2">, </span><span class="s1">cls).setup_class()</span>
        <span class="s1">family_link = sm.families.Gaussian(sm.families.links.Log())</span>
        <span class="s1">cls.res1 = GLM(cls.endog</span><span class="s2">, </span><span class="s1">cls.exog</span><span class="s2">,</span>
                       <span class="s1">freq_weights=cls.weight</span><span class="s2">,</span>
                       <span class="s1">family=family_link).fit()</span>
        <span class="s1">cls.res2 = GLM(cls.endog_big</span><span class="s2">, </span><span class="s1">cls.exog_big</span><span class="s2">,</span>
                       <span class="s1">family=family_link).fit()</span>


<span class="s2">class </span><span class="s1">TestWtdGlmInverseGaussian(CheckWtdDuplicationMixin):</span>
    <span class="s1">@classmethod</span>
    <span class="s2">def </span><span class="s1">setup_class(cls):</span>
        <span class="s0">''' 
        Tests InverseGaussian family with log link. 
        '''</span>
        <span class="s1">super(TestWtdGlmInverseGaussian</span><span class="s2">, </span><span class="s1">cls).setup_class()</span>
        <span class="s1">family_link = sm.families.InverseGaussian(sm.families.links.Log())</span>
        <span class="s1">cls.res1 = GLM(cls.endog</span><span class="s2">, </span><span class="s1">cls.exog</span><span class="s2">,</span>
                       <span class="s1">freq_weights=cls.weight</span><span class="s2">,</span>
                       <span class="s1">family=family_link).fit()</span>
        <span class="s1">cls.res2 = GLM(cls.endog_big</span><span class="s2">, </span><span class="s1">cls.exog_big</span><span class="s2">,</span>
                       <span class="s1">family=family_link).fit()</span>


<span class="s2">class </span><span class="s1">TestWtdGlmGammaNewton(CheckWtdDuplicationMixin):</span>
    <span class="s1">@classmethod</span>
    <span class="s2">def </span><span class="s1">setup_class(cls):</span>
        <span class="s0">''' 
        Tests Gamma family with log link. 
        '''</span>
        <span class="s1">super(TestWtdGlmGammaNewton</span><span class="s2">, </span><span class="s1">cls).setup_class()</span>
        <span class="s1">family_link = sm.families.Gamma(sm.families.links.Log())</span>
        <span class="s1">cls.res1 = GLM(cls.endog</span><span class="s2">, </span><span class="s1">cls.exog</span><span class="s2">,</span>
                       <span class="s1">freq_weights=cls.weight</span><span class="s2">,</span>
                       <span class="s1">family=family_link</span>
                       <span class="s1">).fit(method=</span><span class="s5">'newton'</span><span class="s1">)</span>
        <span class="s1">cls.res2 = GLM(cls.endog_big</span><span class="s2">, </span><span class="s1">cls.exog_big</span><span class="s2">,</span>
                       <span class="s1">family=family_link</span>
                       <span class="s1">).fit(method=</span><span class="s5">'newton'</span><span class="s1">)</span>

    <span class="s2">def </span><span class="s1">test_init_kwargs(self):</span>
        <span class="s1">family_link = sm.families.Gamma(sm.families.links.Log())</span>

        <span class="s2">with </span><span class="s1">pytest.warns(ValueWarning</span><span class="s2">, </span><span class="s1">match=</span><span class="s5">&quot;unknown kwargs&quot;</span><span class="s1">):</span>
            <span class="s1">GLM(self.endog</span><span class="s2">, </span><span class="s1">self.exog</span><span class="s2">, </span><span class="s1">family=family_link</span><span class="s2">,</span>
                <span class="s1">weights=self.weight</span><span class="s2">,  </span><span class="s3"># incorrect keyword</span>
                <span class="s1">)</span>


<span class="s2">class </span><span class="s1">TestWtdGlmGammaScale_X2(CheckWtdDuplicationMixin):</span>
    <span class="s1">@classmethod</span>
    <span class="s2">def </span><span class="s1">setup_class(cls):</span>
        <span class="s0">''' 
        Tests Gamma family with log link. 
        '''</span>
        <span class="s1">super(TestWtdGlmGammaScale_X2</span><span class="s2">, </span><span class="s1">cls).setup_class()</span>
        <span class="s1">family_link = sm.families.Gamma(sm.families.links.Log())</span>
        <span class="s1">cls.res1 = GLM(cls.endog</span><span class="s2">, </span><span class="s1">cls.exog</span><span class="s2">,</span>
                       <span class="s1">freq_weights=cls.weight</span><span class="s2">,</span>
                       <span class="s1">family=family_link</span><span class="s2">,</span>
                       <span class="s1">).fit(scale=</span><span class="s5">'X2'</span><span class="s1">)</span>
        <span class="s1">cls.res2 = GLM(cls.endog_big</span><span class="s2">, </span><span class="s1">cls.exog_big</span><span class="s2">,</span>
                       <span class="s1">family=family_link</span><span class="s2">,</span>
                       <span class="s1">).fit(scale=</span><span class="s5">'X2'</span><span class="s1">)</span>


<span class="s2">class </span><span class="s1">TestWtdGlmGammaScale_dev(CheckWtdDuplicationMixin):</span>
    <span class="s1">@classmethod</span>
    <span class="s2">def </span><span class="s1">setup_class(cls):</span>
        <span class="s0">''' 
        Tests Gamma family with log link. 
        '''</span>
        <span class="s1">super(TestWtdGlmGammaScale_dev</span><span class="s2">, </span><span class="s1">cls).setup_class()</span>
        <span class="s1">family_link = sm.families.Gamma(sm.families.links.Log())</span>
        <span class="s1">cls.res1 = GLM(cls.endog</span><span class="s2">, </span><span class="s1">cls.exog</span><span class="s2">,</span>
                       <span class="s1">freq_weights=cls.weight</span><span class="s2">,</span>
                       <span class="s1">family=family_link</span><span class="s2">,</span>
                       <span class="s1">).fit(scale=</span><span class="s5">'dev'</span><span class="s1">)</span>
        <span class="s1">cls.res2 = GLM(cls.endog_big</span><span class="s2">, </span><span class="s1">cls.exog_big</span><span class="s2">,</span>
                       <span class="s1">family=family_link</span><span class="s2">,</span>
                       <span class="s1">).fit(scale=</span><span class="s5">'dev'</span><span class="s1">)</span>

    <span class="s2">def </span><span class="s1">test_missing(self):</span>
        <span class="s1">endog = self.data.endog.copy()</span>
        <span class="s1">exog = self.data.exog.copy()</span>
        <span class="s1">exog[</span><span class="s4">0</span><span class="s2">, </span><span class="s4">0</span><span class="s1">] = np.nan</span>
        <span class="s1">endog[[</span><span class="s4">2</span><span class="s2">, </span><span class="s4">4</span><span class="s2">, </span><span class="s4">6</span><span class="s2">, </span><span class="s4">8</span><span class="s1">]] = np.nan</span>
        <span class="s1">freq_weights = self.weight</span>
        <span class="s1">mod_misisng = GLM(endog</span><span class="s2">, </span><span class="s1">exog</span><span class="s2">, </span><span class="s1">family=self.res1.model.family</span><span class="s2">,</span>
                          <span class="s1">freq_weights=freq_weights</span><span class="s2">, </span><span class="s1">missing=</span><span class="s5">'drop'</span><span class="s1">)</span>
        <span class="s1">assert_equal(mod_misisng.freq_weights.shape[</span><span class="s4">0</span><span class="s1">]</span><span class="s2">,</span>
                     <span class="s1">mod_misisng.endog.shape[</span><span class="s4">0</span><span class="s1">])</span>
        <span class="s1">assert_equal(mod_misisng.freq_weights.shape[</span><span class="s4">0</span><span class="s1">]</span><span class="s2">,</span>
                     <span class="s1">mod_misisng.exog.shape[</span><span class="s4">0</span><span class="s1">])</span>
        <span class="s1">keep_idx = np.array([</span><span class="s4">1</span><span class="s2">,  </span><span class="s4">3</span><span class="s2">,  </span><span class="s4">5</span><span class="s2">,  </span><span class="s4">7</span><span class="s2">,  </span><span class="s4">9</span><span class="s2">, </span><span class="s4">10</span><span class="s2">, </span><span class="s4">11</span><span class="s2">, </span><span class="s4">12</span><span class="s2">, </span><span class="s4">13</span><span class="s2">, </span><span class="s4">14</span><span class="s2">, </span><span class="s4">15</span><span class="s2">, </span><span class="s4">16</span><span class="s1">])</span>
        <span class="s1">assert_equal(mod_misisng.freq_weights</span><span class="s2">, </span><span class="s1">self.weight[keep_idx])</span>


<span class="s2">class </span><span class="s1">TestWtdTweedieLog(CheckWtdDuplicationMixin):</span>
    <span class="s1">@classmethod</span>
    <span class="s2">def </span><span class="s1">setup_class(cls):</span>
        <span class="s0">''' 
        Tests Tweedie family with log link and var_power=1. 
        '''</span>
        <span class="s1">super(TestWtdTweedieLog</span><span class="s2">, </span><span class="s1">cls).setup_class()</span>
        <span class="s1">family_link = sm.families.Tweedie(link=sm.families.links.Log()</span><span class="s2">,</span>
                                          <span class="s1">var_power=</span><span class="s4">1</span><span class="s1">)</span>
        <span class="s1">cls.res1 = GLM(cls.endog</span><span class="s2">, </span><span class="s1">cls.exog</span><span class="s2">,</span>
                        <span class="s1">freq_weights=cls.weight</span><span class="s2">,</span>
                        <span class="s1">family=family_link).fit()</span>
        <span class="s1">cls.res2 = GLM(cls.endog_big</span><span class="s2">, </span><span class="s1">cls.exog_big</span><span class="s2">,</span>
                        <span class="s1">family=family_link).fit()</span>


<span class="s2">class </span><span class="s1">TestWtdTweediePower2(CheckWtdDuplicationMixin):</span>
    <span class="s1">@classmethod</span>
    <span class="s2">def </span><span class="s1">setup_class(cls):</span>
        <span class="s0">''' 
        Tests Tweedie family with Power(1) link and var_power=2. 
        '''</span>
        <span class="s1">cls.data = cpunish.load_pandas()</span>
        <span class="s1">cls.endog = cls.data.endog</span>
        <span class="s1">cls.exog = cls.data.exog[[</span><span class="s5">'INCOME'</span><span class="s2">, </span><span class="s5">'SOUTH'</span><span class="s1">]]</span>
        <span class="s1">np.random.seed(</span><span class="s4">1234</span><span class="s1">)</span>
        <span class="s1">cls.weight = np.random.randint(</span><span class="s4">5</span><span class="s2">, </span><span class="s4">100</span><span class="s2">, </span><span class="s1">len(cls.endog))</span>
        <span class="s1">cls.endog_big = np.repeat(cls.endog.values</span><span class="s2">, </span><span class="s1">cls.weight)</span>
        <span class="s1">cls.exog_big = np.repeat(cls.exog.values</span><span class="s2">, </span><span class="s1">cls.weight</span><span class="s2">, </span><span class="s1">axis=</span><span class="s4">0</span><span class="s1">)</span>
        <span class="s1">link = sm.families.links.Power()</span>
        <span class="s1">family_link = sm.families.Tweedie(link=link</span><span class="s2">, </span><span class="s1">var_power=</span><span class="s4">2</span><span class="s1">)</span>
        <span class="s1">cls.res1 = GLM(cls.endog</span><span class="s2">, </span><span class="s1">cls.exog</span><span class="s2">,</span>
                       <span class="s1">freq_weights=cls.weight</span><span class="s2">,</span>
                       <span class="s1">family=family_link).fit()</span>
        <span class="s1">cls.res2 = GLM(cls.endog_big</span><span class="s2">, </span><span class="s1">cls.exog_big</span><span class="s2">,</span>
                       <span class="s1">family=family_link).fit()</span>


<span class="s2">class </span><span class="s1">TestWtdTweediePower15(CheckWtdDuplicationMixin):</span>
    <span class="s1">@classmethod</span>
    <span class="s2">def </span><span class="s1">setup_class(cls):</span>
        <span class="s0">''' 
        Tests Tweedie family with Power(0.5) link and var_power=1.5. 
        '''</span>
        <span class="s1">super(TestWtdTweediePower15</span><span class="s2">, </span><span class="s1">cls).setup_class()</span>
        <span class="s1">family_link = sm.families.Tweedie(link=sm.families.links.Power(</span><span class="s4">0.5</span><span class="s1">)</span><span class="s2">,</span>
                                          <span class="s1">var_power=</span><span class="s4">1.5</span><span class="s1">)</span>
        <span class="s1">cls.res1 = GLM(cls.endog</span><span class="s2">, </span><span class="s1">cls.exog</span><span class="s2">,</span>
                        <span class="s1">freq_weights=cls.weight</span><span class="s2">,</span>
                        <span class="s1">family=family_link).fit()</span>
        <span class="s1">cls.res2 = GLM(cls.endog_big</span><span class="s2">, </span><span class="s1">cls.exog_big</span><span class="s2">,</span>
                        <span class="s1">family=family_link).fit()</span>


<span class="s2">def </span><span class="s1">test_wtd_patsy_missing():</span>
    <span class="s2">import </span><span class="s1">pandas </span><span class="s2">as </span><span class="s1">pd</span>
    <span class="s1">data = cpunish.load()</span>
    <span class="s1">data.endog = np.require(data.endog</span><span class="s2">, </span><span class="s1">requirements=</span><span class="s5">&quot;W&quot;</span><span class="s1">)</span>
    <span class="s1">data.exog = np.require(data.exog</span><span class="s2">, </span><span class="s1">requirements=</span><span class="s5">&quot;W&quot;</span><span class="s1">)</span>
    <span class="s1">data.exog[</span><span class="s4">0</span><span class="s2">, </span><span class="s4">0</span><span class="s1">] = np.nan</span>
    <span class="s1">data.endog[[</span><span class="s4">2</span><span class="s2">, </span><span class="s4">4</span><span class="s2">, </span><span class="s4">6</span><span class="s2">, </span><span class="s4">8</span><span class="s1">]] = np.nan</span>
    <span class="s1">data.pandas = pd.DataFrame(data.exog</span><span class="s2">, </span><span class="s1">columns=data.exog_name)</span>
    <span class="s1">data.pandas[</span><span class="s5">'EXECUTIONS'</span><span class="s1">] = data.endog</span>
    <span class="s1">weights = np.arange(</span><span class="s4">1</span><span class="s2">, </span><span class="s1">len(data.endog)+</span><span class="s4">1</span><span class="s1">)</span>
    <span class="s1">formula = </span><span class="s5">&quot;&quot;&quot;EXECUTIONS ~ INCOME + PERPOVERTY + PERBLACK + VC100k96 + 
                 SOUTH + DEGREE&quot;&quot;&quot;</span>
    <span class="s1">mod_misisng = GLM.from_formula(formula</span><span class="s2">, </span><span class="s1">data=data.pandas</span><span class="s2">,</span>
                                   <span class="s1">freq_weights=weights)</span>
    <span class="s1">assert_equal(mod_misisng.freq_weights.shape[</span><span class="s4">0</span><span class="s1">]</span><span class="s2">,</span>
                 <span class="s1">mod_misisng.endog.shape[</span><span class="s4">0</span><span class="s1">])</span>
    <span class="s1">assert_equal(mod_misisng.freq_weights.shape[</span><span class="s4">0</span><span class="s1">]</span><span class="s2">,</span>
                 <span class="s1">mod_misisng.exog.shape[</span><span class="s4">0</span><span class="s1">])</span>
    <span class="s1">assert_equal(mod_misisng.freq_weights.shape[</span><span class="s4">0</span><span class="s1">]</span><span class="s2">, </span><span class="s4">12</span><span class="s1">)</span>
    <span class="s1">keep_weights = np.array([</span><span class="s4">2</span><span class="s2">,  </span><span class="s4">4</span><span class="s2">,  </span><span class="s4">6</span><span class="s2">,  </span><span class="s4">8</span><span class="s2">, </span><span class="s4">10</span><span class="s2">, </span><span class="s4">11</span><span class="s2">, </span><span class="s4">12</span><span class="s2">, </span><span class="s4">13</span><span class="s2">, </span><span class="s4">14</span><span class="s2">, </span><span class="s4">15</span><span class="s2">, </span><span class="s4">16</span><span class="s2">, </span><span class="s4">17</span><span class="s1">])</span>
    <span class="s1">assert_equal(mod_misisng.freq_weights</span><span class="s2">, </span><span class="s1">keep_weights)</span>


<span class="s2">class </span><span class="s1">CheckTweedie:</span>
    <span class="s2">def </span><span class="s1">test_resid(self):</span>
        <span class="s1">idx1 = len(self.res1.resid_response) - </span><span class="s4">1</span>
        <span class="s1">idx2 = len(self.res2.resid_response) - </span><span class="s4">1</span>
        <span class="s1">assert_allclose(np.concatenate((self.res1.resid_response[:</span><span class="s4">17</span><span class="s1">]</span><span class="s2">,</span>
                                        <span class="s1">[self.res1.resid_response[idx1]]))</span><span class="s2">,</span>
                        <span class="s1">np.concatenate((self.res2.resid_response[:</span><span class="s4">17</span><span class="s1">]</span><span class="s2">,</span>
                                        <span class="s1">[self.res2.resid_response[idx2]]))</span><span class="s2">,</span>
                        <span class="s1">rtol=</span><span class="s4">1e-5</span><span class="s2">, </span><span class="s1">atol=</span><span class="s4">1e-5</span><span class="s1">)</span>
        <span class="s1">assert_allclose(np.concatenate((self.res1.resid_pearson[:</span><span class="s4">17</span><span class="s1">]</span><span class="s2">,</span>
                                        <span class="s1">[self.res1.resid_pearson[idx1]]))</span><span class="s2">,</span>
                        <span class="s1">np.concatenate((self.res2.resid_pearson[:</span><span class="s4">17</span><span class="s1">]</span><span class="s2">,</span>
                                        <span class="s1">[self.res2.resid_pearson[idx2]]))</span><span class="s2">,</span>
                        <span class="s1">rtol=</span><span class="s4">1e-5</span><span class="s2">, </span><span class="s1">atol=</span><span class="s4">1e-5</span><span class="s1">)</span>
        <span class="s1">assert_allclose(np.concatenate((self.res1.resid_deviance[:</span><span class="s4">17</span><span class="s1">]</span><span class="s2">,</span>
                                        <span class="s1">[self.res1.resid_deviance[idx1]]))</span><span class="s2">,</span>
                        <span class="s1">np.concatenate((self.res2.resid_deviance[:</span><span class="s4">17</span><span class="s1">]</span><span class="s2">,</span>
                                        <span class="s1">[self.res2.resid_deviance[idx2]]))</span><span class="s2">,</span>
                        <span class="s1">rtol=</span><span class="s4">1e-5</span><span class="s2">, </span><span class="s1">atol=</span><span class="s4">1e-5</span><span class="s1">)</span>

        <span class="s1">assert_allclose(np.concatenate((self.res1.resid_working[:</span><span class="s4">17</span><span class="s1">]</span><span class="s2">,</span>
                                        <span class="s1">[self.res1.resid_working[idx1]]))</span><span class="s2">,</span>
                        <span class="s1">np.concatenate((self.res2.resid_working[:</span><span class="s4">17</span><span class="s1">]</span><span class="s2">,</span>
                                        <span class="s1">[self.res2.resid_working[idx2]]))</span><span class="s2">,</span>
                        <span class="s1">rtol=</span><span class="s4">1e-5</span><span class="s2">, </span><span class="s1">atol=</span><span class="s4">1e-5</span><span class="s1">)</span>


    <span class="s2">def </span><span class="s1">test_bse(self):</span>
        <span class="s1">assert_allclose(self.res1.bse</span><span class="s2">, </span><span class="s1">self.res2.bse</span><span class="s2">, </span><span class="s1">atol=</span><span class="s4">1e-6</span><span class="s2">, </span><span class="s1">rtol=</span><span class="s4">1e6</span><span class="s1">)</span>

    <span class="s2">def </span><span class="s1">test_params(self):</span>
        <span class="s1">assert_allclose(self.res1.params</span><span class="s2">, </span><span class="s1">self.res2.params</span><span class="s2">, </span><span class="s1">atol=</span><span class="s4">1e-5</span><span class="s2">,</span>
                        <span class="s1">rtol=</span><span class="s4">1e-5</span><span class="s1">)</span>

    <span class="s2">def </span><span class="s1">test_deviance(self):</span>
        <span class="s1">assert_allclose(self.res1.deviance</span><span class="s2">, </span><span class="s1">self.res2.deviance</span><span class="s2">, </span><span class="s1">atol=</span><span class="s4">1e-6</span><span class="s2">,</span>
                        <span class="s1">rtol=</span><span class="s4">1e-6</span><span class="s1">)</span>

    <span class="s2">def </span><span class="s1">test_df(self):</span>
        <span class="s1">assert_equal(self.res1.df_model</span><span class="s2">, </span><span class="s1">self.res2.df_model)</span>
        <span class="s1">assert_equal(self.res1.df_resid</span><span class="s2">, </span><span class="s1">self.res2.df_resid)</span>

    <span class="s2">def </span><span class="s1">test_fittedvalues(self):</span>
        <span class="s1">idx1 = len(self.res1.fittedvalues) - </span><span class="s4">1</span>
        <span class="s1">idx2 = len(self.res2.resid_response) - </span><span class="s4">1</span>
        <span class="s1">assert_allclose(np.concatenate((self.res1.fittedvalues[:</span><span class="s4">17</span><span class="s1">]</span><span class="s2">,</span>
                                        <span class="s1">[self.res1.fittedvalues[idx1]]))</span><span class="s2">,</span>
                        <span class="s1">np.concatenate((self.res2.fittedvalues[:</span><span class="s4">17</span><span class="s1">]</span><span class="s2">,</span>
                                        <span class="s1">[self.res2.fittedvalues[idx2]]))</span><span class="s2">,</span>
                        <span class="s1">atol=</span><span class="s4">1e-4</span><span class="s2">, </span><span class="s1">rtol=</span><span class="s4">1e-4</span><span class="s1">)</span>

    <span class="s2">def </span><span class="s1">test_summary(self):</span>
        <span class="s1">self.res1.summary()</span>
        <span class="s1">self.res1.summary2()</span>


<span class="s2">class </span><span class="s1">TestTweediePower15(CheckTweedie):</span>
    <span class="s1">@classmethod</span>
    <span class="s2">def </span><span class="s1">setup_class(cls):</span>
        <span class="s2">from </span><span class="s1">.results.results_glm </span><span class="s2">import </span><span class="s1">CpunishTweediePower15</span>
        <span class="s1">cls.data = cpunish.load_pandas()</span>
        <span class="s1">cls.exog = cls.data.exog[[</span><span class="s5">'INCOME'</span><span class="s2">, </span><span class="s5">'SOUTH'</span><span class="s1">]]</span>
        <span class="s1">cls.endog = cls.data.endog</span>
        <span class="s1">family_link = sm.families.Tweedie(link=sm.families.links.Power(</span><span class="s4">1</span><span class="s1">)</span><span class="s2">,</span>
                                          <span class="s1">var_power=</span><span class="s4">1.5</span><span class="s1">)</span>
        <span class="s1">cls.res1 = sm.GLM(endog=cls.data.endog</span><span class="s2">,</span>
                          <span class="s1">exog=cls.data.exog[[</span><span class="s5">'INCOME'</span><span class="s2">, </span><span class="s5">'SOUTH'</span><span class="s1">]]</span><span class="s2">,</span>
                          <span class="s1">family=family_link).fit()</span>
        <span class="s1">cls.res2 = CpunishTweediePower15()</span>


<span class="s2">class </span><span class="s1">TestTweediePower2(CheckTweedie):</span>
    <span class="s1">@classmethod</span>
    <span class="s2">def </span><span class="s1">setup_class(cls):</span>
        <span class="s2">from </span><span class="s1">.results.results_glm </span><span class="s2">import </span><span class="s1">CpunishTweediePower2</span>
        <span class="s1">cls.data = cpunish.load_pandas()</span>
        <span class="s1">cls.exog = cls.data.exog[[</span><span class="s5">'INCOME'</span><span class="s2">, </span><span class="s5">'SOUTH'</span><span class="s1">]]</span>
        <span class="s1">cls.endog = cls.data.endog</span>
        <span class="s1">family_link = sm.families.Tweedie(link=sm.families.links.Power(</span><span class="s4">1</span><span class="s1">)</span><span class="s2">,</span>
                                          <span class="s1">var_power=</span><span class="s4">2.</span><span class="s1">)</span>
        <span class="s1">cls.res1 = sm.GLM(endog=cls.data.endog</span><span class="s2">,</span>
                          <span class="s1">exog=cls.data.exog[[</span><span class="s5">'INCOME'</span><span class="s2">, </span><span class="s5">'SOUTH'</span><span class="s1">]]</span><span class="s2">,</span>
                          <span class="s1">family=family_link).fit()</span>
        <span class="s1">cls.res2 = CpunishTweediePower2()</span>


<span class="s2">class </span><span class="s1">TestTweedieLog1(CheckTweedie):</span>
    <span class="s1">@classmethod</span>
    <span class="s2">def </span><span class="s1">setup_class(cls):</span>
        <span class="s2">from </span><span class="s1">.results.results_glm </span><span class="s2">import </span><span class="s1">CpunishTweedieLog1</span>
        <span class="s1">cls.data = cpunish.load_pandas()</span>
        <span class="s1">cls.exog = cls.data.exog[[</span><span class="s5">'INCOME'</span><span class="s2">, </span><span class="s5">'SOUTH'</span><span class="s1">]]</span>
        <span class="s1">cls.endog = cls.data.endog</span>
        <span class="s1">family_link = sm.families.Tweedie(link=sm.families.links.Log()</span><span class="s2">,</span>
                                          <span class="s1">var_power=</span><span class="s4">1.</span><span class="s1">)</span>
        <span class="s1">cls.res1 = sm.GLM(endog=cls.data.endog</span><span class="s2">,</span>
                          <span class="s1">exog=cls.data.exog[[</span><span class="s5">'INCOME'</span><span class="s2">, </span><span class="s5">'SOUTH'</span><span class="s1">]]</span><span class="s2">,</span>
                          <span class="s1">family=family_link).fit()</span>
        <span class="s1">cls.res2 = CpunishTweedieLog1()</span>


<span class="s2">class </span><span class="s1">TestTweedieLog15Fair(CheckTweedie):</span>
    <span class="s1">@classmethod</span>
    <span class="s2">def </span><span class="s1">setup_class(cls):</span>
        <span class="s2">from </span><span class="s1">statsmodels.datasets.fair </span><span class="s2">import </span><span class="s1">load_pandas</span>

        <span class="s2">from </span><span class="s1">.results.results_glm </span><span class="s2">import </span><span class="s1">FairTweedieLog15</span>
        <span class="s1">data = load_pandas()</span>
        <span class="s1">family_link = sm.families.Tweedie(link=sm.families.links.Log()</span><span class="s2">,</span>
                                          <span class="s1">var_power=</span><span class="s4">1.5</span><span class="s1">)</span>
        <span class="s1">cls.res1 = sm.GLM(endog=data.endog</span><span class="s2">,</span>
                          <span class="s1">exog=data.exog[[</span><span class="s5">'rate_marriage'</span><span class="s2">, </span><span class="s5">'age'</span><span class="s2">,</span>
                                          <span class="s5">'yrs_married'</span><span class="s1">]]</span><span class="s2">,</span>
                          <span class="s1">family=family_link).fit()</span>
        <span class="s1">cls.res2 = FairTweedieLog15()</span>


<span class="s2">class </span><span class="s1">CheckTweedieSpecial:</span>
    <span class="s2">def </span><span class="s1">test_mu(self):</span>
        <span class="s1">assert_allclose(self.res1.mu</span><span class="s2">, </span><span class="s1">self.res2.mu</span><span class="s2">, </span><span class="s1">rtol=</span><span class="s4">1e-5</span><span class="s2">, </span><span class="s1">atol=</span><span class="s4">1e-5</span><span class="s1">)</span>

    <span class="s2">def </span><span class="s1">test_resid(self):</span>
        <span class="s1">assert_allclose(self.res1.resid_response</span><span class="s2">, </span><span class="s1">self.res2.resid_response</span><span class="s2">,</span>
                        <span class="s1">rtol=</span><span class="s4">1e-5</span><span class="s2">, </span><span class="s1">atol=</span><span class="s4">1e-5</span><span class="s1">)</span>
        <span class="s1">assert_allclose(self.res1.resid_pearson</span><span class="s2">, </span><span class="s1">self.res2.resid_pearson</span><span class="s2">,</span>
                        <span class="s1">rtol=</span><span class="s4">1e-5</span><span class="s2">, </span><span class="s1">atol=</span><span class="s4">1e-5</span><span class="s1">)</span>
        <span class="s1">assert_allclose(self.res1.resid_deviance</span><span class="s2">, </span><span class="s1">self.res2.resid_deviance</span><span class="s2">,</span>
                        <span class="s1">rtol=</span><span class="s4">1e-5</span><span class="s2">, </span><span class="s1">atol=</span><span class="s4">1e-5</span><span class="s1">)</span>
        <span class="s1">assert_allclose(self.res1.resid_working</span><span class="s2">, </span><span class="s1">self.res2.resid_working</span><span class="s2">,</span>
                        <span class="s1">rtol=</span><span class="s4">1e-5</span><span class="s2">, </span><span class="s1">atol=</span><span class="s4">1e-5</span><span class="s1">)</span>
        <span class="s1">assert_allclose(self.res1.resid_anscombe_unscaled</span><span class="s2">,</span>
                        <span class="s1">self.res2.resid_anscombe_unscaled</span><span class="s2">,</span>
                        <span class="s1">rtol=</span><span class="s4">1e-5</span><span class="s2">, </span><span class="s1">atol=</span><span class="s4">1e-5</span><span class="s1">)</span>


<span class="s2">class </span><span class="s1">TestTweedieSpecialLog0(CheckTweedieSpecial):</span>
    <span class="s1">@classmethod</span>
    <span class="s2">def </span><span class="s1">setup_class(cls):</span>
        <span class="s1">cls.data = cpunish.load_pandas()</span>
        <span class="s1">cls.exog = cls.data.exog[[</span><span class="s5">'INCOME'</span><span class="s2">, </span><span class="s5">'SOUTH'</span><span class="s1">]]</span>
        <span class="s1">cls.endog = cls.data.endog</span>
        <span class="s1">family1 = sm.families.Gaussian(link=sm.families.links.Log())</span>
        <span class="s1">cls.res1 = sm.GLM(endog=cls.data.endog</span><span class="s2">,</span>
                          <span class="s1">exog=cls.data.exog[[</span><span class="s5">'INCOME'</span><span class="s2">, </span><span class="s5">'SOUTH'</span><span class="s1">]]</span><span class="s2">,</span>
                          <span class="s1">family=family1).fit()</span>
        <span class="s1">family2 = sm.families.Tweedie(link=sm.families.links.Log()</span><span class="s2">,</span>
                                      <span class="s1">var_power=</span><span class="s4">0</span><span class="s1">)</span>
        <span class="s1">cls.res2 = sm.GLM(endog=cls.data.endog</span><span class="s2">,</span>
                          <span class="s1">exog=cls.data.exog[[</span><span class="s5">'INCOME'</span><span class="s2">, </span><span class="s5">'SOUTH'</span><span class="s1">]]</span><span class="s2">,</span>
                          <span class="s1">family=family2).fit()</span>


<span class="s2">class </span><span class="s1">TestTweedieSpecialLog1(CheckTweedieSpecial):</span>
    <span class="s1">@classmethod</span>
    <span class="s2">def </span><span class="s1">setup_class(cls):</span>
        <span class="s1">cls.data = cpunish.load_pandas()</span>
        <span class="s1">cls.exog = cls.data.exog[[</span><span class="s5">'INCOME'</span><span class="s2">, </span><span class="s5">'SOUTH'</span><span class="s1">]]</span>
        <span class="s1">cls.endog = cls.data.endog</span>
        <span class="s1">family1 = sm.families.Poisson(link=sm.families.links.Log())</span>
        <span class="s1">cls.res1 = sm.GLM(endog=cls.data.endog</span><span class="s2">,</span>
                          <span class="s1">exog=cls.data.exog[[</span><span class="s5">'INCOME'</span><span class="s2">, </span><span class="s5">'SOUTH'</span><span class="s1">]]</span><span class="s2">,</span>
                          <span class="s1">family=family1).fit()</span>
        <span class="s1">family2 = sm.families.Tweedie(link=sm.families.links.Log()</span><span class="s2">,</span>
                                      <span class="s1">var_power=</span><span class="s4">1</span><span class="s1">)</span>
        <span class="s1">cls.res2 = sm.GLM(endog=cls.data.endog</span><span class="s2">,</span>
                          <span class="s1">exog=cls.data.exog[[</span><span class="s5">'INCOME'</span><span class="s2">, </span><span class="s5">'SOUTH'</span><span class="s1">]]</span><span class="s2">,</span>
                          <span class="s1">family=family2).fit()</span>


<span class="s2">class </span><span class="s1">TestTweedieSpecialLog2(CheckTweedieSpecial):</span>
    <span class="s1">@classmethod</span>
    <span class="s2">def </span><span class="s1">setup_class(cls):</span>
        <span class="s1">cls.data = cpunish.load_pandas()</span>
        <span class="s1">cls.exog = cls.data.exog[[</span><span class="s5">'INCOME'</span><span class="s2">, </span><span class="s5">'SOUTH'</span><span class="s1">]]</span>
        <span class="s1">cls.endog = cls.data.endog</span>
        <span class="s1">family1 = sm.families.Gamma(link=sm.families.links.Log())</span>
        <span class="s1">cls.res1 = sm.GLM(endog=cls.data.endog</span><span class="s2">,</span>
                          <span class="s1">exog=cls.data.exog[[</span><span class="s5">'INCOME'</span><span class="s2">, </span><span class="s5">'SOUTH'</span><span class="s1">]]</span><span class="s2">,</span>
                          <span class="s1">family=family1).fit()</span>
        <span class="s1">family2 = sm.families.Tweedie(link=sm.families.links.Log()</span><span class="s2">,</span>
                                      <span class="s1">var_power=</span><span class="s4">2</span><span class="s1">)</span>
        <span class="s1">cls.res2 = sm.GLM(endog=cls.data.endog</span><span class="s2">,</span>
                          <span class="s1">exog=cls.data.exog[[</span><span class="s5">'INCOME'</span><span class="s2">, </span><span class="s5">'SOUTH'</span><span class="s1">]]</span><span class="s2">,</span>
                          <span class="s1">family=family2).fit()</span>


<span class="s2">class </span><span class="s1">TestTweedieSpecialLog3(CheckTweedieSpecial):</span>
    <span class="s1">@classmethod</span>
    <span class="s2">def </span><span class="s1">setup_class(cls):</span>
        <span class="s1">cls.data = cpunish.load_pandas()</span>
        <span class="s1">cls.exog = cls.data.exog[[</span><span class="s5">'INCOME'</span><span class="s2">, </span><span class="s5">'SOUTH'</span><span class="s1">]]</span>
        <span class="s1">cls.endog = cls.data.endog</span>
        <span class="s1">family1 = sm.families.InverseGaussian(link=sm.families.links.Log())</span>
        <span class="s1">cls.res1 = sm.GLM(endog=cls.data.endog</span><span class="s2">,</span>
                          <span class="s1">exog=cls.data.exog[[</span><span class="s5">'INCOME'</span><span class="s2">, </span><span class="s5">'SOUTH'</span><span class="s1">]]</span><span class="s2">,</span>
                          <span class="s1">family=family1).fit()</span>
        <span class="s1">family2 = sm.families.Tweedie(link=sm.families.links.Log()</span><span class="s2">,</span>
                                      <span class="s1">var_power=</span><span class="s4">3</span><span class="s1">)</span>
        <span class="s1">cls.res2 = sm.GLM(endog=cls.data.endog</span><span class="s2">,</span>
                          <span class="s1">exog=cls.data.exog[[</span><span class="s5">'INCOME'</span><span class="s2">, </span><span class="s5">'SOUTH'</span><span class="s1">]]</span><span class="s2">,</span>
                          <span class="s1">family=family2).fit()</span>

<span class="s2">def </span><span class="s1">gen_tweedie(p):</span>

    <span class="s1">np.random.seed(</span><span class="s4">3242</span><span class="s1">)</span>
    <span class="s1">n = </span><span class="s4">500</span>
    <span class="s1">x = np.random.normal(size=(n</span><span class="s2">, </span><span class="s4">4</span><span class="s1">))</span>
    <span class="s1">lpr = np.dot(x</span><span class="s2">, </span><span class="s1">np.r_[</span><span class="s4">1</span><span class="s2">, </span><span class="s1">-</span><span class="s4">1</span><span class="s2">, </span><span class="s4">0</span><span class="s2">, </span><span class="s4">0.5</span><span class="s1">])</span>
    <span class="s1">mu = np.exp(lpr)</span>
    <span class="s1">lam = </span><span class="s4">10 </span><span class="s1">* mu**(</span><span class="s4">2 </span><span class="s1">- p) / (</span><span class="s4">2 </span><span class="s1">- p)</span>
    <span class="s1">alp = (</span><span class="s4">2 </span><span class="s1">- p) / (p - </span><span class="s4">1</span><span class="s1">)</span>
    <span class="s1">bet = </span><span class="s4">10 </span><span class="s1">* mu**(</span><span class="s4">1 </span><span class="s1">- p) / (p - </span><span class="s4">1</span><span class="s1">)</span>

    <span class="s3"># Generate Tweedie values using commpound Poisson distribution</span>
    <span class="s1">y = np.empty(n)</span>
    <span class="s1">N = np.random.poisson(lam)</span>
    <span class="s2">for </span><span class="s1">i </span><span class="s2">in </span><span class="s1">range(n):</span>
        <span class="s1">y[i] = np.random.gamma(alp</span><span class="s2">, </span><span class="s4">1 </span><span class="s1">/ bet[i]</span><span class="s2">, </span><span class="s1">N[i]).sum()</span>

    <span class="s2">return </span><span class="s1">y</span><span class="s2">, </span><span class="s1">x</span>

<span class="s1">@pytest.mark.filterwarnings(</span><span class="s5">&quot;ignore:GLM ridge optimization&quot;</span><span class="s1">)</span>
<span class="s2">def </span><span class="s1">test_tweedie_EQL():</span>
    <span class="s3"># All tests below are regression tests, but the results</span>
    <span class="s3"># are very close to the population values.</span>

    <span class="s1">p = </span><span class="s4">1.5</span>
    <span class="s1">y</span><span class="s2">, </span><span class="s1">x = gen_tweedie(p)</span>

    <span class="s3"># Un-regularized fit using gradients</span>
    <span class="s1">fam = sm.families.Tweedie(var_power=p</span><span class="s2">, </span><span class="s1">eql=</span><span class="s2">True</span><span class="s1">)</span>
    <span class="s1">model1 = sm.GLM(y</span><span class="s2">, </span><span class="s1">x</span><span class="s2">, </span><span class="s1">family=fam)</span>
    <span class="s1">result1 = model1.fit(method=</span><span class="s5">&quot;newton&quot;</span><span class="s1">)</span>
    <span class="s1">assert_allclose(result1.params</span><span class="s2">,</span>
       <span class="s1">np.array([</span><span class="s4">1.00350497</span><span class="s2">, </span><span class="s1">-</span><span class="s4">0.99656954</span><span class="s2">, </span><span class="s4">0.00802702</span><span class="s2">, </span><span class="s4">0.50713209</span><span class="s1">])</span><span class="s2">,</span>
       <span class="s1">rtol=</span><span class="s4">1e-5</span><span class="s2">, </span><span class="s1">atol=</span><span class="s4">1e-5</span><span class="s1">)</span>

    <span class="s3"># Un-regularized fit using IRLS</span>
    <span class="s1">model1x = sm.GLM(y</span><span class="s2">, </span><span class="s1">x</span><span class="s2">, </span><span class="s1">family=fam)</span>
    <span class="s1">result1x = model1x.fit(method=</span><span class="s5">&quot;irls&quot;</span><span class="s1">)</span>
    <span class="s1">assert_allclose(result1.params</span><span class="s2">, </span><span class="s1">result1x.params)</span>
    <span class="s1">assert_allclose(result1.bse</span><span class="s2">, </span><span class="s1">result1x.bse</span><span class="s2">, </span><span class="s1">rtol=</span><span class="s4">1e-2</span><span class="s1">)</span>

    <span class="s3"># Lasso fit using coordinate-wise descent</span>
    <span class="s3"># TODO: The search gets trapped in an infinite oscillation, so use</span>
    <span class="s3"># a slack convergence tolerance.</span>
    <span class="s1">model2 = sm.GLM(y</span><span class="s2">, </span><span class="s1">x</span><span class="s2">, </span><span class="s1">family=fam)</span>
    <span class="s1">result2 = model2.fit_regularized(L1_wt=</span><span class="s4">1</span><span class="s2">, </span><span class="s1">alpha=</span><span class="s4">0.07</span><span class="s2">, </span><span class="s1">maxiter=</span><span class="s4">200</span><span class="s2">,</span>
                   <span class="s1">cnvrg_tol=</span><span class="s4">0.01</span><span class="s1">)</span>

    <span class="s1">rtol</span><span class="s2">, </span><span class="s1">atol = </span><span class="s4">1e-2</span><span class="s2">, </span><span class="s4">1e-4</span>
    <span class="s1">assert_allclose(result2.params</span><span class="s2">,</span>
        <span class="s1">np.array([</span><span class="s4">0.976831</span><span class="s2">, </span><span class="s1">-</span><span class="s4">0.952854</span><span class="s2">, </span><span class="s4">0.</span><span class="s2">, </span><span class="s4">0.470171</span><span class="s1">])</span><span class="s2">,</span>
        <span class="s1">rtol=rtol</span><span class="s2">, </span><span class="s1">atol=atol)</span>

    <span class="s3"># Series of ridge fits using gradients</span>
    <span class="s1">ev = (np.array([</span><span class="s4">1.001778</span><span class="s2">, </span><span class="s1">-</span><span class="s4">0.99388</span><span class="s2">, </span><span class="s4">0.00797</span><span class="s2">, </span><span class="s4">0.506183</span><span class="s1">])</span><span class="s2">,</span>
          <span class="s1">np.array([</span><span class="s4">0.98586638</span><span class="s2">, </span><span class="s1">-</span><span class="s4">0.96953481</span><span class="s2">, </span><span class="s4">0.00749983</span><span class="s2">, </span><span class="s4">0.4975267</span><span class="s1">])</span><span class="s2">,</span>
          <span class="s1">np.array([</span><span class="s4">0.206429</span><span class="s2">, </span><span class="s1">-</span><span class="s4">0.164547</span><span class="s2">, </span><span class="s4">0.000235</span><span class="s2">, </span><span class="s4">0.102489</span><span class="s1">]))</span>
    <span class="s2">for </span><span class="s1">j</span><span class="s2">, </span><span class="s1">alpha </span><span class="s2">in </span><span class="s1">enumerate([</span><span class="s4">0.05</span><span class="s2">, </span><span class="s4">0.5</span><span class="s2">, </span><span class="s4">0.7</span><span class="s1">]):</span>
        <span class="s1">model3 = sm.GLM(y</span><span class="s2">, </span><span class="s1">x</span><span class="s2">, </span><span class="s1">family=fam)</span>
        <span class="s1">result3 = model3.fit_regularized(L1_wt=</span><span class="s4">0</span><span class="s2">, </span><span class="s1">alpha=alpha)</span>
        <span class="s1">assert_allclose(result3.params</span><span class="s2">, </span><span class="s1">ev[j]</span><span class="s2">, </span><span class="s1">rtol=rtol</span><span class="s2">, </span><span class="s1">atol=atol)</span>
        <span class="s1">result4 = model3.fit_regularized(L1_wt=</span><span class="s4">0</span><span class="s2">, </span><span class="s1">alpha=alpha * np.ones(x.shape[</span><span class="s4">1</span><span class="s1">]))</span>
        <span class="s1">assert_allclose(result4.params</span><span class="s2">, </span><span class="s1">result3.params</span><span class="s2">, </span><span class="s1">rtol=rtol</span><span class="s2">, </span><span class="s1">atol=atol)</span>
        <span class="s1">alpha = alpha * np.ones(x.shape[</span><span class="s4">1</span><span class="s1">])</span>
        <span class="s1">alpha[</span><span class="s4">0</span><span class="s1">] = </span><span class="s4">0</span>
        <span class="s1">result5 = model3.fit_regularized(L1_wt=</span><span class="s4">0</span><span class="s2">, </span><span class="s1">alpha=alpha)</span>
        <span class="s2">assert not </span><span class="s1">np.allclose(result5.params</span><span class="s2">, </span><span class="s1">result4.params)</span>

<span class="s2">def </span><span class="s1">test_tweedie_elastic_net():</span>
    <span class="s3"># Check that the coefficients vanish one-by-one</span>
    <span class="s3"># when using the elastic net.</span>

    <span class="s1">p = </span><span class="s4">1.5 </span><span class="s3"># Tweedie variance exponent</span>
    <span class="s1">y</span><span class="s2">, </span><span class="s1">x = gen_tweedie(p)</span>

    <span class="s3"># Un-regularized fit using gradients</span>
    <span class="s1">fam = sm.families.Tweedie(var_power=p</span><span class="s2">, </span><span class="s1">eql=</span><span class="s2">True</span><span class="s1">)</span>
    <span class="s1">model1 = sm.GLM(y</span><span class="s2">, </span><span class="s1">x</span><span class="s2">, </span><span class="s1">family=fam)</span>

    <span class="s1">nnz = []</span>
    <span class="s2">for </span><span class="s1">alpha </span><span class="s2">in </span><span class="s1">np.linspace(</span><span class="s4">0</span><span class="s2">, </span><span class="s4">10</span><span class="s2">, </span><span class="s4">20</span><span class="s1">):</span>
        <span class="s1">result1 = model1.fit_regularized(L1_wt=</span><span class="s4">0.5</span><span class="s2">, </span><span class="s1">alpha=alpha)</span>
        <span class="s1">nnz.append((np.abs(result1.params) &gt; </span><span class="s4">0</span><span class="s1">).sum())</span>
    <span class="s1">nnz = np.unique(nnz)</span>
    <span class="s2">assert </span><span class="s1">len(nnz) == </span><span class="s4">5</span>


<span class="s2">def </span><span class="s1">test_tweedie_EQL_poisson_limit():</span>
    <span class="s3"># Test the limiting Poisson case of the Nelder/Pregibon/Tweedie</span>
    <span class="s3"># EQL.</span>

    <span class="s1">np.random.seed(</span><span class="s4">3242</span><span class="s1">)</span>
    <span class="s1">n = </span><span class="s4">500</span>

    <span class="s1">x = np.random.normal(size=(n</span><span class="s2">, </span><span class="s4">3</span><span class="s1">))</span>
    <span class="s1">x[:</span><span class="s2">, </span><span class="s4">0</span><span class="s1">] = </span><span class="s4">1</span>
    <span class="s1">lpr = </span><span class="s4">4 </span><span class="s1">+ x[:</span><span class="s2">, </span><span class="s4">1</span><span class="s1">:].sum(</span><span class="s4">1</span><span class="s1">)</span>
    <span class="s1">mn = np.exp(lpr)</span>
    <span class="s1">y = np.random.poisson(mn)</span>

    <span class="s2">for </span><span class="s1">scale </span><span class="s2">in </span><span class="s4">1.0</span><span class="s2">, </span><span class="s5">'x2'</span><span class="s2">, </span><span class="s5">'dev'</span><span class="s1">:</span>

        <span class="s3"># Un-regularized fit using gradients not IRLS</span>
        <span class="s1">fam = sm.families.Tweedie(var_power=</span><span class="s4">1</span><span class="s2">, </span><span class="s1">eql=</span><span class="s2">True</span><span class="s1">)</span>
        <span class="s1">model1 = sm.GLM(y</span><span class="s2">, </span><span class="s1">x</span><span class="s2">, </span><span class="s1">family=fam)</span>
        <span class="s1">result1 = model1.fit(method=</span><span class="s5">&quot;newton&quot;</span><span class="s2">, </span><span class="s1">scale=scale)</span>

        <span class="s3"># Poisson GLM</span>
        <span class="s1">model2 = sm.GLM(y</span><span class="s2">, </span><span class="s1">x</span><span class="s2">, </span><span class="s1">family=sm.families.Poisson())</span>
        <span class="s1">result2 = model2.fit(method=</span><span class="s5">&quot;newton&quot;</span><span class="s2">, </span><span class="s1">scale=scale)</span>

        <span class="s1">assert_allclose(result1.params</span><span class="s2">, </span><span class="s1">result2.params</span><span class="s2">, </span><span class="s1">atol=</span><span class="s4">1e-6</span><span class="s2">, </span><span class="s1">rtol=</span><span class="s4">1e-6</span><span class="s1">)</span>
        <span class="s1">assert_allclose(result1.bse</span><span class="s2">, </span><span class="s1">result2.bse</span><span class="s2">, </span><span class="s4">1e-6</span><span class="s2">, </span><span class="s4">1e-6</span><span class="s1">)</span>


<span class="s2">def </span><span class="s1">test_tweedie_EQL_upper_limit():</span>
    <span class="s3"># Test the limiting case of the Nelder/Pregibon/Tweedie</span>
    <span class="s3"># EQL with var = mean^2.  These are tests against population</span>
    <span class="s3"># values so accuracy is not high.</span>

    <span class="s1">np.random.seed(</span><span class="s4">3242</span><span class="s1">)</span>
    <span class="s1">n = </span><span class="s4">500</span>

    <span class="s1">x = np.random.normal(size=(n</span><span class="s2">, </span><span class="s4">3</span><span class="s1">))</span>
    <span class="s1">x[:</span><span class="s2">, </span><span class="s4">0</span><span class="s1">] = </span><span class="s4">1</span>
    <span class="s1">lpr = </span><span class="s4">4 </span><span class="s1">+ x[:</span><span class="s2">, </span><span class="s4">1</span><span class="s1">:].sum(</span><span class="s4">1</span><span class="s1">)</span>
    <span class="s1">mn = np.exp(lpr)</span>
    <span class="s1">y = np.random.poisson(mn)</span>

    <span class="s2">for </span><span class="s1">scale </span><span class="s2">in </span><span class="s5">'x2'</span><span class="s2">, </span><span class="s5">'dev'</span><span class="s2">, </span><span class="s4">1.0</span><span class="s1">:</span>

        <span class="s3"># Un-regularized fit using gradients not IRLS</span>
        <span class="s1">fam = sm.families.Tweedie(var_power=</span><span class="s4">2</span><span class="s2">, </span><span class="s1">eql=</span><span class="s2">True</span><span class="s1">)</span>
        <span class="s1">model1 = sm.GLM(y</span><span class="s2">, </span><span class="s1">x</span><span class="s2">, </span><span class="s1">family=fam)</span>
        <span class="s1">result1 = model1.fit(method=</span><span class="s5">&quot;newton&quot;</span><span class="s2">, </span><span class="s1">scale=scale)</span>
        <span class="s1">assert_allclose(result1.params</span><span class="s2">, </span><span class="s1">np.r_[</span><span class="s4">4</span><span class="s2">, </span><span class="s4">1</span><span class="s2">, </span><span class="s4">1</span><span class="s1">]</span><span class="s2">, </span><span class="s1">atol=</span><span class="s4">1e-3</span><span class="s2">, </span><span class="s1">rtol=</span><span class="s4">1e-1</span><span class="s1">)</span>


<span class="s2">def </span><span class="s1">testTweediePowerEstimate():</span>
    <span class="s3"># Test the Pearson estimate of the Tweedie variance and scale parameters.</span>
    <span class="s3">#</span>
    <span class="s3"># Ideally, this would match the following R code, but I cannot make it work...</span>
    <span class="s3">#</span>
    <span class="s3"># setwd('c:/workspace')</span>
    <span class="s3"># data &lt;- read.csv('cpunish.csv', sep=&quot;,&quot;)</span>
    <span class="s3">#</span>
    <span class="s3"># library(tweedie)</span>
    <span class="s3">#</span>
    <span class="s3"># y &lt;- c(1.00113835e+05,   6.89668315e+03,   6.15726842e+03,</span>
    <span class="s3">#        1.41718806e+03,   5.11776456e+02,   2.55369154e+02,</span>
    <span class="s3">#        1.07147443e+01,   3.56874698e+00,   4.06797842e-02,</span>
    <span class="s3">#        7.06996731e-05,   2.10165106e-07,   4.34276938e-08,</span>
    <span class="s3">#        1.56354040e-09,   0.00000000e+00,   0.00000000e+00,</span>
    <span class="s3">#        0.00000000e+00,   0.00000000e+00)</span>
    <span class="s3">#</span>
    <span class="s3"># data$NewY &lt;- y</span>
    <span class="s3">#</span>
    <span class="s3"># out &lt;- tweedie.profile( NewY ~ INCOME + SOUTH - 1,</span>
    <span class="s3">#                         p.vec=c(1.1, 1.2, 1.3, 1.4, 1.5, 1.6, 1.7, 1.8,</span>
    <span class="s3">#                                 1.9), link.power=0,</span>
    <span class="s3">#                         data=data,do.plot = TRUE)</span>
    <span class="s1">data = cpunish.load_pandas()</span>
    <span class="s1">y = [</span><span class="s4">1.00113835e+05</span><span class="s2">,   </span><span class="s4">6.89668315e+03</span><span class="s2">,   </span><span class="s4">6.15726842e+03</span><span class="s2">,</span>
         <span class="s4">1.41718806e+03</span><span class="s2">,   </span><span class="s4">5.11776456e+02</span><span class="s2">,   </span><span class="s4">2.55369154e+02</span><span class="s2">,</span>
         <span class="s4">1.07147443e+01</span><span class="s2">,   </span><span class="s4">3.56874698e+00</span><span class="s2">,   </span><span class="s4">4.06797842e-02</span><span class="s2">,</span>
         <span class="s4">7.06996731e-05</span><span class="s2">,   </span><span class="s4">2.10165106e-07</span><span class="s2">,   </span><span class="s4">4.34276938e-08</span><span class="s2">,</span>
         <span class="s4">1.56354040e-09</span><span class="s2">,   </span><span class="s4">0.00000000e+00</span><span class="s2">,   </span><span class="s4">0.00000000e+00</span><span class="s2">,</span>
         <span class="s4">0.00000000e+00</span><span class="s2">,   </span><span class="s4">0.00000000e+00</span><span class="s1">]</span>
    <span class="s1">model1 = sm.GLM(y</span><span class="s2">, </span><span class="s1">data.exog[[</span><span class="s5">'INCOME'</span><span class="s2">, </span><span class="s5">'SOUTH'</span><span class="s1">]]</span><span class="s2">,</span>
                    <span class="s1">family=sm.families.Tweedie(link=sm.families.links.Log()</span><span class="s2">,</span>
                                               <span class="s1">var_power=</span><span class="s4">1.5</span><span class="s1">))</span>
    <span class="s1">res1 = model1.fit()</span>
    <span class="s1">model2 = sm.GLM((y - res1.mu) ** </span><span class="s4">2</span><span class="s2">,</span>
                    <span class="s1">np.column_stack((np.ones(len(res1.mu))</span><span class="s2">, </span><span class="s1">np.log(res1.mu)))</span><span class="s2">,</span>
                    <span class="s1">family=sm.families.Gamma(sm.families.links.Log()))</span>
    <span class="s1">res2 = model2.fit()</span>
    <span class="s3"># Sample may be too small for this...</span>
    <span class="s3"># assert_allclose(res1.scale, np.exp(res2.params[0]), rtol=0.25)</span>
    <span class="s1">p = model1.estimate_tweedie_power(res1.mu)</span>
    <span class="s1">assert_allclose(p</span><span class="s2">, </span><span class="s1">res2.params[</span><span class="s4">1</span><span class="s1">]</span><span class="s2">, </span><span class="s1">rtol=</span><span class="s4">0.25</span><span class="s1">)</span>

<span class="s2">def </span><span class="s1">test_glm_lasso_6431():</span>

    <span class="s3"># Based on issue #6431</span>
    <span class="s3"># Fails with newton-cg as optimizer</span>
    <span class="s1">np.random.seed(</span><span class="s4">123</span><span class="s1">)</span>

    <span class="s2">from </span><span class="s1">statsmodels.regression.linear_model </span><span class="s2">import </span><span class="s1">OLS</span>

    <span class="s1">n = </span><span class="s4">50</span>
    <span class="s1">x = np.ones((n</span><span class="s2">, </span><span class="s4">2</span><span class="s1">))</span>
    <span class="s1">x[:</span><span class="s2">, </span><span class="s4">1</span><span class="s1">] = np.arange(</span><span class="s4">0</span><span class="s2">, </span><span class="s1">n)</span>
    <span class="s1">y = </span><span class="s4">1000 </span><span class="s1">+ x[:</span><span class="s2">, </span><span class="s4">1</span><span class="s1">] + np.random.normal(</span><span class="s4">0</span><span class="s2">, </span><span class="s4">1</span><span class="s2">, </span><span class="s1">n)</span>

    <span class="s1">params = np.r_[</span><span class="s4">999.82244338</span><span class="s2">, </span><span class="s4">1.0077889</span><span class="s1">]</span>

    <span class="s2">for </span><span class="s1">method </span><span class="s2">in </span><span class="s5">&quot;bfgs&quot;</span><span class="s2">, None</span><span class="s1">:</span>
        <span class="s2">for </span><span class="s1">fun </span><span class="s2">in </span><span class="s1">[OLS</span><span class="s2">, </span><span class="s1">GLM]:</span>

            <span class="s3"># Changing L1_wtValue from 0 to 1e-9 changes</span>
            <span class="s3"># the algorithm from scipy gradient optimization</span>
            <span class="s3"># to statsmodels coordinate descent</span>
            <span class="s2">for </span><span class="s1">L1_wtValue </span><span class="s2">in </span><span class="s1">[</span><span class="s4">0</span><span class="s2">, </span><span class="s4">1e-9</span><span class="s1">]:</span>
                <span class="s1">model = fun(y</span><span class="s2">, </span><span class="s1">x)</span>
                <span class="s2">if </span><span class="s1">fun == OLS:</span>
                    <span class="s1">fit = model.fit_regularized(alpha=</span><span class="s4">0</span><span class="s2">, </span><span class="s1">L1_wt=L1_wtValue)</span>
                <span class="s2">else</span><span class="s1">:</span>
                    <span class="s1">fit = model._fit_ridge(alpha=</span><span class="s4">0</span><span class="s2">, </span><span class="s1">start_params=</span><span class="s2">None, </span><span class="s1">method=method)</span>
                <span class="s1">assert_allclose(params</span><span class="s2">, </span><span class="s1">fit.params</span><span class="s2">, </span><span class="s1">atol=</span><span class="s4">1e-6</span><span class="s2">, </span><span class="s1">rtol=</span><span class="s4">1e-6</span><span class="s1">)</span>

<span class="s2">class </span><span class="s1">TestRegularized:</span>

    <span class="s2">def </span><span class="s1">test_regularized(self):</span>

        <span class="s2">import </span><span class="s1">os</span>

        <span class="s2">from </span><span class="s1">.results </span><span class="s2">import </span><span class="s1">glmnet_r_results</span>

        <span class="s2">for </span><span class="s1">dtype </span><span class="s2">in </span><span class="s5">&quot;binomial&quot;</span><span class="s2">, </span><span class="s5">&quot;poisson&quot;</span><span class="s1">:</span>

            <span class="s1">cur_dir = os.path.dirname(os.path.abspath(__file__))</span>
            <span class="s1">data = np.loadtxt(os.path.join(cur_dir</span><span class="s2">, </span><span class="s5">&quot;results&quot;</span><span class="s2">, </span><span class="s5">&quot;enet_%s.csv&quot; </span><span class="s1">% dtype)</span><span class="s2">,</span>
                              <span class="s1">delimiter=</span><span class="s5">&quot;,&quot;</span><span class="s1">)</span>

            <span class="s1">endog = data[:</span><span class="s2">, </span><span class="s4">0</span><span class="s1">]</span>
            <span class="s1">exog = data[:</span><span class="s2">, </span><span class="s4">1</span><span class="s1">:]</span>

            <span class="s1">fam = {</span><span class="s5">&quot;binomial&quot; </span><span class="s1">: sm.families.Binomial</span><span class="s2">,</span>
                   <span class="s5">&quot;poisson&quot; </span><span class="s1">: sm.families.Poisson}[dtype]</span>

            <span class="s2">for </span><span class="s1">j </span><span class="s2">in </span><span class="s1">range(</span><span class="s4">9</span><span class="s1">):</span>

                <span class="s1">vn = </span><span class="s5">&quot;rslt_%s_%d&quot; </span><span class="s1">% (dtype</span><span class="s2">, </span><span class="s1">j)</span>
                <span class="s1">r_result = getattr(glmnet_r_results</span><span class="s2">, </span><span class="s1">vn)</span>
                <span class="s1">L1_wt = r_result[</span><span class="s4">0</span><span class="s1">]</span>
                <span class="s1">alpha = r_result[</span><span class="s4">1</span><span class="s1">]</span>
                <span class="s1">params = r_result[</span><span class="s4">2</span><span class="s1">:]</span>

                <span class="s1">model = GLM(endog</span><span class="s2">, </span><span class="s1">exog</span><span class="s2">, </span><span class="s1">family=fam())</span>
                <span class="s1">sm_result = model.fit_regularized(L1_wt=L1_wt</span><span class="s2">, </span><span class="s1">alpha=alpha)</span>

                <span class="s3"># Agreement is OK, see below for further check</span>
                <span class="s1">assert_allclose(params</span><span class="s2">, </span><span class="s1">sm_result.params</span><span class="s2">, </span><span class="s1">atol=</span><span class="s4">1e-2</span><span class="s2">, </span><span class="s1">rtol=</span><span class="s4">0.3</span><span class="s1">)</span>

                <span class="s3"># The penalized log-likelihood that we are maximizing.</span>
                <span class="s2">def </span><span class="s1">plf(params):</span>
                    <span class="s1">llf = model.loglike(params) / len(endog)</span>
                    <span class="s1">llf = llf - alpha * ((</span><span class="s4">1 </span><span class="s1">- L1_wt)*np.sum(params**</span><span class="s4">2</span><span class="s1">) / </span><span class="s4">2 </span><span class="s1">+ L1_wt*np.sum(np.abs(params)))</span>
                    <span class="s2">return </span><span class="s1">llf</span>

                <span class="s3"># Confirm that we are doing better than glmnet.</span>
                <span class="s1">llf_r = plf(params)</span>
                <span class="s1">llf_sm = plf(sm_result.params)</span>
                <span class="s1">assert_equal(np.sign(llf_sm - llf_r)</span><span class="s2">, </span><span class="s4">1</span><span class="s1">)</span>


<span class="s2">class </span><span class="s1">TestConvergence:</span>
    <span class="s1">@classmethod</span>
    <span class="s2">def </span><span class="s1">setup_class(cls):</span>
        <span class="s0">''' 
        Test Binomial family with canonical logit link using star98 dataset. 
        '''</span>
        <span class="s2">from </span><span class="s1">statsmodels.datasets.star98 </span><span class="s2">import </span><span class="s1">load</span>
        <span class="s1">data = load()</span>
        <span class="s1">data.exog = add_constant(data.exog</span><span class="s2">, </span><span class="s1">prepend=</span><span class="s2">False</span><span class="s1">)</span>
        <span class="s1">cls.model = GLM(data.endog</span><span class="s2">, </span><span class="s1">data.exog</span><span class="s2">,</span>
                         <span class="s1">family=sm.families.Binomial())</span>

    <span class="s2">def </span><span class="s1">_when_converged(self</span><span class="s2">, </span><span class="s1">atol=</span><span class="s4">1e-8</span><span class="s2">, </span><span class="s1">rtol=</span><span class="s4">0</span><span class="s2">, </span><span class="s1">tol_criterion=</span><span class="s5">'deviance'</span><span class="s1">):</span>
        <span class="s2">for </span><span class="s1">i</span><span class="s2">, </span><span class="s1">dev </span><span class="s2">in </span><span class="s1">enumerate(self.res.fit_history[tol_criterion]):</span>
            <span class="s1">orig = self.res.fit_history[tol_criterion][i]</span>
            <span class="s1">new = self.res.fit_history[tol_criterion][i + </span><span class="s4">1</span><span class="s1">]</span>
            <span class="s2">if </span><span class="s1">np.allclose(orig</span><span class="s2">, </span><span class="s1">new</span><span class="s2">, </span><span class="s1">atol=atol</span><span class="s2">, </span><span class="s1">rtol=rtol):</span>
                <span class="s2">return </span><span class="s1">i</span>
        <span class="s2">raise </span><span class="s1">ValueError(</span><span class="s5">'CONVERGENCE CHECK: It seems this doens</span><span class="s2">\'</span><span class="s5">t converge!'</span><span class="s1">)</span>

    <span class="s2">def </span><span class="s1">test_convergence_atol_only(self):</span>
        <span class="s1">atol = </span><span class="s4">1e-8</span>
        <span class="s1">rtol = </span><span class="s4">0</span>
        <span class="s1">self.res = self.model.fit(atol=atol</span><span class="s2">, </span><span class="s1">rtol=rtol)</span>
        <span class="s1">expected_iterations = self._when_converged(atol=atol</span><span class="s2">, </span><span class="s1">rtol=rtol)</span>
        <span class="s1">actual_iterations = self.res.fit_history[</span><span class="s5">'iteration'</span><span class="s1">]</span>
        <span class="s3"># Note the first value is the list is np.inf. The second value</span>
        <span class="s3"># is the initial guess based off of start_params or the</span>
        <span class="s3"># estimate thereof. The third value (index = 2) is the actual &quot;first</span>
        <span class="s3"># iteration&quot;</span>
        <span class="s1">assert_equal(expected_iterations</span><span class="s2">, </span><span class="s1">actual_iterations)</span>
        <span class="s1">assert_equal(len(self.res.fit_history[</span><span class="s5">'deviance'</span><span class="s1">]) - </span><span class="s4">2</span><span class="s2">,</span>
                     <span class="s1">actual_iterations)</span>

    <span class="s2">def </span><span class="s1">test_convergence_rtol_only(self):</span>
        <span class="s1">atol = </span><span class="s4">0</span>
        <span class="s1">rtol = </span><span class="s4">1e-8</span>
        <span class="s1">self.res = self.model.fit(atol=atol</span><span class="s2">, </span><span class="s1">rtol=rtol)</span>
        <span class="s1">expected_iterations = self._when_converged(atol=atol</span><span class="s2">, </span><span class="s1">rtol=rtol)</span>
        <span class="s1">actual_iterations = self.res.fit_history[</span><span class="s5">'iteration'</span><span class="s1">]</span>
        <span class="s3"># Note the first value is the list is np.inf. The second value</span>
        <span class="s3"># is the initial guess based off of start_params or the</span>
        <span class="s3"># estimate thereof. The third value (index = 2) is the actual &quot;first</span>
        <span class="s3"># iteration&quot;</span>
        <span class="s1">assert_equal(expected_iterations</span><span class="s2">, </span><span class="s1">actual_iterations)</span>
        <span class="s1">assert_equal(len(self.res.fit_history[</span><span class="s5">'deviance'</span><span class="s1">]) - </span><span class="s4">2</span><span class="s2">,</span>
                     <span class="s1">actual_iterations)</span>

    <span class="s2">def </span><span class="s1">test_convergence_atol_rtol(self):</span>
        <span class="s1">atol = </span><span class="s4">1e-8</span>
        <span class="s1">rtol = </span><span class="s4">1e-8</span>
        <span class="s1">self.res = self.model.fit(atol=atol</span><span class="s2">, </span><span class="s1">rtol=rtol)</span>
        <span class="s1">expected_iterations = self._when_converged(atol=atol</span><span class="s2">, </span><span class="s1">rtol=rtol)</span>
        <span class="s1">actual_iterations = self.res.fit_history[</span><span class="s5">'iteration'</span><span class="s1">]</span>
        <span class="s3"># Note the first value is the list is np.inf. The second value</span>
        <span class="s3"># is the initial guess based off of start_params or the</span>
        <span class="s3"># estimate thereof. The third value (index = 2) is the actual &quot;first</span>
        <span class="s3"># iteration&quot;</span>
        <span class="s1">assert_equal(expected_iterations</span><span class="s2">, </span><span class="s1">actual_iterations)</span>
        <span class="s1">assert_equal(len(self.res.fit_history[</span><span class="s5">'deviance'</span><span class="s1">]) - </span><span class="s4">2</span><span class="s2">,</span>
                     <span class="s1">actual_iterations)</span>

    <span class="s2">def </span><span class="s1">test_convergence_atol_only_params(self):</span>
        <span class="s1">atol = </span><span class="s4">1e-8</span>
        <span class="s1">rtol = </span><span class="s4">0</span>
        <span class="s1">self.res = self.model.fit(atol=atol</span><span class="s2">, </span><span class="s1">rtol=rtol</span><span class="s2">, </span><span class="s1">tol_criterion=</span><span class="s5">'params'</span><span class="s1">)</span>
        <span class="s1">expected_iterations = self._when_converged(atol=atol</span><span class="s2">, </span><span class="s1">rtol=rtol</span><span class="s2">,</span>
                                                   <span class="s1">tol_criterion=</span><span class="s5">'params'</span><span class="s1">)</span>
        <span class="s1">actual_iterations = self.res.fit_history[</span><span class="s5">'iteration'</span><span class="s1">]</span>
        <span class="s3"># Note the first value is the list is np.inf. The second value</span>
        <span class="s3"># is the initial guess based off of start_params or the</span>
        <span class="s3"># estimate thereof. The third value (index = 2) is the actual &quot;first</span>
        <span class="s3"># iteration&quot;</span>
        <span class="s1">assert_equal(expected_iterations</span><span class="s2">, </span><span class="s1">actual_iterations)</span>
        <span class="s1">assert_equal(len(self.res.fit_history[</span><span class="s5">'deviance'</span><span class="s1">]) - </span><span class="s4">2</span><span class="s2">,</span>
                     <span class="s1">actual_iterations)</span>

    <span class="s2">def </span><span class="s1">test_convergence_rtol_only_params(self):</span>
        <span class="s1">atol = </span><span class="s4">0</span>
        <span class="s1">rtol = </span><span class="s4">1e-8</span>
        <span class="s1">self.res = self.model.fit(atol=atol</span><span class="s2">, </span><span class="s1">rtol=rtol</span><span class="s2">, </span><span class="s1">tol_criterion=</span><span class="s5">'params'</span><span class="s1">)</span>
        <span class="s1">expected_iterations = self._when_converged(atol=atol</span><span class="s2">, </span><span class="s1">rtol=rtol</span><span class="s2">,</span>
                                                   <span class="s1">tol_criterion=</span><span class="s5">'params'</span><span class="s1">)</span>
        <span class="s1">actual_iterations = self.res.fit_history[</span><span class="s5">'iteration'</span><span class="s1">]</span>
        <span class="s3"># Note the first value is the list is np.inf. The second value</span>
        <span class="s3"># is the initial guess based off of start_params or the</span>
        <span class="s3"># estimate thereof. The third value (index = 2) is the actual &quot;first</span>
        <span class="s3"># iteration&quot;</span>
        <span class="s1">assert_equal(expected_iterations</span><span class="s2">, </span><span class="s1">actual_iterations)</span>
        <span class="s1">assert_equal(len(self.res.fit_history[</span><span class="s5">'deviance'</span><span class="s1">]) - </span><span class="s4">2</span><span class="s2">,</span>
                     <span class="s1">actual_iterations)</span>

    <span class="s2">def </span><span class="s1">test_convergence_atol_rtol_params(self):</span>
        <span class="s1">atol = </span><span class="s4">1e-8</span>
        <span class="s1">rtol = </span><span class="s4">1e-8</span>
        <span class="s1">self.res = self.model.fit(atol=atol</span><span class="s2">, </span><span class="s1">rtol=rtol</span><span class="s2">, </span><span class="s1">tol_criterion=</span><span class="s5">'params'</span><span class="s1">)</span>
        <span class="s1">expected_iterations = self._when_converged(atol=atol</span><span class="s2">, </span><span class="s1">rtol=rtol</span><span class="s2">,</span>
                                                   <span class="s1">tol_criterion=</span><span class="s5">'params'</span><span class="s1">)</span>
        <span class="s1">actual_iterations = self.res.fit_history[</span><span class="s5">'iteration'</span><span class="s1">]</span>
        <span class="s3"># Note the first value is the list is np.inf. The second value</span>
        <span class="s3"># is the initial guess based off of start_params or the</span>
        <span class="s3"># estimate thereof. The third value (index = 2) is the actual &quot;first</span>
        <span class="s3"># iteration&quot;</span>
        <span class="s1">assert_equal(expected_iterations</span><span class="s2">, </span><span class="s1">actual_iterations)</span>
        <span class="s1">assert_equal(len(self.res.fit_history[</span><span class="s5">'deviance'</span><span class="s1">]) - </span><span class="s4">2</span><span class="s2">,</span>
                     <span class="s1">actual_iterations)</span>


<span class="s2">def </span><span class="s1">test_poisson_deviance():</span>
    <span class="s3"># see #3355 missing term in deviance if resid_response.sum() != 0</span>
    <span class="s1">np.random.seed(</span><span class="s4">123987</span><span class="s1">)</span>
    <span class="s1">nobs</span><span class="s2">, </span><span class="s1">k_vars = </span><span class="s4">50</span><span class="s2">, </span><span class="s4">3</span><span class="s1">-</span><span class="s4">1</span>
    <span class="s1">x = sm.add_constant(np.random.randn(nobs</span><span class="s2">, </span><span class="s1">k_vars))</span>

    <span class="s1">mu_true = np.exp(x.sum(</span><span class="s4">1</span><span class="s1">))</span>
    <span class="s1">y = np.random.poisson(mu_true</span><span class="s2">, </span><span class="s1">size=nobs)</span>

    <span class="s1">mod = sm.GLM(y</span><span class="s2">, </span><span class="s1">x[:</span><span class="s2">, </span><span class="s1">:]</span><span class="s2">, </span><span class="s1">family=sm.genmod.families.Poisson())</span>
    <span class="s1">res = mod.fit()</span>

    <span class="s1">d_i = res.resid_deviance</span>
    <span class="s1">d = res.deviance</span>
    <span class="s1">lr = (mod.family.loglike(y</span><span class="s2">, </span><span class="s1">y+</span><span class="s4">1e-20</span><span class="s1">) -</span>
          <span class="s1">mod.family.loglike(y</span><span class="s2">, </span><span class="s1">res.fittedvalues)) * </span><span class="s4">2</span>

    <span class="s1">assert_allclose(d</span><span class="s2">, </span><span class="s1">(d_i**</span><span class="s4">2</span><span class="s1">).sum()</span><span class="s2">, </span><span class="s1">rtol=</span><span class="s4">1e-12</span><span class="s1">)</span>
    <span class="s1">assert_allclose(d</span><span class="s2">, </span><span class="s1">lr</span><span class="s2">, </span><span class="s1">rtol=</span><span class="s4">1e-12</span><span class="s1">)</span>

    <span class="s3"># case without constant, resid_response.sum() != 0</span>
    <span class="s1">mod_nc = sm.GLM(y</span><span class="s2">, </span><span class="s1">x[:</span><span class="s2">, </span><span class="s4">1</span><span class="s1">:]</span><span class="s2">, </span><span class="s1">family=sm.genmod.families.Poisson())</span>
    <span class="s1">res_nc = mod_nc.fit()</span>

    <span class="s1">d_i = res_nc.resid_deviance</span>
    <span class="s1">d = res_nc.deviance</span>
    <span class="s1">lr = (mod.family.loglike(y</span><span class="s2">, </span><span class="s1">y+</span><span class="s4">1e-20</span><span class="s1">) -</span>
          <span class="s1">mod.family.loglike(y</span><span class="s2">, </span><span class="s1">res_nc.fittedvalues)) * </span><span class="s4">2</span>

    <span class="s1">assert_allclose(d</span><span class="s2">, </span><span class="s1">(d_i**</span><span class="s4">2</span><span class="s1">).sum()</span><span class="s2">, </span><span class="s1">rtol=</span><span class="s4">1e-12</span><span class="s1">)</span>
    <span class="s1">assert_allclose(d</span><span class="s2">, </span><span class="s1">lr</span><span class="s2">, </span><span class="s1">rtol=</span><span class="s4">1e-12</span><span class="s1">)</span>


<span class="s2">def </span><span class="s1">test_non_invertible_hessian_fails_summary():</span>
    <span class="s3"># Test when the hessian fails the summary is still available.</span>
    <span class="s1">data = cpunish.load_pandas()</span>

    <span class="s1">data.endog[:] = </span><span class="s4">1</span>
    <span class="s2">with </span><span class="s1">warnings.catch_warnings():</span>
        <span class="s3"># we filter DomainWarning, the convergence problems</span>
        <span class="s3"># and warnings in summary</span>
        <span class="s1">warnings.simplefilter(</span><span class="s5">&quot;ignore&quot;</span><span class="s1">)</span>
        <span class="s1">mod = sm.GLM(data.endog</span><span class="s2">, </span><span class="s1">data.exog</span><span class="s2">, </span><span class="s1">family=sm.families.Gamma())</span>
        <span class="s1">res = mod.fit(maxiter=</span><span class="s4">1</span><span class="s2">, </span><span class="s1">method=</span><span class="s5">'bfgs'</span><span class="s2">, </span><span class="s1">max_start_irls=</span><span class="s4">0</span><span class="s1">)</span>
        <span class="s1">res.summary()</span>


<span class="s2">def </span><span class="s1">test_int_scale():</span>
    <span class="s3"># GH-6627, make sure it works with int scale</span>
    <span class="s1">data = longley.load()</span>
    <span class="s1">mod = GLM(data.endog</span><span class="s2">, </span><span class="s1">data.exog</span><span class="s2">, </span><span class="s1">family=sm.families.Gaussian())</span>
    <span class="s1">res = mod.fit(scale=</span><span class="s4">1</span><span class="s1">)</span>
    <span class="s2">assert </span><span class="s1">isinstance(res.params</span><span class="s2">, </span><span class="s1">pd.Series)</span>
    <span class="s2">assert </span><span class="s1">res.scale.dtype == np.float64</span>


<span class="s1">@pytest.mark.parametrize(</span><span class="s5">&quot;dtype&quot;</span><span class="s2">, </span><span class="s1">[np.int8</span><span class="s2">, </span><span class="s1">np.int16</span><span class="s2">, </span><span class="s1">np.int32</span><span class="s2">, </span><span class="s1">np.int64])</span>
<span class="s2">def </span><span class="s1">test_int_exog(dtype):</span>
    <span class="s3"># GH-6627, make use of floats internally</span>
    <span class="s1">count1</span><span class="s2">, </span><span class="s1">n1</span><span class="s2">, </span><span class="s1">count2</span><span class="s2">, </span><span class="s1">n2 = </span><span class="s4">60</span><span class="s2">, </span><span class="s4">51477.5</span><span class="s2">, </span><span class="s4">30</span><span class="s2">, </span><span class="s4">54308.7</span>
    <span class="s1">y = [count1</span><span class="s2">, </span><span class="s1">count2]</span>
    <span class="s1">x = np.asarray([[</span><span class="s4">1</span><span class="s2">, </span><span class="s4">1</span><span class="s1">]</span><span class="s2">, </span><span class="s1">[</span><span class="s4">1</span><span class="s2">, </span><span class="s4">0</span><span class="s1">]]).astype(dtype)</span>
    <span class="s1">exposure = np.asarray([n1</span><span class="s2">, </span><span class="s1">n2])</span>
    <span class="s1">mod = GLM(y</span><span class="s2">, </span><span class="s1">x</span><span class="s2">, </span><span class="s1">exposure=exposure</span><span class="s2">, </span><span class="s1">family=sm.families.Poisson())</span>
    <span class="s1">res = mod.fit(method=</span><span class="s5">'bfgs'</span><span class="s2">, </span><span class="s1">max_start_irls=</span><span class="s4">0</span><span class="s1">)</span>
    <span class="s2">assert </span><span class="s1">isinstance(res.params</span><span class="s2">, </span><span class="s1">np.ndarray)</span>


<span class="s2">def </span><span class="s1">test_glm_bic(iris):</span>
    <span class="s1">X = np.c_[np.ones(</span><span class="s4">100</span><span class="s1">)</span><span class="s2">, </span><span class="s1">iris[</span><span class="s4">50</span><span class="s1">:</span><span class="s2">, </span><span class="s1">:</span><span class="s4">4</span><span class="s1">]]</span>
    <span class="s1">y = np.array(iris)[</span><span class="s4">50</span><span class="s1">:</span><span class="s2">, </span><span class="s4">4</span><span class="s1">].astype(np.int32)</span>
    <span class="s1">y -= </span><span class="s4">1</span>
    <span class="s1">SET_USE_BIC_LLF(</span><span class="s2">True</span><span class="s1">)</span>
    <span class="s1">model = GLM(y</span><span class="s2">, </span><span class="s1">X</span><span class="s2">, </span><span class="s1">family=sm.families.Binomial()).fit()</span>
    <span class="s3"># 34.9244 is what glm() of R yields</span>
    <span class="s1">assert_almost_equal(model.bic</span><span class="s2">, </span><span class="s4">34.9244</span><span class="s2">, </span><span class="s1">decimal=</span><span class="s4">3</span><span class="s1">)</span>
    <span class="s1">assert_almost_equal(model.bic_llf</span><span class="s2">, </span><span class="s4">34.9244</span><span class="s2">, </span><span class="s1">decimal=</span><span class="s4">3</span><span class="s1">)</span>
    <span class="s1">SET_USE_BIC_LLF(</span><span class="s2">False</span><span class="s1">)</span>
    <span class="s1">assert_almost_equal(model.bic</span><span class="s2">, </span><span class="s1">model.bic_deviance</span><span class="s2">, </span><span class="s1">decimal=</span><span class="s4">3</span><span class="s1">)</span>
    <span class="s1">SET_USE_BIC_LLF(</span><span class="s2">None</span><span class="s1">)</span>


<span class="s2">def </span><span class="s1">test_glm_bic_warning(iris):</span>
    <span class="s1">X = np.c_[np.ones(</span><span class="s4">100</span><span class="s1">)</span><span class="s2">, </span><span class="s1">iris[</span><span class="s4">50</span><span class="s1">:</span><span class="s2">, </span><span class="s1">:</span><span class="s4">4</span><span class="s1">]]</span>
    <span class="s1">y = np.array(iris)[</span><span class="s4">50</span><span class="s1">:</span><span class="s2">, </span><span class="s4">4</span><span class="s1">].astype(np.int32)</span>
    <span class="s1">y -= </span><span class="s4">1</span>
    <span class="s1">model = GLM(y</span><span class="s2">, </span><span class="s1">X</span><span class="s2">, </span><span class="s1">family=sm.families.Binomial()).fit()</span>
    <span class="s2">with </span><span class="s1">pytest.warns(FutureWarning</span><span class="s2">, </span><span class="s1">match=</span><span class="s5">&quot;The bic&quot;</span><span class="s1">):</span>
        <span class="s2">assert </span><span class="s1">isinstance(model.bic</span><span class="s2">, </span><span class="s1">float)</span>


<span class="s2">def </span><span class="s1">test_output_exposure_null(reset_randomstate):</span>
    <span class="s3"># GH 6953</span>

    <span class="s1">x0 = [np.sin(i / </span><span class="s4">20</span><span class="s1">) + </span><span class="s4">2 </span><span class="s2">for </span><span class="s1">i </span><span class="s2">in </span><span class="s1">range(</span><span class="s4">1000</span><span class="s1">)]</span>
    <span class="s1">rs = np.random.RandomState(</span><span class="s4">0</span><span class="s1">)</span>
    <span class="s3"># Variable exposures for each observation</span>
    <span class="s1">exposure = rs.randint(</span><span class="s4">100</span><span class="s2">, </span><span class="s4">200</span><span class="s2">, </span><span class="s1">size=</span><span class="s4">1000</span><span class="s1">)</span>
    <span class="s1">y = [np.sum(rs.poisson(x</span><span class="s2">, </span><span class="s1">size=e)) </span><span class="s2">for </span><span class="s1">x</span><span class="s2">, </span><span class="s1">e </span><span class="s2">in </span><span class="s1">zip(x0</span><span class="s2">, </span><span class="s1">exposure)]</span>
    <span class="s1">x = add_constant(x0)</span>

    <span class="s1">model = GLM(</span>
        <span class="s1">endog=y</span><span class="s2">, </span><span class="s1">exog=x</span><span class="s2">, </span><span class="s1">exposure=exposure</span><span class="s2">, </span><span class="s1">family=sm.families.Poisson()</span>
    <span class="s1">).fit()</span>
    <span class="s1">null_model = GLM(</span>
        <span class="s1">endog=y</span><span class="s2">, </span><span class="s1">exog=x[:</span><span class="s2">, </span><span class="s4">0</span><span class="s1">]</span><span class="s2">, </span><span class="s1">exposure=exposure</span><span class="s2">, </span><span class="s1">family=sm.families.Poisson()</span>
    <span class="s1">).fit()</span>
    <span class="s1">null_model_without_exposure = GLM(</span>
        <span class="s1">endog=y</span><span class="s2">, </span><span class="s1">exog=x[:</span><span class="s2">, </span><span class="s4">0</span><span class="s1">]</span><span class="s2">, </span><span class="s1">family=sm.families.Poisson()</span>
    <span class="s1">).fit()</span>
    <span class="s1">assert_allclose(model.llnull</span><span class="s2">, </span><span class="s1">null_model.llf)</span>
    <span class="s3"># Check that they are different</span>
    <span class="s2">assert </span><span class="s1">np.abs(null_model_without_exposure.llf - model.llnull) &gt; </span><span class="s4">1</span>


<span class="s2">def </span><span class="s1">test_qaic():</span>

    <span class="s3"># Example from documentation of R package MuMIn</span>
    <span class="s2">import </span><span class="s1">patsy</span>
    <span class="s1">ldose = np.concatenate((np.arange(</span><span class="s4">6</span><span class="s1">)</span><span class="s2">, </span><span class="s1">np.arange(</span><span class="s4">6</span><span class="s1">)))</span>
    <span class="s1">sex = [</span><span class="s5">&quot;M&quot;</span><span class="s1">]*</span><span class="s4">6 </span><span class="s1">+ [</span><span class="s5">&quot;F&quot;</span><span class="s1">]*</span><span class="s4">6</span>
    <span class="s1">numdead = [</span><span class="s4">10</span><span class="s2">, </span><span class="s4">4</span><span class="s2">, </span><span class="s4">9</span><span class="s2">, </span><span class="s4">12</span><span class="s2">, </span><span class="s4">18</span><span class="s2">, </span><span class="s4">20</span><span class="s2">, </span><span class="s4">0</span><span class="s2">, </span><span class="s4">2</span><span class="s2">, </span><span class="s4">6</span><span class="s2">, </span><span class="s4">10</span><span class="s2">, </span><span class="s4">12</span><span class="s2">, </span><span class="s4">16</span><span class="s1">]</span>
    <span class="s1">df = pd.DataFrame({</span><span class="s5">&quot;ldose&quot;</span><span class="s1">: ldose</span><span class="s2">, </span><span class="s5">&quot;sex&quot;</span><span class="s1">: sex</span><span class="s2">, </span><span class="s5">&quot;numdead&quot;</span><span class="s1">: numdead})</span>
    <span class="s1">df[</span><span class="s5">&quot;numalive&quot;</span><span class="s1">] = </span><span class="s4">20 </span><span class="s1">- df[</span><span class="s5">&quot;numdead&quot;</span><span class="s1">]</span>
    <span class="s1">df[</span><span class="s5">&quot;SF&quot;</span><span class="s1">] = df[</span><span class="s5">&quot;numdead&quot;</span><span class="s1">]</span>

    <span class="s1">y = df[[</span><span class="s5">&quot;numalive&quot;</span><span class="s2">, </span><span class="s5">&quot;numdead&quot;</span><span class="s1">]].values</span>
    <span class="s1">x = patsy.dmatrix(</span><span class="s5">&quot;sex*ldose&quot;</span><span class="s2">, </span><span class="s1">data=df</span><span class="s2">, </span><span class="s1">return_type=</span><span class="s5">'dataframe'</span><span class="s1">)</span>
    <span class="s1">m = GLM(y</span><span class="s2">, </span><span class="s1">x</span><span class="s2">, </span><span class="s1">family=sm.families.Binomial())</span>
    <span class="s1">r = m.fit()</span>
    <span class="s1">scale = </span><span class="s4">2.412699</span>
    <span class="s1">qaic = r.info_criteria(crit=</span><span class="s5">&quot;qaic&quot;</span><span class="s2">, </span><span class="s1">scale=scale)</span>

    <span class="s3"># R gives 31.13266 because it uses a df that is 1 greater,</span>
    <span class="s3"># presumably because they count the scale parameter in df.</span>
    <span class="s3"># This won't matter when comparing models by differencing</span>
    <span class="s3"># QAICs.</span>
    <span class="s3"># Binomial doesn't have a scale parameter, so adding +1 is not correct.</span>
    <span class="s1">assert_allclose(qaic</span><span class="s2">, </span><span class="s4">29.13266</span><span class="s2">, </span><span class="s1">rtol=</span><span class="s4">1e-5</span><span class="s2">, </span><span class="s1">atol=</span><span class="s4">1e-5</span><span class="s1">)</span>
    <span class="s1">qaic1 = r.info_criteria(crit=</span><span class="s5">&quot;qaic&quot;</span><span class="s2">, </span><span class="s1">scale=scale</span><span class="s2">, </span><span class="s1">dk_params=</span><span class="s4">1</span><span class="s1">)</span>
    <span class="s1">assert_allclose(qaic1</span><span class="s2">, </span><span class="s4">31.13266</span><span class="s2">, </span><span class="s1">rtol=</span><span class="s4">1e-5</span><span class="s2">, </span><span class="s1">atol=</span><span class="s4">1e-5</span><span class="s1">)</span>


<span class="s2">def </span><span class="s1">test_tweedie_score():</span>

    <span class="s1">np.random.seed(</span><span class="s4">3242</span><span class="s1">)</span>
    <span class="s1">n = </span><span class="s4">500</span>
    <span class="s1">x = np.random.normal(size=(n</span><span class="s2">, </span><span class="s4">4</span><span class="s1">))</span>
    <span class="s1">lpr = np.dot(x</span><span class="s2">, </span><span class="s1">np.r_[</span><span class="s4">1</span><span class="s2">, </span><span class="s1">-</span><span class="s4">1</span><span class="s2">, </span><span class="s4">0</span><span class="s2">, </span><span class="s4">0.5</span><span class="s1">])</span>
    <span class="s1">mu = np.exp(lpr)</span>

    <span class="s1">p0 = </span><span class="s4">1.5</span>
    <span class="s1">lam = </span><span class="s4">10 </span><span class="s1">* mu**(</span><span class="s4">2 </span><span class="s1">- p0) / (</span><span class="s4">2 </span><span class="s1">- p0)</span>
    <span class="s1">alp = (</span><span class="s4">2 </span><span class="s1">- p0) / (p0 - </span><span class="s4">1</span><span class="s1">)</span>
    <span class="s1">bet = </span><span class="s4">10 </span><span class="s1">* mu**(</span><span class="s4">1 </span><span class="s1">- p0) / (p0 - </span><span class="s4">1</span><span class="s1">)</span>
    <span class="s1">y = np.empty(n)</span>
    <span class="s1">N = np.random.poisson(lam)</span>
    <span class="s2">for </span><span class="s1">i </span><span class="s2">in </span><span class="s1">range(n):</span>
        <span class="s1">y[i] = np.random.gamma(alp</span><span class="s2">, </span><span class="s4">1 </span><span class="s1">/ bet[i]</span><span class="s2">, </span><span class="s1">N[i]).sum()</span>

    <span class="s2">for </span><span class="s1">eql </span><span class="s2">in </span><span class="s1">[</span><span class="s2">True, False</span><span class="s1">]:</span>
        <span class="s2">for </span><span class="s1">p </span><span class="s2">in </span><span class="s1">[</span><span class="s4">1</span><span class="s2">, </span><span class="s4">1.5</span><span class="s2">, </span><span class="s4">2</span><span class="s1">]:</span>
            <span class="s2">if </span><span class="s1">eql </span><span class="s2">is False and </span><span class="s1">SP_LT_17:</span>
                <span class="s1">pytest.skip(</span><span class="s5">'skip, scipy too old, no bessel_wright'</span><span class="s1">)</span>

            <span class="s1">fam = sm.families.Tweedie(var_power=p</span><span class="s2">, </span><span class="s1">eql=eql)</span>
            <span class="s1">model = GLM(y</span><span class="s2">, </span><span class="s1">x</span><span class="s2">, </span><span class="s1">family=fam)</span>
            <span class="s1">result = model.fit()</span>

            <span class="s1">pa = result.params + </span><span class="s4">0.2</span><span class="s1">*np.random.normal(size=result.params.size)</span>

            <span class="s1">ngrad = approx_fprime_cs(pa</span><span class="s2">, lambda </span><span class="s1">x: model.loglike(x</span><span class="s2">, </span><span class="s1">scale=</span><span class="s4">1</span><span class="s1">))</span>
            <span class="s1">agrad = model.score(pa</span><span class="s2">, </span><span class="s1">scale=</span><span class="s4">1</span><span class="s1">)</span>
            <span class="s1">assert_allclose(ngrad</span><span class="s2">, </span><span class="s1">agrad</span><span class="s2">, </span><span class="s1">atol=</span><span class="s4">1e-8</span><span class="s2">, </span><span class="s1">rtol=</span><span class="s4">1e-8</span><span class="s1">)</span>

            <span class="s1">nhess = approx_hess_cs(pa</span><span class="s2">, lambda </span><span class="s1">x: model.loglike(x</span><span class="s2">, </span><span class="s1">scale=</span><span class="s4">1</span><span class="s1">))</span>
            <span class="s1">ahess = model.hessian(pa</span><span class="s2">, </span><span class="s1">scale=</span><span class="s4">1</span><span class="s1">)</span>
            <span class="s1">assert_allclose(nhess</span><span class="s2">, </span><span class="s1">ahess</span><span class="s2">, </span><span class="s1">atol=</span><span class="s4">5e-8</span><span class="s2">, </span><span class="s1">rtol=</span><span class="s4">5e-8</span><span class="s1">)</span>
</pre>
</body>
</html>