<html>
<head>
<title>descriptive.py</title>
<meta http-equiv="Content-Type" content="text/html; charset=utf-8">
<style type="text/css">
.s0 { color: #629755; font-style: italic;}
.s1 { color: #a9b7c6;}
.s2 { color: #cc7832;}
.s3 { color: #6897bb;}
.s4 { color: #808080;}
.s5 { color: #6a8759;}
</style>
</head>
<body bgcolor="#2b2b2b">
<table CELLSPACING=0 CELLPADDING=5 COLS=1 WIDTH="100%" BGCOLOR="#606060" >
<tr><td><center>
<font face="Arial, Helvetica" color="#000000">
descriptive.py</font>
</center></td></tr></table>
<pre><span class="s0">&quot;&quot;&quot; 
Empirical likelihood inference on descriptive statistics 
 
This module conducts hypothesis tests and constructs confidence 
intervals for the mean, variance, skewness, kurtosis and correlation. 
 
If matplotlib is installed, this module can also generate multivariate 
confidence region plots as well as mean-variance contour plots. 
 
See _OptFuncts docstring for technical details and optimization variable 
definitions. 
 
General References: 
------------------ 
Owen, A. (2001). &quot;Empirical Likelihood.&quot; Chapman and Hall 
 
&quot;&quot;&quot;</span>
<span class="s2">import </span><span class="s1">numpy </span><span class="s2">as </span><span class="s1">np</span>
<span class="s2">from </span><span class="s1">scipy </span><span class="s2">import </span><span class="s1">optimize</span>
<span class="s2">from </span><span class="s1">scipy.stats </span><span class="s2">import </span><span class="s1">chi2</span><span class="s2">, </span><span class="s1">skew</span><span class="s2">, </span><span class="s1">kurtosis</span>
<span class="s2">from </span><span class="s1">statsmodels.base.optimizer </span><span class="s2">import </span><span class="s1">_fit_newton</span>
<span class="s2">import </span><span class="s1">itertools</span>
<span class="s2">from </span><span class="s1">statsmodels.graphics </span><span class="s2">import </span><span class="s1">utils</span>


<span class="s2">def </span><span class="s1">DescStat(endog):</span>
    <span class="s0">&quot;&quot;&quot; 
    Returns an instance to conduct inference on descriptive statistics 
    via empirical likelihood.  See DescStatUV and DescStatMV for more 
    information. 
 
    Parameters 
    ---------- 
    endog : ndarray 
         Array of data 
 
    Returns : DescStat instance 
        If k=1, the function returns a univariate instance, DescStatUV. 
        If k&gt;1, the function returns a multivariate instance, DescStatMV. 
    &quot;&quot;&quot;</span>
    <span class="s2">if </span><span class="s1">endog.ndim == </span><span class="s3">1</span><span class="s1">:</span>
        <span class="s1">endog = endog.reshape(len(endog)</span><span class="s2">, </span><span class="s3">1</span><span class="s1">)</span>
    <span class="s2">if </span><span class="s1">endog.shape[</span><span class="s3">1</span><span class="s1">] == </span><span class="s3">1</span><span class="s1">:</span>
        <span class="s2">return </span><span class="s1">DescStatUV(endog)</span>
    <span class="s2">if </span><span class="s1">endog.shape[</span><span class="s3">1</span><span class="s1">] &gt; </span><span class="s3">1</span><span class="s1">:</span>
        <span class="s2">return </span><span class="s1">DescStatMV(endog)</span>


<span class="s2">class </span><span class="s1">_OptFuncts:</span>
    <span class="s0">&quot;&quot;&quot; 
    A class that holds functions that are optimized/solved. 
 
    The general setup of the class is simple.  Any method that starts with 
    _opt_ creates a vector of estimating equations named est_vect such that 
    np.dot(p, (est_vect))=0 where p is the weight on each 
    observation as a 1 x n array and est_vect is n x k.  Then _modif_Newton is 
    called to determine the optimal p by solving for the Lagrange multiplier 
    (eta) in the profile likelihood maximization problem.  In the presence 
    of nuisance parameters, _opt_ functions are  optimized over to profile 
    out the nuisance parameters. 
 
    Any method starting with _ci_limits calculates the log likelihood 
    ratio for a specific value of a parameter and then subtracts a 
    pre-specified critical value.  This is solved so that llr - crit = 0. 
    &quot;&quot;&quot;</span>

    <span class="s2">def </span><span class="s1">__init__(self</span><span class="s2">, </span><span class="s1">endog):</span>
        <span class="s2">pass</span>

    <span class="s2">def </span><span class="s1">_log_star(self</span><span class="s2">, </span><span class="s1">eta</span><span class="s2">, </span><span class="s1">est_vect</span><span class="s2">, </span><span class="s1">weights</span><span class="s2">, </span><span class="s1">nobs):</span>
        <span class="s0">&quot;&quot;&quot; 
        Transforms the log of observation probabilities in terms of the 
        Lagrange multiplier to the log 'star' of the probabilities. 
 
        Parameters 
        ---------- 
        eta : float 
            Lagrange multiplier 
 
        est_vect : ndarray (n,k) 
            Estimating equations vector 
 
        wts : nx1 array 
            Observation weights 
 
        Returns 
        ------ 
        data_star : ndarray 
            The weighted logstar of the estimting equations 
 
        Notes 
        ----- 
        This function is only a placeholder for the _fit_Newton. 
        The function value is not used in optimization and the optimal value 
        is disregarded when computing the log likelihood ratio. 
        &quot;&quot;&quot;</span>
        <span class="s1">data_star = np.log(weights) + (np.sum(weights) +\</span>
                                       <span class="s1">np.dot(est_vect</span><span class="s2">, </span><span class="s1">eta))</span>
        <span class="s1">idx = data_star &lt; </span><span class="s3">1. </span><span class="s1">/ nobs</span>
        <span class="s1">not_idx = ~idx</span>
        <span class="s1">nx = nobs * data_star[idx]</span>
        <span class="s1">data_star[idx] = np.log(</span><span class="s3">1. </span><span class="s1">/ nobs) - </span><span class="s3">1.5 </span><span class="s1">+ nx * (</span><span class="s3">2. </span><span class="s1">- nx / </span><span class="s3">2</span><span class="s1">)</span>
        <span class="s1">data_star[not_idx] = np.log(data_star[not_idx])</span>
        <span class="s2">return </span><span class="s1">data_star</span>

    <span class="s2">def </span><span class="s1">_hess(self</span><span class="s2">, </span><span class="s1">eta</span><span class="s2">, </span><span class="s1">est_vect</span><span class="s2">, </span><span class="s1">weights</span><span class="s2">, </span><span class="s1">nobs):</span>
        <span class="s0">&quot;&quot;&quot; 
        Calculates the hessian of a weighted empirical likelihood 
        problem. 
 
        Parameters 
        ---------- 
        eta : ndarray, (1,m) 
            Lagrange multiplier in the profile likelihood maximization 
 
        est_vect : ndarray (n,k) 
            Estimating equations vector 
 
        weights : 1darray 
            Observation weights 
 
        Returns 
        ------- 
        hess : m x m array 
            Weighted hessian used in _wtd_modif_newton 
        &quot;&quot;&quot;</span>
        <span class="s4">#eta = np.squeeze(eta)</span>
        <span class="s1">data_star_doub_prime = np.sum(weights) + np.dot(est_vect</span><span class="s2">, </span><span class="s1">eta)</span>
        <span class="s1">idx = data_star_doub_prime &lt; </span><span class="s3">1. </span><span class="s1">/ nobs</span>
        <span class="s1">not_idx = ~idx</span>
        <span class="s1">data_star_doub_prime[idx] = - nobs ** </span><span class="s3">2</span>
        <span class="s1">data_star_doub_prime[not_idx] = - (data_star_doub_prime[not_idx]) ** -</span><span class="s3">2</span>
        <span class="s1">wtd_dsdp = weights * data_star_doub_prime</span>
        <span class="s2">return </span><span class="s1">np.dot(est_vect.T</span><span class="s2">, </span><span class="s1">wtd_dsdp[:</span><span class="s2">, None</span><span class="s1">] * est_vect)</span>

    <span class="s2">def </span><span class="s1">_grad(self</span><span class="s2">, </span><span class="s1">eta</span><span class="s2">, </span><span class="s1">est_vect</span><span class="s2">, </span><span class="s1">weights</span><span class="s2">, </span><span class="s1">nobs):</span>
        <span class="s0">&quot;&quot;&quot; 
        Calculates the gradient of a weighted empirical likelihood 
        problem 
 
        Parameters 
        ---------- 
        eta : ndarray, (1,m) 
            Lagrange multiplier in the profile likelihood maximization 
 
        est_vect : ndarray, (n,k) 
            Estimating equations vector 
 
        weights : 1darray 
            Observation weights 
 
        Returns 
        ------- 
        gradient : ndarray (m,1) 
            The gradient used in _wtd_modif_newton 
        &quot;&quot;&quot;</span>
        <span class="s4">#eta = np.squeeze(eta)</span>
        <span class="s1">data_star_prime = np.sum(weights) + np.dot(est_vect</span><span class="s2">, </span><span class="s1">eta)</span>
        <span class="s1">idx = data_star_prime &lt; </span><span class="s3">1. </span><span class="s1">/ nobs</span>
        <span class="s1">not_idx = ~idx</span>
        <span class="s1">data_star_prime[idx] = nobs * (</span><span class="s3">2 </span><span class="s1">- nobs * data_star_prime[idx])</span>
        <span class="s1">data_star_prime[not_idx] = </span><span class="s3">1. </span><span class="s1">/ data_star_prime[not_idx]</span>
        <span class="s2">return </span><span class="s1">np.dot(weights * data_star_prime</span><span class="s2">, </span><span class="s1">est_vect)</span>

    <span class="s2">def </span><span class="s1">_modif_newton(self</span><span class="s2">,  </span><span class="s1">eta</span><span class="s2">, </span><span class="s1">est_vect</span><span class="s2">, </span><span class="s1">weights):</span>
        <span class="s0">&quot;&quot;&quot; 
        Modified Newton's method for maximizing the log 'star' equation.  This 
        function calls _fit_newton to find the optimal values of eta. 
 
        Parameters 
        ---------- 
        eta : ndarray, (1,m) 
            Lagrange multiplier in the profile likelihood maximization 
 
        est_vect : ndarray, (n,k) 
            Estimating equations vector 
 
        weights : 1darray 
            Observation weights 
 
        Returns 
        ------- 
        params : 1xm array 
            Lagrange multiplier that maximizes the log-likelihood 
        &quot;&quot;&quot;</span>
        <span class="s1">nobs = len(est_vect)</span>
        <span class="s1">f = </span><span class="s2">lambda </span><span class="s1">x0: - np.sum(self._log_star(x0</span><span class="s2">, </span><span class="s1">est_vect</span><span class="s2">, </span><span class="s1">weights</span><span class="s2">, </span><span class="s1">nobs))</span>
        <span class="s1">grad = </span><span class="s2">lambda </span><span class="s1">x0: - self._grad(x0</span><span class="s2">, </span><span class="s1">est_vect</span><span class="s2">, </span><span class="s1">weights</span><span class="s2">, </span><span class="s1">nobs)</span>
        <span class="s1">hess = </span><span class="s2">lambda </span><span class="s1">x0: - self._hess(x0</span><span class="s2">, </span><span class="s1">est_vect</span><span class="s2">, </span><span class="s1">weights</span><span class="s2">, </span><span class="s1">nobs)</span>
        <span class="s1">kwds = {</span><span class="s5">'tol'</span><span class="s1">: </span><span class="s3">1e-8</span><span class="s1">}</span>
        <span class="s1">eta = eta.squeeze()</span>
        <span class="s1">res = _fit_newton(f</span><span class="s2">, </span><span class="s1">grad</span><span class="s2">, </span><span class="s1">eta</span><span class="s2">, </span><span class="s1">()</span><span class="s2">, </span><span class="s1">kwds</span><span class="s2">, </span><span class="s1">hess=hess</span><span class="s2">, </span><span class="s1">maxiter=</span><span class="s3">50</span><span class="s2">, </span><span class="s1">\</span>
                              <span class="s1">disp=</span><span class="s3">0</span><span class="s1">)</span>
        <span class="s2">return </span><span class="s1">res[</span><span class="s3">0</span><span class="s1">]</span>

    <span class="s2">def </span><span class="s1">_find_eta(self</span><span class="s2">, </span><span class="s1">eta):</span>
        <span class="s0">&quot;&quot;&quot; 
        Finding the root of sum(xi-h0)/(1+eta(xi-mu)) solves for 
        eta when computing ELR for univariate mean. 
 
        Parameters 
        ---------- 
        eta : float 
            Lagrange multiplier in the empirical likelihood maximization 
 
        Returns 
        ------- 
        llr : float 
            n times the log likelihood value for a given value of eta 
        &quot;&quot;&quot;</span>
        <span class="s2">return </span><span class="s1">np.sum((self.endog - self.mu0) / \</span>
              <span class="s1">(</span><span class="s3">1. </span><span class="s1">+ eta * (self.endog - self.mu0)))</span>

    <span class="s2">def </span><span class="s1">_ci_limits_mu(self</span><span class="s2">, </span><span class="s1">mu):</span>
        <span class="s0">&quot;&quot;&quot; 
        Calculates the difference between the log likelihood of mu_test and a 
        specified critical value. 
 
        Parameters 
        ---------- 
        mu : float 
           Hypothesized value of the mean. 
 
        Returns 
        ------- 
        diff : float 
            The difference between the log likelihood value of mu0 and 
            a specified value. 
        &quot;&quot;&quot;</span>
        <span class="s2">return </span><span class="s1">self.test_mean(mu)[</span><span class="s3">0</span><span class="s1">] - self.r0</span>

    <span class="s2">def </span><span class="s1">_find_gamma(self</span><span class="s2">, </span><span class="s1">gamma):</span>
        <span class="s0">&quot;&quot;&quot; 
        Finds gamma that satisfies 
        sum(log(n * w(gamma))) - log(r0) = 0 
 
        Used for confidence intervals for the mean 
 
        Parameters 
        ---------- 
        gamma : float 
            Lagrange multiplier when computing confidence interval 
 
        Returns 
        ------- 
        diff : float 
            The difference between the log-liklihood when the Lagrange 
            multiplier is gamma and a pre-specified value 
        &quot;&quot;&quot;</span>
        <span class="s1">denom = np.sum((self.endog - gamma) ** -</span><span class="s3">1</span><span class="s1">)</span>
        <span class="s1">new_weights = (self.endog - gamma) ** -</span><span class="s3">1 </span><span class="s1">/ denom</span>
        <span class="s2">return </span><span class="s1">-</span><span class="s3">2 </span><span class="s1">* np.sum(np.log(self.nobs * new_weights)) - \</span>
            <span class="s1">self.r0</span>

    <span class="s2">def </span><span class="s1">_opt_var(self</span><span class="s2">, </span><span class="s1">nuisance_mu</span><span class="s2">, </span><span class="s1">pval=</span><span class="s2">False</span><span class="s1">):</span>
        <span class="s0">&quot;&quot;&quot; 
        This is the function to be optimized over a nuisance mean parameter 
        to determine the likelihood ratio for the variance 
 
        Parameters 
        ---------- 
        nuisance_mu : float 
            Value of a nuisance mean parameter 
 
        Returns 
        ------- 
        llr : float 
            Log likelihood of a pre-specified variance holding the nuisance 
            parameter constant 
        &quot;&quot;&quot;</span>
        <span class="s1">endog = self.endog</span>
        <span class="s1">nobs = self.nobs</span>
        <span class="s1">sig_data = ((endog - nuisance_mu) ** </span><span class="s3">2 </span><span class="s1">\</span>
                    <span class="s1">- self.sig2_0)</span>
        <span class="s1">mu_data = (endog - nuisance_mu)</span>
        <span class="s1">est_vect = np.column_stack((mu_data</span><span class="s2">, </span><span class="s1">sig_data))</span>
        <span class="s1">eta_star = self._modif_newton(np.array([</span><span class="s3">1. </span><span class="s1">/ nobs</span><span class="s2">,</span>
                                               <span class="s3">1. </span><span class="s1">/ nobs])</span><span class="s2">, </span><span class="s1">est_vect</span><span class="s2">,</span>
                                                <span class="s1">np.ones(nobs) * (</span><span class="s3">1. </span><span class="s1">/ nobs))</span>

        <span class="s1">denom = </span><span class="s3">1 </span><span class="s1">+ np.dot(eta_star</span><span class="s2">, </span><span class="s1">est_vect.T)</span>
        <span class="s1">self.new_weights = </span><span class="s3">1. </span><span class="s1">/ nobs * </span><span class="s3">1. </span><span class="s1">/ denom</span>
        <span class="s1">llr = np.sum(np.log(nobs * self.new_weights))</span>
        <span class="s2">if </span><span class="s1">pval:  </span><span class="s4"># Used for contour plotting</span>
            <span class="s2">return </span><span class="s1">chi2.sf(-</span><span class="s3">2 </span><span class="s1">* llr</span><span class="s2">, </span><span class="s3">1</span><span class="s1">)</span>
        <span class="s2">return </span><span class="s1">-</span><span class="s3">2 </span><span class="s1">* llr</span>

    <span class="s2">def </span><span class="s1">_ci_limits_var(self</span><span class="s2">, </span><span class="s1">var):</span>
        <span class="s0">&quot;&quot;&quot; 
        Used to determine the confidence intervals for the variance. 
        It calls test_var and when called by an optimizer, 
        finds the value of sig2_0 that is chi2.ppf(significance-level) 
 
        Parameters 
        ---------- 
        var_test : float 
            Hypothesized value of the variance 
 
        Returns 
        ------- 
        diff : float 
            The difference between the log likelihood ratio at var_test and a 
            pre-specified value. 
        &quot;&quot;&quot;</span>
        <span class="s2">return </span><span class="s1">self.test_var(var)[</span><span class="s3">0</span><span class="s1">] - self.r0</span>

    <span class="s2">def </span><span class="s1">_opt_skew(self</span><span class="s2">, </span><span class="s1">nuis_params):</span>
        <span class="s0">&quot;&quot;&quot; 
        Called by test_skew.  This function is optimized over 
        nuisance parameters mu and sigma 
 
        Parameters 
        ---------- 
        nuis_params : 1darray 
            An array with a  nuisance mean and variance parameter 
 
        Returns 
        ------- 
        llr : float 
            The log likelihood ratio of a pre-specified skewness holding 
            the nuisance parameters constant. 
        &quot;&quot;&quot;</span>
        <span class="s1">endog = self.endog</span>
        <span class="s1">nobs = self.nobs</span>
        <span class="s1">mu_data = endog - nuis_params[</span><span class="s3">0</span><span class="s1">]</span>
        <span class="s1">sig_data = ((endog - nuis_params[</span><span class="s3">0</span><span class="s1">]) ** </span><span class="s3">2</span><span class="s1">) - nuis_params[</span><span class="s3">1</span><span class="s1">]</span>
        <span class="s1">skew_data = ((((endog - nuis_params[</span><span class="s3">0</span><span class="s1">]) ** </span><span class="s3">3</span><span class="s1">) /</span>
                    <span class="s1">(nuis_params[</span><span class="s3">1</span><span class="s1">] ** </span><span class="s3">1.5</span><span class="s1">))) - self.skew0</span>
        <span class="s1">est_vect = np.column_stack((mu_data</span><span class="s2">, </span><span class="s1">sig_data</span><span class="s2">, </span><span class="s1">skew_data))</span>
        <span class="s1">eta_star = self._modif_newton(np.array([</span><span class="s3">1. </span><span class="s1">/ nobs</span><span class="s2">,</span>
                                               <span class="s3">1. </span><span class="s1">/ nobs</span><span class="s2">,</span>
                                               <span class="s3">1. </span><span class="s1">/ nobs])</span><span class="s2">, </span><span class="s1">est_vect</span><span class="s2">,</span>
                                               <span class="s1">np.ones(nobs) * (</span><span class="s3">1. </span><span class="s1">/ nobs))</span>
        <span class="s1">denom = </span><span class="s3">1. </span><span class="s1">+ np.dot(eta_star</span><span class="s2">, </span><span class="s1">est_vect.T)</span>
        <span class="s1">self.new_weights = </span><span class="s3">1. </span><span class="s1">/ nobs * </span><span class="s3">1. </span><span class="s1">/ denom</span>
        <span class="s1">llr = np.sum(np.log(nobs * self.new_weights))</span>
        <span class="s2">return </span><span class="s1">-</span><span class="s3">2 </span><span class="s1">* llr</span>

    <span class="s2">def </span><span class="s1">_opt_kurt(self</span><span class="s2">, </span><span class="s1">nuis_params):</span>
        <span class="s0">&quot;&quot;&quot; 
        Called by test_kurt.  This function is optimized over 
        nuisance parameters mu and sigma 
 
        Parameters 
        ---------- 
        nuis_params : 1darray 
            An array with a nuisance mean and variance parameter 
 
        Returns 
        ------- 
        llr : float 
            The log likelihood ratio of a pre-speified kurtosis holding the 
            nuisance parameters constant 
        &quot;&quot;&quot;</span>
        <span class="s1">endog = self.endog</span>
        <span class="s1">nobs = self.nobs</span>
        <span class="s1">mu_data = endog - nuis_params[</span><span class="s3">0</span><span class="s1">]</span>
        <span class="s1">sig_data = ((endog - nuis_params[</span><span class="s3">0</span><span class="s1">]) ** </span><span class="s3">2</span><span class="s1">) - nuis_params[</span><span class="s3">1</span><span class="s1">]</span>
        <span class="s1">kurt_data = (((((endog - nuis_params[</span><span class="s3">0</span><span class="s1">]) ** </span><span class="s3">4</span><span class="s1">) / \</span>
                    <span class="s1">(nuis_params[</span><span class="s3">1</span><span class="s1">] ** </span><span class="s3">2</span><span class="s1">))) - </span><span class="s3">3</span><span class="s1">) - self.kurt0</span>
        <span class="s1">est_vect = np.column_stack((mu_data</span><span class="s2">, </span><span class="s1">sig_data</span><span class="s2">, </span><span class="s1">kurt_data))</span>
        <span class="s1">eta_star = self._modif_newton(np.array([</span><span class="s3">1. </span><span class="s1">/ nobs</span><span class="s2">,</span>
                                               <span class="s3">1. </span><span class="s1">/ nobs</span><span class="s2">,</span>
                                               <span class="s3">1. </span><span class="s1">/ nobs])</span><span class="s2">, </span><span class="s1">est_vect</span><span class="s2">,</span>
                                               <span class="s1">np.ones(nobs) * (</span><span class="s3">1. </span><span class="s1">/ nobs))</span>
        <span class="s1">denom = </span><span class="s3">1 </span><span class="s1">+ np.dot(eta_star</span><span class="s2">, </span><span class="s1">est_vect.T)</span>
        <span class="s1">self.new_weights = </span><span class="s3">1. </span><span class="s1">/ nobs * </span><span class="s3">1. </span><span class="s1">/ denom</span>
        <span class="s1">llr = np.sum(np.log(nobs * self.new_weights))</span>
        <span class="s2">return </span><span class="s1">-</span><span class="s3">2 </span><span class="s1">* llr</span>

    <span class="s2">def </span><span class="s1">_opt_skew_kurt(self</span><span class="s2">, </span><span class="s1">nuis_params):</span>
        <span class="s0">&quot;&quot;&quot; 
        Called by test_joint_skew_kurt.  This function is optimized over 
        nuisance parameters mu and sigma 
 
        Parameters 
        ---------- 
        nuis_params : 1darray 
            An array with a nuisance mean and variance parameter 
 
        Returns 
        ------ 
        llr : float 
            The log likelihood ratio of a pre-speified skewness and 
            kurtosis holding the nuisance parameters constant. 
        &quot;&quot;&quot;</span>
        <span class="s1">endog = self.endog</span>
        <span class="s1">nobs = self.nobs</span>
        <span class="s1">mu_data = endog - nuis_params[</span><span class="s3">0</span><span class="s1">]</span>
        <span class="s1">sig_data = ((endog - nuis_params[</span><span class="s3">0</span><span class="s1">]) ** </span><span class="s3">2</span><span class="s1">) - nuis_params[</span><span class="s3">1</span><span class="s1">]</span>
        <span class="s1">skew_data = ((((endog - nuis_params[</span><span class="s3">0</span><span class="s1">]) ** </span><span class="s3">3</span><span class="s1">) / \</span>
                    <span class="s1">(nuis_params[</span><span class="s3">1</span><span class="s1">] ** </span><span class="s3">1.5</span><span class="s1">))) - self.skew0</span>
        <span class="s1">kurt_data = (((((endog - nuis_params[</span><span class="s3">0</span><span class="s1">]) ** </span><span class="s3">4</span><span class="s1">) / \</span>
                    <span class="s1">(nuis_params[</span><span class="s3">1</span><span class="s1">] ** </span><span class="s3">2</span><span class="s1">))) - </span><span class="s3">3</span><span class="s1">) - self.kurt0</span>
        <span class="s1">est_vect = np.column_stack((mu_data</span><span class="s2">, </span><span class="s1">sig_data</span><span class="s2">, </span><span class="s1">skew_data</span><span class="s2">, </span><span class="s1">kurt_data))</span>
        <span class="s1">eta_star = self._modif_newton(np.array([</span><span class="s3">1. </span><span class="s1">/ nobs</span><span class="s2">,</span>
                                               <span class="s3">1. </span><span class="s1">/ nobs</span><span class="s2">,</span>
                                               <span class="s3">1. </span><span class="s1">/ nobs</span><span class="s2">,</span>
                                               <span class="s3">1. </span><span class="s1">/ nobs])</span><span class="s2">, </span><span class="s1">est_vect</span><span class="s2">,</span>
                                               <span class="s1">np.ones(nobs) * (</span><span class="s3">1. </span><span class="s1">/ nobs))</span>
        <span class="s1">denom = </span><span class="s3">1. </span><span class="s1">+ np.dot(eta_star</span><span class="s2">, </span><span class="s1">est_vect.T)</span>
        <span class="s1">self.new_weights = </span><span class="s3">1. </span><span class="s1">/ nobs * </span><span class="s3">1. </span><span class="s1">/ denom</span>
        <span class="s1">llr = np.sum(np.log(nobs * self.new_weights))</span>
        <span class="s2">return </span><span class="s1">-</span><span class="s3">2 </span><span class="s1">* llr</span>

    <span class="s2">def </span><span class="s1">_ci_limits_skew(self</span><span class="s2">, </span><span class="s1">skew):</span>
        <span class="s0">&quot;&quot;&quot; 
        Parameters 
        ---------- 
        skew0 : float 
            Hypothesized value of skewness 
 
        Returns 
        ------- 
        diff : float 
            The difference between the log likelihood ratio at skew and a 
            pre-specified value. 
        &quot;&quot;&quot;</span>
        <span class="s2">return </span><span class="s1">self.test_skew(skew)[</span><span class="s3">0</span><span class="s1">] - self.r0</span>

    <span class="s2">def </span><span class="s1">_ci_limits_kurt(self</span><span class="s2">, </span><span class="s1">kurt):</span>
        <span class="s0">&quot;&quot;&quot; 
        Parameters 
        ---------- 
        skew0 : float 
            Hypothesized value of kurtosis 
 
        Returns 
        ------- 
        diff : float 
            The difference between the log likelihood ratio at kurt and a 
            pre-specified value. 
        &quot;&quot;&quot;</span>
        <span class="s2">return </span><span class="s1">self.test_kurt(kurt)[</span><span class="s3">0</span><span class="s1">] - self.r0</span>

    <span class="s2">def </span><span class="s1">_opt_correl(self</span><span class="s2">, </span><span class="s1">nuis_params</span><span class="s2">, </span><span class="s1">corr0</span><span class="s2">, </span><span class="s1">endog</span><span class="s2">, </span><span class="s1">nobs</span><span class="s2">, </span><span class="s1">x0</span><span class="s2">, </span><span class="s1">weights0):</span>
        <span class="s0">&quot;&quot;&quot; 
        Parameters 
        ---------- 
        nuis_params : 1darray 
            Array containing two nuisance means and two nuisance variances 
 
        Returns 
        ------- 
        llr : float 
            The log-likelihood of the correlation coefficient holding nuisance 
            parameters constant 
        &quot;&quot;&quot;</span>
        <span class="s1">mu1_data</span><span class="s2">, </span><span class="s1">mu2_data = (endog - nuis_params[::</span><span class="s3">2</span><span class="s1">]).T</span>
        <span class="s1">sig1_data = mu1_data ** </span><span class="s3">2 </span><span class="s1">- nuis_params[</span><span class="s3">1</span><span class="s1">]</span>
        <span class="s1">sig2_data = mu2_data ** </span><span class="s3">2 </span><span class="s1">- nuis_params[</span><span class="s3">3</span><span class="s1">]</span>
        <span class="s1">correl_data = ((mu1_data * mu2_data) - corr0 *</span>
                    <span class="s1">(nuis_params[</span><span class="s3">1</span><span class="s1">] * nuis_params[</span><span class="s3">3</span><span class="s1">]) ** </span><span class="s3">.5</span><span class="s1">)</span>
        <span class="s1">est_vect = np.column_stack((mu1_data</span><span class="s2">, </span><span class="s1">sig1_data</span><span class="s2">,</span>
                                    <span class="s1">mu2_data</span><span class="s2">, </span><span class="s1">sig2_data</span><span class="s2">, </span><span class="s1">correl_data))</span>
        <span class="s1">eta_star = self._modif_newton(x0</span><span class="s2">, </span><span class="s1">est_vect</span><span class="s2">, </span><span class="s1">weights0)</span>
        <span class="s1">denom = </span><span class="s3">1. </span><span class="s1">+ np.dot(est_vect</span><span class="s2">, </span><span class="s1">eta_star)</span>
        <span class="s1">self.new_weights = </span><span class="s3">1. </span><span class="s1">/ nobs * </span><span class="s3">1. </span><span class="s1">/ denom</span>
        <span class="s1">llr = np.sum(np.log(nobs * self.new_weights))</span>
        <span class="s2">return </span><span class="s1">-</span><span class="s3">2 </span><span class="s1">* llr</span>

    <span class="s2">def </span><span class="s1">_ci_limits_corr(self</span><span class="s2">, </span><span class="s1">corr):</span>
        <span class="s2">return </span><span class="s1">self.test_corr(corr)[</span><span class="s3">0</span><span class="s1">] - self.r0</span>


<span class="s2">class </span><span class="s1">DescStatUV(_OptFuncts):</span>
    <span class="s0">&quot;&quot;&quot; 
    A class to compute confidence intervals and hypothesis tests involving 
    mean, variance, kurtosis and skewness of a univariate random variable. 
 
    Parameters 
    ---------- 
    endog : 1darray 
        Data to be analyzed 
 
    Attributes 
    ---------- 
    endog : 1darray 
        Data to be analyzed 
 
    nobs : float 
        Number of observations 
    &quot;&quot;&quot;</span>

    <span class="s2">def </span><span class="s1">__init__(self</span><span class="s2">, </span><span class="s1">endog):</span>
        <span class="s1">self.endog = np.squeeze(endog)</span>
        <span class="s1">self.nobs = endog.shape[</span><span class="s3">0</span><span class="s1">]</span>

    <span class="s2">def </span><span class="s1">test_mean(self</span><span class="s2">, </span><span class="s1">mu0</span><span class="s2">, </span><span class="s1">return_weights=</span><span class="s2">False</span><span class="s1">):</span>
        <span class="s0">&quot;&quot;&quot; 
        Returns - 2 x log-likelihood ratio, p-value and weights 
        for a hypothesis test of the mean. 
 
        Parameters 
        ---------- 
        mu0 : float 
            Mean value to be tested 
 
        return_weights : bool 
            If return_weights is True the function returns 
            the weights of the observations under the null hypothesis. 
            Default is False 
 
        Returns 
        ------- 
        test_results : tuple 
            The log-likelihood ratio and p-value of mu0 
        &quot;&quot;&quot;</span>
        <span class="s1">self.mu0 = mu0</span>
        <span class="s1">endog = self.endog</span>
        <span class="s1">nobs = self.nobs</span>
        <span class="s1">eta_min = (</span><span class="s3">1. </span><span class="s1">- (</span><span class="s3">1. </span><span class="s1">/ nobs)) / (self.mu0 - max(endog))</span>
        <span class="s1">eta_max = (</span><span class="s3">1. </span><span class="s1">- (</span><span class="s3">1. </span><span class="s1">/ nobs)) / (self.mu0 - min(endog))</span>
        <span class="s1">eta_star = optimize.brentq(self._find_eta</span><span class="s2">, </span><span class="s1">eta_min</span><span class="s2">, </span><span class="s1">eta_max)</span>
        <span class="s1">new_weights = (</span><span class="s3">1. </span><span class="s1">/ nobs) * </span><span class="s3">1. </span><span class="s1">/ (</span><span class="s3">1. </span><span class="s1">+ eta_star * (endog - self.mu0))</span>
        <span class="s1">llr = -</span><span class="s3">2 </span><span class="s1">* np.sum(np.log(nobs * new_weights))</span>
        <span class="s2">if </span><span class="s1">return_weights:</span>
            <span class="s2">return </span><span class="s1">llr</span><span class="s2">, </span><span class="s1">chi2.sf(llr</span><span class="s2">, </span><span class="s3">1</span><span class="s1">)</span><span class="s2">, </span><span class="s1">new_weights</span>
        <span class="s2">else</span><span class="s1">:</span>
            <span class="s2">return </span><span class="s1">llr</span><span class="s2">, </span><span class="s1">chi2.sf(llr</span><span class="s2">, </span><span class="s3">1</span><span class="s1">)</span>

    <span class="s2">def </span><span class="s1">ci_mean(self</span><span class="s2">, </span><span class="s1">sig=</span><span class="s3">.05</span><span class="s2">, </span><span class="s1">method=</span><span class="s5">'gamma'</span><span class="s2">, </span><span class="s1">epsilon=</span><span class="s3">10 </span><span class="s1">** -</span><span class="s3">8</span><span class="s2">,</span>
                 <span class="s1">gamma_low=-</span><span class="s3">10 </span><span class="s1">** </span><span class="s3">10</span><span class="s2">, </span><span class="s1">gamma_high=</span><span class="s3">10 </span><span class="s1">** </span><span class="s3">10</span><span class="s1">):</span>
        <span class="s0">&quot;&quot;&quot; 
        Returns the confidence interval for the mean. 
 
        Parameters 
        ---------- 
        sig : float 
            significance level. Default is .05 
 
        method : str 
            Root finding method,  Can be 'nested-brent' or 
            'gamma'.  Default is 'gamma' 
 
            'gamma' Tries to solve for the gamma parameter in the 
            Lagrange (see Owen pg 22) and then determine the weights. 
 
            'nested brent' uses brents method to find the confidence 
            intervals but must maximize the likelihood ratio on every 
            iteration. 
 
            gamma is generally much faster.  If the optimizations does not 
            converge, try expanding the gamma_high and gamma_low 
            variable. 
 
        gamma_low : float 
            Lower bound for gamma when finding lower limit. 
            If function returns f(a) and f(b) must have different signs, 
            consider lowering gamma_low. 
 
        gamma_high : float 
            Upper bound for gamma when finding upper limit. 
            If function returns f(a) and f(b) must have different signs, 
            consider raising gamma_high. 
 
        epsilon : float 
            When using 'nested-brent', amount to decrease (increase) 
            from the maximum (minimum) of the data when 
            starting the search.  This is to protect against the 
            likelihood ratio being zero at the maximum (minimum) 
            value of the data.  If data is very small in absolute value 
            (&lt;10 ``**`` -6) consider shrinking epsilon 
 
            When using 'gamma', amount to decrease (increase) the 
            minimum (maximum) by to start the search for gamma. 
            If function returns f(a) and f(b) must have different signs, 
            consider lowering epsilon. 
 
        Returns 
        ------- 
        Interval : tuple 
            Confidence interval for the mean 
        &quot;&quot;&quot;</span>
        <span class="s1">endog = self.endog</span>
        <span class="s1">sig = </span><span class="s3">1 </span><span class="s1">- sig</span>
        <span class="s2">if </span><span class="s1">method == </span><span class="s5">'nested-brent'</span><span class="s1">:</span>
            <span class="s1">self.r0 = chi2.ppf(sig</span><span class="s2">, </span><span class="s3">1</span><span class="s1">)</span>
            <span class="s1">middle = np.mean(endog)</span>
            <span class="s1">epsilon_u = (max(endog) - np.mean(endog)) * epsilon</span>
            <span class="s1">epsilon_l = (np.mean(endog) - min(endog)) * epsilon</span>
            <span class="s1">ulim = optimize.brentq(self._ci_limits_mu</span><span class="s2">, </span><span class="s1">middle</span><span class="s2">,</span>
                <span class="s1">max(endog) - epsilon_u)</span>
            <span class="s1">llim = optimize.brentq(self._ci_limits_mu</span><span class="s2">, </span><span class="s1">middle</span><span class="s2">,</span>
                <span class="s1">min(endog) + epsilon_l)</span>
            <span class="s2">return </span><span class="s1">llim</span><span class="s2">, </span><span class="s1">ulim</span>

        <span class="s2">if </span><span class="s1">method == </span><span class="s5">'gamma'</span><span class="s1">:</span>
            <span class="s1">self.r0 = chi2.ppf(sig</span><span class="s2">, </span><span class="s3">1</span><span class="s1">)</span>
            <span class="s1">gamma_star_l = optimize.brentq(self._find_gamma</span><span class="s2">, </span><span class="s1">gamma_low</span><span class="s2">,</span>
                <span class="s1">min(endog) - epsilon)</span>
            <span class="s1">gamma_star_u = optimize.brentq(self._find_gamma</span><span class="s2">, </span><span class="s1">\</span>
                         <span class="s1">max(endog) + epsilon</span><span class="s2">, </span><span class="s1">gamma_high)</span>
            <span class="s1">weights_low = ((endog - gamma_star_l) ** -</span><span class="s3">1</span><span class="s1">) / \</span>
                <span class="s1">np.sum((endog - gamma_star_l) ** -</span><span class="s3">1</span><span class="s1">)</span>
            <span class="s1">weights_high = ((endog - gamma_star_u) ** -</span><span class="s3">1</span><span class="s1">) / \</span>
                <span class="s1">np.sum((endog - gamma_star_u) ** -</span><span class="s3">1</span><span class="s1">)</span>
            <span class="s1">mu_low = np.sum(weights_low * endog)</span>
            <span class="s1">mu_high = np.sum(weights_high * endog)</span>
            <span class="s2">return </span><span class="s1">mu_low</span><span class="s2">,  </span><span class="s1">mu_high</span>

    <span class="s2">def </span><span class="s1">test_var(self</span><span class="s2">, </span><span class="s1">sig2_0</span><span class="s2">, </span><span class="s1">return_weights=</span><span class="s2">False</span><span class="s1">):</span>
        <span class="s0">&quot;&quot;&quot; 
        Returns  -2 x log-likelihood ratio and the p-value for the 
        hypothesized variance 
 
        Parameters 
        ---------- 
        sig2_0 : float 
            Hypothesized variance to be tested 
 
        return_weights : bool 
            If True, returns the weights that maximize the 
            likelihood of observing sig2_0. Default is False 
 
        Returns 
        ------- 
        test_results : tuple 
            The  log-likelihood ratio and the p_value  of sig2_0 
 
        Examples 
        -------- 
        &gt;&gt;&gt; import numpy as np 
        &gt;&gt;&gt; import statsmodels.api as sm 
        &gt;&gt;&gt; random_numbers = np.random.standard_normal(1000)*100 
        &gt;&gt;&gt; el_analysis = sm.emplike.DescStat(random_numbers) 
        &gt;&gt;&gt; hyp_test = el_analysis.test_var(9500) 
        &quot;&quot;&quot;</span>
        <span class="s1">self.sig2_0 = sig2_0</span>
        <span class="s1">mu_max = max(self.endog)</span>
        <span class="s1">mu_min = min(self.endog)</span>
        <span class="s1">llr = optimize.fminbound(self._opt_var</span><span class="s2">, </span><span class="s1">mu_min</span><span class="s2">, </span><span class="s1">mu_max</span><span class="s2">, </span><span class="s1">\</span>
                                 <span class="s1">full_output=</span><span class="s3">1</span><span class="s1">)[</span><span class="s3">1</span><span class="s1">]</span>
        <span class="s1">p_val = chi2.sf(llr</span><span class="s2">, </span><span class="s3">1</span><span class="s1">)</span>
        <span class="s2">if </span><span class="s1">return_weights:</span>
            <span class="s2">return </span><span class="s1">llr</span><span class="s2">, </span><span class="s1">p_val</span><span class="s2">, </span><span class="s1">self.new_weights.T</span>
        <span class="s2">else</span><span class="s1">:</span>
            <span class="s2">return </span><span class="s1">llr</span><span class="s2">, </span><span class="s1">p_val</span>

    <span class="s2">def </span><span class="s1">ci_var(self</span><span class="s2">, </span><span class="s1">lower_bound=</span><span class="s2">None, </span><span class="s1">upper_bound=</span><span class="s2">None, </span><span class="s1">sig=</span><span class="s3">.05</span><span class="s1">):</span>
        <span class="s0">&quot;&quot;&quot; 
        Returns the confidence interval for the variance. 
 
        Parameters 
        ---------- 
        lower_bound : float 
            The minimum value the lower confidence interval can 
            take. The p-value from test_var(lower_bound) must be lower 
            than 1 - significance level. Default is .99 confidence 
            limit assuming normality 
 
        upper_bound : float 
            The maximum value the upper confidence interval 
            can take. The p-value from test_var(upper_bound) must be lower 
            than 1 - significance level.  Default is .99 confidence 
            limit assuming normality 
 
        sig : float 
            The significance level. Default is .05 
 
        Returns 
        ------- 
        Interval : tuple 
            Confidence interval for the variance 
 
        Examples 
        -------- 
        &gt;&gt;&gt; import numpy as np 
        &gt;&gt;&gt; import statsmodels.api as sm 
        &gt;&gt;&gt; random_numbers = np.random.standard_normal(100) 
        &gt;&gt;&gt; el_analysis = sm.emplike.DescStat(random_numbers) 
        &gt;&gt;&gt; el_analysis.ci_var() 
        (0.7539322567470305, 1.229998852496268) 
        &gt;&gt;&gt; el_analysis.ci_var(.5, 2) 
        (0.7539322567469926, 1.2299988524962664) 
 
        Notes 
        ----- 
        If the function returns the error f(a) and f(b) must have 
        different signs, consider lowering lower_bound and raising 
        upper_bound. 
        &quot;&quot;&quot;</span>
        <span class="s1">endog = self.endog</span>
        <span class="s2">if </span><span class="s1">upper_bound </span><span class="s2">is None</span><span class="s1">:</span>
            <span class="s1">upper_bound = ((self.nobs - </span><span class="s3">1</span><span class="s1">) * endog.var()) / \</span>
              <span class="s1">(chi2.ppf(</span><span class="s3">.0001</span><span class="s2">, </span><span class="s1">self.nobs - </span><span class="s3">1</span><span class="s1">))</span>
        <span class="s2">if </span><span class="s1">lower_bound </span><span class="s2">is None</span><span class="s1">:</span>
            <span class="s1">lower_bound = ((self.nobs - </span><span class="s3">1</span><span class="s1">) * endog.var()) / \</span>
              <span class="s1">(chi2.ppf(</span><span class="s3">.9999</span><span class="s2">, </span><span class="s1">self.nobs - </span><span class="s3">1</span><span class="s1">))</span>
        <span class="s1">self.r0 = chi2.ppf(</span><span class="s3">1 </span><span class="s1">- sig</span><span class="s2">, </span><span class="s3">1</span><span class="s1">)</span>
        <span class="s1">llim = optimize.brentq(self._ci_limits_var</span><span class="s2">, </span><span class="s1">lower_bound</span><span class="s2">, </span><span class="s1">endog.var())</span>
        <span class="s1">ulim = optimize.brentq(self._ci_limits_var</span><span class="s2">, </span><span class="s1">endog.var()</span><span class="s2">, </span><span class="s1">upper_bound)</span>
        <span class="s2">return </span><span class="s1">llim</span><span class="s2">, </span><span class="s1">ulim</span>

    <span class="s2">def </span><span class="s1">plot_contour(self</span><span class="s2">, </span><span class="s1">mu_low</span><span class="s2">, </span><span class="s1">mu_high</span><span class="s2">, </span><span class="s1">var_low</span><span class="s2">, </span><span class="s1">var_high</span><span class="s2">, </span><span class="s1">mu_step</span><span class="s2">,</span>
                        <span class="s1">var_step</span><span class="s2">,</span>
                        <span class="s1">levs=[</span><span class="s3">.2</span><span class="s2">, </span><span class="s3">.1</span><span class="s2">, </span><span class="s3">.05</span><span class="s2">, </span><span class="s3">.01</span><span class="s2">, </span><span class="s3">.001</span><span class="s1">]):</span>
        <span class="s0">&quot;&quot;&quot; 
        Returns a plot of the confidence region for a univariate 
        mean and variance. 
 
        Parameters 
        ---------- 
        mu_low : float 
            Lowest value of the mean to plot 
 
        mu_high : float 
            Highest value of the mean to plot 
 
        var_low : float 
            Lowest value of the variance to plot 
 
        var_high : float 
            Highest value of the variance to plot 
 
        mu_step : float 
            Increments to evaluate the mean 
 
        var_step : float 
            Increments to evaluate the mean 
 
        levs : list 
            Which values of significance the contour lines will be drawn. 
            Default is [.2, .1, .05, .01, .001] 
 
        Returns 
        ------- 
        Figure 
            The contour plot 
        &quot;&quot;&quot;</span>
        <span class="s1">fig</span><span class="s2">, </span><span class="s1">ax = utils.create_mpl_ax()</span>
        <span class="s1">ax.set_ylabel(</span><span class="s5">'Variance'</span><span class="s1">)</span>
        <span class="s1">ax.set_xlabel(</span><span class="s5">'Mean'</span><span class="s1">)</span>
        <span class="s1">mu_vect = list(np.arange(mu_low</span><span class="s2">, </span><span class="s1">mu_high</span><span class="s2">, </span><span class="s1">mu_step))</span>
        <span class="s1">var_vect = list(np.arange(var_low</span><span class="s2">, </span><span class="s1">var_high</span><span class="s2">, </span><span class="s1">var_step))</span>
        <span class="s1">z = []</span>
        <span class="s2">for </span><span class="s1">sig0 </span><span class="s2">in </span><span class="s1">var_vect:</span>
            <span class="s1">self.sig2_0 = sig0</span>
            <span class="s2">for </span><span class="s1">mu0 </span><span class="s2">in </span><span class="s1">mu_vect:</span>
                <span class="s1">z.append(self._opt_var(mu0</span><span class="s2">, </span><span class="s1">pval=</span><span class="s2">True</span><span class="s1">))</span>
        <span class="s1">z = np.asarray(z).reshape(len(var_vect)</span><span class="s2">, </span><span class="s1">len(mu_vect))</span>
        <span class="s1">ax.contour(mu_vect</span><span class="s2">, </span><span class="s1">var_vect</span><span class="s2">, </span><span class="s1">z</span><span class="s2">, </span><span class="s1">levels=levs)</span>
        <span class="s2">return </span><span class="s1">fig</span>

    <span class="s2">def </span><span class="s1">test_skew(self</span><span class="s2">, </span><span class="s1">skew0</span><span class="s2">, </span><span class="s1">return_weights=</span><span class="s2">False</span><span class="s1">):</span>
        <span class="s0">&quot;&quot;&quot; 
        Returns  -2 x log-likelihood and p-value for the hypothesized 
        skewness. 
 
        Parameters 
        ---------- 
        skew0 : float 
            Skewness value to be tested 
 
        return_weights : bool 
            If True, function also returns the weights that 
            maximize the likelihood ratio. Default is False. 
 
        Returns 
        ------- 
        test_results : tuple 
            The log-likelihood ratio and p_value of skew0 
        &quot;&quot;&quot;</span>
        <span class="s1">self.skew0 = skew0</span>
        <span class="s1">start_nuisance = np.array([self.endog.mean()</span><span class="s2">,</span>
                                       <span class="s1">self.endog.var()])</span>

        <span class="s1">llr = optimize.fmin_powell(self._opt_skew</span><span class="s2">, </span><span class="s1">start_nuisance</span><span class="s2">,</span>
                                     <span class="s1">full_output=</span><span class="s3">1</span><span class="s2">, </span><span class="s1">disp=</span><span class="s3">0</span><span class="s1">)[</span><span class="s3">1</span><span class="s1">]</span>
        <span class="s1">p_val = chi2.sf(llr</span><span class="s2">, </span><span class="s3">1</span><span class="s1">)</span>
        <span class="s2">if </span><span class="s1">return_weights:</span>
            <span class="s2">return </span><span class="s1">llr</span><span class="s2">, </span><span class="s1">p_val</span><span class="s2">,  </span><span class="s1">self.new_weights.T</span>
        <span class="s2">return </span><span class="s1">llr</span><span class="s2">, </span><span class="s1">p_val</span>

    <span class="s2">def </span><span class="s1">test_kurt(self</span><span class="s2">, </span><span class="s1">kurt0</span><span class="s2">, </span><span class="s1">return_weights=</span><span class="s2">False</span><span class="s1">):</span>
        <span class="s0">&quot;&quot;&quot; 
        Returns -2 x log-likelihood and the p-value for the hypothesized 
        kurtosis. 
 
        Parameters 
        ---------- 
        kurt0 : float 
            Kurtosis value to be tested 
 
        return_weights : bool 
            If True, function also returns the weights that 
            maximize the likelihood ratio. Default is False. 
 
        Returns 
        ------- 
        test_results : tuple 
            The log-likelihood ratio and p-value of kurt0 
        &quot;&quot;&quot;</span>
        <span class="s1">self.kurt0 = kurt0</span>
        <span class="s1">start_nuisance = np.array([self.endog.mean()</span><span class="s2">,</span>
                                       <span class="s1">self.endog.var()])</span>

        <span class="s1">llr = optimize.fmin_powell(self._opt_kurt</span><span class="s2">, </span><span class="s1">start_nuisance</span><span class="s2">,</span>
                                     <span class="s1">full_output=</span><span class="s3">1</span><span class="s2">, </span><span class="s1">disp=</span><span class="s3">0</span><span class="s1">)[</span><span class="s3">1</span><span class="s1">]</span>
        <span class="s1">p_val = chi2.sf(llr</span><span class="s2">, </span><span class="s3">1</span><span class="s1">)</span>
        <span class="s2">if </span><span class="s1">return_weights:</span>
            <span class="s2">return </span><span class="s1">llr</span><span class="s2">, </span><span class="s1">p_val</span><span class="s2">, </span><span class="s1">self.new_weights.T</span>
        <span class="s2">return </span><span class="s1">llr</span><span class="s2">, </span><span class="s1">p_val</span>

    <span class="s2">def </span><span class="s1">test_joint_skew_kurt(self</span><span class="s2">, </span><span class="s1">skew0</span><span class="s2">, </span><span class="s1">kurt0</span><span class="s2">, </span><span class="s1">return_weights=</span><span class="s2">False</span><span class="s1">):</span>
        <span class="s0">&quot;&quot;&quot; 
        Returns - 2 x log-likelihood and the p-value for the joint 
        hypothesis test for skewness and kurtosis 
 
        Parameters 
        ---------- 
        skew0 : float 
            Skewness value to be tested 
        kurt0 : float 
            Kurtosis value to be tested 
 
        return_weights : bool 
            If True, function also returns the weights that 
            maximize the likelihood ratio. Default is False. 
 
        Returns 
        ------- 
        test_results : tuple 
            The log-likelihood ratio and p-value  of the joint hypothesis test. 
        &quot;&quot;&quot;</span>
        <span class="s1">self.skew0 = skew0</span>
        <span class="s1">self.kurt0 = kurt0</span>
        <span class="s1">start_nuisance = np.array([self.endog.mean()</span><span class="s2">,</span>
                                       <span class="s1">self.endog.var()])</span>

        <span class="s1">llr = optimize.fmin_powell(self._opt_skew_kurt</span><span class="s2">, </span><span class="s1">start_nuisance</span><span class="s2">,</span>
                                     <span class="s1">full_output=</span><span class="s3">1</span><span class="s2">, </span><span class="s1">disp=</span><span class="s3">0</span><span class="s1">)[</span><span class="s3">1</span><span class="s1">]</span>
        <span class="s1">p_val = chi2.sf(llr</span><span class="s2">, </span><span class="s3">2</span><span class="s1">)</span>
        <span class="s2">if </span><span class="s1">return_weights:</span>
            <span class="s2">return </span><span class="s1">llr</span><span class="s2">, </span><span class="s1">p_val</span><span class="s2">, </span><span class="s1">self.new_weights.T</span>
        <span class="s2">return </span><span class="s1">llr</span><span class="s2">, </span><span class="s1">p_val</span>

    <span class="s2">def </span><span class="s1">ci_skew(self</span><span class="s2">, </span><span class="s1">sig=</span><span class="s3">.05</span><span class="s2">, </span><span class="s1">upper_bound=</span><span class="s2">None, </span><span class="s1">lower_bound=</span><span class="s2">None</span><span class="s1">):</span>
        <span class="s0">&quot;&quot;&quot; 
        Returns the confidence interval for skewness. 
 
        Parameters 
        ---------- 
        sig : float 
            The significance level.  Default is .05 
 
        upper_bound : float 
            Maximum value of skewness the upper limit can be. 
            Default is .99 confidence limit assuming normality. 
 
        lower_bound : float 
            Minimum value of skewness the lower limit can be. 
            Default is .99 confidence level assuming normality. 
 
        Returns 
        ------- 
        Interval : tuple 
            Confidence interval for the skewness 
 
        Notes 
        ----- 
        If function returns f(a) and f(b) must have different signs, consider 
        expanding lower and upper bounds 
        &quot;&quot;&quot;</span>
        <span class="s1">nobs = self.nobs</span>
        <span class="s1">endog = self.endog</span>
        <span class="s2">if </span><span class="s1">upper_bound </span><span class="s2">is None</span><span class="s1">:</span>
            <span class="s1">upper_bound = skew(endog) + \</span>
            <span class="s3">2.5 </span><span class="s1">* ((</span><span class="s3">6. </span><span class="s1">* nobs * (nobs - </span><span class="s3">1.</span><span class="s1">)) / \</span>
              <span class="s1">((nobs - </span><span class="s3">2.</span><span class="s1">) * (nobs + </span><span class="s3">1.</span><span class="s1">) * \</span>
               <span class="s1">(nobs + </span><span class="s3">3.</span><span class="s1">))) ** </span><span class="s3">.5</span>
        <span class="s2">if </span><span class="s1">lower_bound </span><span class="s2">is None</span><span class="s1">:</span>
            <span class="s1">lower_bound = skew(endog) - \</span>
            <span class="s3">2.5 </span><span class="s1">* ((</span><span class="s3">6. </span><span class="s1">* nobs * (nobs - </span><span class="s3">1.</span><span class="s1">)) / \</span>
              <span class="s1">((nobs - </span><span class="s3">2.</span><span class="s1">) * (nobs + </span><span class="s3">1.</span><span class="s1">) * \</span>
               <span class="s1">(nobs + </span><span class="s3">3.</span><span class="s1">))) ** </span><span class="s3">.5</span>
        <span class="s1">self.r0 = chi2.ppf(</span><span class="s3">1 </span><span class="s1">- sig</span><span class="s2">, </span><span class="s3">1</span><span class="s1">)</span>
        <span class="s1">llim = optimize.brentq(self._ci_limits_skew</span><span class="s2">, </span><span class="s1">lower_bound</span><span class="s2">, </span><span class="s1">skew(endog))</span>
        <span class="s1">ulim = optimize.brentq(self._ci_limits_skew</span><span class="s2">, </span><span class="s1">skew(endog)</span><span class="s2">, </span><span class="s1">upper_bound)</span>
        <span class="s2">return </span><span class="s1">llim</span><span class="s2">, </span><span class="s1">ulim</span>

    <span class="s2">def </span><span class="s1">ci_kurt(self</span><span class="s2">, </span><span class="s1">sig=</span><span class="s3">.05</span><span class="s2">, </span><span class="s1">upper_bound=</span><span class="s2">None, </span><span class="s1">lower_bound=</span><span class="s2">None</span><span class="s1">):</span>
        <span class="s0">&quot;&quot;&quot; 
        Returns the confidence interval for kurtosis. 
 
        Parameters 
        ---------- 
 
        sig : float 
            The significance level.  Default is .05 
 
        upper_bound : float 
            Maximum value of kurtosis the upper limit can be. 
            Default is .99 confidence limit assuming normality. 
 
        lower_bound : float 
            Minimum value of kurtosis the lower limit can be. 
            Default is .99 confidence limit assuming normality. 
 
        Returns 
        ------- 
        Interval : tuple 
            Lower and upper confidence limit 
 
        Notes 
        ----- 
        For small n, upper_bound and lower_bound may have to be 
        provided by the user.  Consider using test_kurt to find 
        values close to the desired significance level. 
 
        If function returns f(a) and f(b) must have different signs, consider 
        expanding the bounds. 
        &quot;&quot;&quot;</span>
        <span class="s1">endog = self.endog</span>
        <span class="s1">nobs = self.nobs</span>
        <span class="s2">if </span><span class="s1">upper_bound </span><span class="s2">is None</span><span class="s1">:</span>
            <span class="s1">upper_bound = kurtosis(endog) + \</span>
            <span class="s1">(</span><span class="s3">2.5 </span><span class="s1">* (</span><span class="s3">2. </span><span class="s1">* ((</span><span class="s3">6. </span><span class="s1">* nobs * (nobs - </span><span class="s3">1.</span><span class="s1">)) / \</span>
              <span class="s1">((nobs - </span><span class="s3">2.</span><span class="s1">) * (nobs + </span><span class="s3">1.</span><span class="s1">) * \</span>
               <span class="s1">(nobs + </span><span class="s3">3.</span><span class="s1">))) ** </span><span class="s3">.5</span><span class="s1">) * \</span>
               <span class="s1">(((nobs ** </span><span class="s3">2.</span><span class="s1">) - </span><span class="s3">1.</span><span class="s1">) / ((nobs - </span><span class="s3">3.</span><span class="s1">) *\</span>
                 <span class="s1">(nobs + </span><span class="s3">5.</span><span class="s1">))) ** </span><span class="s3">.5</span><span class="s1">)</span>
        <span class="s2">if </span><span class="s1">lower_bound </span><span class="s2">is None</span><span class="s1">:</span>
            <span class="s1">lower_bound = kurtosis(endog) - \</span>
            <span class="s1">(</span><span class="s3">2.5 </span><span class="s1">* (</span><span class="s3">2. </span><span class="s1">* ((</span><span class="s3">6. </span><span class="s1">* nobs * (nobs - </span><span class="s3">1.</span><span class="s1">)) / \</span>
              <span class="s1">((nobs - </span><span class="s3">2.</span><span class="s1">) * (nobs + </span><span class="s3">1.</span><span class="s1">) * \</span>
               <span class="s1">(nobs + </span><span class="s3">3.</span><span class="s1">))) ** </span><span class="s3">.5</span><span class="s1">) * \</span>
               <span class="s1">(((nobs ** </span><span class="s3">2.</span><span class="s1">) - </span><span class="s3">1.</span><span class="s1">) / ((nobs - </span><span class="s3">3.</span><span class="s1">) *\</span>
                 <span class="s1">(nobs + </span><span class="s3">5.</span><span class="s1">))) ** </span><span class="s3">.5</span><span class="s1">)</span>
        <span class="s1">self.r0 = chi2.ppf(</span><span class="s3">1 </span><span class="s1">- sig</span><span class="s2">, </span><span class="s3">1</span><span class="s1">)</span>
        <span class="s1">llim = optimize.brentq(self._ci_limits_kurt</span><span class="s2">, </span><span class="s1">lower_bound</span><span class="s2">, </span><span class="s1">\</span>
                             <span class="s1">kurtosis(endog))</span>
        <span class="s1">ulim = optimize.brentq(self._ci_limits_kurt</span><span class="s2">, </span><span class="s1">kurtosis(endog)</span><span class="s2">, </span><span class="s1">\</span>
                             <span class="s1">upper_bound)</span>
        <span class="s2">return </span><span class="s1">llim</span><span class="s2">, </span><span class="s1">ulim</span>


<span class="s2">class </span><span class="s1">DescStatMV(_OptFuncts):</span>
    <span class="s0">&quot;&quot;&quot; 
    A class for conducting inference on multivariate means and correlation. 
 
    Parameters 
    ---------- 
    endog : ndarray 
        Data to be analyzed 
 
    Attributes 
    ---------- 
    endog : ndarray 
        Data to be analyzed 
 
    nobs : float 
        Number of observations 
    &quot;&quot;&quot;</span>

    <span class="s2">def </span><span class="s1">__init__(self</span><span class="s2">, </span><span class="s1">endog):</span>
        <span class="s1">self.endog = endog</span>
        <span class="s1">self.nobs = endog.shape[</span><span class="s3">0</span><span class="s1">]</span>

    <span class="s2">def </span><span class="s1">mv_test_mean(self</span><span class="s2">, </span><span class="s1">mu_array</span><span class="s2">, </span><span class="s1">return_weights=</span><span class="s2">False</span><span class="s1">):</span>
        <span class="s0">&quot;&quot;&quot; 
        Returns -2 x log likelihood and the p-value 
        for a multivariate hypothesis test of the mean 
 
        Parameters 
        ---------- 
        mu_array  : 1d array 
            Hypothesized values for the mean.  Must have same number of 
            elements as columns in endog 
 
        return_weights : bool 
            If True, returns the weights that maximize the 
            likelihood of mu_array. Default is False. 
 
        Returns 
        ------- 
        test_results : tuple 
            The log-likelihood ratio and p-value for mu_array 
        &quot;&quot;&quot;</span>
        <span class="s1">endog = self.endog</span>
        <span class="s1">nobs = self.nobs</span>
        <span class="s2">if </span><span class="s1">len(mu_array) != endog.shape[</span><span class="s3">1</span><span class="s1">]:</span>
            <span class="s2">raise </span><span class="s1">ValueError(</span><span class="s5">'mu_array must have the same number of '</span>
                             <span class="s5">'elements as the columns of the data.'</span><span class="s1">)</span>
        <span class="s1">mu_array = mu_array.reshape(</span><span class="s3">1</span><span class="s2">, </span><span class="s1">endog.shape[</span><span class="s3">1</span><span class="s1">])</span>
        <span class="s1">means = np.ones((endog.shape[</span><span class="s3">0</span><span class="s1">]</span><span class="s2">, </span><span class="s1">endog.shape[</span><span class="s3">1</span><span class="s1">]))</span>
        <span class="s1">means = mu_array * means</span>
        <span class="s1">est_vect = endog - means</span>
        <span class="s1">start_vals = </span><span class="s3">1. </span><span class="s1">/ nobs * np.ones(endog.shape[</span><span class="s3">1</span><span class="s1">])</span>
        <span class="s1">eta_star = self._modif_newton(start_vals</span><span class="s2">, </span><span class="s1">est_vect</span><span class="s2">,</span>
                                      <span class="s1">np.ones(nobs) * (</span><span class="s3">1. </span><span class="s1">/ nobs))</span>
        <span class="s1">denom = </span><span class="s3">1 </span><span class="s1">+ np.dot(eta_star</span><span class="s2">, </span><span class="s1">est_vect.T)</span>
        <span class="s1">self.new_weights = </span><span class="s3">1 </span><span class="s1">/ nobs * </span><span class="s3">1 </span><span class="s1">/ denom</span>
        <span class="s1">llr = -</span><span class="s3">2 </span><span class="s1">* np.sum(np.log(nobs * self.new_weights))</span>
        <span class="s1">p_val = chi2.sf(llr</span><span class="s2">, </span><span class="s1">mu_array.shape[</span><span class="s3">1</span><span class="s1">])</span>
        <span class="s2">if </span><span class="s1">return_weights:</span>
            <span class="s2">return </span><span class="s1">llr</span><span class="s2">, </span><span class="s1">p_val</span><span class="s2">,  </span><span class="s1">self.new_weights.T</span>
        <span class="s2">else</span><span class="s1">:</span>
            <span class="s2">return </span><span class="s1">llr</span><span class="s2">, </span><span class="s1">p_val</span>

    <span class="s2">def </span><span class="s1">mv_mean_contour(self</span><span class="s2">, </span><span class="s1">mu1_low</span><span class="s2">, </span><span class="s1">mu1_upp</span><span class="s2">, </span><span class="s1">mu2_low</span><span class="s2">, </span><span class="s1">mu2_upp</span><span class="s2">, </span><span class="s1">step1</span><span class="s2">, </span><span class="s1">step2</span><span class="s2">,</span>
                        <span class="s1">levs=(</span><span class="s3">.001</span><span class="s2">, </span><span class="s3">.01</span><span class="s2">, </span><span class="s3">.05</span><span class="s2">, </span><span class="s3">.1</span><span class="s2">, </span><span class="s3">.2</span><span class="s1">)</span><span class="s2">, </span><span class="s1">var1_name=</span><span class="s2">None,</span>
                        <span class="s1">var2_name=</span><span class="s2">None, </span><span class="s1">plot_dta=</span><span class="s2">False</span><span class="s1">):</span>
        <span class="s0">&quot;&quot;&quot; 
        Creates a confidence region plot for the mean of bivariate data 
 
        Parameters 
        ---------- 
        m1_low : float 
            Minimum value of the mean for variable 1 
        m1_upp : float 
            Maximum value of the mean for variable 1 
        mu2_low : float 
            Minimum value of the mean for variable 2 
        mu2_upp : float 
            Maximum value of the mean for variable 2 
        step1 : float 
            Increment of evaluations for variable 1 
        step2 : float 
            Increment of evaluations for variable 2 
        levs : list 
            Levels to be drawn on the contour plot. 
            Default =  (.001, .01, .05, .1, .2) 
        plot_dta : bool 
            If True, makes a scatter plot of the data on 
            top of the contour plot. Defaultis False. 
        var1_name : str 
            Name of variable 1 to be plotted on the x-axis 
        var2_name : str 
            Name of variable 2 to be plotted on the y-axis 
 
        Notes 
        ----- 
        The smaller the step size, the more accurate the intervals 
        will be 
 
        If the function returns optimization failed, consider narrowing 
        the boundaries of the plot 
 
        Examples 
        -------- 
        &gt;&gt;&gt; import statsmodels.api as sm 
        &gt;&gt;&gt; two_rvs = np.random.standard_normal((20,2)) 
        &gt;&gt;&gt; el_analysis = sm.emplike.DescStat(two_rvs) 
        &gt;&gt;&gt; contourp = el_analysis.mv_mean_contour(-2, 2, -2, 2, .1, .1) 
        &gt;&gt;&gt; contourp.show() 
        &quot;&quot;&quot;</span>
        <span class="s2">if </span><span class="s1">self.endog.shape[</span><span class="s3">1</span><span class="s1">] != </span><span class="s3">2</span><span class="s1">:</span>
            <span class="s2">raise </span><span class="s1">ValueError(</span><span class="s5">'Data must contain exactly two variables'</span><span class="s1">)</span>
        <span class="s1">fig</span><span class="s2">, </span><span class="s1">ax = utils.create_mpl_ax()</span>
        <span class="s2">if </span><span class="s1">var2_name </span><span class="s2">is None</span><span class="s1">:</span>
            <span class="s1">ax.set_ylabel(</span><span class="s5">'Variable 2'</span><span class="s1">)</span>
        <span class="s2">else</span><span class="s1">:</span>
            <span class="s1">ax.set_ylabel(var2_name)</span>
        <span class="s2">if </span><span class="s1">var1_name </span><span class="s2">is None</span><span class="s1">:</span>
            <span class="s1">ax.set_xlabel(</span><span class="s5">'Variable 1'</span><span class="s1">)</span>
        <span class="s2">else</span><span class="s1">:</span>
            <span class="s1">ax.set_xlabel(var1_name)</span>
        <span class="s1">x = np.arange(mu1_low</span><span class="s2">, </span><span class="s1">mu1_upp</span><span class="s2">, </span><span class="s1">step1)</span>
        <span class="s1">y = np.arange(mu2_low</span><span class="s2">, </span><span class="s1">mu2_upp</span><span class="s2">, </span><span class="s1">step2)</span>
        <span class="s1">pairs = itertools.product(x</span><span class="s2">, </span><span class="s1">y)</span>
        <span class="s1">z = []</span>
        <span class="s2">for </span><span class="s1">i </span><span class="s2">in </span><span class="s1">pairs:</span>
            <span class="s1">z.append(self.mv_test_mean(np.asarray(i))[</span><span class="s3">0</span><span class="s1">])</span>
        <span class="s1">X</span><span class="s2">, </span><span class="s1">Y = np.meshgrid(x</span><span class="s2">, </span><span class="s1">y)</span>
        <span class="s1">z = np.asarray(z)</span>
        <span class="s1">z = z.reshape(X.shape[</span><span class="s3">1</span><span class="s1">]</span><span class="s2">, </span><span class="s1">Y.shape[</span><span class="s3">0</span><span class="s1">])</span>
        <span class="s1">ax.contour(x</span><span class="s2">, </span><span class="s1">y</span><span class="s2">, </span><span class="s1">z.T</span><span class="s2">, </span><span class="s1">levels=levs)</span>
        <span class="s2">if </span><span class="s1">plot_dta:</span>
            <span class="s1">ax.plot(self.endog[:</span><span class="s2">, </span><span class="s3">0</span><span class="s1">]</span><span class="s2">, </span><span class="s1">self.endog[:</span><span class="s2">, </span><span class="s3">1</span><span class="s1">]</span><span class="s2">, </span><span class="s5">'bo'</span><span class="s1">)</span>
        <span class="s2">return </span><span class="s1">fig</span>

    <span class="s2">def </span><span class="s1">test_corr(self</span><span class="s2">, </span><span class="s1">corr0</span><span class="s2">, </span><span class="s1">return_weights=</span><span class="s3">0</span><span class="s1">):</span>
        <span class="s0">&quot;&quot;&quot; 
        Returns -2 x log-likelihood ratio and  p-value for the 
        correlation coefficient between 2 variables 
 
        Parameters 
        ---------- 
        corr0 : float 
            Hypothesized value to be tested 
 
        return_weights : bool 
            If true, returns the weights that maximize 
            the log-likelihood at the hypothesized value 
        &quot;&quot;&quot;</span>
        <span class="s1">nobs = self.nobs</span>
        <span class="s1">endog = self.endog</span>
        <span class="s2">if </span><span class="s1">endog.shape[</span><span class="s3">1</span><span class="s1">] != </span><span class="s3">2</span><span class="s1">:</span>
            <span class="s2">raise </span><span class="s1">NotImplementedError(</span><span class="s5">'Correlation matrix not yet implemented'</span><span class="s1">)</span>
        <span class="s1">nuis0 = np.array([endog[:</span><span class="s2">, </span><span class="s3">0</span><span class="s1">].mean()</span><span class="s2">,</span>
                              <span class="s1">endog[:</span><span class="s2">, </span><span class="s3">0</span><span class="s1">].var()</span><span class="s2">,</span>
                              <span class="s1">endog[:</span><span class="s2">, </span><span class="s3">1</span><span class="s1">].mean()</span><span class="s2">,</span>
                              <span class="s1">endog[:</span><span class="s2">, </span><span class="s3">1</span><span class="s1">].var()])</span>

        <span class="s1">x0 = np.zeros(</span><span class="s3">5</span><span class="s1">)</span>
        <span class="s1">weights0 = np.array([</span><span class="s3">1. </span><span class="s1">/ nobs] * int(nobs))</span>
        <span class="s1">args = (corr0</span><span class="s2">, </span><span class="s1">endog</span><span class="s2">, </span><span class="s1">nobs</span><span class="s2">, </span><span class="s1">x0</span><span class="s2">, </span><span class="s1">weights0)</span>
        <span class="s1">llr = optimize.fmin(self._opt_correl</span><span class="s2">, </span><span class="s1">nuis0</span><span class="s2">, </span><span class="s1">args=args</span><span class="s2">,</span>
                                     <span class="s1">full_output=</span><span class="s3">1</span><span class="s2">, </span><span class="s1">disp=</span><span class="s3">0</span><span class="s1">)[</span><span class="s3">1</span><span class="s1">]</span>
        <span class="s1">p_val = chi2.sf(llr</span><span class="s2">, </span><span class="s3">1</span><span class="s1">)</span>
        <span class="s2">if </span><span class="s1">return_weights:</span>
            <span class="s2">return </span><span class="s1">llr</span><span class="s2">, </span><span class="s1">p_val</span><span class="s2">, </span><span class="s1">self.new_weights.T</span>
        <span class="s2">return </span><span class="s1">llr</span><span class="s2">, </span><span class="s1">p_val</span>

    <span class="s2">def </span><span class="s1">ci_corr(self</span><span class="s2">, </span><span class="s1">sig=</span><span class="s3">.05</span><span class="s2">, </span><span class="s1">upper_bound=</span><span class="s2">None, </span><span class="s1">lower_bound=</span><span class="s2">None</span><span class="s1">):</span>
        <span class="s0">&quot;&quot;&quot; 
        Returns the confidence intervals for the correlation coefficient 
 
        Parameters 
        ---------- 
        sig : float 
            The significance level.  Default is .05 
 
        upper_bound : float 
            Maximum value the upper confidence limit can be. 
            Default is  99% confidence limit assuming normality. 
 
        lower_bound : float 
            Minimum value the lower confidence limit can be. 
            Default is 99% confidence limit assuming normality. 
 
        Returns 
        ------- 
        interval : tuple 
            Confidence interval for the correlation 
        &quot;&quot;&quot;</span>
        <span class="s1">endog = self.endog</span>
        <span class="s1">nobs = self.nobs</span>
        <span class="s1">self.r0 = chi2.ppf(</span><span class="s3">1 </span><span class="s1">- sig</span><span class="s2">, </span><span class="s3">1</span><span class="s1">)</span>
        <span class="s1">point_est = np.corrcoef(endog[:</span><span class="s2">, </span><span class="s3">0</span><span class="s1">]</span><span class="s2">, </span><span class="s1">endog[:</span><span class="s2">, </span><span class="s3">1</span><span class="s1">])[</span><span class="s3">0</span><span class="s2">, </span><span class="s3">1</span><span class="s1">]</span>
        <span class="s2">if </span><span class="s1">upper_bound </span><span class="s2">is None</span><span class="s1">:</span>
            <span class="s1">upper_bound = min(</span><span class="s3">.999</span><span class="s2">, </span><span class="s1">point_est + \</span>
                          <span class="s3">2.5 </span><span class="s1">* ((</span><span class="s3">1. </span><span class="s1">- point_est ** </span><span class="s3">2.</span><span class="s1">) / \</span>
                          <span class="s1">(nobs - </span><span class="s3">2.</span><span class="s1">)) ** </span><span class="s3">.5</span><span class="s1">)</span>

        <span class="s2">if </span><span class="s1">lower_bound </span><span class="s2">is None</span><span class="s1">:</span>
            <span class="s1">lower_bound = max(- </span><span class="s3">.999</span><span class="s2">, </span><span class="s1">point_est - \</span>
                          <span class="s3">2.5 </span><span class="s1">* (np.sqrt((</span><span class="s3">1. </span><span class="s1">- point_est ** </span><span class="s3">2.</span><span class="s1">) / \</span>
                          <span class="s1">(nobs - </span><span class="s3">2.</span><span class="s1">))))</span>

        <span class="s1">llim = optimize.brenth(self._ci_limits_corr</span><span class="s2">, </span><span class="s1">lower_bound</span><span class="s2">, </span><span class="s1">point_est)</span>
        <span class="s1">ulim = optimize.brenth(self._ci_limits_corr</span><span class="s2">, </span><span class="s1">point_est</span><span class="s2">, </span><span class="s1">upper_bound)</span>
        <span class="s2">return </span><span class="s1">llim</span><span class="s2">, </span><span class="s1">ulim</span>
</pre>
</body>
</html>