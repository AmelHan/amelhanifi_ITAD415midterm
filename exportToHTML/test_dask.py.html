<html>
<head>
<title>test_dask.py</title>
<meta http-equiv="Content-Type" content="text/html; charset=utf-8">
<style type="text/css">
.s0 { color: #cc7832;}
.s1 { color: #a9b7c6;}
.s2 { color: #6a8759;}
.s3 { color: #808080;}
.s4 { color: #6897bb;}
</style>
</head>
<body bgcolor="#2b2b2b">
<table CELLSPACING=0 CELLPADDING=5 COLS=1 WIDTH="100%" BGCOLOR="#606060" >
<tr><td><center>
<font face="Arial, Helvetica" color="#000000">
test_dask.py</font>
</center></td></tr></table>
<pre><span class="s0">from </span><span class="s1">__future__ </span><span class="s0">import </span><span class="s1">print_function</span><span class="s0">, </span><span class="s1">division</span><span class="s0">, </span><span class="s1">absolute_import</span>
<span class="s0">import </span><span class="s1">os</span>
<span class="s0">import </span><span class="s1">warnings</span>

<span class="s0">import </span><span class="s1">pytest</span>
<span class="s0">from </span><span class="s1">random </span><span class="s0">import </span><span class="s1">random</span>
<span class="s0">from </span><span class="s1">uuid </span><span class="s0">import </span><span class="s1">uuid4</span>
<span class="s0">from </span><span class="s1">time </span><span class="s0">import </span><span class="s1">sleep</span>

<span class="s0">from </span><span class="s1">.. </span><span class="s0">import </span><span class="s1">Parallel</span><span class="s0">, </span><span class="s1">delayed</span><span class="s0">, </span><span class="s1">parallel_config</span>
<span class="s0">from </span><span class="s1">..parallel </span><span class="s0">import </span><span class="s1">ThreadingBackend</span><span class="s0">, </span><span class="s1">AutoBatchingMixin</span>
<span class="s0">from </span><span class="s1">.._dask </span><span class="s0">import </span><span class="s1">DaskDistributedBackend</span>

<span class="s1">distributed = pytest.importorskip(</span><span class="s2">'distributed'</span><span class="s1">)</span>
<span class="s1">dask = pytest.importorskip(</span><span class="s2">'dask'</span><span class="s1">)</span>

<span class="s3"># These imports need to be after the pytest.importorskip hence the noqa: E402</span>
<span class="s0">from </span><span class="s1">distributed </span><span class="s0">import </span><span class="s1">Client</span><span class="s0">, </span><span class="s1">LocalCluster</span><span class="s0">, </span><span class="s1">get_client  </span><span class="s3"># noqa: E402</span>
<span class="s0">from </span><span class="s1">distributed.metrics </span><span class="s0">import </span><span class="s1">time  </span><span class="s3"># noqa: E402</span>
<span class="s0">from </span><span class="s1">distributed.utils_test </span><span class="s0">import </span><span class="s1">cluster</span><span class="s0">, </span><span class="s1">inc  </span><span class="s3"># noqa: E402</span>


<span class="s0">def </span><span class="s1">noop(*args</span><span class="s0">, </span><span class="s1">**kwargs):</span>
    <span class="s0">pass</span>


<span class="s0">def </span><span class="s1">slow_raise_value_error(condition</span><span class="s0">, </span><span class="s1">duration=</span><span class="s4">0.05</span><span class="s1">):</span>
    <span class="s1">sleep(duration)</span>
    <span class="s0">if </span><span class="s1">condition:</span>
        <span class="s0">raise </span><span class="s1">ValueError(</span><span class="s2">&quot;condition evaluated to True&quot;</span><span class="s1">)</span>


<span class="s0">def </span><span class="s1">count_events(event_name</span><span class="s0">, </span><span class="s1">client):</span>
    <span class="s1">worker_events = client.run(</span><span class="s0">lambda </span><span class="s1">dask_worker: dask_worker.log)</span>
    <span class="s1">event_counts = {}</span>
    <span class="s0">for </span><span class="s1">w</span><span class="s0">, </span><span class="s1">events </span><span class="s0">in </span><span class="s1">worker_events.items():</span>
        <span class="s1">event_counts[w] = len([event </span><span class="s0">for </span><span class="s1">event </span><span class="s0">in </span><span class="s1">list(events)</span>
                               <span class="s0">if </span><span class="s1">event[</span><span class="s4">1</span><span class="s1">] == event_name])</span>
    <span class="s0">return </span><span class="s1">event_counts</span>


<span class="s0">def </span><span class="s1">test_simple(loop):</span>
    <span class="s0">with </span><span class="s1">cluster() </span><span class="s0">as </span><span class="s1">(s</span><span class="s0">, </span><span class="s1">[a</span><span class="s0">, </span><span class="s1">b]):</span>
        <span class="s0">with </span><span class="s1">Client(s[</span><span class="s2">'address'</span><span class="s1">]</span><span class="s0">, </span><span class="s1">loop=loop) </span><span class="s0">as </span><span class="s1">client:  </span><span class="s3"># noqa: F841</span>
            <span class="s0">with </span><span class="s1">parallel_config(backend=</span><span class="s2">'dask'</span><span class="s1">):</span>
                <span class="s1">seq = Parallel()(delayed(inc)(i) </span><span class="s0">for </span><span class="s1">i </span><span class="s0">in </span><span class="s1">range(</span><span class="s4">10</span><span class="s1">))</span>
                <span class="s0">assert </span><span class="s1">seq == [inc(i) </span><span class="s0">for </span><span class="s1">i </span><span class="s0">in </span><span class="s1">range(</span><span class="s4">10</span><span class="s1">)]</span>

                <span class="s0">with </span><span class="s1">pytest.raises(ValueError):</span>
                    <span class="s1">Parallel()(delayed(slow_raise_value_error)(i == </span><span class="s4">3</span><span class="s1">)</span>
                               <span class="s0">for </span><span class="s1">i </span><span class="s0">in </span><span class="s1">range(</span><span class="s4">10</span><span class="s1">))</span>

                <span class="s1">seq = Parallel()(delayed(inc)(i) </span><span class="s0">for </span><span class="s1">i </span><span class="s0">in </span><span class="s1">range(</span><span class="s4">10</span><span class="s1">))</span>
                <span class="s0">assert </span><span class="s1">seq == [inc(i) </span><span class="s0">for </span><span class="s1">i </span><span class="s0">in </span><span class="s1">range(</span><span class="s4">10</span><span class="s1">)]</span>


<span class="s0">def </span><span class="s1">test_dask_backend_uses_autobatching(loop):</span>
    <span class="s0">assert </span><span class="s1">(DaskDistributedBackend.compute_batch_size</span>
            <span class="s0">is </span><span class="s1">AutoBatchingMixin.compute_batch_size)</span>

    <span class="s0">with </span><span class="s1">cluster() </span><span class="s0">as </span><span class="s1">(s</span><span class="s0">, </span><span class="s1">[a</span><span class="s0">, </span><span class="s1">b]):</span>
        <span class="s0">with </span><span class="s1">Client(s[</span><span class="s2">'address'</span><span class="s1">]</span><span class="s0">, </span><span class="s1">loop=loop) </span><span class="s0">as </span><span class="s1">client:  </span><span class="s3"># noqa: F841</span>
            <span class="s0">with </span><span class="s1">parallel_config(backend=</span><span class="s2">'dask'</span><span class="s1">):</span>
                <span class="s0">with </span><span class="s1">Parallel() </span><span class="s0">as </span><span class="s1">parallel:</span>
                    <span class="s3"># The backend should be initialized with a default</span>
                    <span class="s3"># batch size of 1:</span>
                    <span class="s1">backend = parallel._backend</span>
                    <span class="s0">assert </span><span class="s1">isinstance(backend</span><span class="s0">, </span><span class="s1">DaskDistributedBackend)</span>
                    <span class="s0">assert </span><span class="s1">backend.parallel </span><span class="s0">is </span><span class="s1">parallel</span>
                    <span class="s0">assert </span><span class="s1">backend._effective_batch_size == </span><span class="s4">1</span>

                    <span class="s3"># Launch many short tasks that should trigger</span>
                    <span class="s3"># auto-batching:</span>
                    <span class="s1">parallel(</span>
                        <span class="s1">delayed(</span><span class="s0">lambda</span><span class="s1">: </span><span class="s0">None</span><span class="s1">)()</span>
                        <span class="s0">for </span><span class="s1">_ </span><span class="s0">in </span><span class="s1">range(int(</span><span class="s4">1e4</span><span class="s1">))</span>
                    <span class="s1">)</span>
                    <span class="s0">assert </span><span class="s1">backend._effective_batch_size &gt; </span><span class="s4">10</span>


<span class="s0">def </span><span class="s1">random2():</span>
    <span class="s0">return </span><span class="s1">random()</span>


<span class="s0">def </span><span class="s1">test_dont_assume_function_purity(loop):</span>
    <span class="s0">with </span><span class="s1">cluster() </span><span class="s0">as </span><span class="s1">(s</span><span class="s0">, </span><span class="s1">[a</span><span class="s0">, </span><span class="s1">b]):</span>
        <span class="s0">with </span><span class="s1">Client(s[</span><span class="s2">'address'</span><span class="s1">]</span><span class="s0">, </span><span class="s1">loop=loop) </span><span class="s0">as </span><span class="s1">client:  </span><span class="s3"># noqa: F841</span>
            <span class="s0">with </span><span class="s1">parallel_config(backend=</span><span class="s2">'dask'</span><span class="s1">):</span>
                <span class="s1">x</span><span class="s0">, </span><span class="s1">y = Parallel()(delayed(random2)() </span><span class="s0">for </span><span class="s1">i </span><span class="s0">in </span><span class="s1">range(</span><span class="s4">2</span><span class="s1">))</span>
                <span class="s0">assert </span><span class="s1">x != y</span>


<span class="s1">@pytest.mark.parametrize(</span><span class="s2">&quot;mixed&quot;</span><span class="s0">, </span><span class="s1">[</span><span class="s0">True, False</span><span class="s1">])</span>
<span class="s0">def </span><span class="s1">test_dask_funcname(loop</span><span class="s0">, </span><span class="s1">mixed):</span>
    <span class="s0">from </span><span class="s1">joblib._dask </span><span class="s0">import </span><span class="s1">Batch</span>
    <span class="s0">if not </span><span class="s1">mixed:</span>
        <span class="s1">tasks = [delayed(inc)(i) </span><span class="s0">for </span><span class="s1">i </span><span class="s0">in </span><span class="s1">range(</span><span class="s4">4</span><span class="s1">)]</span>
        <span class="s1">batch_repr = </span><span class="s2">'batch_of_inc_4_calls'</span>
    <span class="s0">else</span><span class="s1">:</span>
        <span class="s1">tasks = [</span>
            <span class="s1">delayed(abs)(i) </span><span class="s0">if </span><span class="s1">i % </span><span class="s4">2 </span><span class="s0">else </span><span class="s1">delayed(inc)(i) </span><span class="s0">for </span><span class="s1">i </span><span class="s0">in </span><span class="s1">range(</span><span class="s4">4</span><span class="s1">)</span>
        <span class="s1">]</span>
        <span class="s1">batch_repr = </span><span class="s2">'mixed_batch_of_inc_4_calls'</span>

    <span class="s0">assert </span><span class="s1">repr(Batch(tasks)) == batch_repr</span>

    <span class="s0">with </span><span class="s1">cluster() </span><span class="s0">as </span><span class="s1">(s</span><span class="s0">, </span><span class="s1">[a</span><span class="s0">, </span><span class="s1">b]):</span>
        <span class="s0">with </span><span class="s1">Client(s[</span><span class="s2">'address'</span><span class="s1">]</span><span class="s0">, </span><span class="s1">loop=loop) </span><span class="s0">as </span><span class="s1">client:</span>
            <span class="s0">with </span><span class="s1">parallel_config(backend=</span><span class="s2">'dask'</span><span class="s1">):</span>
                <span class="s1">_ = Parallel(batch_size=</span><span class="s4">2</span><span class="s0">, </span><span class="s1">pre_dispatch=</span><span class="s2">'all'</span><span class="s1">)(tasks)</span>

            <span class="s0">def </span><span class="s1">f(dask_scheduler):</span>
                <span class="s0">return </span><span class="s1">list(dask_scheduler.transition_log)</span>
            <span class="s1">batch_repr = batch_repr.replace(</span><span class="s2">'4'</span><span class="s0">, </span><span class="s2">'2'</span><span class="s1">)</span>
            <span class="s1">log = client.run_on_scheduler(f)</span>
            <span class="s0">assert </span><span class="s1">all(</span><span class="s2">'batch_of_inc' </span><span class="s0">in </span><span class="s1">tup[</span><span class="s4">0</span><span class="s1">] </span><span class="s0">for </span><span class="s1">tup </span><span class="s0">in </span><span class="s1">log)</span>


<span class="s0">def </span><span class="s1">test_no_undesired_distributed_cache_hit(loop):</span>
    <span class="s3"># Dask has a pickle cache for callables that are called many times. Because</span>
    <span class="s3"># the dask backends used to wrap both the functions and the arguments</span>
    <span class="s3"># under instances of the Batch callable class this caching mechanism could</span>
    <span class="s3"># lead to bugs as described in: https://github.com/joblib/joblib/pull/1055</span>
    <span class="s3"># The joblib-dask backend has been refactored to avoid bundling the</span>
    <span class="s3"># arguments as an attribute of the Batch instance to avoid this problem.</span>
    <span class="s3"># This test serves as non-regression problem.</span>

    <span class="s3"># Use a large number of input arguments to give the AutoBatchingMixin</span>
    <span class="s3"># enough tasks to kick-in.</span>
    <span class="s1">lists = [[] </span><span class="s0">for </span><span class="s1">_ </span><span class="s0">in </span><span class="s1">range(</span><span class="s4">100</span><span class="s1">)]</span>
    <span class="s1">np = pytest.importorskip(</span><span class="s2">'numpy'</span><span class="s1">)</span>
    <span class="s1">X = np.arange(int(</span><span class="s4">1e6</span><span class="s1">))</span>

    <span class="s0">def </span><span class="s1">isolated_operation(list_</span><span class="s0">, </span><span class="s1">data=</span><span class="s0">None</span><span class="s1">):</span>
        <span class="s0">if </span><span class="s1">data </span><span class="s0">is not None</span><span class="s1">:</span>
            <span class="s1">np.testing.assert_array_equal(data</span><span class="s0">, </span><span class="s1">X)</span>
        <span class="s1">list_.append(uuid4().hex)</span>
        <span class="s0">return </span><span class="s1">list_</span>

    <span class="s1">cluster = LocalCluster(n_workers=</span><span class="s4">1</span><span class="s0">, </span><span class="s1">threads_per_worker=</span><span class="s4">2</span><span class="s1">)</span>
    <span class="s1">client = Client(cluster)</span>
    <span class="s0">try</span><span class="s1">:</span>
        <span class="s0">with </span><span class="s1">parallel_config(backend=</span><span class="s2">'dask'</span><span class="s1">):</span>
            <span class="s3"># dispatches joblib.parallel.BatchedCalls</span>
            <span class="s1">res = Parallel()(</span>
                <span class="s1">delayed(isolated_operation)(list_) </span><span class="s0">for </span><span class="s1">list_ </span><span class="s0">in </span><span class="s1">lists</span>
            <span class="s1">)</span>

        <span class="s3"># The original arguments should not have been mutated as the mutation</span>
        <span class="s3"># happens in the dask worker process.</span>
        <span class="s0">assert </span><span class="s1">lists == [[] </span><span class="s0">for </span><span class="s1">_ </span><span class="s0">in </span><span class="s1">range(</span><span class="s4">100</span><span class="s1">)]</span>

        <span class="s3"># Here we did not pass any large numpy array as argument to</span>
        <span class="s3"># isolated_operation so no scattering event should happen under the</span>
        <span class="s3"># hood.</span>
        <span class="s1">counts = count_events(</span><span class="s2">'receive-from-scatter'</span><span class="s0">, </span><span class="s1">client)</span>
        <span class="s0">assert </span><span class="s1">sum(counts.values()) == </span><span class="s4">0</span>
        <span class="s0">assert </span><span class="s1">all([len(r) == </span><span class="s4">1 </span><span class="s0">for </span><span class="s1">r </span><span class="s0">in </span><span class="s1">res])</span>

        <span class="s0">with </span><span class="s1">parallel_config(backend=</span><span class="s2">'dask'</span><span class="s1">):</span>
            <span class="s3"># Append a large array which will be scattered by dask, and</span>
            <span class="s3"># dispatch joblib._dask.Batch</span>
            <span class="s1">res = Parallel()(</span>
                <span class="s1">delayed(isolated_operation)(list_</span><span class="s0">, </span><span class="s1">data=X) </span><span class="s0">for </span><span class="s1">list_ </span><span class="s0">in </span><span class="s1">lists</span>
            <span class="s1">)</span>

        <span class="s3"># This time, auto-scattering should have kicked it.</span>
        <span class="s1">counts = count_events(</span><span class="s2">'receive-from-scatter'</span><span class="s0">, </span><span class="s1">client)</span>
        <span class="s0">assert </span><span class="s1">sum(counts.values()) &gt; </span><span class="s4">0</span>
        <span class="s0">assert </span><span class="s1">all([len(r) == </span><span class="s4">1 </span><span class="s0">for </span><span class="s1">r </span><span class="s0">in </span><span class="s1">res])</span>
    <span class="s0">finally</span><span class="s1">:</span>
        <span class="s1">client.close()</span>
        <span class="s1">cluster.close()</span>


<span class="s0">class </span><span class="s1">CountSerialized(object):</span>
    <span class="s0">def </span><span class="s1">__init__(self</span><span class="s0">, </span><span class="s1">x):</span>
        <span class="s1">self.x = x</span>
        <span class="s1">self.count = </span><span class="s4">0</span>

    <span class="s0">def </span><span class="s1">__add__(self</span><span class="s0">, </span><span class="s1">other):</span>
        <span class="s0">return </span><span class="s1">self.x + getattr(other</span><span class="s0">, </span><span class="s2">'x'</span><span class="s0">, </span><span class="s1">other)</span>

    <span class="s1">__radd__ = __add__</span>

    <span class="s0">def </span><span class="s1">__reduce__(self):</span>
        <span class="s1">self.count += </span><span class="s4">1</span>
        <span class="s0">return </span><span class="s1">(CountSerialized</span><span class="s0">, </span><span class="s1">(self.x</span><span class="s0">,</span><span class="s1">))</span>


<span class="s0">def </span><span class="s1">add5(a</span><span class="s0">, </span><span class="s1">b</span><span class="s0">, </span><span class="s1">c</span><span class="s0">, </span><span class="s1">d=</span><span class="s4">0</span><span class="s0">, </span><span class="s1">e=</span><span class="s4">0</span><span class="s1">):</span>
    <span class="s0">return </span><span class="s1">a + b + c + d + e</span>


<span class="s0">def </span><span class="s1">test_manual_scatter(loop):</span>
    <span class="s1">x = CountSerialized(</span><span class="s4">1</span><span class="s1">)</span>
    <span class="s1">y = CountSerialized(</span><span class="s4">2</span><span class="s1">)</span>
    <span class="s1">z = CountSerialized(</span><span class="s4">3</span><span class="s1">)</span>

    <span class="s0">with </span><span class="s1">cluster() </span><span class="s0">as </span><span class="s1">(s</span><span class="s0">, </span><span class="s1">[a</span><span class="s0">, </span><span class="s1">b]):</span>
        <span class="s0">with </span><span class="s1">Client(s[</span><span class="s2">'address'</span><span class="s1">]</span><span class="s0">, </span><span class="s1">loop=loop) </span><span class="s0">as </span><span class="s1">client:  </span><span class="s3"># noqa: F841</span>
            <span class="s0">with </span><span class="s1">parallel_config(backend=</span><span class="s2">'dask'</span><span class="s0">, </span><span class="s1">scatter=[x</span><span class="s0">, </span><span class="s1">y]):</span>
                <span class="s1">f = delayed(add5)</span>
                <span class="s1">tasks = [f(x</span><span class="s0">, </span><span class="s1">y</span><span class="s0">, </span><span class="s1">z</span><span class="s0">, </span><span class="s1">d=</span><span class="s4">4</span><span class="s0">, </span><span class="s1">e=</span><span class="s4">5</span><span class="s1">)</span><span class="s0">,</span>
                         <span class="s1">f(x</span><span class="s0">, </span><span class="s1">z</span><span class="s0">, </span><span class="s1">y</span><span class="s0">, </span><span class="s1">d=</span><span class="s4">5</span><span class="s0">, </span><span class="s1">e=</span><span class="s4">4</span><span class="s1">)</span><span class="s0">,</span>
                         <span class="s1">f(y</span><span class="s0">, </span><span class="s1">x</span><span class="s0">, </span><span class="s1">z</span><span class="s0">, </span><span class="s1">d=x</span><span class="s0">, </span><span class="s1">e=</span><span class="s4">5</span><span class="s1">)</span><span class="s0">,</span>
                         <span class="s1">f(z</span><span class="s0">, </span><span class="s1">z</span><span class="s0">, </span><span class="s1">x</span><span class="s0">, </span><span class="s1">d=z</span><span class="s0">, </span><span class="s1">e=y)]</span>
                <span class="s1">expected = [func(*args</span><span class="s0">, </span><span class="s1">**kwargs)</span>
                            <span class="s0">for </span><span class="s1">func</span><span class="s0">, </span><span class="s1">args</span><span class="s0">, </span><span class="s1">kwargs </span><span class="s0">in </span><span class="s1">tasks]</span>
                <span class="s1">results = Parallel()(tasks)</span>

            <span class="s3"># Scatter must take a list/tuple</span>
            <span class="s0">with </span><span class="s1">pytest.raises(TypeError):</span>
                <span class="s0">with </span><span class="s1">parallel_config(backend=</span><span class="s2">'dask'</span><span class="s0">, </span><span class="s1">loop=loop</span><span class="s0">, </span><span class="s1">scatter=</span><span class="s4">1</span><span class="s1">):</span>
                    <span class="s0">pass</span>

    <span class="s0">assert </span><span class="s1">results == expected</span>

    <span class="s3"># Scattered variables only serialized once</span>
    <span class="s0">assert </span><span class="s1">x.count == </span><span class="s4">1</span>
    <span class="s0">assert </span><span class="s1">y.count == </span><span class="s4">1</span>
    <span class="s3"># Depending on the version of distributed, the unscattered z variable</span>
    <span class="s3"># is either pickled 4 or 6 times, possibly because of the memoization</span>
    <span class="s3"># of objects that appear several times in the arguments of a delayed</span>
    <span class="s3"># task.</span>
    <span class="s0">assert </span><span class="s1">z.count </span><span class="s0">in </span><span class="s1">(</span><span class="s4">4</span><span class="s0">, </span><span class="s4">6</span><span class="s1">)</span>


<span class="s3"># When the same IOLoop is used for multiple clients in a row, use</span>
<span class="s3"># loop_in_thread instead of loop to prevent the Client from closing it.  See</span>
<span class="s3"># dask/distributed #4112</span>
<span class="s0">def </span><span class="s1">test_auto_scatter(loop_in_thread):</span>
    <span class="s1">np = pytest.importorskip(</span><span class="s2">'numpy'</span><span class="s1">)</span>
    <span class="s1">data1 = np.ones(int(</span><span class="s4">1e4</span><span class="s1">)</span><span class="s0">, </span><span class="s1">dtype=np.uint8)</span>
    <span class="s1">data2 = np.ones(int(</span><span class="s4">1e4</span><span class="s1">)</span><span class="s0">, </span><span class="s1">dtype=np.uint8)</span>
    <span class="s1">data_to_process = ([data1] * </span><span class="s4">3</span><span class="s1">) + ([data2] * </span><span class="s4">3</span><span class="s1">)</span>

    <span class="s0">with </span><span class="s1">cluster() </span><span class="s0">as </span><span class="s1">(s</span><span class="s0">, </span><span class="s1">[a</span><span class="s0">, </span><span class="s1">b]):</span>
        <span class="s0">with </span><span class="s1">Client(s[</span><span class="s2">'address'</span><span class="s1">]</span><span class="s0">, </span><span class="s1">loop=loop_in_thread) </span><span class="s0">as </span><span class="s1">client:</span>
            <span class="s0">with </span><span class="s1">parallel_config(backend=</span><span class="s2">'dask'</span><span class="s1">):</span>
                <span class="s3"># Passing the same data as arg and kwarg triggers a single</span>
                <span class="s3"># scatter operation whose result is reused.</span>
                <span class="s1">Parallel()(delayed(noop)(data</span><span class="s0">, </span><span class="s1">data</span><span class="s0">, </span><span class="s1">i</span><span class="s0">, </span><span class="s1">opt=data)</span>
                           <span class="s0">for </span><span class="s1">i</span><span class="s0">, </span><span class="s1">data </span><span class="s0">in </span><span class="s1">enumerate(data_to_process))</span>
            <span class="s3"># By default large array are automatically scattered with</span>
            <span class="s3"># broadcast=1 which means that one worker must directly receive</span>
            <span class="s3"># the data from the scatter operation once.</span>
            <span class="s1">counts = count_events(</span><span class="s2">'receive-from-scatter'</span><span class="s0">, </span><span class="s1">client)</span>
            <span class="s0">assert </span><span class="s1">counts[a[</span><span class="s2">'address'</span><span class="s1">]] + counts[b[</span><span class="s2">'address'</span><span class="s1">]] == </span><span class="s4">2</span>

    <span class="s0">with </span><span class="s1">cluster() </span><span class="s0">as </span><span class="s1">(s</span><span class="s0">, </span><span class="s1">[a</span><span class="s0">, </span><span class="s1">b]):</span>
        <span class="s0">with </span><span class="s1">Client(s[</span><span class="s2">'address'</span><span class="s1">]</span><span class="s0">, </span><span class="s1">loop=loop_in_thread) </span><span class="s0">as </span><span class="s1">client:</span>
            <span class="s0">with </span><span class="s1">parallel_config(backend=</span><span class="s2">'dask'</span><span class="s1">):</span>
                <span class="s1">Parallel()(delayed(noop)(data1[:</span><span class="s4">3</span><span class="s1">]</span><span class="s0">, </span><span class="s1">i) </span><span class="s0">for </span><span class="s1">i </span><span class="s0">in </span><span class="s1">range(</span><span class="s4">5</span><span class="s1">))</span>
            <span class="s3"># Small arrays are passed within the task definition without going</span>
            <span class="s3"># through a scatter operation.</span>
            <span class="s1">counts = count_events(</span><span class="s2">'receive-from-scatter'</span><span class="s0">, </span><span class="s1">client)</span>
            <span class="s0">assert </span><span class="s1">counts[a[</span><span class="s2">'address'</span><span class="s1">]] == </span><span class="s4">0</span>
            <span class="s0">assert </span><span class="s1">counts[b[</span><span class="s2">'address'</span><span class="s1">]] == </span><span class="s4">0</span>


<span class="s1">@pytest.mark.parametrize(</span><span class="s2">&quot;retry_no&quot;</span><span class="s0">, </span><span class="s1">list(range(</span><span class="s4">2</span><span class="s1">)))</span>
<span class="s0">def </span><span class="s1">test_nested_scatter(loop</span><span class="s0">, </span><span class="s1">retry_no):</span>

    <span class="s1">np = pytest.importorskip(</span><span class="s2">'numpy'</span><span class="s1">)</span>

    <span class="s1">NUM_INNER_TASKS = </span><span class="s4">10</span>
    <span class="s1">NUM_OUTER_TASKS = </span><span class="s4">10</span>

    <span class="s0">def </span><span class="s1">my_sum(x</span><span class="s0">, </span><span class="s1">i</span><span class="s0">, </span><span class="s1">j):</span>
        <span class="s0">return </span><span class="s1">np.sum(x)</span>

    <span class="s0">def </span><span class="s1">outer_function_joblib(array</span><span class="s0">, </span><span class="s1">i):</span>
        <span class="s1">client = get_client()  </span><span class="s3"># noqa</span>
        <span class="s0">with </span><span class="s1">parallel_config(backend=</span><span class="s2">&quot;dask&quot;</span><span class="s1">):</span>
            <span class="s1">results = Parallel()(</span>
                <span class="s1">delayed(my_sum)(array[j:]</span><span class="s0">, </span><span class="s1">i</span><span class="s0">, </span><span class="s1">j) </span><span class="s0">for </span><span class="s1">j </span><span class="s0">in </span><span class="s1">range(</span>
                    <span class="s1">NUM_INNER_TASKS)</span>
            <span class="s1">)</span>
        <span class="s0">return </span><span class="s1">sum(results)</span>

    <span class="s0">with </span><span class="s1">cluster() </span><span class="s0">as </span><span class="s1">(s</span><span class="s0">, </span><span class="s1">[a</span><span class="s0">, </span><span class="s1">b]):</span>
        <span class="s0">with </span><span class="s1">Client(s[</span><span class="s2">'address'</span><span class="s1">]</span><span class="s0">, </span><span class="s1">loop=loop) </span><span class="s0">as </span><span class="s1">_:</span>
            <span class="s0">with </span><span class="s1">parallel_config(backend=</span><span class="s2">&quot;dask&quot;</span><span class="s1">):</span>
                <span class="s1">my_array = np.ones(</span><span class="s4">10000</span><span class="s1">)</span>
                <span class="s1">_ = Parallel()(</span>
                    <span class="s1">delayed(outer_function_joblib)(</span>
                        <span class="s1">my_array[i:]</span><span class="s0">, </span><span class="s1">i) </span><span class="s0">for </span><span class="s1">i </span><span class="s0">in </span><span class="s1">range(NUM_OUTER_TASKS)</span>
                <span class="s1">)</span>


<span class="s0">def </span><span class="s1">test_nested_backend_context_manager(loop_in_thread):</span>
    <span class="s0">def </span><span class="s1">get_nested_pids():</span>
        <span class="s1">pids = set(Parallel(n_jobs=</span><span class="s4">2</span><span class="s1">)(delayed(os.getpid)() </span><span class="s0">for </span><span class="s1">_ </span><span class="s0">in </span><span class="s1">range(</span><span class="s4">2</span><span class="s1">)))</span>
        <span class="s1">pids |= set(Parallel(n_jobs=</span><span class="s4">2</span><span class="s1">)(delayed(os.getpid)() </span><span class="s0">for </span><span class="s1">_ </span><span class="s0">in </span><span class="s1">range(</span><span class="s4">2</span><span class="s1">)))</span>
        <span class="s0">return </span><span class="s1">pids</span>

    <span class="s0">with </span><span class="s1">cluster() </span><span class="s0">as </span><span class="s1">(s</span><span class="s0">, </span><span class="s1">[a</span><span class="s0">, </span><span class="s1">b]):</span>
        <span class="s0">with </span><span class="s1">Client(s[</span><span class="s2">'address'</span><span class="s1">]</span><span class="s0">, </span><span class="s1">loop=loop_in_thread) </span><span class="s0">as </span><span class="s1">client:</span>
            <span class="s0">with </span><span class="s1">parallel_config(backend=</span><span class="s2">'dask'</span><span class="s1">):</span>
                <span class="s1">pid_groups = Parallel(n_jobs=</span><span class="s4">2</span><span class="s1">)(</span>
                    <span class="s1">delayed(get_nested_pids)()</span>
                    <span class="s0">for </span><span class="s1">_ </span><span class="s0">in </span><span class="s1">range(</span><span class="s4">10</span><span class="s1">)</span>
                <span class="s1">)</span>
                <span class="s0">for </span><span class="s1">pid_group </span><span class="s0">in </span><span class="s1">pid_groups:</span>
                    <span class="s0">assert </span><span class="s1">len(set(pid_group)) &lt;= </span><span class="s4">2</span>

        <span class="s3"># No deadlocks</span>
        <span class="s0">with </span><span class="s1">Client(s[</span><span class="s2">'address'</span><span class="s1">]</span><span class="s0">, </span><span class="s1">loop=loop_in_thread) </span><span class="s0">as </span><span class="s1">client:  </span><span class="s3"># noqa: F841</span>
            <span class="s0">with </span><span class="s1">parallel_config(backend=</span><span class="s2">'dask'</span><span class="s1">):</span>
                <span class="s1">pid_groups = Parallel(n_jobs=</span><span class="s4">2</span><span class="s1">)(</span>
                    <span class="s1">delayed(get_nested_pids)()</span>
                    <span class="s0">for </span><span class="s1">_ </span><span class="s0">in </span><span class="s1">range(</span><span class="s4">10</span><span class="s1">)</span>
                <span class="s1">)</span>
                <span class="s0">for </span><span class="s1">pid_group </span><span class="s0">in </span><span class="s1">pid_groups:</span>
                    <span class="s0">assert </span><span class="s1">len(set(pid_group)) &lt;= </span><span class="s4">2</span>


<span class="s0">def </span><span class="s1">test_nested_backend_context_manager_implicit_n_jobs(loop):</span>
    <span class="s3"># Check that Parallel with no explicit n_jobs value automatically selects</span>
    <span class="s3"># all the dask workers, including in nested calls.</span>

    <span class="s0">def </span><span class="s1">_backend_type(p):</span>
        <span class="s0">return </span><span class="s1">p._backend.__class__.__name__</span>

    <span class="s0">def </span><span class="s1">get_nested_implicit_n_jobs():</span>
        <span class="s0">with </span><span class="s1">Parallel() </span><span class="s0">as </span><span class="s1">p:</span>
            <span class="s0">return </span><span class="s1">_backend_type(p)</span><span class="s0">, </span><span class="s1">p.n_jobs</span>

    <span class="s0">with </span><span class="s1">cluster() </span><span class="s0">as </span><span class="s1">(s</span><span class="s0">, </span><span class="s1">[a</span><span class="s0">, </span><span class="s1">b]):</span>
        <span class="s0">with </span><span class="s1">Client(s[</span><span class="s2">'address'</span><span class="s1">]</span><span class="s0">, </span><span class="s1">loop=loop) </span><span class="s0">as </span><span class="s1">client:  </span><span class="s3"># noqa: F841</span>
            <span class="s0">with </span><span class="s1">parallel_config(backend=</span><span class="s2">'dask'</span><span class="s1">):</span>
                <span class="s0">with </span><span class="s1">Parallel() </span><span class="s0">as </span><span class="s1">p:</span>
                    <span class="s0">assert </span><span class="s1">_backend_type(p) == </span><span class="s2">&quot;DaskDistributedBackend&quot;</span>
                    <span class="s0">assert </span><span class="s1">p.n_jobs == -</span><span class="s4">1</span>
                    <span class="s1">all_nested_n_jobs = p(</span>
                        <span class="s1">delayed(get_nested_implicit_n_jobs)()</span>
                        <span class="s0">for </span><span class="s1">_ </span><span class="s0">in </span><span class="s1">range(</span><span class="s4">2</span><span class="s1">)</span>
                    <span class="s1">)</span>
                <span class="s0">for </span><span class="s1">backend_type</span><span class="s0">, </span><span class="s1">nested_n_jobs </span><span class="s0">in </span><span class="s1">all_nested_n_jobs:</span>
                    <span class="s0">assert </span><span class="s1">backend_type == </span><span class="s2">&quot;DaskDistributedBackend&quot;</span>
                    <span class="s0">assert </span><span class="s1">nested_n_jobs == -</span><span class="s4">1</span>


<span class="s0">def </span><span class="s1">test_errors(loop):</span>
    <span class="s0">with </span><span class="s1">pytest.raises(ValueError) </span><span class="s0">as </span><span class="s1">info:</span>
        <span class="s0">with </span><span class="s1">parallel_config(backend=</span><span class="s2">'dask'</span><span class="s1">):</span>
            <span class="s0">pass</span>

    <span class="s0">assert </span><span class="s2">&quot;create a dask client&quot; </span><span class="s0">in </span><span class="s1">str(info.value).lower()</span>


<span class="s0">def </span><span class="s1">test_correct_nested_backend(loop):</span>
    <span class="s0">with </span><span class="s1">cluster() </span><span class="s0">as </span><span class="s1">(s</span><span class="s0">, </span><span class="s1">[a</span><span class="s0">, </span><span class="s1">b]):</span>
        <span class="s0">with </span><span class="s1">Client(s[</span><span class="s2">'address'</span><span class="s1">]</span><span class="s0">, </span><span class="s1">loop=loop) </span><span class="s0">as </span><span class="s1">client:  </span><span class="s3"># noqa: F841</span>
            <span class="s3"># No requirement, should be us</span>
            <span class="s0">with </span><span class="s1">parallel_config(backend=</span><span class="s2">'dask'</span><span class="s1">):</span>
                <span class="s1">result = Parallel(n_jobs=</span><span class="s4">2</span><span class="s1">)(</span>
                    <span class="s1">delayed(outer)(nested_require=</span><span class="s0">None</span><span class="s1">) </span><span class="s0">for </span><span class="s1">_ </span><span class="s0">in </span><span class="s1">range(</span><span class="s4">1</span><span class="s1">))</span>
                <span class="s0">assert </span><span class="s1">isinstance(result[</span><span class="s4">0</span><span class="s1">][</span><span class="s4">0</span><span class="s1">][</span><span class="s4">0</span><span class="s1">]</span><span class="s0">, </span><span class="s1">DaskDistributedBackend)</span>

            <span class="s3"># Require threads, should be threading</span>
            <span class="s0">with </span><span class="s1">parallel_config(backend=</span><span class="s2">'dask'</span><span class="s1">):</span>
                <span class="s1">result = Parallel(n_jobs=</span><span class="s4">2</span><span class="s1">)(</span>
                    <span class="s1">delayed(outer)(nested_require=</span><span class="s2">'sharedmem'</span><span class="s1">)</span>
                    <span class="s0">for </span><span class="s1">_ </span><span class="s0">in </span><span class="s1">range(</span><span class="s4">1</span><span class="s1">))</span>
                <span class="s0">assert </span><span class="s1">isinstance(result[</span><span class="s4">0</span><span class="s1">][</span><span class="s4">0</span><span class="s1">][</span><span class="s4">0</span><span class="s1">]</span><span class="s0">, </span><span class="s1">ThreadingBackend)</span>


<span class="s0">def </span><span class="s1">outer(nested_require):</span>
    <span class="s0">return </span><span class="s1">Parallel(n_jobs=</span><span class="s4">2</span><span class="s0">, </span><span class="s1">prefer=</span><span class="s2">'threads'</span><span class="s1">)(</span>
        <span class="s1">delayed(middle)(nested_require) </span><span class="s0">for </span><span class="s1">_ </span><span class="s0">in </span><span class="s1">range(</span><span class="s4">1</span><span class="s1">)</span>
    <span class="s1">)</span>


<span class="s0">def </span><span class="s1">middle(require):</span>
    <span class="s0">return </span><span class="s1">Parallel(n_jobs=</span><span class="s4">2</span><span class="s0">, </span><span class="s1">require=require)(</span>
        <span class="s1">delayed(inner)() </span><span class="s0">for </span><span class="s1">_ </span><span class="s0">in </span><span class="s1">range(</span><span class="s4">1</span><span class="s1">)</span>
    <span class="s1">)</span>


<span class="s0">def </span><span class="s1">inner():</span>
    <span class="s0">return </span><span class="s1">Parallel()._backend</span>


<span class="s0">def </span><span class="s1">test_secede_with_no_processes(loop):</span>
    <span class="s3"># https://github.com/dask/distributed/issues/1775</span>
    <span class="s0">with </span><span class="s1">Client(loop=loop</span><span class="s0">, </span><span class="s1">processes=</span><span class="s0">False, </span><span class="s1">set_as_default=</span><span class="s0">True</span><span class="s1">):</span>
        <span class="s0">with </span><span class="s1">parallel_config(backend=</span><span class="s2">'dask'</span><span class="s1">):</span>
            <span class="s1">Parallel(n_jobs=</span><span class="s4">4</span><span class="s1">)(delayed(id)(i) </span><span class="s0">for </span><span class="s1">i </span><span class="s0">in </span><span class="s1">range(</span><span class="s4">2</span><span class="s1">))</span>


<span class="s0">def </span><span class="s1">_worker_address(_):</span>
    <span class="s0">from </span><span class="s1">distributed </span><span class="s0">import </span><span class="s1">get_worker</span>
    <span class="s0">return </span><span class="s1">get_worker().address</span>


<span class="s0">def </span><span class="s1">test_dask_backend_keywords(loop):</span>
    <span class="s0">with </span><span class="s1">cluster() </span><span class="s0">as </span><span class="s1">(s</span><span class="s0">, </span><span class="s1">[a</span><span class="s0">, </span><span class="s1">b]):</span>
        <span class="s0">with </span><span class="s1">Client(s[</span><span class="s2">'address'</span><span class="s1">]</span><span class="s0">, </span><span class="s1">loop=loop) </span><span class="s0">as </span><span class="s1">client:  </span><span class="s3"># noqa: F841</span>
            <span class="s0">with </span><span class="s1">parallel_config(backend=</span><span class="s2">'dask'</span><span class="s0">, </span><span class="s1">workers=a[</span><span class="s2">'address'</span><span class="s1">]):</span>
                <span class="s1">seq = Parallel()(</span>
                    <span class="s1">delayed(_worker_address)(i) </span><span class="s0">for </span><span class="s1">i </span><span class="s0">in </span><span class="s1">range(</span><span class="s4">10</span><span class="s1">))</span>
                <span class="s0">assert </span><span class="s1">seq == [a[</span><span class="s2">'address'</span><span class="s1">]] * </span><span class="s4">10</span>

            <span class="s0">with </span><span class="s1">parallel_config(backend=</span><span class="s2">'dask'</span><span class="s0">, </span><span class="s1">workers=b[</span><span class="s2">'address'</span><span class="s1">]):</span>
                <span class="s1">seq = Parallel()(</span>
                    <span class="s1">delayed(_worker_address)(i) </span><span class="s0">for </span><span class="s1">i </span><span class="s0">in </span><span class="s1">range(</span><span class="s4">10</span><span class="s1">))</span>
                <span class="s0">assert </span><span class="s1">seq == [b[</span><span class="s2">'address'</span><span class="s1">]] * </span><span class="s4">10</span>


<span class="s0">def </span><span class="s1">test_cleanup(loop):</span>
    <span class="s0">with </span><span class="s1">Client(processes=</span><span class="s0">False, </span><span class="s1">loop=loop) </span><span class="s0">as </span><span class="s1">client:</span>
        <span class="s0">with </span><span class="s1">parallel_config(backend=</span><span class="s2">'dask'</span><span class="s1">):</span>
            <span class="s1">Parallel()(delayed(inc)(i) </span><span class="s0">for </span><span class="s1">i </span><span class="s0">in </span><span class="s1">range(</span><span class="s4">10</span><span class="s1">))</span>

        <span class="s1">start = time()</span>
        <span class="s0">while </span><span class="s1">client.cluster.scheduler.tasks:</span>
            <span class="s1">sleep(</span><span class="s4">0.01</span><span class="s1">)</span>
            <span class="s0">assert </span><span class="s1">time() &lt; start + </span><span class="s4">5</span>

        <span class="s0">assert not </span><span class="s1">client.futures</span>


<span class="s1">@pytest.mark.parametrize(</span><span class="s2">&quot;cluster_strategy&quot;</span><span class="s0">, </span><span class="s1">[</span><span class="s2">&quot;adaptive&quot;</span><span class="s0">, </span><span class="s2">&quot;late_scaling&quot;</span><span class="s1">])</span>
<span class="s1">@pytest.mark.skipif(</span>
    <span class="s1">distributed.__version__ &lt;= </span><span class="s2">'2.1.1' </span><span class="s0">and </span><span class="s1">distributed.__version__ &gt;= </span><span class="s2">'1.28.0'</span><span class="s0">,</span>
    <span class="s1">reason=</span><span class="s2">&quot;distributed bug - https://github.com/dask/distributed/pull/2841&quot;</span><span class="s1">)</span>
<span class="s0">def </span><span class="s1">test_wait_for_workers(cluster_strategy):</span>
    <span class="s1">cluster = LocalCluster(n_workers=</span><span class="s4">0</span><span class="s0">, </span><span class="s1">processes=</span><span class="s0">False, </span><span class="s1">threads_per_worker=</span><span class="s4">2</span><span class="s1">)</span>
    <span class="s1">client = Client(cluster)</span>
    <span class="s0">if </span><span class="s1">cluster_strategy == </span><span class="s2">&quot;adaptive&quot;</span><span class="s1">:</span>
        <span class="s1">cluster.adapt(minimum=</span><span class="s4">0</span><span class="s0">, </span><span class="s1">maximum=</span><span class="s4">2</span><span class="s1">)</span>
    <span class="s0">elif </span><span class="s1">cluster_strategy == </span><span class="s2">&quot;late_scaling&quot;</span><span class="s1">:</span>
        <span class="s3"># Tell the cluster to start workers but this is a non-blocking call</span>
        <span class="s3"># and new workers might take time to connect. In this case the Parallel</span>
        <span class="s3"># call should wait for at least one worker to come up before starting</span>
        <span class="s3"># to schedule work.</span>
        <span class="s1">cluster.scale(</span><span class="s4">2</span><span class="s1">)</span>
    <span class="s0">try</span><span class="s1">:</span>
        <span class="s0">with </span><span class="s1">parallel_config(backend=</span><span class="s2">'dask'</span><span class="s1">):</span>
            <span class="s3"># The following should wait a bit for at least one worker to</span>
            <span class="s3"># become available.</span>
            <span class="s1">Parallel()(delayed(inc)(i) </span><span class="s0">for </span><span class="s1">i </span><span class="s0">in </span><span class="s1">range(</span><span class="s4">10</span><span class="s1">))</span>
    <span class="s0">finally</span><span class="s1">:</span>
        <span class="s1">client.close()</span>
        <span class="s1">cluster.close()</span>


<span class="s0">def </span><span class="s1">test_wait_for_workers_timeout():</span>
    <span class="s3"># Start a cluster with 0 worker:</span>
    <span class="s1">cluster = LocalCluster(n_workers=</span><span class="s4">0</span><span class="s0">, </span><span class="s1">processes=</span><span class="s0">False, </span><span class="s1">threads_per_worker=</span><span class="s4">2</span><span class="s1">)</span>
    <span class="s1">client = Client(cluster)</span>
    <span class="s0">try</span><span class="s1">:</span>
        <span class="s0">with </span><span class="s1">parallel_config(backend=</span><span class="s2">'dask'</span><span class="s0">, </span><span class="s1">wait_for_workers_timeout=</span><span class="s4">0.1</span><span class="s1">):</span>
            <span class="s3"># Short timeout: DaskDistributedBackend</span>
            <span class="s1">msg = </span><span class="s2">&quot;DaskDistributedBackend has no worker after 0.1 seconds.&quot;</span>
            <span class="s0">with </span><span class="s1">pytest.raises(TimeoutError</span><span class="s0">, </span><span class="s1">match=msg):</span>
                <span class="s1">Parallel()(delayed(inc)(i) </span><span class="s0">for </span><span class="s1">i </span><span class="s0">in </span><span class="s1">range(</span><span class="s4">10</span><span class="s1">))</span>

        <span class="s0">with </span><span class="s1">parallel_config(backend=</span><span class="s2">'dask'</span><span class="s0">, </span><span class="s1">wait_for_workers_timeout=</span><span class="s4">0</span><span class="s1">):</span>
            <span class="s3"># No timeout: fallback to generic joblib failure:</span>
            <span class="s1">msg = </span><span class="s2">&quot;DaskDistributedBackend has no active worker&quot;</span>
            <span class="s0">with </span><span class="s1">pytest.raises(RuntimeError</span><span class="s0">, </span><span class="s1">match=msg):</span>
                <span class="s1">Parallel()(delayed(inc)(i) </span><span class="s0">for </span><span class="s1">i </span><span class="s0">in </span><span class="s1">range(</span><span class="s4">10</span><span class="s1">))</span>
    <span class="s0">finally</span><span class="s1">:</span>
        <span class="s1">client.close()</span>
        <span class="s1">cluster.close()</span>


<span class="s1">@pytest.mark.parametrize(</span><span class="s2">&quot;backend&quot;</span><span class="s0">, </span><span class="s1">[</span><span class="s2">&quot;loky&quot;</span><span class="s0">, </span><span class="s2">&quot;multiprocessing&quot;</span><span class="s1">])</span>
<span class="s0">def </span><span class="s1">test_joblib_warning_inside_dask_daemonic_worker(backend):</span>
    <span class="s1">cluster = LocalCluster(n_workers=</span><span class="s4">2</span><span class="s1">)</span>
    <span class="s1">client = Client(cluster)</span>

    <span class="s0">def </span><span class="s1">func_using_joblib_parallel():</span>
        <span class="s3"># Somehow trying to check the warning type here (e.g. with</span>
        <span class="s3"># pytest.warns(UserWarning)) make the test hang. Work-around: return</span>
        <span class="s3"># the warning record to the client and the warning check is done</span>
        <span class="s3"># client-side.</span>
        <span class="s0">with </span><span class="s1">warnings.catch_warnings(record=</span><span class="s0">True</span><span class="s1">) </span><span class="s0">as </span><span class="s1">record:</span>
            <span class="s1">Parallel(n_jobs=</span><span class="s4">2</span><span class="s0">, </span><span class="s1">backend=backend)(</span>
                <span class="s1">delayed(inc)(i) </span><span class="s0">for </span><span class="s1">i </span><span class="s0">in </span><span class="s1">range(</span><span class="s4">10</span><span class="s1">))</span>

        <span class="s0">return </span><span class="s1">record</span>

    <span class="s1">fut = client.submit(func_using_joblib_parallel)</span>
    <span class="s1">record = fut.result()</span>

    <span class="s0">assert </span><span class="s1">len(record) == </span><span class="s4">1</span>
    <span class="s1">warning = record[</span><span class="s4">0</span><span class="s1">].message</span>
    <span class="s0">assert </span><span class="s1">isinstance(warning</span><span class="s0">, </span><span class="s1">UserWarning)</span>
    <span class="s0">assert </span><span class="s2">&quot;distributed.worker.daemon&quot; </span><span class="s0">in </span><span class="s1">str(warning)</span>
</pre>
</body>
</html>